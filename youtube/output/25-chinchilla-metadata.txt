TYTUÅ:
Chinchilla: Jak DeepMind zmieniÅ‚ zasady wyÅ›cigu AI | Deep Dive

OPIS:
ğŸ™ï¸ Omawiamy przeÅ‚omowy artykuÅ‚ "Training Compute-Optimal Large Language Models" od DeepMind, ktÃ³ry udowodniÅ‚ Å¼e dotychczasowe gigantyczne modele jÄ™zykowe byÅ‚y znaczÄ…co niedotrenowane.

W tym odcinku omawiamy:
â€¢ Dlaczego przez lata branÅ¼a AI podÄ…Å¼aÅ‚a za bÅ‚Ä™dnÄ… filozofiÄ… "wiÄ™ksze znaczy lepsze"
â€¢ Prawa skalowania Kaplana i ich wpÅ‚yw na rozwÃ³j modeli jÄ™zykowych
â€¢ Fundamentalne pytanie DeepMind: jak optymalnie zbalansowaÄ‡ rozmiar modelu i iloÅ›Ä‡ danych
â€¢ MetodologiÄ™ eksperymentu: 400+ modeli i profile Izo-FLOP
â€¢ PorÃ³wnanie Chinchilla (70B) vs Gopher (280B) - jak 4x mniejszy model wygraÅ‚
â€¢ Wyniki benchmarkÃ³w: MMLU, RACE-h, BIG-bench i dlaczego to zmienia wszystko
â€¢ Nowy paradygmat: od wyÅ›cigu na parametry do wyÅ›cigu na dane

ğŸ“„ Oryginalny artykuÅ‚: https://arxiv.org/abs/2203.15556

Autorzy: Jordan Hoffmann et al. (DeepMind)

TAGI:
#AI #MachineLearning #DeepLearning #Chinchilla #DeepMind #LLM #NLP #ScalingLaws #GPT #Gopher #TransformerModels #ComputeOptimal #AIResearch #SztucznaInteligencja
