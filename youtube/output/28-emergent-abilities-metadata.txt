TYTUÅ:
Emergent Abilities of Large Language Models | Deep Dive

OPIS:
ğŸ™ï¸ Czy modele AI mogÄ… nagle nabyÄ‡ nowych zdolnoÅ›ci, ktÃ³rych nie miaÅ‚y wczeÅ›niej? W tym odcinku zgÅ‚Ä™biamy fascynujÄ…cy artykuÅ‚ "Emergent Abilities of Large Language Models", ktÃ³ry rzuca wyzwanie prostej mantrze "im wiÄ™kszy, tym lepszy" i pokazuje, Å¼e skalowanie modeli jÄ™zykowych przypomina przejÅ›cia fazowe w fizyce.

W tym odcinku omawiamy:
â€¢ Czym sÄ… zdolnoÅ›ci emergentne i jak je zdefiniowaÄ‡
â€¢ Analogia przejÅ›cia fazowego - jak zmiana iloÅ›ciowa prowadzi do zmiany jakoÅ›ciowej
â€¢ Few-shot prompting jako protokÃ³Å‚ testowy dla emergencji
â€¢ Konkretne dowody: arytmetyka, Word in Context, Chain of Thought
â€¢ Paradoks instruction finetuning - techniki, ktÃ³re szkodzÄ… mniejszym modelom
â€¢ Emergentne ryzyka - ciemna strona skalowania

ğŸ“„ Oryginalny artykuÅ‚: https://arxiv.org/abs/2206.07682

Autorzy: Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus (Google Research, Stanford University, DeepMind)

TAGI:
#AI #MachineLearning #DeepLearning #LLM #EmergentAbilities #GPT #PaLM #Skalowanie #NLP #SztucznaInteligencja #PrzejscieFazowe #ChainOfThought #FewShotLearning
