TYTUÅ:
Attention Is All You Need - ArtykuÅ‚, ktÃ³ry zmieniÅ‚ AI na zawsze | Deep Dive

OPIS:
ğŸ™ï¸ GÅ‚Ä™bokie zanurzenie w przeÅ‚omowy artykuÅ‚ z 2017 roku, ktÃ³ry wprowadziÅ‚ architekturÄ™ Transformer i zrewolucjonizowaÅ‚ sztucznÄ… inteligencjÄ™.

W tym odcinku omawiamy:
â€¢ Dlaczego modele sekwencyjne (RNN/LSTM) osiÄ…gnÄ™Å‚y swÃ³j limit
â€¢ Jak mechanizm self-attention rozwiÄ…zuje problem dÅ‚ugich zaleÅ¼noÅ›ci
â€¢ Architektura encoder-decoder i multi-head attention
â€¢ Kodowanie pozycyjne - jak zachowaÄ‡ kolejnoÅ›Ä‡ bez sekwencji
â€¢ Wyniki: +2 BLEU na WMT 2014, trening w 3.5 dnia
â€¢ WpÅ‚yw na BERT, GPT i caÅ‚Ä… nowoczesnÄ… AI

ğŸ“„ Oryginalny artykuÅ‚: https://arxiv.org/abs/1706.03762

Autorzy: Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, Polosukhin (Google Brain / Google Research)

TAGI:
#AI #MachineLearning #Transformer #DeepLearning #NLP #AttentionMechanism
