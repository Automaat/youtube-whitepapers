TYTU≈Å:
GPT-3: Jak 175 miliard√≥w parametr√≥w zmieni≈Ço sztucznƒÖ inteligencjƒô | Deep Dive

OPIS:
üéôÔ∏è W tym odcinku analizujemy prze≈Çomowy artyku≈Ç "Language Models are Few-Shot Learners" z 2020 roku, kt√≥ry przedstawi≈Ç ≈õwiatu GPT-3 i zrewolucjonizowa≈Ç podej≈õcie do uczenia maszynowego.

W tym odcinku omawiamy:
‚Ä¢ Paradygmat AI przed GPT-3: pre-training i fine-tuning
‚Ä¢ Rewolucja in-context learning - uczenie siƒô bez treningu
‚Ä¢ Trzy poziomy promptowania: zero-shot, one-shot, few-shot
‚Ä¢ Prze≈Çomowe wyniki na benchmarkach TriviaQA i LAMBADA
‚Ä¢ Emergencja meta-uczenia i skalowanie modeli
‚Ä¢ Fundamentalne ograniczenia GPT-3 i problem bias
‚Ä¢ Wp≈Çyw na przysz≈Ço≈õƒá AI i przewidywania autor√≥w

üìÑ Oryginalny artyku≈Ç: https://arxiv.org/abs/2005.14165

Autorzy: Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei (OpenAI)

TAGI:
#AI #MachineLearning #DeepLearning #GPT3 #OpenAI #NLP #LargeLanguageModels #InContextLearning #FewShotLearning #TransformerModels #NeuralNetworks #AIResearch #SztucznaInteligencja #UczenieMaszynowe
