TYTUÅ:
ZeRO: Jak Microsoft PrzeÅ‚amaÅ‚ BarierÄ™ PamiÄ™ci GPU | Deep Dive

OPIS:
ğŸ™ï¸ Omawiamy przeÅ‚omowy artykuÅ‚ Microsoftu z 2019 roku "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models" â€“ technologia, ktÃ³ra zrewolucjonizowaÅ‚a trenowanie gigantycznych modeli jÄ™zykowych poprzez eliminacjÄ™ redundancji w pamiÄ™ci GPU.

W tym odcinku omawiamy:
â€¢ Problem muru pamiÄ™ci: dlaczego model GPT-2 (3 GB wag) potrzebuje 24 GB pamiÄ™ci do treningu
â€¢ Ograniczenia Data Parallelism (DP) i Model Parallelism (MP): redundancja vs komunikacyjny koszmar
â€¢ Anatomia pamiÄ™ci GPU: 16 bajtÃ³w na parametr przez stany optymalizatora Adam
â€¢ Trzy etapy ZeRO-DP: partycjonowanie stanÃ³w optymalizatora (4x), gradientÃ³w (8x) i parametrÃ³w (64x redukcji)
â€¢ ZeRO-R: zarzÄ…dzanie aktywacjami, buforami i defragmentacja pamiÄ™ci
â€¢ Superliniowa skalowalnoÅ›Ä‡: jak uwolniona pamiÄ™Ä‡ pozwala na wiÄ™ksze batch sizes
â€¢ Demokratyzacja AI: trenowanie modeli 13B parametrÃ³w bez skomplikowanego model parallelism
â€¢ Turing-NLG: 17 miliardÃ³w parametrÃ³w â€“ najwiÄ™kszy model jÄ™zykowy w momencie publikacji

ğŸ“„ Oryginalny artykuÅ‚: https://arxiv.org/abs/1910.02054

Autorzy: Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He (Microsoft, 2019)

TAGI:
#AI #MachineLearning #DeepLearning #ZeRO #Microsoft #DeepSpeed #NLP #Transformer #DistributedTraining #LanguageModels #ArtificialIntelligence #SztucznaInteligencja #DeepDive #GPUMemory #DataParallelism #ModelParallelism #TuringNLG #AdamOptimizer #MixedPrecision #ScalableAI #PL
