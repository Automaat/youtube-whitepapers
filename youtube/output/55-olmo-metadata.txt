TYTUÅ:
OLMo: Pierwszy Prawdziwie Otwarty Model JÄ™zykowy | GÅ‚Ä™boki Odczyt

OPIS:
ğŸ™ï¸ OLMo (Open Language Model) to przeÅ‚omowy projekt Allen Institute for AI, ktÃ³ry redefiniuje znaczenie â€otwartoÅ›ci" w sztucznej inteligencji. W przeciwieÅ„stwie do czÄ™Å›ciowo otwartych modeli jak LLaMA czy Mixtral, OLMo udostÄ™pnia kompletny ekosystem: peÅ‚ny kod treningowy, 3 biliony tokenÃ³w danych Dolma, ponad 500 checkpointÃ³w treningowych oraz wszystkie narzÄ™dzia ewaluacyjne.

W tym odcinku omawiamy:
â€¢ ArchitekturÄ™ modelu: decoder-only Transformer bez biasÃ³w, SwiGLU, RoPE i non-parametric layer norm
â€¢ ZbiÃ³r danych Dolma: transparentny korpus 3T tokenÃ³w z peÅ‚nÄ… dokumentacjÄ… ÅºrÃ³deÅ‚
â€¢ InfrastrukturÄ™ treningowÄ…: rÃ³wnolegÅ‚y trening na NVIDIA A100 i AMD MI250X (PyTorch + FSDP)
â€¢ StrategiÄ™ ewaluacji: ciÄ…gÅ‚y monitoring w czasie rzeczywistym i wykrywanie anomalii
â€¢ Wyniki benchmarkÃ³w: porÃ³wnanie z LLaMA 2 7B i Falcon 7B na CommonSense Reasoning
â€¢ Proces dekontaminacji: uczciwa ewaluacja na benchmark Paloma
â€¢ AdaptacjÄ™ do asystenta: SFT + DPO â†’ OLMo-7B-Instruct z poprawÄ… bezpieczeÅ„stwa
â€¢ Kompletny framework badawczy: narzÄ™dzia Catwalk, peÅ‚ne logi Weights & Biases

ğŸ“„ Oryginalny artykuÅ‚: https://arxiv.org/abs/2402.00838

Autorzy: Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, et al. (Allen Institute for AI)

TAGI:
#AI #MachineLearning #DeepLearning #OpenSource #LLM #OLMo #AllenAI #NLP #Transformers #OpenScience #ResearchTransparency #LanguageModels #AIResearch