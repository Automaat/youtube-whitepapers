TYTUÅ:
RETRO: Model 25x Mniejszy Pokonuje GPT-3 DziÄ™ki ZewnÄ™trznej PamiÄ™ci | Deep Dive

OPIS:
ğŸ™ï¸ W tym odcinku analizujemy przeÅ‚omowÄ… pracÄ™ DeepMind "Improving Language Models by Retrieving from Trillions of Tokens", ktÃ³ra rzuca wyzwanie dominujÄ…cemu paradygmatowi "wiÄ™kszy znaczy lepszy" w dziedzinie AI.

W tym odcinku omawiamy:
â€¢ ğŸ§  Architektura RETRO - jak model wykorzystuje bazÄ™ 2 bilionÃ³w tokenÃ³w jako zewnÄ™trznÄ… pamiÄ™Ä‡
â€¢ ğŸ”’ ZamroÅ¼ony koder BERT - pragmatyczny kompromis miÄ™dzy wydajnoÅ›ciÄ… a praktycznoÅ›ciÄ…
â€¢ âš¡ Mechanizm Chunked Cross-Attention (CCA) - integracja wiedzy z generowaniem tekstu
â€¢ ğŸ“Š Wyniki - jak 7.5B model pokonuje 178B Jurassic-1 na benchmarku The Pile
â€¢ ğŸ”„ Retro-fitting - modernizacja istniejÄ…cych modeli z wykorzystaniem tylko 3% danych
â€¢ ğŸ¯ Test set leakage - odpowiedÅº na zarzuty o "kopiuj-wklej"
â€¢ ğŸ”® Trzy fundamentalne korzyÅ›ci: interpretowalnoÅ›Ä‡, Å‚atwe aktualizacje, bezpieczeÅ„stwo

ğŸ“„ Oryginalny artykuÅ‚: https://arxiv.org/abs/2112.04426

Autorzy: Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, Laurent Sifre (DeepMind)

TAGI:
#AI #MachineLearning #DeepLearning #RETRO #DeepMind #NLP #LLM #Transformers #RAG #Retrieval #GPT3 #SztucznaInteligencja #ModelJÄ™zykowy #NeuralNetworks
