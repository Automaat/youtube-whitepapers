TYTUÅ:
InstructGPT: Jak 1.3B parametrÃ³w pokonaÅ‚o 175B? Rewolucja RLHF | Deep Dive

OPIS:
ğŸ™ï¸ W tym odcinku zagÅ‚Ä™biamy siÄ™ w przeÅ‚omowÄ… pracÄ™ InstructGPT od OpenAI, ktÃ³ra zdefiniowaÅ‚a na nowo jak trenowaÄ‡ modele jÄ™zykowe, by naprawdÄ™ rozumiaÅ‚y nasze intencje.

W tym odcinku omawiamy:
â€¢ Problem niedopasowania (misalignment) wielkich modeli jÄ™zykowych i framework 3H (Helpful, Honest, Harmless)
â€¢ SzokujÄ…cy wynik: model 1.3B parametrÃ³w preferowany przez ludzi nad GPT-3 (175B) - ponad 100x rÃ³Å¼nica!
â€¢ TrÃ³jetapowy proces RLHF: Supervised Fine-Tuning â†’ Reward Model â†’ PPO
â€¢ Wyniki: 85% preferencji, redukcja halucynacji z 41% do 21%, 25% mniej toksycznych treÅ›ci
â€¢ "Podatek od dopasowania" (alignment tax) i eleganckie rozwiÄ…zanie PPO-PTX
â€¢ Generalizacja do zadaÅ„ poza danymi treningowymi (np. kod)
â€¢ Ograniczenia i "problem 40 ewaluatorÃ³w" - czyje wartoÅ›ci reprezentuje AI?

ğŸ“„ Oryginalny artykuÅ‚: https://arxiv.org/abs/2203.02155

Autorzy: Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe (OpenAI)

TAGI:
#AI #MachineLearning #DeepLearning #InstructGPT #RLHF #OpenAI #GPT3 #LanguageModels #AIAlignment #ReinforcementLearning #PPO #NLP #AIBezpieczeÅ„stwo #SztucznaInteligencja #LLM
