TYTU≈Å:
BERT: Jak Google nauczy≈Ç maszyny rozumieƒá kontekst | Deep Dive

OPIS:
üéôÔ∏è Omawiamy prze≈Çomowy artyku≈Ç Google AI Language z 2018 roku "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" ‚Äì model, kt√≥ry zrewolucjonizowa≈Ç przetwarzanie jƒôzyka naturalnego.

W tym odcinku omawiamy:
‚Ä¢ Problem kontekstu w NLP: dlaczego maszyny zna≈Çy s≈Çowa, ale nie rozumia≈Çy ich znaczenia
‚Ä¢ Ograniczenia poprzednik√≥w (GPT, ELMo): jednokierunkowo≈õƒá jako g≈Ç√≥wna przeszkoda
‚Ä¢ Innowacja nr 1 ‚Äì Masked Language Model (MLM): genialna gra w zgadywanie zamaskowanych s≈Ç√≥w
‚Ä¢ Strategia maskowania 80/10/10: klucz do unikniƒôcia rozbie≈ºno≈õci trening-zastosowanie
‚Ä¢ Innowacja nr 2 ‚Äì Next Sentence Prediction (NSP): nauka logiki narracji
‚Ä¢ Wyniki: 11 nowych rekord√≥w SOTA, pierwszy system pokonujƒÖcy ludzi w SQuAD
‚Ä¢ Paradygmat pre-training + fine-tuning: demokratyzacja NLP
‚Ä¢ Badania ablacyjne: dow√≥d na znaczenie dwukierunkowo≈õci i NSP
‚Ä¢ Efekt skali: wiƒôksze modele = lepsze wyniki nawet na ma≈Çych zbiorach danych

üìÑ Oryginalny artyku≈Ç: https://arxiv.org/abs/1810.04805

Autorzy: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova (Google AI Language, 2018)

TAGI:
#AI #MachineLearning #DeepLearning #BERT #Google #NLP #Transformer #PreTraining #FineTuning #LanguageModels #ArtificialIntelligence #SztucznaInteligencja #DeepDive #NaturalLanguageProcessing #MLM #NSP #PL
