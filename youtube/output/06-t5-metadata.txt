TYTU≈Å:
T5: Jeden Model, by Wszystkimi RzƒÖdziƒá - Paradygmat Text-to-Text | Deep Dive

OPIS:
üéôÔ∏è Omawiamy prze≈Çomowy artyku≈Ç Google Research z 2019 roku "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" ‚Äì praca, kt√≥ra zaproponowa≈Ça rewolucyjne podej≈õcie: ka≈ºde zadanie NLP jako generowanie tekstu.

W tym odcinku omawiamy:
‚Ä¢ Paradygmat Text-to-Text: jeden model do t≈Çumacze≈Ñ, streszczania, klasyfikacji i wielu innych zada≈Ñ
‚Ä¢ C4 (Colossal Clean Crawled Corpus): 750 GB starannie przefiltrowanych danych z internetu
‚Ä¢ Powr√≥t do klasyki: dlaczego architektura Encoder-Decoder pokona≈Ça wyspecjalizowane alternatywy
‚Ä¢ Span Corruption: innowacyjna metoda pre-treningu przez "odszumianie" tekstu
‚Ä¢ Lekcja z danych: r√≥≈ºnorodno≈õƒá lepsza ni≈º powt√≥rzenia ‚Äì problem zapamiƒôtywania vs generalizacji
‚Ä¢ Strategie Fine-tuningu: pe≈Çne dostrojenie vs Adapter Layers
‚Ä¢ Gorzka Lekcja Skalowania: wiƒôksze modele + wiƒôcej danych = lepsze wyniki
‚Ä¢ Rodzina modeli T5: od 60M do 11B parametr√≥w, SOTA na 18/24 benchmarkach
‚Ä¢ SuperGLUE: wynik niemal dor√≥wnujƒÖcy ludzkiej wydajno≈õci (88.9 vs 89.8)

üìÑ Oryginalny artyku≈Ç: https://arxiv.org/abs/1910.10683

Autorzy: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu (Google Research, 2019)

TAGI:
#AI #MachineLearning #DeepLearning #T5 #Google #NLP #Transformer #TransferLearning #TextToText #LanguageModels #ArtificialIntelligence #SztucznaInteligencja #DeepDive #BERT #GPT #SuperGLUE #C4 #PreTraining #FineTuning #ScalableAI #PL
