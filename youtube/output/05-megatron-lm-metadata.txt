TYTUÅ:
Megatron-LM: Jak NVIDIA PrzeÅ‚amaÅ‚a BarierÄ™ PamiÄ™ci GPU | Deep Dive

OPIS:
ğŸ™ï¸ Omawiamy przeÅ‚omowy artykuÅ‚ NVIDIA z 2019 roku "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism" â€“ technika, ktÃ³ra otworzyÅ‚a erÄ™ gigantycznych modeli jÄ™zykowych.

W tym odcinku omawiamy:
â€¢ Paradoks skalowania AI: jak zmieÅ›ciÄ‡ "ocean wiedzy w szklance pamiÄ™ci" GPU
â€¢ Ograniczenia Pipeline Parallelism (GPipe) i Mesh TensorFlow: problem "bÄ…belkÃ³w" i nieefektywnoÅ›ci
â€¢ Innowacja Intralayer Model Parallelism: rÃ³wnolegÅ‚e obliczenia wewnÄ…trz warstwy Transformer
â€¢ Strategia podziaÅ‚u blokÃ³w MLP i Self-Attention: kolumnowy vs wierszowy podziaÅ‚ macierzy
â€¢ AllReduce: tylko 2 operacje komunikacji na warstwÄ™ â€“ klucz do wydajnoÅ›ci
â€¢ Wyniki: 8.3 miliarda parametrÃ³w, 512 GPU V100, 76% efektywnoÅ›ci skalowania
â€¢ Nowe rekordy SOTA: WikiText-103 (Perplexity 10.8), LAMBADA (66.5% celnoÅ›ci)
â€¢ Odkrycie przy okazji: jak przestawienie LayerNorm naprawiÅ‚o niestabilnoÅ›Ä‡ BERT
â€¢ Dziedzictwo: fundament dla Turing-NLG, GPT-3, PaLM i BLOOM

ğŸ“„ Oryginalny artykuÅ‚: https://arxiv.org/abs/1909.08053

Autorzy: Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro (NVIDIA, 2019)

TAGI:
#AI #MachineLearning #DeepLearning #MegatronLM #NVIDIA #NLP #Transformer #ModelParallelism #DistributedTraining #LanguageModels #ArtificialIntelligence #SztucznaInteligencja #DeepDive #GPT2 #BERT #GPU #ScalableAI #PL
