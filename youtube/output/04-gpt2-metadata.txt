TYTUÅ:
GPT-2: Narodziny Modeli JÄ™zykowych Nowej Generacji | Deep Dive

OPIS:
ğŸ™ï¸ DogÅ‚Ä™bna analiza przeÅ‚omowej pracy OpenAI z 2019 roku "Language Models Are Unsupervised Multitask Learners", ktÃ³ra zapoczÄ…tkowaÅ‚a rewolucjÄ™ Wielkich Modeli JÄ™zykowych (LLM).

W tym odcinku omawiamy:
â€¢ ZmianÄ™ paradygmatu: od wÄ…skich specjalistÃ³w do uniwersalnych modeli jÄ™zykowych
â€¢ Nienadzorowane uczenie wielozadaniowe - jak z jednego prostego zadania (przewidywanie nastÄ™pnego sÅ‚owa) wyÅ‚aniajÄ… siÄ™ zÅ‚oÅ¼one umiejÄ™tnoÅ›ci
â€¢ WebText: 40GB jakoÅ›ciowych danych z Reddita i celowe wykluczenie Wikipedii
â€¢ Architektura Transformer i skala: od 117M do 1.5B parametrÃ³w
â€¢ Rewolucyjne wyniki zero-shot: tÅ‚umaczenie, czytanie ze zrozumieniem, odpowiadanie na pytania
â€¢ Dowody na prawdziwÄ… generalizacjÄ™ vs zapamiÄ™tywanie (Bloom Filters, krzywe uczenia)
â€¢ Emergentne wÅ‚aÅ›ciwoÅ›ci i metapoznanie modelu

ğŸ“„ Oryginalny artykuÅ‚: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf

Autorzy: Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever (OpenAI)

TAGI:
#GPT2 #OpenAI #AI #MachineLearning #DeepLearning #NLP #LanguageModels #LLM #Transformer #ZeroShot #EmergentAbilities #ArtificialIntelligence #SztucznaInteligencja #NeuralNetworks #BPE
