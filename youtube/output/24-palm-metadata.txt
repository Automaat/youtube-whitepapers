TYTUÅ:
PaLM: Model 540 miliardÃ³w parametrÃ³w, ktÃ³ry zmieniÅ‚ zasady skalowania AI | Deep Dive

OPIS:
ğŸ™ï¸ Omawiamy przeÅ‚omowÄ… pracÄ™ Google nad modelem jÄ™zykowym PaLM (Pathways Language Model) - model o 540 miliardach parametrÃ³w, ktÃ³ry osiÄ…gnÄ…Å‚ bezprecedensowÄ… efektywnoÅ›Ä‡ treningu i ujawniÅ‚ zaskakujÄ…ce emergentne zdolnoÅ›ci AI.

W tym odcinku omawiamy:
â€¢ ğŸ“Š System Pathways - rewolucyjna architektura treningu na 6000+ chipach TPU v4
â€¢ âš¡ Model Flops Utilization (MFU) - ponad 2x lepsza efektywnoÅ›Ä‡ niÅ¼ GPT-3 (46% vs 21%)
â€¢ ğŸ§  Chain of Thought Prompting - technika rozumowania krok po kroku
â€¢ ğŸ˜‚ Emergentne zdolnoÅ›ci - rozumienie Å¼artÃ³w, przysÅ‚Ã³w i nieciÄ…gÅ‚e ulepszenia
â€¢ ğŸ’» PaLM-Coder - state-of-the-art w generowaniu i naprawianiu kodu
â€¢ ğŸŒ Paradoks wielojÄ™zyczny - 78% danych po angielsku, a model tÅ‚umaczy lepiej niÅ¼ specjalistyczne modele
â€¢ âš–ï¸ Prawa skalowania Chinchilla - czy "wiÄ™kszy" zawsze znaczy "lepszy"?

ğŸ“„ Oryginalny artykuÅ‚: https://arxiv.org/abs/2204.02311

Autorzy: Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, et al. (Google Research)

TAGI:
#AI #MachineLearning #DeepLearning #PaLM #Google #LLM #NLP #Transformers #Pathways #ChainOfThought #EmergentAbilities #Chinchilla #ScalingLaws #LanguageModels #ArtificialIntelligence #SztucznaInteligencja
