TYTUÅ:
Codex - Jak AI NauczyÅ‚o SiÄ™ ProgramowaÄ‡ | Deep Dive

OPIS:
ğŸ™ï¸ Omawiamy przeÅ‚omowÄ… pracÄ™ OpenAI przedstawiajÄ…cÄ… model Codex - technologiÄ™ stojÄ…cÄ… za GitHub Copilot. Autorzy pokazujÄ…, jak specjalistyczny fine-tuning modelu GPT na 159GB kodu z GitHuba przeksztaÅ‚ciÅ‚ go w narzÄ™dzie zdolne do generowania dziaÅ‚ajÄ…cego kodu z opisÃ³w w jÄ™zyku naturalnym.

W tym odcinku omawiamy:
â€¢ SyntezÄ™ programÃ³w - od science fiction do rzeczywistoÅ›ci dziÄ™ki Codex
â€¢ Benchmark HumanEval - 164 problemy programistyczne do oceny poprawnoÅ›ci funkcjonalnej
â€¢ MetrykÄ™ pass@k - rewolucyjne podejÅ›cie do mierzenia skutecznoÅ›ci generowania kodu
â€¢ Wyniki: skok z 0% (GPT-3) do 28.8% (Codex) przy jednej prÃ³bie
â€¢ Ograniczenia modelu - problemy z wiÄ…zaniem zmiennych i misalignment
â€¢ Ryzyka bezpieczeÅ„stwa - nadmierne zaufanie i generowanie kodu z lukami

ğŸ“„ Oryginalny artykuÅ‚: https://arxiv.org/abs/2107.03374

Autorzy: Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan i zespÃ³Å‚ OpenAI

TAGI:
#AI #MachineLearning #DeepLearning #Codex #GitHubCopilot #OpenAI #GPT #CodeGeneration #NLP #ProgramSynthesis #HumanEval #SztucznaInteligencja #Programowanie
