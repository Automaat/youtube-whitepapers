TYTUÅ:
Megatron-Turing NLG 530B: Jak Microsoft i NVIDIA zbudowali gigantyczny model AI | Deep Dive

OPIS:
ğŸ™ï¸ WspÃ³lna wyprawa badaczy z Microsoft i NVIDIA dokumentujÄ…ca budowÄ™ jednego z najwiÄ™kszych i najpotÄ™Å¼niejszych modeli jÄ™zykowych w historii - Megatron-Turing NLG z 530 miliardami parametrÃ³w.

W tym odcinku omawiamy:
â€¢ ğŸ§  Problem skali - dlaczego 530 miliardÃ³w parametrÃ³w wymaga ponad 10 terabajtÃ³w pamiÄ™ci i jak to przezwyciÄ™Å¼ono
â€¢ âš¡ Architektura 3D Parallelizm - genialne poÅ‚Ä…czenie tensor, pipeline i data parallelizmu
â€¢ ğŸ—ºï¸ Topologia Superkomputera Selene - jak zmapowano architekturÄ™ 3D na fizyczny ukÅ‚ad GPU
â€¢ ğŸ“Š Kuracja danych treningowych - filtracja jakoÅ›ciowa, deduplikacja semantyczna i unikanie "Å›ciÄ…gania"
â€¢ ğŸ† Wyniki SOTA - rekordowe rezultaty na benchmarkach LAMBADA, HellaSwag, BoolQ
â€¢ ğŸ§ª Test HANS - czy wiÄ™ksze modele naprawdÄ™ lepiej rozumiejÄ… jÄ™zyk?
â€¢ âš ï¸ Ograniczenia - in-context learning i krucha rÃ³wnowaga przykÅ‚adÃ³w w promptcie

ğŸ“„ Oryginalny artykuÅ‚: https://arxiv.org/abs/2201.11990

Autorzy: Smith, Patwary, Norick, LeGresley, Rajbhandari, Casper, Liu, Prabhumoye, Zerveas, Korthikanti, Zhang, Child, Aminabadi, Bernauer, Song, Shoeybi, He, Houston, Tiwary, Catanzaro (Microsoft & NVIDIA)

TAGI:
#AI #MachineLearning #DeepLearning #NLP #GPT3 #LLM #LanguageModel #MegatronTuring #Microsoft #NVIDIA #ScaleAI #Transformers #530B #AIResearch #NeuralNetworks #ZeroShot #FewShot #3DParallelism
