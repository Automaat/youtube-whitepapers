Generate 11 presentation slides based on the podcast about RFC 959 - File Transfer Protocol (FTP).

## Visual Style

- Minimal, clean design with dark blue headers
- White/light gray background
- Sans-serif typography throughout
- Simple outline icons only (no stock photos, no AI-generated images)
- Consistent layout: title at top, bullets left-aligned
- Same spacing and margins across all slides
- Use diagrams/flowcharts for technical concepts where appropriate

---

## Slide 1: Origins and Context of FTP

- First FTP specification appeared before RFC 959 - protocol evolved over multiple iterations
- RFC 959 published in 1985 by Jon Postel, representing mature version of the protocol
- Designed to solve file transfer problems across heterogeneous systems (different OSes, architectures)
- Built on top of TCP/IP, leveraging reliable transport layer
- Became fundamental protocol for Internet file sharing before HTTP dominance

## Slide 2: The Three Fundamental Problems

- Problem 1: Different file systems across platforms (Unix, IBM mainframes, DEC systems)
- Problem 2: Diverse data representations - ASCII variants, EBCDIC, binary formats
- Problem 3: Network heterogeneity - different byte orders, character encodings, line terminators
- FTP needed abstraction layer to handle all these differences transparently
- Goal: universal file transfer mechanism independent of underlying systems

## Slide 3: Dual Connection Architecture

- FTP uses two separate TCP connections simultaneously
- Control Connection: persistent channel for commands and replies (port 21)
- Data Connection: ephemeral channel established per transfer operation
- Control connection uses Telnet protocol for command exchange
- Separation enables independent management of control flow and data flow

## Slide 4: Control vs Data Connection Details

- Control connection established first, remains open throughout session
- Commands sent as ASCII text over control connection (USER, PASS, LIST, RETR, STOR)
- Server responds with 3-digit status codes (similar to HTTP: 2xx success, 4xx client error, 5xx server error)
- Data connection created on-demand for each file transfer or directory listing
- Two modes: active (server connects to client) and passive (client connects to server)

## Slide 5: Data Type Negotiation

- Before each transfer, client must negotiate data representation type
- TYPE command with four options: ASCII, EBCDIC, Image (binary), Local
- ASCII type handles text files with automatic line ending conversion (CRLF normalization)
- Image type for binary files - raw bytes without interpretation
- Type setting affects how data is interpreted and converted between systems

## Slide 6: Structure and Mode Dimensions

- Structure dimension defines file organization: File, Record, Page
- File structure: continuous byte stream (most common)
- Record structure: file composed of sequential records
- Mode dimension controls transmission format: Stream, Block, Compressed
- These orthogonal parameters create flexible transfer matrix

## Slide 7: Stream Mode Limitations

- Stream mode: simplest approach, continuous data flow until EOF
- Fundamental flaw: no mechanism to distinguish data from control information
- Cannot reliably detect premature connection termination vs normal EOF
- No way to resume interrupted transfers
- Led to development of Block and Compressed modes

## Slide 8: Block Mode Solution

- Block mode encapsulates data in structured blocks with headers
- Each block contains: descriptor byte, byte count, data payload
- Descriptor indicates block type: data, EOF marker, error recovery info
- Enables restart capability - transfer can resume from last complete block
- More overhead but significantly more robust than stream mode

## Slide 9: Compressed Mode Efficiency

- Designed for slow network links and expensive bandwidth (1980s reality)
- Run-length encoding: instead of sending 100 spaces, send "100 Ã— space"
- Compression happens at protocol level, transparent to applications
- Format: compressed byte, repeat count, actual data
- Trade-off: CPU overhead for bandwidth savings

## Slide 10: Historical Significance and Legacy

- RFC 959 demonstrates engineering thinking from resource-constrained era
- Every byte mattered - compression built into protocol, not application layer
- Assumptions optimized for 1980s networks: slow links, expensive bandwidth, heterogeneous systems
- Today's context differs: fast networks, cheap bandwidth, homogeneous environments
- Shows how protocol design reflects technological constraints of its time

## Slide 11: Question for You

Will our current protocol design assumptions seem equally naive in another 40 years?
