{"text": " Modele j\u0119zykowe staj\u0105 si\u0119 coraz pot\u0119\u017cniejsze, ale no w\u0142a\u015bnie, co to tak naprawd\u0119 znaczy w praktyce? To jest dobre pytanie, bo wszyscy s\u0142yszymy te liczby, prawda? Miliardy parametr\u00f3w. Dok\u0142adnie. Ale co za tym idzie? Dzisiaj zajrzyjmy prosto do maszynowi. Mamy przed sob\u0105 artyku\u0142 naukowy z Deep Mind, Scaling Language Models, Methods Analysis and Insights from Training Gopher. I to jest w zasadzie dziennik pok\u0142adowy stworzenia jednego z najwi\u0119kszych modeli na \u015bwiecie, gofera. A on ma, przypomnijmy, 280 miliard\u00f3w parametr\u00f3w. Na celem tej naszej analizy jest zrozumienie, co si\u0119 dzieje, gdy model osi\u0105ga tak\u0105 skal\u0119. No w\u0142a\u015bnie. I to nie jest prosta historia o tym, \u017ce wi\u0119kszy znaczy lepszy. Autorzy zadali sobie fundamentalne pytanie. Jakie nowe zdolno\u015bci si\u0119 pojawiaj\u0105? Gdzie model, hmm, udarza w \u015bcian\u0119. I co to wszystko m\u00f3wi nam o przysz\u0142o\u015bci sztucznej inteligencji? W\u0142a\u015bnie. A poniewa\u017c opieramy si\u0119 na pracy badawczej, mamy dost\u0119p do brutalnie szczerych danych, a nie, wiesz, marketingowych obietnic. Ok, czyli misja na dzi\u015b. Zrozumie\u0107, czy zbudowali\u015bmy cyfrowego geniusze, czy po prostu, no, najwi\u0119ksz\u0105 na \u015bwiecie papug\u0119. Gopher jest oparty na architekturze Transformer. Czyli tej samej, co wi\u0119kszo\u015b\u0107 znanych nam modeli. Jego zadaniem jest w zasadzie przewidywanie nast\u0119pnego s\u0142owa w zdaniu. Ale przecie\u017c nie jest pierwszym gigantem. Mieli\u015bmy GPT-3, Jurassic One, Megatron, Turing, NLG. Czym on si\u0119 tak naprawd\u0119 wyr\u00f3\u017cnia? I to jest \u015bwietne pytanie, bo artyku\u0142 pozycjonuje gofera nie jako kolejnego wi\u0119kszego, tylko jako precyzyjne narz\u0119dzie badawcze. W sensie, \u017ce stworzyli ca\u0142\u0105 rodzin\u0119 modeli? Dok\u0142adnie. Od takich maluch\u00f3w z kilkudziesi\u0119cioma milionami parametr\u00f3w, a\u017c po tego 280 miliardowego kolosa. I zrobili to po to, by precyzyjnie zmierzy\u0107, w kt\u00f3rym momencie i w jakich zadaniach sama skala zaczyna by\u0107, no, game changerem. Czyli pytanie nie brzmi, czy jest lepszy, ale gdzie, dlaczego i jakim kosztem staje si\u0119 lepszy? Tak jest. No dobrze. Ale sedno tego badania musi opieracie na jakie\u015b niewyobra\u017calne in\u017cynierii. Jak w og\u00f3le trenuje si\u0119 takiego potwora? Oj, tak. To jest gigantyczne wyzwanie. W artykule jest mowa, \u017ce same parametry i stan optymalizatora gofera zajmuj\u0105 2,5 terabajta. Przeci\u0119tny dysk w laptopie ma mo\u017ce 1 terabajt. A pojedynczy rdze\u0144 ich uk\u0142adu TPU V3 ma tylko 16 GB pami\u0119ci. To jest, jak sama zauwa\u017cy\u0142a\u015b, pr\u00f3ba wlania oceanu do szklanki. I jak to zrobili? Fizyczne ograniczenia pami\u0119ci. I zastosowano ca\u0142y arsena\u0142 sztuczek. Na przyk\u0142ad model paralelizm. Co to znaczy w praktyce? Wyobra\u017a sobie linie produkcyjn\u0105 w fabryce samochod\u00f3w. Jeden robot nie buduje ca\u0142ego auta, prawda? No jasne. Jeden montuje drwi, drugi silnik. Dok\u0142adnie. Tutaj jest podobnie. Model jest tak du\u017cy, \u017ce tnie si\u0119 go na kawa\u0142ki i ka\u017cdy kawa\u0142ek mieszka na innym procesorze. Komunikuj\u0105 si\u0119 ze sob\u0105, ale \u017caden pojedynczy uk\u0142ad nie musi mie\u015bci\u0107 ca\u0142o\u015bci. Okej. To ma sens. Ale w artykule jest te\u017c mowa o czym\u015b, co nazywa si\u0119 rematerializacen. To ju\u017c brzmi bardziej tajemniczo. Bo to jest jeszcze sprytniejsze. Wyobra\u017a sobie, \u017ce pieczesz bardzo skomplikowane ciasto. Zamiast przygotowywa\u0107 wszystkie kremy i biszkopty naraz i trzyma\u0107 je na blacie, co zaj\u0119\u0142oby mn\u00f3stwo miejsca... Za ka\u017cdym razem, gdy potrzebuje kremu, robi\u0119 go od nowa. W\u0142a\u015bnie. To troch\u0119 w\u00f3lniejsze, ale pozwala zaoszcz\u0119dzi\u0107 ogromn\u0105 ilo\u015b\u0107 pami\u0119ci na kuchennym blacie. Oni robili dok\u0142adnie to samo z wynikami po\u015brednich oblicze\u0144. Niesamowite. Czyli to nie jest jeden superkomputer, tylko ca\u0142a precyzyjnie synchronizowana orkiestra tysi\u0119cy ma\u0142ych jednostek. Kt\u00f3re bez przerwy ze sob\u0105 rozmawiaj\u0105, dziel\u0105 zadania i odtwarzaj\u0105 obliczenia, \u017ceby nie zapcha\u0107 sobie pami\u0119ci. A czym te orkiestry nakarmiono? Jakie paliwo nap\u0119dza Gofer'a? Paliwo to zbi\u00f3r danych o nazwie Masyw Text. M\u00f3wimy tu o 15 terabajtach tekstu. 15 terabajt\u00f3w? To jest absolutnie gigantyczna biblioteka cyfrowa. Zawiera strony internetowe, ksi\u0105\u017cki, artyku\u0142y z wiadomo\u015bciami, kod z Githaba. I tu jest co\u015b, co przyku\u0142o moj\u0105 uwag\u0119. Ksi\u0105\u017cki stanowi\u0105 a\u017c 27% danych treningowych. Tak to sporo. W przypadku s\u0142ynnego GPT-3 by\u0142o to zdaje si\u0119 tylko 16%. Czy to naprawd\u0119 robi a\u017c tak\u0105 r\u00f3\u017cnic\u0119? Domy\u015blam si\u0119, \u017ce j\u0119zyk w ksi\u0105\u017ckach jest generalnie bardziej uporz\u0105dkowany. Autorzy sugeruj\u0105, \u017ce tak, \u017ce to mo\u017ce by\u0107 jeden z powod\u00f3w jego lepszych wynik\u00f3w. Ksi\u0105\u017cki oferuj\u0105 d\u0142ugie, sp\u00f3jne narracje, bogatsze s\u0142ownictwo. To mo\u017ce uczy\u0107 model bardziej z\u0142o\u017conych zale\u017cno\u015bci w j\u0119zyku. A co z filtrowaniem tych danych? To jest r\u00f3wnie ciekawe. Zamiast budowa\u0107 jaki\u015b skomplikowany klasyfikator, kt\u00f3ry mia\u0142by ocenia\u0107 jako\u015b\u0107 tekstu, u\u017cyli bardzo prostych heurystyk. Czyli? Wyrzucali \u015bmieci. Mniej wi\u0119cej. Usuwali teksty, kt\u00f3re by\u0142y za kr\u00f3tkie, mia\u0142y za du\u017co powt\u00f3rze\u0144, albo dziwn\u0105, \u015bredni\u0105 d\u0142ugo\u015b\u0107 s\u0142owa. I to by\u0142o celowe? Tak. Chcieli unikn\u0105\u0107 sytuacji, w kt\u00f3rej zaawansowany filtr, nauczony na z\u0142otym standardzie tekst\u00f3w w stylu wikipedii, nie\u015bwiadomie odrzuci\u0142by nietypowe, ale warto\u015bciowe formy wypowiedzi. Na przyk\u0142ad r\u00f3\u017cne dialekty czy style literackie. Dok\u0142adnie. To by\u0142a \u015bwiadoma decyzja, \u017ceby zachowa\u0107 wi\u0119ksz\u0105 r\u00f3\u017cnorodno\u015b\u0107 danych. Dobra. Wiemy, jak go zbudowano i czym nakarmiono. Czas na danie g\u0142\u00f3wne. Wyniki. Przegl\u0105da\u0142em ten artyku\u0142 i tam jest tabela, kt\u00f3ra ma chyba ze 150 r\u00f3\u017cnych zada\u0144. Mhm. I jeden z tych wynik\u00f3w po prostu zwali\u0142 mnie znuk. Chodzi o ten test czytania ze zrozumieniem dla licealist\u00f3w, RAC. Pami\u0119tasz, jaki tam by\u0142 wynik? Oczywi\u015bcie. To jest jeden z g\u0142\u00f3wnych bohater\u00f3w tej publikacji. Gofer osi\u0105gn\u0105\u0142 tam 71,6% skuteczno\u015bci. A \u017ceby da\u0107 kontekst? Inny pot\u0119\u017cny model, Megatron Turing, mia\u0142 w tym samym testie 47,9%. To nie jest przyrost, to jest skok do innej ligi. Wynik Gofera zbli\u017ca si\u0119 do poziomu osi\u0105ganego przez ludzi. I ta chyba idealnie pokazuje, o co chodzi w tym skalowaniu. To nie jest tak, \u017ce model staje si\u0119 po prostu o kilka procent lepszy we wszystkim. W\u0142a\u015bnie. W pewnych obszarach przy odpowiedniej skali co\u015b klika i model nagle zyskuje zupe\u0142nie now\u0105 jako\u015b\u0107. Dok\u0142adnie. Autorzy nazywaj\u0105 to odblokowywaniem zdolno\u015bci. Mniejsza modele z tej rodziny w tym zadaniu radzi\u0142y sobie fatalnie. Dopiero te najwi\u0119ksze wersje nagle zaczyna\u0142y rozumie\u0107 kontekst na tyle dobrze, by odpowiada\u0107 na podchwytliwe pytania. Czyli pewnych umiej\u0119tno\u015bci nie da si\u0119 nauczy\u0107 troch\u0119. Albo model ich nie ma, albo po przekroczeniu pewnego progu skali nagle je zyskuje. Tak to wygl\u0105da. Poza czytaniem ze zrozumieniem ogromne post\u0119py wida\u0107 te\u017c by\u0142o w zadaniach zwi\u0105zanych z weryfikacj\u0105 fakt\u00f3w i co ciekawe w identyfikacji toksycznego j\u0119zyka. No w\u0142a\u015bnie. Ale skoro s\u0105 takie jasne punkty, to musz\u0105 by\u0107 te\u017c i cienie. Co jest r\u00f3wnie fascynuj\u0105ce to fakt, \u017ce s\u0105 obszary, gdzie skala nie tylko nie pomog\u0142a, ale wr\u0119cz zaszkodzi\u0142a. Zgadza si\u0119. I to jest chyba najbardziej otrze\u017awiaj\u0105cy wniosek z ca\u0142ego artyku\u0142u. Najmniejsze post\u0119py, a w niekt\u00f3rych przypadkach nawet pogorszenie wynik\u00f3w, odnotowano w zadaniach wymagaj\u0105cych rozumowania logicznego i matematycznego. Chwila to jest kompletnie wbryw intuicji. M\u00f3wisz, \u017ce dodanie mocy obliczeniowej i danych sprawi\u0142o, \u017ce w pewnych zadaniach model sta\u0142 si\u0119 g\u0142upszy? Brzmi niewiarygodnie, ale tak. Jak to w og\u00f3le jest mo\u017cliwe? We\u017amy przyk\u0142ad z benchmarku MMLU, kt\u00f3ry sprawdza wiedz\u0119 z 57 dziedzin akademickich. W podzadaniach z matematyki czy algebry abstrakcyjnej Gofer wypad\u0142 gorzej ni\u017c niekt\u00f3re mniejsze modele z tej samej rodziny. Dlaczego? Autorzy spekuluj\u0105, \u017ce to mo\u017ce by\u0107 fundamentalne ograniczenie obecnej architektury. Celem modelu jest przewidzenie kolejnego tokenu. To \u015bwietnie dzia\u0142a do nauki fakt\u00f3w, skojarze\u0144 i stylu j\u0119zykowego, ale wieloetapowe abstrakcyjne rozumowanie. Jakie jest potrzebne do rozwi\u0105zania zadania matematycznego to zupe\u0142nie inna bajka. Dok\u0142adnie tak. Czyli to troch\u0119 wraca do tego, co m\u00f3wili\u015bmy o danych. Internet i ksi\u0105\u017cki s\u0105 pe\u0142ne fakt\u00f3w typu Stolic\u0105 Francji jest Pary\u017c, ale rzadziej zawieraj\u0105 formalne krok po kroku do wody matematyczne. Mo\u017ce on po prostu nie mia\u0142 si\u0119 na czym tego nauczy\u0107. To jest cz\u0119\u015b\u0107 wyja\u015bnienia. \u0141atwiej jest zapami\u0119ta\u0107 ogromn\u0105 liczb\u0119 fakt\u00f3w, ni\u017c nauczy\u0107 si\u0119 uniwersalnej metody rozwi\u0105zywania problem\u00f3w. Mimo wszystko w og\u00f3lnej klasyfikacji tego benchmarku MMLU Gofer osi\u0105gn\u0105\u0142 60%, deklasuj\u0105c GPT-3 z wynikiem 43,9%. A co ciekawe, platforma prognostyczna HyperMind, gdzie ludzie obstawiaj\u0105 przysz\u0142o\u015b\u0107 AI, przewidywa\u0142a osi\u0105gni\u0119cie takiego poziomu dopiero za rok lub dwa. Gofer znowu wyprzedzi\u0142 oczekiwania, ale pokaza\u0142 te\u017c, gdzie le\u017c\u0105 twarde granice. A ta niesamowita zdolno\u015b\u0107 do na\u015bladowania ludzkiego j\u0119zyka, kt\u00f3r\u0105 wida\u0107 w testach na czytanie musi mie\u0107 te\u017c swoj\u0105 ciemn\u0105 stron\u0119, co z toksyczno\u015bci\u0105 i uprzedzeniami. I tu dochodzimy do fascynuj\u0105cego paradoksu. To jedno z najwa\u017cniejszych odkry\u0107 w tym artykule. To znaczy... Skala ma bardzo z\u0142o\u017cony wp\u0142yw na toksyczno\u015b\u0107. Z jednej strony, je\u015bli dasz modelowi toksyczny prompt, czyli jak\u0105\u015b obra\u017cliw\u0105 zaczepk\u0119, wi\u0119ksze modele s\u0105 bardziej sk\u0142one do wygenerowania r\u00f3wnie toksycznej odpowiedzi. Czyli s\u0105 lepszymi papugami. Nad dobrej nadz\u0142e. Po prostu lepiej na\u015bladuj\u0105 styl tego, co dostaj\u0105 na wej\u015bciu. Ok, czyli im wi\u0119kszy model, tym wi\u0119ksze ryzyko, \u017ce odwdzi\u0119czy si\u0119 pi\u0119knym za nadobne. Ale powiedzia\u0142e\u015b, \u017ce to paradoks, wi\u0119c musi by\u0107 te\u017c druga strona medalu. Dok\u0142adnie. Jednocze\u015bnie te same wi\u0119ksze modele s\u0105 znacznie lepsze w zadaniu klasyfikowania tekst\u00f3w jako toksyczne. Jak to? Liwy. Odpowied\u017a du\u017co wi\u0119ksz\u0105 precyzj\u0105 ni\u017c jego mniejsi kuzyni. Potrafi lepiej rozpozna\u0107 mow\u0119 nienawi\u015bci, mimo \u017ce sam potrafi j\u0105 lepiej generowa\u0107, je\u015bli si\u0119 go do tego sprowokuje. Czyli to jest kluczowa lekcja dla ka\u017cdego, kto buduje takie systemy. Sama wielko\u015b\u0107 modelu to nie wszystko. A nawet mo\u017ce by\u0107 pu\u0142apk\u0105. Bez odpowiedniego prowadzenia za r\u0119k\u0119 dostajemy po prostu m\u0105drzejsz\u0105, ale i potencjalnie bardziej z\u0142o\u015bliw\u0105 papug\u0119. I tu chyba wchodzi ca\u0142a sztuka promptingu. W\u0142a\u015bnie. A prompting to nic innego jak sztuka zadawania pyta\u0144 i wydawania instrukcji modelowi. W artykule opisano eksperyment Dialog Prompted Gopher. I co tam zrobili? Na samym pocz\u0105tku rozmowy model dosta\u0142 prost\u0105 instrukcj\u0119. Jeste\u015b pomocnym, uprzejmym i inkluzywnym asystentem. I sta\u0142a si\u0119 magia. Co si\u0119 sta\u0142o? Ca\u0142y trend si\u0119 odwr\u00f3ci\u0142. W tym trybie sk\u0142onno\u015b\u0107 do generowania toksycznych odpowiedzi nie ros\u0142a wraz ze skal\u0105 modelu. Wr\u0119cz przeciwnie wi\u0119ksze modele lepiej trzyma\u0142y si\u0119 instrukcji bycia mi\u0142ym asystentem. Bo pokazuje, \u017ce to jak poprosimy model o wykonanie zadania jest r\u00f3wnie wa\u017cne jak to co on wie. A co z innymi bardziej subtelnymi uprzedzeniami. Na przyk\u0142ad dotycz\u0105cymi p\u0142ci czy dialekt\u00f3w. Tu sprawa jest jeszcze bardziej skomplikowana. W przypadku uprzedze\u0144 p\u0142ciowych badacze nie znale\u017ali prostej zale\u017cno\u015bci od skali. Nic. Co wi\u0119cej odkryli jak niestabilne i zawodne s\u0105 obecne metody pomiaru. Wystarczy\u0142o zmieni\u0107 jedno s\u0142owo w szablonie zapytania. Na przyk\u0142ad was na is w zdaniu the zaw\u00f3d was p\u0142e\u0107. \u017beby wyniki pomiaru uprzedze\u0144 wywr\u00f3ci\u0142y si\u0119 do g\u00f3ry nogami. Czyli jeste\u015bmy jeszcze daleko od rzetelnego mierzenia tego problemu. A uprzedzenia dialektowe. Tutaj wyniki s\u0105 bardziej jednozlaczne i niestety niepokoj\u0105ce. Modele wykazywa\u0142y wy\u017csz\u0105 perpleksyti. Czyli by\u0142y bardziej zaskoczone tekstem. Gorzej sobie z nim radzi\u0142y. Tak. Na tweetach napisanych w dialekcie afroameryka\u0144skim, czyli African American Aligned English w por\u00f3wnaniu do standardowego angielskiego. I co najgorsze, ta r\u00f3\u017cnica wcale nie mala\u0142a wraz ze wzrostem skali modelu. Luka pozostawa\u0142a taka sama. Czyli samo dorzucanie wi\u0119kszej ilo\u015bci danych i parametr\u00f3w nie rozwi\u0105zuje problemu niedostatecznej reprezentacji pewnych gr\u00f3b w internecie. Dok\u0142adnie. Skala nie jest panaceum. To fundamentalny problem, kt\u00f3ry wymaga zupe\u0142nie innych rozwi\u0105za\u0144. Mimo tych wyzwa\u0144 widz\u0119, \u017ce autorzy widz\u0105 w tych modelach ogromny potencja\u0142. Tak, tak\u017ce jako narz\u0119dzia do badania bezpiecze\u0144stwa samej sztucznej inteligencji. Na przyk\u0142ad. Proponuj\u0105 u\u017cycie ich do symulowania debaty mi\u0119dzy dwoma systemami II. Je\u015bli jeden system ma jaki\u015b pogl\u0105d, drugi mo\u017ce pr\u00f3bowa\u0107 znale\u017a\u0107 w nim luki. Taki zautomatyzowany adwokat, diab\u0142a. Podoba mi si\u0119 to. Zamiast ba\u0107 si\u0119 pot\u0119\u017cnej AI u\u017cyjmy jej do wzajemnej kontroli. Artyku\u0142 ko\u0144czy si\u0119 te\u017c kilkoma bardzo cennymi, technicznymi lekcjami. Tak, z samego procesu treningu. Okaza\u0142o si\u0119 na przyk\u0142ad, \u017ce pewne algorytmy, kt\u00f3re \u015bwietnie dzia\u0142a\u0142y w mniejszej skali kompletnie zawodzi\u0142y przy rozmiarze GoFer'a. Jakie na przyk\u0142ad? Optymalizator Adam okaza\u0142 si\u0119 du\u017co stabilniejszy ni\u017c popularny Adafactor. Trening z ni\u017csz\u0105 precyzj\u0105 liczbow\u0105, Blowfet 16, powodowa\u0142, \u017ce niekt\u00f3re parametry zamra\u017ca\u0142y si\u0119 i przestawa\u0142y si\u0119 uczy\u0107. To s\u0105 bezcenne wskaz\u00f3wki dla innych zespo\u0142\u00f3w. Ale chyba najciekawszy by\u0142 wynik pr\u00f3b kompresji modelu. Pr\u00f3bowali go jako\u015b zmniejszy\u0107? Tak, u\u017cywaj\u0105c technik, jak pruning, czyli przycinanie najmniej wa\u017cnych po\u0142\u0105cze\u0144. Czy distillation, czyli pr\u00f3by nauczenia mniejszego modelu przez ten du\u017cy? Obie metody da\u0142y bardzo skromne rezultaty. Czyli te gigantyczne modele s\u0105 jak suflet. Pr\u00f3ba zmniejszenia go ko\u0144czy si\u0119 katastrof\u0105. Albo jak wie\u017ca d\u017c\u0119ga, wyci\u0105gni\u0119cie nawet kilku, ma\u0142o wa\u017cnych klock\u00f3w mo\u017ce spowodowa\u0107 zawalenie si\u0119 ca\u0142ej struktury. To jest \u015bwietna analogia. Wygl\u0105da na to, \u017ce wiedza w tych modelach jest rozproszona w bardzo delikatny i g\u0119sty spos\u00f3b po ca\u0142ej sieci. Nie ma tam zb\u0119dnych cz\u0119\u015bci. Czyli konkluzja autor\u00f3w jest jasna, skala daje niesamowite rezultaty w zadaniach opartych na wiedzy, ale ma swoje granice w rozumowaniu. A architektura Transformer to prawdopodobnie tylko etap przej\u015bciowy na drodze do czego\u015b zupe\u0142nie nowego. Zosta\u0142a mi w g\u0142owie jeszcze jedna ostatnia my\u015bl. W dodatkach do artyku\u0142u opisano test, w kt\u00f3rym zwi\u0119kszono d\u0142ugo\u015b\u0107 kontekstu, z kt\u00f3rym pracuje model. Z tysi\u0105ca do ponad 2 tysi\u0119cy token\u00f3w. Okaza\u0142o si\u0119, \u017ce bardzo pomog\u0142o to w analizie kodu i artyku\u0142\u00f3w naukowych. Co jest logiczne? Zar\u00f3wno kod, jak i prace naukowe maj\u0105 zale\u017cno\u015bci rozci\u0105gni\u0119te na wiele kapit\u00f3w. Wi\u0119kszy kontekst pozwala je lepiej wychwyci\u0107. No w\u0142a\u015bnie. I tu jest zagadka. Ten sam zabieg da\u0142 znacznie mniejszy zysk w przypadku analizy ksi\u0105\u017cek z klasyki literatury. A przecie\u017c wydawa\u0142oby si\u0119, \u017ce to w\u0142a\u015bnie w powie\u015bciach d\u0142ugi kontekst jest absolutnie kluczowy. To jest rzeczywi\u015bcie frapuj\u0105ce. Autorzy podsuwaj\u0105 dwie r\u00f3wnie intryguj\u0105ce mo\u017cliwo\u015bci. Pierwsza mo\u017ce te ksi\u0105\u017cki, nasze wielkie dzie\u0142a literackie, zawieraj\u0105 w rzeczywisto\u015bci mniej formalnych, d\u0142ugodystansowych zale\u017cno\u015bci ni\u017c nam si\u0119 wydaje. \u017be ich z\u0142o\u017cono\u015b\u0107 le\u017cy gdzie indziej? W emocjach, metaforach, pod tekstach? Mo\u017ce. A nie w skomplikowanej sk\u0142adni rozci\u0105gni\u0119tej na dziesi\u0105tki stron. A druga mo\u017cliwo\u015b\u0107? Druga jest taka, \u017ce gofer mimo swojej gigantycznej skali po prostu nie jest jeszcze na tyle pot\u0119\u017cny, by te subtelne literackie zale\u017cno\u015bci wychwyci\u0107 i wykorzysta\u0107. I to pozostawia nas z fundamentalnym pytaniem. Mianowicie. Czy nasze najbardziej z\u0142o\u017cone dzie\u0142a kultury s\u0105 w rzeczywisto\u015bci prostsze ni\u017c s\u0105dzimy? Czy te\u017c nasza sztuczna inteligencja jest wci\u0105\u017c \u015blepa na prawdziw\u0105 natur\u0119 ich z\u0142o\u017cono\u015bci?", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.8, "text": " Modele j\u0119zykowe staj\u0105 si\u0119 coraz pot\u0119\u017cniejsze, ale no w\u0142a\u015bnie, co to tak naprawd\u0119 znaczy w praktyce?", "tokens": [50364, 20500, 306, 49055, 74, 6880, 342, 11133, 3244, 25899, 1847, 1274, 1427, 44258, 11, 6775, 572, 14234, 11, 598, 281, 991, 20970, 36584, 261, 3206, 74, 874, 384, 30, 50754], "temperature": 0.0, "avg_logprob": -0.17626472533218504, "compression_ratio": 1.3519163763066202, "no_speech_prob": 0.009067724458873272}, {"id": 1, "seek": 0, "start": 7.8, "end": 12.0, "text": " To jest dobre pytanie, bo wszyscy s\u0142yszymy te liczby, prawda?", "tokens": [50754, 1407, 3492, 41959, 36610, 11, 748, 44232, 15116, 749, 1229, 2226, 535, 6169, 89, 2322, 11, 43607, 30, 50964], "temperature": 0.0, "avg_logprob": -0.17626472533218504, "compression_ratio": 1.3519163763066202, "no_speech_prob": 0.009067724458873272}, {"id": 2, "seek": 0, "start": 12.0, "end": 13.8, "text": " Miliardy parametr\u00f3w.", "tokens": [50964, 7036, 72, 515, 88, 6220, 27965, 3901, 13, 51054], "temperature": 0.0, "avg_logprob": -0.17626472533218504, "compression_ratio": 1.3519163763066202, "no_speech_prob": 0.009067724458873272}, {"id": 3, "seek": 0, "start": 13.8, "end": 17.2, "text": " Dok\u0142adnie. Ale co za tym idzie?", "tokens": [51054, 29768, 10358, 2766, 13, 9366, 598, 7949, 8107, 4496, 3283, 30, 51224], "temperature": 0.0, "avg_logprob": -0.17626472533218504, "compression_ratio": 1.3519163763066202, "no_speech_prob": 0.009067724458873272}, {"id": 4, "seek": 0, "start": 17.2, "end": 19.8, "text": " Dzisiaj zajrzyjmy prosto do maszynowi.", "tokens": [51224, 39448, 22356, 33729, 13047, 73, 2226, 10293, 78, 360, 2300, 1229, 3785, 72, 13, 51354], "temperature": 0.0, "avg_logprob": -0.17626472533218504, "compression_ratio": 1.3519163763066202, "no_speech_prob": 0.009067724458873272}, {"id": 5, "seek": 0, "start": 19.8, "end": 28.1, "text": " Mamy przed sob\u0105 artyku\u0142 naukowy z Deep Mind, Scaling Language Models, Methods Analysis and Insights from Training Gopher.", "tokens": [51354, 376, 7804, 18334, 18253, 1611, 594, 874, 5279, 1221, 35616, 74, 10089, 710, 14895, 13719, 11, 2747, 4270, 24445, 6583, 1625, 11, 25285, 82, 38172, 293, 9442, 5761, 490, 20620, 460, 16754, 13, 51769], "temperature": 0.0, "avg_logprob": -0.17626472533218504, "compression_ratio": 1.3519163763066202, "no_speech_prob": 0.009067724458873272}, {"id": 6, "seek": 2810, "start": 28.1, "end": 35.0, "text": " I to jest w zasadzie dziennik pok\u0142adowy stworzenia jednego z najwi\u0119kszych modeli na \u015bwiecie, gofera.", "tokens": [50364, 286, 281, 3492, 261, 44585, 3283, 9758, 1053, 13123, 13010, 10358, 10089, 342, 28321, 14320, 5232, 11858, 710, 48636, 1694, 28051, 2316, 72, 1667, 40078, 4260, 11, 352, 45635, 13, 50709], "temperature": 0.0, "avg_logprob": -0.0870299194798325, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.004719865042716265}, {"id": 7, "seek": 2810, "start": 35.0, "end": 39.2, "text": " A on ma, przypomnijmy, 280 miliard\u00f3w parametr\u00f3w.", "tokens": [50709, 316, 322, 463, 11, 41780, 38131, 1718, 2226, 11, 41229, 1962, 72, 515, 3901, 6220, 27965, 3901, 13, 50919], "temperature": 0.0, "avg_logprob": -0.0870299194798325, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.004719865042716265}, {"id": 8, "seek": 2810, "start": 39.2, "end": 44.5, "text": " Na celem tej naszej analizy jest zrozumienie, co si\u0119 dzieje, gdy model osi\u0105ga tak\u0105 skal\u0119.", "tokens": [50919, 6056, 1769, 10386, 12573, 42946, 2624, 590, 88, 3492, 710, 27857, 449, 27385, 11, 598, 3244, 17953, 2884, 11, 28405, 2316, 3003, 11404, 3680, 31069, 16890, 1274, 13, 51184], "temperature": 0.0, "avg_logprob": -0.0870299194798325, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.004719865042716265}, {"id": 9, "seek": 2810, "start": 44.5, "end": 49.900000000000006, "text": " No w\u0142a\u015bnie. I to nie jest prosta historia o tym, \u017ce wi\u0119kszy znaczy lepszy.", "tokens": [51184, 883, 14234, 13, 286, 281, 2838, 3492, 582, 8638, 18385, 277, 8107, 11, 3561, 29968, 1229, 36584, 476, 1878, 1229, 13, 51454], "temperature": 0.0, "avg_logprob": -0.0870299194798325, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.004719865042716265}, {"id": 10, "seek": 2810, "start": 49.900000000000006, "end": 52.8, "text": " Autorzy zadali sobie fundamentalne pytanie.", "tokens": [51454, 6049, 284, 1229, 42788, 5103, 13652, 8088, 716, 36610, 13, 51599], "temperature": 0.0, "avg_logprob": -0.0870299194798325, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.004719865042716265}, {"id": 11, "seek": 2810, "start": 52.8, "end": 55.7, "text": " Jakie nowe zdolno\u015bci si\u0119 pojawiaj\u0105?", "tokens": [51599, 15029, 414, 586, 68, 16221, 401, 16438, 3244, 30655, 48125, 30, 51744], "temperature": 0.0, "avg_logprob": -0.0870299194798325, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.004719865042716265}, {"id": 12, "seek": 5570, "start": 55.800000000000004, "end": 58.800000000000004, "text": " Gdzie model, hmm, udarza w \u015bcian\u0119.", "tokens": [50369, 460, 13096, 2316, 11, 16478, 11, 344, 20327, 2394, 261, 220, 6199, 282, 1274, 13, 50519], "temperature": 0.0, "avg_logprob": -0.11270679871728816, "compression_ratio": 1.4107142857142858, "no_speech_prob": 0.003869570093229413}, {"id": 13, "seek": 5570, "start": 58.800000000000004, "end": 62.0, "text": " I co to wszystko m\u00f3wi nam o przysz\u0142o\u015bci sztucznej inteligencji?", "tokens": [50519, 286, 598, 281, 22607, 24592, 8835, 277, 44018, 35059, 262, 2682, 1311, 89, 11794, 24777, 3213, 19649, 30, 50679], "temperature": 0.0, "avg_logprob": -0.11270679871728816, "compression_ratio": 1.4107142857142858, "no_speech_prob": 0.003869570093229413}, {"id": 14, "seek": 5570, "start": 62.0, "end": 63.1, "text": " W\u0142a\u015bnie.", "tokens": [50679, 343, 5024, 12221, 13, 50734], "temperature": 0.0, "avg_logprob": -0.11270679871728816, "compression_ratio": 1.4107142857142858, "no_speech_prob": 0.003869570093229413}, {"id": 15, "seek": 5570, "start": 63.1, "end": 70.3, "text": " A poniewa\u017c opieramy si\u0119 na pracy badawczej, mamy dost\u0119p do brutalnie szczerych danych, a nie, wiesz, marketingowych obietnic.", "tokens": [50734, 316, 32426, 999, 811, 7804, 3244, 1667, 35591, 272, 1538, 86, 9680, 73, 11, 17335, 48209, 360, 17878, 2766, 22090, 2109, 339, 274, 34644, 11, 257, 2838, 11, 261, 15347, 11, 6370, 19605, 1111, 1684, 7692, 13, 51094], "temperature": 0.0, "avg_logprob": -0.11270679871728816, "compression_ratio": 1.4107142857142858, "no_speech_prob": 0.003869570093229413}, {"id": 16, "seek": 5570, "start": 70.3, "end": 79.5, "text": " Ok, czyli misja na dzi\u015b. Zrozumie\u0107, czy zbudowali\u015bmy cyfrowego geniusze, czy po prostu, no, najwi\u0119ksz\u0105 na \u015bwiecie papug\u0119.", "tokens": [51094, 3477, 11, 16591, 3346, 2938, 1667, 31981, 1788, 13, 1176, 27857, 449, 414, 2162, 11, 6430, 710, 18281, 305, 33955, 3185, 69, 1892, 6308, 14017, 1381, 11, 6430, 714, 19518, 11, 572, 11, 48636, 1694, 8925, 1667, 40078, 4260, 5806, 697, 1274, 13, 51554], "temperature": 0.0, "avg_logprob": -0.11270679871728816, "compression_ratio": 1.4107142857142858, "no_speech_prob": 0.003869570093229413}, {"id": 17, "seek": 5570, "start": 79.5, "end": 82.5, "text": " Gopher jest oparty na architekturze Transformer.", "tokens": [51554, 460, 16754, 3492, 999, 446, 88, 1667, 3912, 642, 2320, 374, 1381, 27938, 260, 13, 51704], "temperature": 0.0, "avg_logprob": -0.11270679871728816, "compression_ratio": 1.4107142857142858, "no_speech_prob": 0.003869570093229413}, {"id": 18, "seek": 5570, "start": 82.5, "end": 85.2, "text": " Czyli tej samej, co wi\u0119kszo\u015b\u0107 znanych nam modeli.", "tokens": [51704, 37099, 12573, 912, 73, 11, 598, 29968, 4765, 7753, 15397, 34644, 8835, 2316, 72, 13, 51839], "temperature": 0.0, "avg_logprob": -0.11270679871728816, "compression_ratio": 1.4107142857142858, "no_speech_prob": 0.003869570093229413}, {"id": 19, "seek": 8520, "start": 85.2, "end": 89.60000000000001, "text": " Jego zadaniem jest w zasadzie przewidywanie nast\u0119pnego s\u0142owa w zdaniu.", "tokens": [50364, 508, 6308, 710, 11338, 4907, 3492, 261, 44585, 3283, 39758, 327, 27112, 7155, 39662, 11858, 15116, 5528, 261, 16221, 25849, 13, 50584], "temperature": 0.0, "avg_logprob": -0.07901477112489588, "compression_ratio": 1.3601398601398602, "no_speech_prob": 0.002847632858902216}, {"id": 20, "seek": 8520, "start": 89.60000000000001, "end": 92.3, "text": " Ale przecie\u017c nie jest pierwszym gigantem.", "tokens": [50584, 9366, 8325, 40082, 2838, 3492, 34016, 76, 8741, 394, 443, 13, 50719], "temperature": 0.0, "avg_logprob": -0.07901477112489588, "compression_ratio": 1.3601398601398602, "no_speech_prob": 0.002847632858902216}, {"id": 21, "seek": 8520, "start": 92.3, "end": 97.60000000000001, "text": " Mieli\u015bmy GPT-3, Jurassic One, Megatron, Turing, NLG.", "tokens": [50719, 376, 23099, 10513, 26039, 51, 12, 18, 11, 44730, 1485, 11, 9986, 267, 2044, 11, 314, 1345, 11, 426, 43, 38, 13, 50984], "temperature": 0.0, "avg_logprob": -0.07901477112489588, "compression_ratio": 1.3601398601398602, "no_speech_prob": 0.002847632858902216}, {"id": 22, "seek": 8520, "start": 97.60000000000001, "end": 99.4, "text": " Czym on si\u0119 tak naprawd\u0119 wyr\u00f3\u017cnia?", "tokens": [50984, 19832, 76, 322, 3244, 991, 20970, 4628, 11721, 1427, 12679, 30, 51074], "temperature": 0.0, "avg_logprob": -0.07901477112489588, "compression_ratio": 1.3601398601398602, "no_speech_prob": 0.002847632858902216}, {"id": 23, "seek": 8520, "start": 99.4, "end": 107.9, "text": " I to jest \u015bwietne pytanie, bo artyku\u0142 pozycjonuje gofera nie jako kolejnego wi\u0119kszego, tylko jako precyzyjne narz\u0119dzie badawcze.", "tokens": [51074, 286, 281, 3492, 8299, 39083, 716, 36610, 11, 748, 594, 874, 5279, 1221, 49358, 45677, 13008, 352, 45635, 2838, 17123, 23749, 11858, 29968, 27725, 11, 13219, 17123, 659, 1344, 1229, 73, 716, 6714, 89, 42643, 272, 1538, 86, 9680, 13, 51499], "temperature": 0.0, "avg_logprob": -0.07901477112489588, "compression_ratio": 1.3601398601398602, "no_speech_prob": 0.002847632858902216}, {"id": 24, "seek": 8520, "start": 107.9, "end": 110.10000000000001, "text": " W sensie, \u017ce stworzyli ca\u0142\u0105 rodzin\u0119 modeli?", "tokens": [51499, 343, 2923, 414, 11, 3561, 342, 28321, 1229, 2081, 1335, 15926, 8685, 23584, 1274, 2316, 72, 30, 51609], "temperature": 0.0, "avg_logprob": -0.07901477112489588, "compression_ratio": 1.3601398601398602, "no_speech_prob": 0.002847632858902216}, {"id": 25, "seek": 11010, "start": 110.1, "end": 119.69999999999999, "text": " Dok\u0142adnie. Od takich maluch\u00f3w z kilkudziesi\u0119cioma milionami parametr\u00f3w, a\u017c po tego 280 miliardowego kolosa.", "tokens": [50364, 29768, 10358, 2766, 13, 12210, 29607, 2806, 625, 3901, 710, 5128, 74, 532, 89, 530, 5034, 537, 6440, 1962, 313, 4526, 6220, 27965, 3901, 11, 48134, 714, 8627, 41229, 1962, 72, 515, 26576, 17818, 6447, 13, 50844], "temperature": 0.0, "avg_logprob": -0.09901752792486623, "compression_ratio": 1.3888888888888888, "no_speech_prob": 0.002872506156563759}, {"id": 26, "seek": 11010, "start": 119.69999999999999, "end": 130.2, "text": " I zrobili to po to, by precyzyjnie zmierzy\u0107, w kt\u00f3rym momencie i w jakich zadaniach sama skala zaczyna by\u0107, no, game changerem.", "tokens": [50844, 286, 44399, 2312, 281, 714, 281, 11, 538, 659, 1344, 1229, 73, 2766, 17020, 811, 27150, 11, 261, 30120, 40883, 741, 261, 4207, 480, 42788, 3782, 608, 17768, 1110, 5159, 43811, 629, 15069, 11, 572, 11, 1216, 22822, 443, 13, 51369], "temperature": 0.0, "avg_logprob": -0.09901752792486623, "compression_ratio": 1.3888888888888888, "no_speech_prob": 0.002872506156563759}, {"id": 27, "seek": 11010, "start": 130.2, "end": 136.29999999999998, "text": " Czyli pytanie nie brzmi, czy jest lepszy, ale gdzie, dlaczego i jakim kosztem staje si\u0119 lepszy?", "tokens": [51369, 37099, 36610, 2838, 738, 89, 3057, 11, 6430, 3492, 476, 1878, 1229, 11, 6775, 18922, 11, 37873, 39329, 741, 49410, 19532, 2682, 443, 342, 11153, 3244, 476, 1878, 1229, 30, 51674], "temperature": 0.0, "avg_logprob": -0.09901752792486623, "compression_ratio": 1.3888888888888888, "no_speech_prob": 0.002872506156563759}, {"id": 28, "seek": 11010, "start": 136.29999999999998, "end": 137.29999999999998, "text": " Tak jest.", "tokens": [51674, 9118, 3492, 13, 51724], "temperature": 0.0, "avg_logprob": -0.09901752792486623, "compression_ratio": 1.3888888888888888, "no_speech_prob": 0.002872506156563759}, {"id": 29, "seek": 13730, "start": 137.3, "end": 144.70000000000002, "text": " No dobrze. Ale sedno tego badania musi opieracie na jakie\u015b niewyobra\u017calne in\u017cynierii. Jak w og\u00f3le trenuje si\u0119 takiego potwora?", "tokens": [50364, 883, 28335, 13, 9366, 9643, 1771, 8627, 1578, 5609, 37587, 999, 811, 30805, 1667, 31163, 43622, 88, 24393, 1427, 304, 716, 294, 1427, 2534, 811, 5597, 13, 15029, 261, 29229, 23136, 13008, 3244, 32296, 1847, 86, 3252, 30, 50734], "temperature": 0.0, "avg_logprob": -0.11478327868277566, "compression_ratio": 1.3319148936170213, "no_speech_prob": 0.008451259694993496}, {"id": 30, "seek": 13730, "start": 144.70000000000002, "end": 148.8, "text": " Oj, tak. To jest gigantyczne wyzwanie.", "tokens": [50734, 47100, 11, 991, 13, 1407, 3492, 8741, 394, 17466, 716, 4628, 14406, 7155, 13, 50939], "temperature": 0.0, "avg_logprob": -0.11478327868277566, "compression_ratio": 1.3319148936170213, "no_speech_prob": 0.008451259694993496}, {"id": 31, "seek": 13730, "start": 148.8, "end": 155.70000000000002, "text": " W artykule jest mowa, \u017ce same parametry i stan optymalizatora gofera zajmuj\u0105 2,5 terabajta.", "tokens": [50939, 343, 594, 874, 74, 2271, 3492, 275, 5528, 11, 3561, 912, 6220, 9889, 741, 27984, 2427, 4199, 304, 590, 1639, 64, 352, 45635, 33729, 76, 13263, 568, 11, 20, 1796, 455, 1805, 1328, 13, 51284], "temperature": 0.0, "avg_logprob": -0.11478327868277566, "compression_ratio": 1.3319148936170213, "no_speech_prob": 0.008451259694993496}, {"id": 32, "seek": 13730, "start": 155.70000000000002, "end": 158.8, "text": " Przeci\u0119tny dysk w laptopie ma mo\u017ce 1 terabajt.", "tokens": [51284, 2114, 1381, 537, 46788, 1634, 15243, 74, 261, 10732, 414, 463, 12034, 502, 1796, 455, 1805, 83, 13, 51439], "temperature": 0.0, "avg_logprob": -0.11478327868277566, "compression_ratio": 1.3319148936170213, "no_speech_prob": 0.008451259694993496}, {"id": 33, "seek": 15880, "start": 158.8, "end": 169.70000000000002, "text": " A pojedynczy rdze\u0144 ich uk\u0142adu TPU V3 ma tylko 16 GB pami\u0119ci. To jest, jak sama zauwa\u017cy\u0142a\u015b, pr\u00f3ba wlania oceanu do szklanki.", "tokens": [50364, 316, 714, 40543, 2534, 6522, 367, 67, 49689, 1893, 344, 15317, 84, 314, 8115, 691, 18, 463, 13219, 3165, 26809, 31088, 537, 13, 1407, 3492, 11, 4207, 17768, 710, 1459, 4151, 7735, 5024, 1788, 11, 8565, 4231, 261, 75, 5609, 7810, 84, 360, 7870, 7837, 27203, 13, 50909], "temperature": 0.0, "avg_logprob": -0.1078700411851239, "compression_ratio": 1.3758169934640523, "no_speech_prob": 0.09307607263326645}, {"id": 34, "seek": 15880, "start": 169.70000000000002, "end": 170.9, "text": " I jak to zrobili?", "tokens": [50909, 286, 4207, 281, 44399, 2312, 30, 50969], "temperature": 0.0, "avg_logprob": -0.1078700411851239, "compression_ratio": 1.3758169934640523, "no_speech_prob": 0.09307607263326645}, {"id": 35, "seek": 15880, "start": 170.9, "end": 177.60000000000002, "text": " Fizyczne ograniczenia pami\u0119ci. I zastosowano ca\u0142y arsena\u0142 sztuczek. Na przyk\u0142ad model paralelizm.", "tokens": [50969, 479, 590, 17466, 716, 34416, 30732, 14320, 31088, 537, 13, 286, 36746, 329, 305, 3730, 35226, 594, 82, 4118, 1221, 262, 2682, 1311, 19878, 13, 6056, 23144, 2316, 26009, 338, 590, 76, 13, 51304], "temperature": 0.0, "avg_logprob": -0.1078700411851239, "compression_ratio": 1.3758169934640523, "no_speech_prob": 0.09307607263326645}, {"id": 36, "seek": 15880, "start": 177.60000000000002, "end": 179.3, "text": " Co to znaczy w praktyce?", "tokens": [51304, 3066, 281, 36584, 261, 3206, 74, 874, 384, 30, 51389], "temperature": 0.0, "avg_logprob": -0.1078700411851239, "compression_ratio": 1.3758169934640523, "no_speech_prob": 0.09307607263326645}, {"id": 37, "seek": 15880, "start": 179.3, "end": 185.5, "text": " Wyobra\u017a sobie linie produkcyjn\u0105 w fabryce samochod\u00f3w. Jeden robot nie buduje ca\u0142ego auta, prawda?", "tokens": [51389, 14458, 24393, 10659, 13652, 22896, 414, 33699, 42949, 13113, 261, 5355, 627, 384, 3247, 8997, 378, 3901, 13, 508, 6876, 7881, 2838, 3265, 13008, 35224, 6308, 1476, 64, 11, 43607, 30, 51699], "temperature": 0.0, "avg_logprob": -0.1078700411851239, "compression_ratio": 1.3758169934640523, "no_speech_prob": 0.09307607263326645}, {"id": 38, "seek": 15880, "start": 185.5, "end": 188.70000000000002, "text": " No jasne. Jeden montuje drwi, drugi silnik.", "tokens": [51699, 883, 361, 296, 716, 13, 508, 6876, 8143, 13008, 1224, 6253, 11, 4110, 72, 3425, 13123, 13, 51859], "temperature": 0.0, "avg_logprob": -0.1078700411851239, "compression_ratio": 1.3758169934640523, "no_speech_prob": 0.09307607263326645}, {"id": 39, "seek": 18870, "start": 188.7, "end": 196.6, "text": " Dok\u0142adnie. Tutaj jest podobnie. Model jest tak du\u017cy, \u017ce tnie si\u0119 go na kawa\u0142ki i ka\u017cdy kawa\u0142ek mieszka na innym procesorze.", "tokens": [50364, 29768, 10358, 2766, 13, 41819, 3492, 43024, 2766, 13, 17105, 3492, 991, 1581, 7735, 11, 3561, 256, 2766, 3244, 352, 1667, 350, 10449, 1221, 2984, 741, 31615, 350, 10449, 1221, 916, 33039, 2330, 1667, 294, 12996, 17565, 284, 1381, 13, 50759], "temperature": 0.0, "avg_logprob": -0.08662237439836774, "compression_ratio": 1.4879725085910653, "no_speech_prob": 0.0006819625850766897}, {"id": 40, "seek": 18870, "start": 196.6, "end": 201.39999999999998, "text": " Komunikuj\u0105 si\u0119 ze sob\u0105, ale \u017caden pojedynczy uk\u0142ad nie musi mie\u015bci\u0107 ca\u0142o\u015bci.", "tokens": [50759, 14286, 409, 1035, 13263, 3244, 5277, 18253, 1611, 11, 6775, 19625, 14771, 714, 40543, 2534, 6522, 344, 15317, 2838, 37587, 12597, 6199, 2162, 1335, 35059, 13, 50999], "temperature": 0.0, "avg_logprob": -0.08662237439836774, "compression_ratio": 1.4879725085910653, "no_speech_prob": 0.0006819625850766897}, {"id": 41, "seek": 18870, "start": 201.39999999999998, "end": 209.1, "text": " Okej. To ma sens. Ale w artykule jest te\u017c mowa o czym\u015b, co nazywa si\u0119 rematerializacen. To ju\u017c brzmi bardziej tajemniczo.", "tokens": [50999, 29094, 73, 13, 1407, 463, 2923, 13, 9366, 261, 594, 874, 74, 2271, 3492, 9516, 275, 5528, 277, 31466, 1788, 11, 598, 20151, 88, 4151, 3244, 890, 40364, 590, 326, 268, 13, 1407, 10678, 738, 89, 3057, 27209, 256, 1805, 443, 7692, 4765, 13, 51384], "temperature": 0.0, "avg_logprob": -0.08662237439836774, "compression_ratio": 1.4879725085910653, "no_speech_prob": 0.0006819625850766897}, {"id": 42, "seek": 18870, "start": 209.1, "end": 214.29999999999998, "text": " Bo to jest jeszcze sprytniejsze. Wyobra\u017a sobie, \u017ce pieczesz bardzo skomplikowane ciasto.", "tokens": [51384, 3286, 281, 3492, 14168, 637, 627, 83, 44258, 13, 14458, 24393, 10659, 13652, 11, 3561, 1730, 3689, 10430, 9034, 1110, 298, 564, 1035, 23066, 6983, 33869, 13, 51644], "temperature": 0.0, "avg_logprob": -0.08662237439836774, "compression_ratio": 1.4879725085910653, "no_speech_prob": 0.0006819625850766897}, {"id": 43, "seek": 21430, "start": 214.3, "end": 220.10000000000002, "text": " Zamiast przygotowywa\u0107 wszystkie kremy i biszkopty naraz i trzyma\u0107 je na blacie, co zaj\u0119\u0142oby mn\u00f3stwo miejsca...", "tokens": [50364, 1176, 4526, 525, 35914, 10089, 25234, 31723, 350, 2579, 88, 741, 7393, 30154, 404, 874, 6714, 921, 741, 34573, 1696, 2162, 1506, 1667, 888, 30805, 11, 598, 33729, 1274, 1221, 13944, 275, 77, 45052, 6120, 18522, 44239, 485, 50654], "temperature": 0.0, "avg_logprob": -0.08715931080883334, "compression_ratio": 1.4238805970149253, "no_speech_prob": 0.05110834538936615}, {"id": 44, "seek": 21430, "start": 220.10000000000002, "end": 224.4, "text": " Za ka\u017cdym razem, gdy potrzebuje kremu, robi\u0119 go od nowa.", "tokens": [50654, 31440, 31615, 76, 40225, 11, 28405, 28577, 6021, 2884, 350, 2579, 84, 11, 3870, 5034, 352, 3611, 586, 64, 13, 50869], "temperature": 0.0, "avg_logprob": -0.08715931080883334, "compression_ratio": 1.4238805970149253, "no_speech_prob": 0.05110834538936615}, {"id": 45, "seek": 21430, "start": 224.4, "end": 231.0, "text": " W\u0142a\u015bnie. To troch\u0119 w\u00f3lniejsze, ale pozwala zaoszcz\u0119dzi\u0107 ogromn\u0105 ilo\u015b\u0107 pami\u0119ci na kuchennym blacie.", "tokens": [50869, 343, 5024, 12221, 13, 1407, 24926, 261, 15741, 44258, 11, 6775, 40557, 5159, 7949, 329, 43771, 6298, 28496, 34416, 298, 13113, 1930, 78, 7753, 31088, 537, 1667, 350, 11285, 12996, 888, 30805, 13, 51199], "temperature": 0.0, "avg_logprob": -0.08715931080883334, "compression_ratio": 1.4238805970149253, "no_speech_prob": 0.05110834538936615}, {"id": 46, "seek": 21430, "start": 231.0, "end": 234.5, "text": " Oni robili dok\u0142adnie to samo z wynikami po\u015brednich oblicze\u0144.", "tokens": [51199, 1282, 72, 3870, 2312, 45864, 2766, 281, 36422, 710, 31936, 1035, 4526, 714, 1788, 986, 77, 480, 1111, 1050, 49689, 13, 51374], "temperature": 0.0, "avg_logprob": -0.08715931080883334, "compression_ratio": 1.4238805970149253, "no_speech_prob": 0.05110834538936615}, {"id": 47, "seek": 21430, "start": 234.5, "end": 243.3, "text": " Niesamowite. Czyli to nie jest jeden superkomputer, tylko ca\u0142a precyzyjnie synchronizowana orkiestra tysi\u0119cy ma\u0142ych jednostek.", "tokens": [51374, 426, 530, 335, 305, 642, 13, 37099, 281, 2838, 3492, 12906, 1687, 20557, 13849, 11, 13219, 1335, 5024, 659, 1344, 1229, 73, 2766, 19331, 590, 40458, 420, 2984, 32036, 38156, 47303, 463, 47655, 5232, 36414, 916, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08715931080883334, "compression_ratio": 1.4238805970149253, "no_speech_prob": 0.05110834538936615}, {"id": 48, "seek": 24330, "start": 243.3, "end": 250.3, "text": " Kt\u00f3re bez przerwy ze sob\u0105 rozmawiaj\u0105, dziel\u0105 zadania i odtwarzaj\u0105 obliczenia, \u017ceby nie zapcha\u0107 sobie pami\u0119ci.", "tokens": [50364, 591, 4547, 265, 10782, 582, 4527, 9726, 5277, 18253, 1611, 35234, 34953, 8555, 11, 9758, 1187, 1611, 42788, 5609, 741, 3611, 83, 31991, 11133, 1111, 1050, 14320, 11, 11316, 2838, 14223, 4413, 2162, 13652, 31088, 537, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12051981033817415, "compression_ratio": 1.3758389261744965, "no_speech_prob": 0.141379252076149}, {"id": 49, "seek": 24330, "start": 250.3, "end": 254.8, "text": " A czym te orkiestry nakarmiono? Jakie paliwo nap\u0119dza Gofer'a?", "tokens": [50714, 316, 31466, 535, 420, 2984, 47433, 20332, 4452, 49020, 30, 15029, 414, 3984, 72, 6120, 9296, 6298, 2394, 1037, 612, 6, 64, 30, 50939], "temperature": 0.0, "avg_logprob": -0.12051981033817415, "compression_ratio": 1.3758389261744965, "no_speech_prob": 0.141379252076149}, {"id": 50, "seek": 24330, "start": 254.8, "end": 260.8, "text": " Paliwo to zbi\u00f3r danych o nazwie Masyw Text. M\u00f3wimy tu o 15 terabajtach tekstu.", "tokens": [50939, 6116, 72, 6120, 281, 710, 5614, 15614, 274, 34644, 277, 20151, 8699, 5224, 27112, 18643, 13, 376, 3901, 13189, 2604, 277, 2119, 1796, 455, 1805, 83, 608, 16624, 372, 84, 13, 51239], "temperature": 0.0, "avg_logprob": -0.12051981033817415, "compression_ratio": 1.3758389261744965, "no_speech_prob": 0.141379252076149}, {"id": 51, "seek": 24330, "start": 260.8, "end": 262.3, "text": " 15 terabajt\u00f3w?", "tokens": [51239, 2119, 1796, 455, 1805, 83, 3901, 30, 51314], "temperature": 0.0, "avg_logprob": -0.12051981033817415, "compression_ratio": 1.3758389261744965, "no_speech_prob": 0.141379252076149}, {"id": 52, "seek": 24330, "start": 262.3, "end": 266.0, "text": " To jest absolutnie gigantyczna biblioteka cyfrowa.", "tokens": [51314, 1407, 3492, 18757, 2766, 8741, 394, 17466, 629, 34344, 310, 36361, 3185, 69, 1892, 64, 13, 51499], "temperature": 0.0, "avg_logprob": -0.12051981033817415, "compression_ratio": 1.3758389261744965, "no_speech_prob": 0.141379252076149}, {"id": 53, "seek": 24330, "start": 266.0, "end": 271.3, "text": " Zawiera strony internetowe, ksi\u0105\u017cki, artyku\u0142y z wiadomo\u015bciami, kod z Githaba.", "tokens": [51499, 1176, 1607, 10609, 32406, 4705, 6880, 11, 39311, 2984, 11, 594, 874, 5279, 6825, 710, 26393, 40633, 6199, 4526, 11, 350, 378, 710, 460, 355, 5509, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12051981033817415, "compression_ratio": 1.3758389261744965, "no_speech_prob": 0.141379252076149}, {"id": 54, "seek": 27130, "start": 271.3, "end": 277.8, "text": " I tu jest co\u015b, co przyku\u0142o moj\u0105 uwag\u0119. Ksi\u0105\u017cki stanowi\u0105 a\u017c 27% danych treningowych.", "tokens": [50364, 286, 2604, 3492, 19241, 11, 598, 6501, 5279, 5249, 705, 8555, 43696, 13, 591, 7691, 27242, 2984, 27984, 47886, 48134, 7634, 4, 274, 34644, 2192, 773, 19605, 13, 50689], "temperature": 0.0, "avg_logprob": -0.06629403169490089, "compression_ratio": 1.3368794326241136, "no_speech_prob": 0.12420021742582321}, {"id": 55, "seek": 27130, "start": 277.8, "end": 279.3, "text": " Tak to sporo.", "tokens": [50689, 9118, 281, 637, 10780, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06629403169490089, "compression_ratio": 1.3368794326241136, "no_speech_prob": 0.12420021742582321}, {"id": 56, "seek": 27130, "start": 279.3, "end": 284.8, "text": " W przypadku s\u0142ynnego GPT-3 by\u0142o to zdaje si\u0119 tylko 16%.", "tokens": [50764, 343, 41955, 15116, 2534, 11858, 26039, 51, 12, 18, 14811, 281, 16221, 11153, 3244, 13219, 3165, 6856, 51039], "temperature": 0.0, "avg_logprob": -0.06629403169490089, "compression_ratio": 1.3368794326241136, "no_speech_prob": 0.12420021742582321}, {"id": 57, "seek": 27130, "start": 284.8, "end": 291.3, "text": " Czy to naprawd\u0119 robi a\u017c tak\u0105 r\u00f3\u017cnic\u0119? Domy\u015blam si\u0119, \u017ce j\u0119zyk w ksi\u0105\u017ckach jest generalnie bardziej uporz\u0105dkowany.", "tokens": [51039, 19832, 281, 20970, 47380, 48134, 31069, 19637, 7692, 1274, 30, 413, 8488, 1788, 4326, 3244, 11, 3561, 49055, 74, 261, 39311, 41326, 3492, 2674, 2766, 27209, 493, 284, 23876, 74, 23341, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06629403169490089, "compression_ratio": 1.3368794326241136, "no_speech_prob": 0.12420021742582321}, {"id": 58, "seek": 27130, "start": 291.3, "end": 296.8, "text": " Autorzy sugeruj\u0105, \u017ce tak, \u017ce to mo\u017ce by\u0107 jeden z powod\u00f3w jego lepszych wynik\u00f3w.", "tokens": [51364, 6049, 284, 1229, 459, 1321, 13263, 11, 3561, 991, 11, 3561, 281, 12034, 15069, 12906, 710, 3388, 378, 3901, 26542, 476, 1878, 28051, 31936, 1035, 3901, 13, 51639], "temperature": 0.0, "avg_logprob": -0.06629403169490089, "compression_ratio": 1.3368794326241136, "no_speech_prob": 0.12420021742582321}, {"id": 59, "seek": 29680, "start": 296.8, "end": 301.8, "text": " Ksi\u0105\u017cki oferuj\u0105 d\u0142ugie, sp\u00f3jne narracje, bogatsze s\u0142ownictwo.", "tokens": [50364, 591, 7691, 27242, 2984, 295, 260, 13263, 274, 34077, 414, 11, 637, 18999, 716, 6397, 29293, 11, 26132, 1720, 1381, 15116, 648, 985, 6120, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07803498460946838, "compression_ratio": 1.433139534883721, "no_speech_prob": 0.0505635067820549}, {"id": 60, "seek": 29680, "start": 301.8, "end": 305.3, "text": " To mo\u017ce uczy\u0107 model bardziej z\u0142o\u017conych zale\u017cno\u015bci w j\u0119zyku.", "tokens": [50614, 1407, 12034, 344, 33967, 2316, 27209, 710, 5249, 1427, 2526, 339, 710, 45494, 16438, 261, 49055, 5279, 13, 50789], "temperature": 0.0, "avg_logprob": -0.07803498460946838, "compression_ratio": 1.433139534883721, "no_speech_prob": 0.0505635067820549}, {"id": 61, "seek": 29680, "start": 305.3, "end": 307.3, "text": " A co z filtrowaniem tych danych?", "tokens": [50789, 316, 598, 710, 29148, 1892, 282, 4907, 15180, 274, 34644, 30, 50889], "temperature": 0.0, "avg_logprob": -0.07803498460946838, "compression_ratio": 1.433139534883721, "no_speech_prob": 0.0505635067820549}, {"id": 62, "seek": 29680, "start": 307.3, "end": 308.8, "text": " To jest r\u00f3wnie ciekawe.", "tokens": [50889, 1407, 3492, 11416, 14215, 30596, 2330, 826, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07803498460946838, "compression_ratio": 1.433139534883721, "no_speech_prob": 0.0505635067820549}, {"id": 63, "seek": 29680, "start": 308.8, "end": 314.8, "text": " Zamiast budowa\u0107 jaki\u015b skomplikowany klasyfikator, kt\u00f3ry mia\u0142by ocenia\u0107 jako\u015b\u0107 tekstu,", "tokens": [50964, 1176, 4526, 525, 3265, 11445, 34721, 1110, 298, 564, 1035, 23341, 9671, 5871, 31230, 1639, 11, 9913, 27989, 2322, 10409, 268, 654, 2162, 17123, 7753, 16624, 372, 84, 11, 51264], "temperature": 0.0, "avg_logprob": -0.07803498460946838, "compression_ratio": 1.433139534883721, "no_speech_prob": 0.0505635067820549}, {"id": 64, "seek": 29680, "start": 314.8, "end": 316.8, "text": " u\u017cyli bardzo prostych heurystyk.", "tokens": [51264, 34097, 2081, 9034, 10293, 16384, 415, 2598, 25134, 74, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07803498460946838, "compression_ratio": 1.433139534883721, "no_speech_prob": 0.0505635067820549}, {"id": 65, "seek": 29680, "start": 316.8, "end": 317.8, "text": " Czyli?", "tokens": [51364, 37099, 30, 51414], "temperature": 0.0, "avg_logprob": -0.07803498460946838, "compression_ratio": 1.433139534883721, "no_speech_prob": 0.0505635067820549}, {"id": 66, "seek": 29680, "start": 317.8, "end": 318.8, "text": " Wyrzucali \u015bmieci.", "tokens": [51414, 343, 6016, 89, 1311, 5103, 8299, 25210, 537, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07803498460946838, "compression_ratio": 1.433139534883721, "no_speech_prob": 0.0505635067820549}, {"id": 67, "seek": 29680, "start": 318.8, "end": 319.8, "text": " Mniej wi\u0119cej.", "tokens": [51464, 376, 10402, 26004, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07803498460946838, "compression_ratio": 1.433139534883721, "no_speech_prob": 0.0505635067820549}, {"id": 68, "seek": 29680, "start": 319.8, "end": 324.8, "text": " Usuwali teksty, kt\u00f3re by\u0142y za kr\u00f3tkie, mia\u0142y za du\u017co powt\u00f3rze\u0144, albo dziwn\u0105, \u015bredni\u0105 d\u0142ugo\u015b\u0107 s\u0142owa.", "tokens": [51514, 4958, 36824, 5103, 16624, 25134, 11, 8864, 26366, 7949, 42366, 83, 22872, 11, 21290, 6825, 7949, 26673, 3388, 4547, 13503, 5248, 11, 22622, 31981, 895, 1611, 11, 8299, 986, 3722, 1611, 44042, 20746, 7753, 15116, 5528, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07803498460946838, "compression_ratio": 1.433139534883721, "no_speech_prob": 0.0505635067820549}, {"id": 69, "seek": 29680, "start": 324.8, "end": 326.3, "text": " I to by\u0142o celowe?", "tokens": [51764, 286, 281, 14811, 9277, 6880, 30, 51839], "temperature": 0.0, "avg_logprob": -0.07803498460946838, "compression_ratio": 1.433139534883721, "no_speech_prob": 0.0505635067820549}, {"id": 70, "seek": 32630, "start": 326.3, "end": 327.3, "text": " Tak.", "tokens": [50364, 9118, 13, 50414], "temperature": 0.0, "avg_logprob": -0.061558778469379134, "compression_ratio": 1.4060402684563758, "no_speech_prob": 0.003992399666458368}, {"id": 71, "seek": 32630, "start": 327.3, "end": 334.3, "text": " Chcieli unikn\u0105\u0107 sytuacji, w kt\u00f3rej zaawansowany filtr, nauczony na z\u0142otym standardzie tekst\u00f3w w stylu wikipedii,", "tokens": [50414, 761, 537, 10148, 517, 1035, 13113, 2162, 28275, 13152, 11, 261, 36023, 7949, 1607, 599, 23341, 1387, 6903, 11, 49103, 44479, 1667, 31614, 310, 4199, 3832, 3283, 16624, 372, 3901, 261, 7952, 2781, 261, 1035, 647, 292, 5597, 11, 50764], "temperature": 0.0, "avg_logprob": -0.061558778469379134, "compression_ratio": 1.4060402684563758, "no_speech_prob": 0.003992399666458368}, {"id": 72, "seek": 32630, "start": 334.3, "end": 338.3, "text": " nie\u015bwiadomie odrzuci\u0142by nietypowe, ale warto\u015bciowe formy wypowiedzi.", "tokens": [50764, 2838, 37750, 40120, 3611, 81, 11728, 537, 34635, 297, 4014, 79, 6880, 11, 6775, 31830, 6199, 6880, 1254, 88, 4628, 14701, 1091, 3992, 13, 50964], "temperature": 0.0, "avg_logprob": -0.061558778469379134, "compression_ratio": 1.4060402684563758, "no_speech_prob": 0.003992399666458368}, {"id": 73, "seek": 32630, "start": 338.3, "end": 341.3, "text": " Na przyk\u0142ad r\u00f3\u017cne dialekty czy style literackie.", "tokens": [50964, 6056, 23144, 47760, 5502, 916, 874, 6430, 7952, 306, 2733, 501, 414, 13, 51114], "temperature": 0.0, "avg_logprob": -0.061558778469379134, "compression_ratio": 1.4060402684563758, "no_speech_prob": 0.003992399666458368}, {"id": 74, "seek": 32630, "start": 341.3, "end": 342.3, "text": " Dok\u0142adnie.", "tokens": [51114, 29768, 10358, 2766, 13, 51164], "temperature": 0.0, "avg_logprob": -0.061558778469379134, "compression_ratio": 1.4060402684563758, "no_speech_prob": 0.003992399666458368}, {"id": 75, "seek": 32630, "start": 342.3, "end": 346.3, "text": " To by\u0142a \u015bwiadoma decyzja, \u017ceby zachowa\u0107 wi\u0119ksz\u0105 r\u00f3\u017cnorodno\u015b\u0107 danych.", "tokens": [51164, 1407, 23936, 21485, 345, 6440, 979, 37433, 2938, 11, 11316, 29303, 11445, 29968, 8925, 19637, 19048, 378, 23293, 274, 34644, 13, 51364], "temperature": 0.0, "avg_logprob": -0.061558778469379134, "compression_ratio": 1.4060402684563758, "no_speech_prob": 0.003992399666458368}, {"id": 76, "seek": 32630, "start": 346.3, "end": 347.3, "text": " Dobra.", "tokens": [51364, 413, 24393, 13, 51414], "temperature": 0.0, "avg_logprob": -0.061558778469379134, "compression_ratio": 1.4060402684563758, "no_speech_prob": 0.003992399666458368}, {"id": 77, "seek": 32630, "start": 347.3, "end": 349.3, "text": " Wiemy, jak go zbudowano i czym nakarmiono.", "tokens": [51414, 9233, 2226, 11, 4207, 352, 710, 18281, 305, 3730, 741, 31466, 20332, 4452, 49020, 13, 51514], "temperature": 0.0, "avg_logprob": -0.061558778469379134, "compression_ratio": 1.4060402684563758, "no_speech_prob": 0.003992399666458368}, {"id": 78, "seek": 32630, "start": 349.3, "end": 351.3, "text": " Czas na danie g\u0142\u00f3wne.", "tokens": [51514, 383, 24561, 1667, 3277, 414, 18117, 3901, 716, 13, 51614], "temperature": 0.0, "avg_logprob": -0.061558778469379134, "compression_ratio": 1.4060402684563758, "no_speech_prob": 0.003992399666458368}, {"id": 79, "seek": 32630, "start": 351.3, "end": 352.3, "text": " Wyniki.", "tokens": [51614, 343, 2534, 9850, 13, 51664], "temperature": 0.0, "avg_logprob": -0.061558778469379134, "compression_ratio": 1.4060402684563758, "no_speech_prob": 0.003992399666458368}, {"id": 80, "seek": 35230, "start": 352.3, "end": 357.3, "text": " Przegl\u0105da\u0142em ten artyku\u0142 i tam jest tabela, kt\u00f3ra ma chyba ze 150 r\u00f3\u017cnych zada\u0144.", "tokens": [50364, 2114, 89, 1146, 75, 26398, 11126, 2064, 594, 874, 5279, 1221, 741, 7677, 3492, 4421, 4053, 11, 19456, 463, 31532, 5277, 8451, 42602, 710, 1538, 5248, 13, 50614], "temperature": 0.0, "avg_logprob": -0.06734713580873278, "compression_ratio": 1.3228070175438595, "no_speech_prob": 0.31908079981803894}, {"id": 81, "seek": 35230, "start": 357.3, "end": 358.3, "text": " Mhm.", "tokens": [50614, 26272, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06734713580873278, "compression_ratio": 1.3228070175438595, "no_speech_prob": 0.31908079981803894}, {"id": 82, "seek": 35230, "start": 358.3, "end": 361.3, "text": " I jeden z tych wynik\u00f3w po prostu zwali\u0142 mnie znuk.", "tokens": [50664, 286, 12906, 710, 15180, 31936, 1035, 3901, 714, 19518, 11873, 5103, 1221, 17661, 15397, 2034, 13, 50814], "temperature": 0.0, "avg_logprob": -0.06734713580873278, "compression_ratio": 1.3228070175438595, "no_speech_prob": 0.31908079981803894}, {"id": 83, "seek": 35230, "start": 361.3, "end": 365.3, "text": " Chodzi o ten test czytania ze zrozumieniem dla licealist\u00f3w, RAC.", "tokens": [50814, 761, 14543, 277, 2064, 1500, 6430, 83, 5609, 5277, 710, 27857, 449, 1053, 4907, 12285, 287, 573, 304, 468, 3901, 11, 497, 4378, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06734713580873278, "compression_ratio": 1.3228070175438595, "no_speech_prob": 0.31908079981803894}, {"id": 84, "seek": 35230, "start": 365.3, "end": 367.3, "text": " Pami\u0119tasz, jaki tam by\u0142 wynik?", "tokens": [51014, 430, 23806, 83, 19601, 11, 24492, 7677, 16673, 31936, 1035, 30, 51114], "temperature": 0.0, "avg_logprob": -0.06734713580873278, "compression_ratio": 1.3228070175438595, "no_speech_prob": 0.31908079981803894}, {"id": 85, "seek": 35230, "start": 367.3, "end": 368.3, "text": " Oczywi\u015bcie.", "tokens": [51114, 42980, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06734713580873278, "compression_ratio": 1.3228070175438595, "no_speech_prob": 0.31908079981803894}, {"id": 86, "seek": 35230, "start": 368.3, "end": 371.3, "text": " To jest jeden z g\u0142\u00f3wnych bohater\u00f3w tej publikacji.", "tokens": [51164, 1407, 3492, 12906, 710, 18117, 812, 895, 16384, 748, 71, 771, 3901, 12573, 11227, 1035, 13152, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06734713580873278, "compression_ratio": 1.3228070175438595, "no_speech_prob": 0.31908079981803894}, {"id": 87, "seek": 35230, "start": 371.3, "end": 376.3, "text": " Gofer osi\u0105gn\u0105\u0142 tam 71,6% skuteczno\u015bci.", "tokens": [51314, 1037, 612, 3003, 11404, 4568, 1611, 1221, 7677, 30942, 11, 21, 4, 1110, 1169, 3689, 16438, 13, 51564], "temperature": 0.0, "avg_logprob": -0.06734713580873278, "compression_ratio": 1.3228070175438595, "no_speech_prob": 0.31908079981803894}, {"id": 88, "seek": 35230, "start": 376.3, "end": 377.3, "text": " A \u017ceby da\u0107 kontekst?", "tokens": [51564, 316, 11316, 1120, 2162, 14373, 916, 372, 30, 51614], "temperature": 0.0, "avg_logprob": -0.06734713580873278, "compression_ratio": 1.3228070175438595, "no_speech_prob": 0.31908079981803894}, {"id": 89, "seek": 37730, "start": 377.3, "end": 383.3, "text": " Inny pot\u0119\u017cny model, Megatron Turing, mia\u0142 w tym samym testie 47,9%.", "tokens": [50364, 682, 1634, 1847, 1274, 1427, 1634, 2316, 11, 9986, 267, 2044, 314, 1345, 11, 27989, 261, 8107, 3247, 4199, 1500, 414, 16953, 11, 24, 6856, 50664], "temperature": 0.0, "avg_logprob": -0.0654456194709329, "compression_ratio": 1.461290322580645, "no_speech_prob": 0.21122634410858154}, {"id": 90, "seek": 37730, "start": 383.3, "end": 386.3, "text": " To nie jest przyrost, to jest skok do innej ligi.", "tokens": [50664, 1407, 2838, 3492, 6501, 27494, 11, 281, 3492, 1110, 453, 360, 294, 11794, 287, 19789, 13, 50814], "temperature": 0.0, "avg_logprob": -0.0654456194709329, "compression_ratio": 1.461290322580645, "no_speech_prob": 0.21122634410858154}, {"id": 91, "seek": 37730, "start": 386.3, "end": 390.3, "text": " Wynik Gofera zbli\u017ca si\u0119 do poziomu osi\u0105ganego przez ludzi.", "tokens": [50814, 343, 2534, 1035, 1037, 45635, 710, 32117, 35075, 3244, 360, 38503, 298, 84, 3003, 11404, 1275, 6308, 14064, 29586, 13, 51014], "temperature": 0.0, "avg_logprob": -0.0654456194709329, "compression_ratio": 1.461290322580645, "no_speech_prob": 0.21122634410858154}, {"id": 92, "seek": 37730, "start": 390.3, "end": 393.3, "text": " I ta chyba idealnie pokazuje, o co chodzi w tym skalowaniu.", "tokens": [51014, 286, 1846, 31532, 7157, 2766, 13010, 43317, 11, 277, 598, 23998, 261, 8107, 16890, 305, 25849, 13, 51164], "temperature": 0.0, "avg_logprob": -0.0654456194709329, "compression_ratio": 1.461290322580645, "no_speech_prob": 0.21122634410858154}, {"id": 93, "seek": 37730, "start": 393.3, "end": 397.3, "text": " To nie jest tak, \u017ce model staje si\u0119 po prostu o kilka procent lepszy we wszystkim.", "tokens": [51164, 1407, 2838, 3492, 991, 11, 3561, 2316, 342, 11153, 3244, 714, 19518, 277, 36466, 38826, 476, 1878, 1229, 321, 30481, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0654456194709329, "compression_ratio": 1.461290322580645, "no_speech_prob": 0.21122634410858154}, {"id": 94, "seek": 37730, "start": 397.3, "end": 398.3, "text": " W\u0142a\u015bnie.", "tokens": [51364, 343, 5024, 12221, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0654456194709329, "compression_ratio": 1.461290322580645, "no_speech_prob": 0.21122634410858154}, {"id": 95, "seek": 37730, "start": 398.3, "end": 405.3, "text": " W pewnych obszarach przy odpowiedniej skali co\u015b klika i model nagle zyskuje zupe\u0142nie now\u0105 jako\u015b\u0107.", "tokens": [51414, 343, 47160, 16384, 3181, 26236, 608, 6501, 36574, 10402, 1110, 5103, 19241, 9671, 5439, 741, 2316, 297, 15088, 710, 749, 5279, 2884, 49922, 586, 1611, 17123, 7753, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0654456194709329, "compression_ratio": 1.461290322580645, "no_speech_prob": 0.21122634410858154}, {"id": 96, "seek": 37730, "start": 405.3, "end": 406.3, "text": " Dok\u0142adnie.", "tokens": [51764, 29768, 10358, 2766, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0654456194709329, "compression_ratio": 1.461290322580645, "no_speech_prob": 0.21122634410858154}, {"id": 97, "seek": 40630, "start": 406.3, "end": 410.3, "text": " Autorzy nazywaj\u0105 to odblokowywaniem zdolno\u015bci.", "tokens": [50364, 6049, 284, 1229, 20151, 27112, 11133, 281, 3611, 5199, 453, 10089, 7916, 4907, 16221, 401, 16438, 13, 50564], "temperature": 0.0, "avg_logprob": -0.0412990107680812, "compression_ratio": 1.4686346863468636, "no_speech_prob": 0.06819029152393341}, {"id": 98, "seek": 40630, "start": 410.3, "end": 414.3, "text": " Mniejsza modele z tej rodziny w tym zadaniu radzi\u0142y sobie fatalnie.", "tokens": [50564, 376, 30295, 2394, 4391, 306, 710, 12573, 28607, 3519, 261, 8107, 42788, 25849, 2843, 3992, 6825, 13652, 24069, 2766, 13, 50764], "temperature": 0.0, "avg_logprob": -0.0412990107680812, "compression_ratio": 1.4686346863468636, "no_speech_prob": 0.06819029152393341}, {"id": 99, "seek": 40630, "start": 414.3, "end": 422.3, "text": " Dopiero te najwi\u0119ksze wersje nagle zaczyna\u0142y rozumie\u0107 kontekst na tyle dobrze, by odpowiada\u0107 na podchwytliwe pytania.", "tokens": [50764, 42657, 12030, 535, 48636, 1694, 1381, 261, 433, 2884, 297, 15088, 43811, 629, 6825, 48797, 414, 2162, 14373, 916, 372, 1667, 39293, 28335, 11, 538, 24314, 39018, 2162, 1667, 2497, 34655, 4328, 2081, 826, 25878, 5609, 13, 51164], "temperature": 0.0, "avg_logprob": -0.0412990107680812, "compression_ratio": 1.4686346863468636, "no_speech_prob": 0.06819029152393341}, {"id": 100, "seek": 40630, "start": 422.3, "end": 425.3, "text": " Czyli pewnych umiej\u0119tno\u015bci nie da si\u0119 nauczy\u0107 troch\u0119.", "tokens": [51164, 37099, 47160, 16384, 1105, 7764, 46788, 16438, 2838, 1120, 3244, 49103, 27150, 24926, 13, 51314], "temperature": 0.0, "avg_logprob": -0.0412990107680812, "compression_ratio": 1.4686346863468636, "no_speech_prob": 0.06819029152393341}, {"id": 101, "seek": 40630, "start": 425.3, "end": 430.3, "text": " Albo model ich nie ma, albo po przekroczeniu pewnego progu skali nagle je zyskuje.", "tokens": [51314, 967, 1763, 2316, 1893, 2838, 463, 11, 22622, 714, 29785, 24174, 39651, 25889, 11858, 447, 2794, 1110, 5103, 297, 15088, 1506, 710, 749, 5279, 2884, 13, 51564], "temperature": 0.0, "avg_logprob": -0.0412990107680812, "compression_ratio": 1.4686346863468636, "no_speech_prob": 0.06819029152393341}, {"id": 102, "seek": 40630, "start": 430.3, "end": 431.3, "text": " Tak to wygl\u0105da.", "tokens": [51564, 9118, 281, 32015, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0412990107680812, "compression_ratio": 1.4686346863468636, "no_speech_prob": 0.06819029152393341}, {"id": 103, "seek": 43130, "start": 431.3, "end": 438.3, "text": " Poza czytaniem ze zrozumieniem ogromne post\u0119py wida\u0107 te\u017c by\u0142o w zadaniach zwi\u0105zanych z weryfikacj\u0105 fakt\u00f3w", "tokens": [50364, 6165, 2394, 6430, 20356, 4907, 5277, 710, 27857, 449, 1053, 4907, 34416, 298, 716, 2183, 1274, 8200, 261, 46898, 9516, 14811, 261, 42788, 3782, 608, 27741, 34644, 710, 261, 2109, 31230, 326, 8555, 21310, 3901, 50714], "temperature": 0.0, "avg_logprob": -0.05924413174013548, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.33773699402809143}, {"id": 104, "seek": 43130, "start": 438.3, "end": 442.3, "text": " i co ciekawe w identyfikacji toksycznego j\u0119zyka.", "tokens": [50714, 741, 598, 30596, 2330, 826, 261, 2473, 88, 31230, 13152, 281, 1694, 17466, 11858, 42309, 40940, 13, 50914], "temperature": 0.0, "avg_logprob": -0.05924413174013548, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.33773699402809143}, {"id": 105, "seek": 43130, "start": 442.3, "end": 443.3, "text": " No w\u0142a\u015bnie.", "tokens": [50914, 883, 14234, 13, 50964], "temperature": 0.0, "avg_logprob": -0.05924413174013548, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.33773699402809143}, {"id": 106, "seek": 43130, "start": 443.3, "end": 447.3, "text": " Ale skoro s\u0105 takie jasne punkty, to musz\u0105 by\u0107 te\u017c i cienie.", "tokens": [50964, 9366, 1110, 10780, 9015, 15963, 361, 296, 716, 25188, 874, 11, 281, 1038, 8925, 15069, 9516, 741, 269, 27385, 13, 51164], "temperature": 0.0, "avg_logprob": -0.05924413174013548, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.33773699402809143}, {"id": 107, "seek": 43130, "start": 447.3, "end": 454.3, "text": " Co jest r\u00f3wnie fascynuj\u0105ce to fakt, \u017ce s\u0105 obszary, gdzie skala nie tylko nie pomog\u0142a, ale wr\u0119cz zaszkodzi\u0142a.", "tokens": [51164, 3066, 3492, 11416, 14215, 30632, 1344, 77, 13263, 384, 281, 21310, 11, 3561, 9015, 3181, 89, 822, 11, 18922, 1110, 5159, 2838, 13219, 2838, 12991, 664, 5024, 11, 6775, 928, 1274, 3689, 710, 19601, 74, 14543, 5024, 13, 51514], "temperature": 0.0, "avg_logprob": -0.05924413174013548, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.33773699402809143}, {"id": 108, "seek": 43130, "start": 454.3, "end": 455.3, "text": " Zgadza si\u0119.", "tokens": [51514, 1176, 70, 345, 2394, 3244, 13, 51564], "temperature": 0.0, "avg_logprob": -0.05924413174013548, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.33773699402809143}, {"id": 109, "seek": 43130, "start": 455.3, "end": 460.3, "text": " I to jest chyba najbardziej otrze\u017awiaj\u0105cy wniosek z ca\u0142ego artyku\u0142u.", "tokens": [51564, 286, 281, 3492, 31532, 41857, 4337, 13503, 10659, 86, 48125, 1344, 261, 3722, 541, 74, 710, 35224, 6308, 594, 874, 5279, 24066, 13, 51814], "temperature": 0.0, "avg_logprob": -0.05924413174013548, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.33773699402809143}, {"id": 110, "seek": 46030, "start": 460.3, "end": 471.3, "text": " Najmniejsze post\u0119py, a w niekt\u00f3rych przypadkach nawet pogorszenie wynik\u00f3w, odnotowano w zadaniach wymagaj\u0105cych rozumowania logicznego i matematycznego.", "tokens": [50364, 31576, 76, 44258, 2183, 1274, 8200, 11, 257, 261, 2838, 43073, 627, 339, 33100, 41326, 22696, 32037, 830, 16778, 31936, 1035, 3901, 11, 3611, 2247, 305, 3730, 261, 42788, 3782, 608, 29764, 559, 11133, 31306, 48797, 21308, 9952, 89, 11858, 741, 3803, 8615, 17466, 11858, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0740377145220143, "compression_ratio": 1.4122137404580153, "no_speech_prob": 0.019935719668865204}, {"id": 111, "seek": 46030, "start": 471.3, "end": 473.3, "text": " Chwila to jest kompletnie wbryw intuicji.", "tokens": [50914, 761, 86, 7371, 281, 3492, 5207, 14657, 2766, 261, 65, 627, 86, 560, 84, 299, 4013, 13, 51014], "temperature": 0.0, "avg_logprob": -0.0740377145220143, "compression_ratio": 1.4122137404580153, "no_speech_prob": 0.019935719668865204}, {"id": 112, "seek": 46030, "start": 473.3, "end": 480.3, "text": " M\u00f3wisz, \u017ce dodanie mocy obliczeniowej i danych sprawi\u0142o, \u017ce w pewnych zadaniach model sta\u0142 si\u0119 g\u0142upszy?", "tokens": [51014, 376, 3901, 23848, 11, 3561, 13886, 7155, 705, 1344, 1111, 1050, 42124, 21091, 741, 274, 34644, 22734, 72, 5249, 11, 3561, 261, 47160, 16384, 42788, 3782, 608, 2316, 11135, 1221, 3244, 18117, 1010, 7706, 30, 51364], "temperature": 0.0, "avg_logprob": -0.0740377145220143, "compression_ratio": 1.4122137404580153, "no_speech_prob": 0.019935719668865204}, {"id": 113, "seek": 46030, "start": 480.3, "end": 482.3, "text": " Brzmi niewiarygodnie, ale tak.", "tokens": [51364, 1603, 89, 3057, 43622, 29104, 21787, 2766, 11, 6775, 991, 13, 51464], "temperature": 0.0, "avg_logprob": -0.0740377145220143, "compression_ratio": 1.4122137404580153, "no_speech_prob": 0.019935719668865204}, {"id": 114, "seek": 46030, "start": 482.3, "end": 484.3, "text": " Jak to w og\u00f3le jest mo\u017cliwe?", "tokens": [51464, 15029, 281, 261, 29229, 3492, 30854, 826, 30, 51564], "temperature": 0.0, "avg_logprob": -0.0740377145220143, "compression_ratio": 1.4122137404580153, "no_speech_prob": 0.019935719668865204}, {"id": 115, "seek": 48430, "start": 484.3, "end": 491.3, "text": " We\u017amy przyk\u0142ad z benchmarku MMLU, kt\u00f3ry sprawdza wiedz\u0119 z 57 dziedzin akademickich.", "tokens": [50364, 492, 10659, 2226, 23144, 710, 18927, 84, 376, 12683, 52, 11, 9913, 46192, 2394, 46894, 11052, 710, 21423, 9758, 15338, 259, 9308, 49290, 618, 480, 13, 50714], "temperature": 0.0, "avg_logprob": -0.06708127802068536, "compression_ratio": 1.338289962825279, "no_speech_prob": 0.02088792808353901}, {"id": 116, "seek": 48430, "start": 491.3, "end": 499.3, "text": " W podzadaniach z matematyki czy algebry abstrakcyjnej Gofer wypad\u0142 gorzej ni\u017c niekt\u00f3re mniejsze modele z tej samej rodziny.", "tokens": [50714, 343, 2497, 89, 345, 3782, 608, 710, 3803, 8615, 88, 2984, 6430, 419, 432, 65, 627, 10823, 11272, 42949, 11794, 1037, 612, 4628, 13647, 1221, 24012, 16920, 28502, 2838, 43073, 265, 275, 44258, 4391, 306, 710, 12573, 912, 73, 28607, 3519, 13, 51114], "temperature": 0.0, "avg_logprob": -0.06708127802068536, "compression_ratio": 1.338289962825279, "no_speech_prob": 0.02088792808353901}, {"id": 117, "seek": 48430, "start": 499.3, "end": 500.3, "text": " Dlaczego?", "tokens": [51114, 413, 75, 39329, 30, 51164], "temperature": 0.0, "avg_logprob": -0.06708127802068536, "compression_ratio": 1.338289962825279, "no_speech_prob": 0.02088792808353901}, {"id": 118, "seek": 48430, "start": 500.3, "end": 505.3, "text": " Autorzy spekuluj\u0105, \u017ce to mo\u017ce by\u0107 fundamentalne ograniczenie obecnej architektury.", "tokens": [51164, 6049, 284, 1229, 768, 74, 425, 13263, 11, 3561, 281, 12034, 15069, 8088, 716, 34416, 30732, 16778, 49141, 11794, 3912, 642, 2320, 2598, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06708127802068536, "compression_ratio": 1.338289962825279, "no_speech_prob": 0.02088792808353901}, {"id": 119, "seek": 48430, "start": 505.3, "end": 509.3, "text": " Celem modelu jest przewidzenie kolejnego tokenu.", "tokens": [51414, 8257, 10386, 2316, 84, 3492, 39758, 327, 16778, 23749, 11858, 14862, 84, 13, 51614], "temperature": 0.0, "avg_logprob": -0.06708127802068536, "compression_ratio": 1.338289962825279, "no_speech_prob": 0.02088792808353901}, {"id": 120, "seek": 50930, "start": 509.3, "end": 517.3, "text": " To \u015bwietnie dzia\u0142a do nauki fakt\u00f3w, skojarze\u0144 i stylu j\u0119zykowego, ale wieloetapowe abstrakcyjne rozumowanie.", "tokens": [50364, 1407, 8299, 39083, 2766, 37903, 360, 35616, 2984, 21310, 3901, 11, 1110, 78, 10150, 49689, 741, 7952, 2781, 49055, 74, 26576, 11, 6775, 20570, 78, 302, 569, 6880, 10823, 11272, 42949, 716, 48797, 22028, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09280898158711598, "compression_ratio": 1.4359861591695502, "no_speech_prob": 0.13642151653766632}, {"id": 121, "seek": 50930, "start": 517.3, "end": 523.3, "text": " Jakie jest potrzebne do rozwi\u0105zania zadania matematycznego to zupe\u0142nie inna bajka.", "tokens": [50764, 15029, 414, 3492, 37595, 716, 360, 9544, 22620, 5609, 42788, 5609, 3803, 8615, 17466, 11858, 281, 49922, 294, 629, 23589, 2330, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09280898158711598, "compression_ratio": 1.4359861591695502, "no_speech_prob": 0.13642151653766632}, {"id": 122, "seek": 50930, "start": 523.3, "end": 524.3, "text": " Dok\u0142adnie tak.", "tokens": [51064, 29768, 10358, 2766, 991, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09280898158711598, "compression_ratio": 1.4359861591695502, "no_speech_prob": 0.13642151653766632}, {"id": 123, "seek": 50930, "start": 524.3, "end": 527.3, "text": " Czyli to troch\u0119 wraca do tego, co m\u00f3wili\u015bmy o danych.", "tokens": [51114, 37099, 281, 24926, 928, 6628, 360, 8627, 11, 598, 13489, 43912, 277, 274, 34644, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09280898158711598, "compression_ratio": 1.4359861591695502, "no_speech_prob": 0.13642151653766632}, {"id": 124, "seek": 50930, "start": 527.3, "end": 536.3, "text": " Internet i ksi\u0105\u017cki s\u0105 pe\u0142ne fakt\u00f3w typu Stolic\u0105 Francji jest Pary\u017c, ale rzadziej zawieraj\u0105 formalne krok po kroku do wody matematyczne.", "tokens": [51264, 7703, 741, 39311, 2984, 9015, 43205, 716, 21310, 3901, 2125, 84, 745, 7940, 1611, 8686, 4013, 3492, 430, 822, 1427, 11, 6775, 367, 89, 345, 19554, 28165, 811, 11133, 9860, 716, 350, 31621, 714, 45909, 5279, 360, 261, 843, 3803, 8615, 17466, 716, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09280898158711598, "compression_ratio": 1.4359861591695502, "no_speech_prob": 0.13642151653766632}, {"id": 125, "seek": 53630, "start": 536.3, "end": 539.3, "text": " Mo\u017ce on po prostu nie mia\u0142 si\u0119 na czym tego nauczy\u0107.", "tokens": [50364, 43774, 322, 714, 19518, 2838, 27989, 3244, 1667, 31466, 8627, 49103, 27150, 13, 50514], "temperature": 0.0, "avg_logprob": -0.06699109944430265, "compression_ratio": 1.2755905511811023, "no_speech_prob": 0.09397415071725845}, {"id": 126, "seek": 53630, "start": 539.3, "end": 541.3, "text": " To jest cz\u0119\u015b\u0107 wyja\u015bnienia.", "tokens": [50514, 1407, 3492, 47149, 4628, 2938, 1788, 77, 18811, 13, 50614], "temperature": 0.0, "avg_logprob": -0.06699109944430265, "compression_ratio": 1.2755905511811023, "no_speech_prob": 0.09397415071725845}, {"id": 127, "seek": 53630, "start": 541.3, "end": 548.3, "text": " \u0141atwiej jest zapami\u0119ta\u0107 ogromn\u0105 liczb\u0119 fakt\u00f3w, ni\u017c nauczy\u0107 si\u0119 uniwersalnej metody rozwi\u0105zywania problem\u00f3w.", "tokens": [50614, 36901, 267, 86, 7764, 3492, 14223, 23806, 42931, 34416, 298, 13113, 6169, 89, 65, 1274, 21310, 3901, 11, 28502, 49103, 27150, 3244, 36435, 5364, 304, 11794, 1131, 843, 9544, 18234, 1229, 86, 5609, 1154, 3901, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06699109944430265, "compression_ratio": 1.2755905511811023, "no_speech_prob": 0.09397415071725845}, {"id": 128, "seek": 53630, "start": 548.3, "end": 558.3, "text": " Mimo wszystko w og\u00f3lnej klasyfikacji tego benchmarku MMLU Gofer osi\u0105gn\u0105\u0142 60%, deklasuj\u0105c GPT-3 z wynikiem 43,9%.", "tokens": [50964, 376, 6934, 22607, 261, 5360, 15741, 11794, 9671, 5871, 31230, 13152, 8627, 18927, 84, 376, 12683, 52, 1037, 612, 3003, 11404, 4568, 1611, 1221, 4060, 8923, 368, 74, 7743, 44733, 26039, 51, 12, 18, 710, 31936, 1035, 4907, 17914, 11, 24, 6856, 51464], "temperature": 0.0, "avg_logprob": -0.06699109944430265, "compression_ratio": 1.2755905511811023, "no_speech_prob": 0.09397415071725845}, {"id": 129, "seek": 55830, "start": 558.3, "end": 567.3, "text": " A co ciekawe, platforma prognostyczna HyperMind, gdzie ludzie obstawiaj\u0105 przysz\u0142o\u015b\u0107 AI, przewidywa\u0142a osi\u0105gni\u0119cie takiego poziomu dopiero za rok lub dwa.", "tokens": [50364, 316, 598, 30596, 2330, 826, 11, 3663, 64, 447, 4568, 555, 17466, 629, 29592, 44, 471, 11, 18922, 37025, 9579, 34953, 8555, 44018, 44742, 7318, 11, 39758, 38836, 4151, 5024, 3003, 11404, 70, 35938, 4260, 32296, 38503, 298, 84, 21900, 12030, 7949, 35135, 15980, 35045, 13, 50814], "temperature": 0.0, "avg_logprob": -0.05529921003382841, "compression_ratio": 1.4586206896551723, "no_speech_prob": 0.28427639603614807}, {"id": 130, "seek": 55830, "start": 567.3, "end": 572.3, "text": " Gofer znowu wyprzedzi\u0142 oczekiwania, ale pokaza\u0142 te\u017c, gdzie le\u017c\u0105 twarde granice.", "tokens": [50814, 1037, 612, 710, 3785, 84, 4628, 1424, 11312, 3992, 1221, 277, 3689, 14753, 86, 5609, 11, 6775, 13010, 12257, 1221, 9516, 11, 18922, 476, 1427, 1611, 683, 10866, 9370, 573, 13, 51064], "temperature": 0.0, "avg_logprob": -0.05529921003382841, "compression_ratio": 1.4586206896551723, "no_speech_prob": 0.28427639603614807}, {"id": 131, "seek": 55830, "start": 572.3, "end": 583.3, "text": " A ta niesamowita zdolno\u015b\u0107 do na\u015bladowania ludzkiego j\u0119zyka, kt\u00f3r\u0105 wida\u0107 w testach na czytanie musi mie\u0107 te\u017c swoj\u0105 ciemn\u0105 stron\u0119, co z toksyczno\u015bci\u0105 i uprzedzeniami.", "tokens": [51064, 316, 1846, 48100, 335, 305, 2786, 16221, 401, 23293, 360, 1667, 1788, 9290, 21308, 15946, 30154, 12200, 42309, 40940, 11, 37415, 261, 46898, 261, 1500, 608, 1667, 6430, 83, 7155, 37587, 35612, 9516, 49194, 269, 4907, 13113, 45766, 1274, 11, 598, 710, 281, 1694, 17466, 16438, 1611, 741, 493, 81, 11312, 2904, 15568, 13, 51614], "temperature": 0.0, "avg_logprob": -0.05529921003382841, "compression_ratio": 1.4586206896551723, "no_speech_prob": 0.28427639603614807}, {"id": 132, "seek": 58330, "start": 584.3, "end": 590.3, "text": " I tu dochodzimy do fascynuj\u0105cego paradoksu. To jedno z najwa\u017cniejszych odkry\u0107 w tym artykule.", "tokens": [50414, 286, 2604, 9243, 378, 89, 13189, 360, 30632, 1344, 77, 13263, 384, 1571, 13480, 453, 15091, 13, 1407, 5232, 1771, 710, 11212, 27111, 10402, 45021, 3611, 43298, 2162, 261, 8107, 594, 874, 74, 2271, 13, 50714], "temperature": 0.0, "avg_logprob": -0.06582957468215067, "compression_ratio": 1.4394904458598725, "no_speech_prob": 0.4769885540008545}, {"id": 133, "seek": 58330, "start": 590.3, "end": 591.3, "text": " To znaczy...", "tokens": [50714, 1407, 36584, 485, 50764], "temperature": 0.0, "avg_logprob": -0.06582957468215067, "compression_ratio": 1.4394904458598725, "no_speech_prob": 0.4769885540008545}, {"id": 134, "seek": 58330, "start": 591.3, "end": 599.3, "text": " Skala ma bardzo z\u0142o\u017cony wp\u0142yw na toksyczno\u015b\u0107. Z jednej strony, je\u015bli dasz modelowi toksyczny prompt, czyli jak\u0105\u015b obra\u017cliw\u0105 zaczepk\u0119,", "tokens": [50764, 7324, 5159, 463, 9034, 710, 5249, 1427, 2526, 32444, 6825, 86, 1667, 281, 1694, 17466, 23293, 13, 1176, 5232, 11794, 32406, 11, 25630, 1482, 89, 2316, 24503, 281, 1694, 17466, 1634, 12391, 11, 16591, 46719, 1788, 22798, 1427, 2081, 86, 1611, 710, 14875, 595, 15724, 11, 51164], "temperature": 0.0, "avg_logprob": -0.06582957468215067, "compression_ratio": 1.4394904458598725, "no_speech_prob": 0.4769885540008545}, {"id": 135, "seek": 58330, "start": 599.3, "end": 603.3, "text": " wi\u0119ksze modele s\u0105 bardziej sk\u0142one do wygenerowania r\u00f3wnie toksycznej odpowiedzi.", "tokens": [51164, 29968, 1381, 4391, 306, 9015, 27209, 1110, 1221, 546, 360, 4628, 21848, 21308, 11416, 14215, 281, 1694, 17466, 11794, 36574, 3992, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06582957468215067, "compression_ratio": 1.4394904458598725, "no_speech_prob": 0.4769885540008545}, {"id": 136, "seek": 58330, "start": 603.3, "end": 605.3, "text": " Czyli s\u0105 lepszymi papugami.", "tokens": [51364, 37099, 9015, 476, 1878, 1229, 3057, 5806, 697, 4526, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06582957468215067, "compression_ratio": 1.4394904458598725, "no_speech_prob": 0.4769885540008545}, {"id": 137, "seek": 58330, "start": 605.3, "end": 610.3, "text": " Nad dobrej nadz\u0142e. Po prostu lepiej na\u015bladuj\u0105 styl tego, co dostaj\u0105 na wej\u015bciu.", "tokens": [51464, 23269, 41959, 73, 12617, 89, 19827, 13, 6165, 19518, 476, 39699, 1667, 1788, 9290, 13263, 23736, 8627, 11, 598, 20568, 11133, 1667, 321, 73, 6199, 84, 13, 51714], "temperature": 0.0, "avg_logprob": -0.06582957468215067, "compression_ratio": 1.4394904458598725, "no_speech_prob": 0.4769885540008545}, {"id": 138, "seek": 61030, "start": 610.3, "end": 616.3, "text": " Ok, czyli im wi\u0119kszy model, tym wi\u0119ksze ryzyko, \u017ce odwdzi\u0119czy si\u0119 pi\u0119knym za nadobne.", "tokens": [50364, 3477, 11, 16591, 566, 29968, 1229, 2316, 11, 8107, 29968, 1381, 20791, 1229, 4093, 11, 3561, 3611, 86, 67, 16706, 6522, 3244, 48085, 12996, 7949, 12617, 996, 716, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07467378970391736, "compression_ratio": 1.5015290519877675, "no_speech_prob": 0.013989703729748726}, {"id": 139, "seek": 61030, "start": 616.3, "end": 619.3, "text": " Ale powiedzia\u0142e\u015b, \u017ce to paradoks, wi\u0119c musi by\u0107 te\u017c druga strona medalu.", "tokens": [50664, 9366, 48539, 68, 1788, 11, 3561, 281, 13480, 25500, 11, 16677, 37587, 15069, 9516, 4110, 64, 1056, 4037, 1205, 4929, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07467378970391736, "compression_ratio": 1.5015290519877675, "no_speech_prob": 0.013989703729748726}, {"id": 140, "seek": 61030, "start": 619.3, "end": 626.3, "text": " Dok\u0142adnie. Jednocze\u015bnie te same wi\u0119ksze modele s\u0105 znacznie lepsze w zadaniu klasyfikowania tekst\u00f3w jako toksyczne.", "tokens": [50814, 29768, 10358, 2766, 13, 27076, 26694, 1381, 12221, 535, 912, 29968, 1381, 4391, 306, 9015, 15397, 14875, 2766, 476, 1878, 1381, 261, 42788, 25849, 9671, 5871, 31230, 21308, 16624, 372, 3901, 17123, 281, 1694, 17466, 716, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07467378970391736, "compression_ratio": 1.5015290519877675, "no_speech_prob": 0.013989703729748726}, {"id": 141, "seek": 61030, "start": 626.3, "end": 627.3, "text": " Jak to?", "tokens": [51164, 15029, 281, 30, 51214], "temperature": 0.0, "avg_logprob": -0.07467378970391736, "compression_ratio": 1.5015290519877675, "no_speech_prob": 0.013989703729748726}, {"id": 142, "seek": 61030, "start": 627.3, "end": 631.3, "text": " Liwy. Odpowied\u017a du\u017co wi\u0119ksz\u0105 precyzj\u0105 ni\u017c jego mniejsi kuzyni.", "tokens": [51214, 441, 72, 9726, 13, 12210, 14701, 1091, 10659, 26673, 29968, 8925, 659, 1344, 89, 8555, 28502, 26542, 39513, 7691, 17807, 1229, 3722, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07467378970391736, "compression_ratio": 1.5015290519877675, "no_speech_prob": 0.013989703729748726}, {"id": 143, "seek": 61030, "start": 631.3, "end": 639.3, "text": " Potrafi lepiej rozpozna\u0107 mow\u0119 nienawi\u015bci, mimo \u017ce sam potrafi j\u0105 lepiej generowa\u0107, je\u015bli si\u0119 go do tego sprowokuje.", "tokens": [51414, 9145, 10437, 72, 476, 39699, 9544, 2259, 35458, 2162, 275, 305, 1274, 297, 1053, 38402, 6199, 11, 275, 6934, 3561, 3247, 1847, 10437, 72, 35692, 476, 39699, 1337, 11445, 11, 25630, 3244, 352, 360, 8627, 637, 1892, 453, 13008, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07467378970391736, "compression_ratio": 1.5015290519877675, "no_speech_prob": 0.013989703729748726}, {"id": 144, "seek": 63930, "start": 639.3, "end": 643.3, "text": " Czyli to jest kluczowa lekcja dla ka\u017cdego, kto buduje takie systemy.", "tokens": [50364, 37099, 281, 3492, 9671, 1311, 89, 5528, 30863, 34056, 12285, 21912, 67, 6308, 11, 23780, 3265, 13008, 15963, 1185, 88, 13, 50564], "temperature": 0.0, "avg_logprob": -0.053190484831604774, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.00661330996081233}, {"id": 145, "seek": 63930, "start": 643.3, "end": 645.3, "text": " Sama wielko\u015b\u0107 modelu to nie wszystko.", "tokens": [50564, 318, 2404, 20570, 4093, 7753, 2316, 84, 281, 2838, 22607, 13, 50664], "temperature": 0.0, "avg_logprob": -0.053190484831604774, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.00661330996081233}, {"id": 146, "seek": 63930, "start": 645.3, "end": 647.3, "text": " A nawet mo\u017ce by\u0107 pu\u0142apk\u0105.", "tokens": [50664, 316, 22696, 12034, 15069, 2362, 1221, 569, 26304, 13, 50764], "temperature": 0.0, "avg_logprob": -0.053190484831604774, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.00661330996081233}, {"id": 147, "seek": 63930, "start": 647.3, "end": 655.3, "text": " Bez odpowiedniego prowadzenia za r\u0119k\u0119 dostajemy po prostu m\u0105drzejsz\u0105, ale i potencjalnie bardziej z\u0142o\u015bliw\u0105 papug\u0119.", "tokens": [50764, 879, 89, 36574, 2766, 1571, 36590, 14320, 7949, 41197, 15724, 20568, 1805, 3633, 714, 19518, 275, 18962, 13503, 25530, 8925, 11, 6775, 741, 1847, 22660, 22600, 2766, 27209, 710, 5249, 15350, 86, 1611, 5806, 697, 1274, 13, 51164], "temperature": 0.0, "avg_logprob": -0.053190484831604774, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.00661330996081233}, {"id": 148, "seek": 63930, "start": 655.3, "end": 658.3, "text": " I tu chyba wchodzi ca\u0142a sztuka promptingu.", "tokens": [51164, 286, 2604, 31532, 261, 34616, 1335, 5024, 262, 2682, 13599, 12391, 7050, 13, 51314], "temperature": 0.0, "avg_logprob": -0.053190484831604774, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.00661330996081233}, {"id": 149, "seek": 63930, "start": 658.3, "end": 664.3, "text": " W\u0142a\u015bnie. A prompting to nic innego jak sztuka zadawania pyta\u0144 i wydawania instrukcji modelowi.", "tokens": [51314, 343, 5024, 12221, 13, 316, 12391, 278, 281, 6201, 294, 11858, 4207, 262, 2682, 13599, 710, 1538, 86, 5609, 10664, 1328, 5248, 741, 25984, 1607, 5609, 1058, 25126, 19649, 2316, 24503, 13, 51614], "temperature": 0.0, "avg_logprob": -0.053190484831604774, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.00661330996081233}, {"id": 150, "seek": 63930, "start": 664.3, "end": 668.3, "text": " W artykule opisano eksperyment Dialog Prompted Gopher.", "tokens": [51614, 343, 594, 874, 74, 2271, 45477, 3730, 30724, 610, 88, 518, 29658, 664, 15833, 662, 292, 460, 16754, 13, 51814], "temperature": 0.0, "avg_logprob": -0.053190484831604774, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.00661330996081233}, {"id": 151, "seek": 66830, "start": 668.3, "end": 670.3, "text": " I co tam zrobili?", "tokens": [50364, 286, 598, 7677, 44399, 2312, 30, 50464], "temperature": 0.0, "avg_logprob": -0.040413287707737514, "compression_ratio": 1.4514925373134329, "no_speech_prob": 0.0383349172770977}, {"id": 152, "seek": 66830, "start": 670.3, "end": 673.3, "text": " Na samym pocz\u0105tku rozmowy model dosta\u0142 prost\u0105 instrukcj\u0119.", "tokens": [50464, 6056, 3247, 4199, 43959, 35234, 10089, 2316, 274, 8638, 1221, 10293, 1611, 1058, 25126, 41960, 13, 50614], "temperature": 0.0, "avg_logprob": -0.040413287707737514, "compression_ratio": 1.4514925373134329, "no_speech_prob": 0.0383349172770977}, {"id": 153, "seek": 66830, "start": 673.3, "end": 677.3, "text": " Jeste\u015b pomocnym, uprzejmym i inkluzywnym asystentem.", "tokens": [50614, 508, 8887, 1788, 48962, 12996, 11, 493, 13503, 73, 2226, 76, 741, 11276, 2781, 1229, 895, 4199, 382, 38593, 317, 443, 13, 50814], "temperature": 0.0, "avg_logprob": -0.040413287707737514, "compression_ratio": 1.4514925373134329, "no_speech_prob": 0.0383349172770977}, {"id": 154, "seek": 66830, "start": 677.3, "end": 679.3, "text": " I sta\u0142a si\u0119 magia.", "tokens": [50814, 286, 11135, 5024, 3244, 2258, 654, 13, 50914], "temperature": 0.0, "avg_logprob": -0.040413287707737514, "compression_ratio": 1.4514925373134329, "no_speech_prob": 0.0383349172770977}, {"id": 155, "seek": 66830, "start": 679.3, "end": 680.3, "text": " Co si\u0119 sta\u0142o?", "tokens": [50914, 3066, 3244, 11135, 5249, 30, 50964], "temperature": 0.0, "avg_logprob": -0.040413287707737514, "compression_ratio": 1.4514925373134329, "no_speech_prob": 0.0383349172770977}, {"id": 156, "seek": 66830, "start": 680.3, "end": 682.3, "text": " Ca\u0142y trend si\u0119 odwr\u00f3ci\u0142.", "tokens": [50964, 7544, 6825, 6028, 3244, 3611, 7449, 812, 537, 1221, 13, 51064], "temperature": 0.0, "avg_logprob": -0.040413287707737514, "compression_ratio": 1.4514925373134329, "no_speech_prob": 0.0383349172770977}, {"id": 157, "seek": 66830, "start": 682.3, "end": 690.3, "text": " W tym trybie sk\u0142onno\u015b\u0107 do generowania toksycznych odpowiedzi nie ros\u0142a wraz ze skal\u0105 modelu.", "tokens": [51064, 343, 8107, 853, 7392, 1110, 1221, 266, 23293, 360, 1337, 21308, 281, 1694, 17466, 9399, 36574, 3992, 2838, 18953, 5024, 7843, 89, 5277, 16890, 1611, 2316, 84, 13, 51464], "temperature": 0.0, "avg_logprob": -0.040413287707737514, "compression_ratio": 1.4514925373134329, "no_speech_prob": 0.0383349172770977}, {"id": 158, "seek": 66830, "start": 690.3, "end": 696.3, "text": " Wr\u0119cz przeciwnie wi\u0119ksze modele lepiej trzyma\u0142y si\u0119 instrukcji bycia mi\u0142ym asystentem.", "tokens": [51464, 10159, 1274, 3689, 39622, 14215, 29968, 1381, 4391, 306, 476, 39699, 34573, 1696, 6825, 3244, 1058, 25126, 19649, 538, 2755, 2752, 1221, 4199, 382, 38593, 317, 443, 13, 51764], "temperature": 0.0, "avg_logprob": -0.040413287707737514, "compression_ratio": 1.4514925373134329, "no_speech_prob": 0.0383349172770977}, {"id": 159, "seek": 69630, "start": 696.3, "end": 702.3, "text": " Bo pokazuje, \u017ce to jak poprosimy model o wykonanie zadania jest r\u00f3wnie wa\u017cne jak to co on wie.", "tokens": [50364, 3286, 13010, 43317, 11, 3561, 281, 4207, 1665, 2635, 13189, 2316, 277, 46702, 7155, 42788, 5609, 3492, 11416, 14215, 46110, 4207, 281, 598, 322, 3355, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06108461309362341, "compression_ratio": 1.4620938628158844, "no_speech_prob": 0.007101761642843485}, {"id": 160, "seek": 69630, "start": 702.3, "end": 705.3, "text": " A co z innymi bardziej subtelnymi uprzedzeniami.", "tokens": [50664, 316, 598, 710, 294, 31813, 27209, 7257, 338, 31813, 493, 81, 11312, 2904, 15568, 13, 50814], "temperature": 0.0, "avg_logprob": -0.06108461309362341, "compression_ratio": 1.4620938628158844, "no_speech_prob": 0.007101761642843485}, {"id": 161, "seek": 69630, "start": 705.3, "end": 708.3, "text": " Na przyk\u0142ad dotycz\u0105cymi p\u0142ci czy dialekt\u00f3w.", "tokens": [50814, 6056, 23144, 5893, 17466, 1611, 1344, 3057, 28695, 537, 6430, 5502, 8192, 3901, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06108461309362341, "compression_ratio": 1.4620938628158844, "no_speech_prob": 0.007101761642843485}, {"id": 162, "seek": 69630, "start": 708.3, "end": 711.3, "text": " Tu sprawa jest jeszcze bardziej skomplikowana.", "tokens": [50964, 7836, 637, 424, 4151, 3492, 14168, 27209, 1110, 298, 564, 1035, 40458, 13, 51114], "temperature": 0.0, "avg_logprob": -0.06108461309362341, "compression_ratio": 1.4620938628158844, "no_speech_prob": 0.007101761642843485}, {"id": 163, "seek": 69630, "start": 711.3, "end": 716.3, "text": " W przypadku uprzedze\u0144 p\u0142ciowych badacze nie znale\u017ali prostej zale\u017cno\u015bci od skali.", "tokens": [51114, 343, 41955, 493, 81, 11312, 49689, 28695, 537, 19605, 1578, 326, 1381, 2838, 15397, 1220, 10659, 2081, 10293, 40779, 710, 45494, 16438, 3611, 1110, 5103, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06108461309362341, "compression_ratio": 1.4620938628158844, "no_speech_prob": 0.007101761642843485}, {"id": 164, "seek": 69630, "start": 716.3, "end": 717.3, "text": " Nic.", "tokens": [51364, 14776, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06108461309362341, "compression_ratio": 1.4620938628158844, "no_speech_prob": 0.007101761642843485}, {"id": 165, "seek": 69630, "start": 717.3, "end": 723.3, "text": " Co wi\u0119cej odkryli jak niestabilne i zawodne s\u0105 obecne metody pomiaru.", "tokens": [51414, 3066, 26004, 3611, 43298, 2081, 4207, 3867, 377, 5177, 716, 741, 28165, 378, 716, 9015, 49141, 716, 1131, 843, 280, 9220, 16870, 13, 51714], "temperature": 0.0, "avg_logprob": -0.06108461309362341, "compression_ratio": 1.4620938628158844, "no_speech_prob": 0.007101761642843485}, {"id": 166, "seek": 72330, "start": 723.3, "end": 726.3, "text": " Wystarczy\u0142o zmieni\u0107 jedno s\u0142owo w szablonie zapytania.", "tokens": [50364, 14458, 9710, 6522, 5249, 17020, 1053, 12757, 5232, 1771, 15116, 19941, 261, 7870, 455, 14864, 414, 14223, 4328, 5609, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10406044642130534, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.007336883340030909}, {"id": 167, "seek": 72330, "start": 726.3, "end": 732.3, "text": " Na przyk\u0142ad was na is w zdaniu the zaw\u00f3d was p\u0142e\u0107.", "tokens": [50514, 6056, 23144, 390, 1667, 307, 261, 16221, 25849, 264, 28165, 17081, 390, 280, 19827, 2162, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10406044642130534, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.007336883340030909}, {"id": 168, "seek": 72330, "start": 732.3, "end": 736.3, "text": " \u017beby wyniki pomiaru uprzedze\u0144 wywr\u00f3ci\u0142y si\u0119 do g\u00f3ry nogami.", "tokens": [50814, 46864, 2322, 31936, 9850, 280, 9220, 16870, 493, 81, 11312, 49689, 4628, 7449, 812, 537, 6825, 3244, 360, 290, 812, 627, 9638, 4526, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10406044642130534, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.007336883340030909}, {"id": 169, "seek": 72330, "start": 736.3, "end": 740.3, "text": " Czyli jeste\u015bmy jeszcze daleko od rzetelnego mierzenia tego problemu.", "tokens": [51014, 37099, 35928, 14168, 11702, 34241, 3611, 367, 40399, 338, 11858, 47448, 14320, 8627, 1154, 84, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10406044642130534, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.007336883340030909}, {"id": 170, "seek": 72330, "start": 740.3, "end": 742.3, "text": " A uprzedzenia dialektowe.", "tokens": [51214, 316, 493, 81, 11312, 14320, 5502, 8192, 6880, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10406044642130534, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.007336883340030909}, {"id": 171, "seek": 72330, "start": 742.3, "end": 746.3, "text": " Tutaj wyniki s\u0105 bardziej jednozlaczne i niestety niepokoj\u0105ce.", "tokens": [51314, 41819, 31936, 9850, 9015, 27209, 5232, 1771, 89, 75, 14875, 716, 741, 3867, 377, 2210, 2838, 79, 13704, 8555, 384, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10406044642130534, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.007336883340030909}, {"id": 172, "seek": 72330, "start": 746.3, "end": 749.3, "text": " Modele wykazywa\u0142y wy\u017csz\u0105 perpleksyti.", "tokens": [51514, 20500, 306, 39287, 33235, 4151, 6825, 4628, 1427, 82, 8925, 680, 781, 74, 3187, 7317, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10406044642130534, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.007336883340030909}, {"id": 173, "seek": 72330, "start": 749.3, "end": 752.3, "text": " Czyli by\u0142y bardziej zaskoczone tekstem.", "tokens": [51664, 37099, 26366, 27209, 710, 3863, 905, 16896, 16624, 1099, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10406044642130534, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.007336883340030909}, {"id": 174, "seek": 75230, "start": 752.3, "end": 754.3, "text": " Gorzej sobie z nim radzi\u0142y.", "tokens": [50364, 26144, 16920, 13652, 710, 24887, 2843, 3992, 6825, 13, 50464], "temperature": 0.0, "avg_logprob": -0.06879222393035889, "compression_ratio": 1.435374149659864, "no_speech_prob": 0.012459836900234222}, {"id": 175, "seek": 75230, "start": 754.3, "end": 755.3, "text": " Tak.", "tokens": [50464, 9118, 13, 50514], "temperature": 0.0, "avg_logprob": -0.06879222393035889, "compression_ratio": 1.435374149659864, "no_speech_prob": 0.012459836900234222}, {"id": 176, "seek": 75230, "start": 755.3, "end": 758.3, "text": " Na tweetach napisanych w dialekcie afroameryka\u0144skim,", "tokens": [50514, 6056, 15258, 608, 9296, 271, 34644, 261, 5502, 916, 4260, 3238, 340, 335, 2109, 2330, 27125, 332, 11, 50664], "temperature": 0.0, "avg_logprob": -0.06879222393035889, "compression_ratio": 1.435374149659864, "no_speech_prob": 0.012459836900234222}, {"id": 177, "seek": 75230, "start": 758.3, "end": 761.3, "text": " czyli African American Aligned English", "tokens": [50664, 16591, 7312, 2665, 967, 16690, 3669, 50814], "temperature": 0.0, "avg_logprob": -0.06879222393035889, "compression_ratio": 1.435374149659864, "no_speech_prob": 0.012459836900234222}, {"id": 178, "seek": 75230, "start": 761.3, "end": 764.3, "text": " w por\u00f3wnaniu do standardowego angielskiego.", "tokens": [50814, 261, 1515, 812, 895, 25849, 360, 3832, 26576, 2562, 1187, 5161, 12200, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06879222393035889, "compression_ratio": 1.435374149659864, "no_speech_prob": 0.012459836900234222}, {"id": 179, "seek": 75230, "start": 764.3, "end": 769.3, "text": " I co najgorsze, ta r\u00f3\u017cnica wcale nie mala\u0142a wraz ze wzrostem skali modelu.", "tokens": [50964, 286, 598, 11212, 70, 830, 1381, 11, 1846, 19637, 32687, 261, 37088, 2838, 37508, 5024, 7843, 89, 5277, 24809, 27494, 443, 1110, 5103, 2316, 84, 13, 51214], "temperature": 0.0, "avg_logprob": -0.06879222393035889, "compression_ratio": 1.435374149659864, "no_speech_prob": 0.012459836900234222}, {"id": 180, "seek": 75230, "start": 769.3, "end": 772.3, "text": " Luka pozostawa\u0142a taka sama.", "tokens": [51214, 441, 13599, 21281, 555, 10449, 5024, 28017, 17768, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06879222393035889, "compression_ratio": 1.435374149659864, "no_speech_prob": 0.012459836900234222}, {"id": 181, "seek": 75230, "start": 772.3, "end": 775.3, "text": " Czyli samo dorzucanie wi\u0119kszej ilo\u015bci danych i parametr\u00f3w", "tokens": [51364, 37099, 36422, 26313, 89, 1311, 7155, 29968, 16920, 1930, 44468, 274, 34644, 741, 6220, 27965, 3901, 51514], "temperature": 0.0, "avg_logprob": -0.06879222393035889, "compression_ratio": 1.435374149659864, "no_speech_prob": 0.012459836900234222}, {"id": 182, "seek": 75230, "start": 775.3, "end": 779.3, "text": " nie rozwi\u0105zuje problemu niedostatecznej reprezentacji pewnych gr\u00f3b w internecie.", "tokens": [51514, 2838, 9544, 18234, 11728, 2884, 1154, 84, 32488, 555, 473, 3689, 11794, 1085, 265, 14185, 13152, 47160, 16384, 677, 14216, 261, 728, 716, 4260, 13, 51714], "temperature": 0.0, "avg_logprob": -0.06879222393035889, "compression_ratio": 1.435374149659864, "no_speech_prob": 0.012459836900234222}, {"id": 183, "seek": 77930, "start": 779.3, "end": 782.3, "text": " Dok\u0142adnie. Skala nie jest panaceum.", "tokens": [50364, 29768, 10358, 2766, 13, 7324, 5159, 2838, 3492, 2462, 617, 449, 13, 50514], "temperature": 0.0, "avg_logprob": -0.051169032781896454, "compression_ratio": 1.4063492063492065, "no_speech_prob": 0.07027404755353928}, {"id": 184, "seek": 77930, "start": 782.3, "end": 786.3, "text": " To fundamentalny problem, kt\u00f3ry wymaga zupe\u0142nie innych rozwi\u0105za\u0144.", "tokens": [50514, 1407, 8088, 1634, 1154, 11, 9913, 29764, 9286, 49922, 36286, 9544, 18234, 2394, 5248, 13, 50714], "temperature": 0.0, "avg_logprob": -0.051169032781896454, "compression_ratio": 1.4063492063492065, "no_speech_prob": 0.07027404755353928}, {"id": 185, "seek": 77930, "start": 786.3, "end": 791.3, "text": " Mimo tych wyzwa\u0144 widz\u0119, \u017ce autorzy widz\u0105 w tych modelach ogromny potencja\u0142.", "tokens": [50714, 376, 6934, 15180, 4628, 89, 4151, 5248, 5274, 11052, 11, 3561, 19510, 1229, 5274, 8925, 261, 15180, 2316, 608, 34416, 298, 1634, 1847, 22660, 2938, 1221, 13, 50964], "temperature": 0.0, "avg_logprob": -0.051169032781896454, "compression_ratio": 1.4063492063492065, "no_speech_prob": 0.07027404755353928}, {"id": 186, "seek": 77930, "start": 791.3, "end": 795.3, "text": " Tak, tak\u017ce jako narz\u0119dzia do badania bezpiecze\u0144stwa samej sztucznej inteligencji.", "tokens": [50964, 9118, 11, 23306, 17123, 6714, 89, 6298, 40395, 360, 1578, 5609, 47153, 9680, 12229, 4151, 912, 73, 262, 2682, 1311, 89, 11794, 24777, 3213, 19649, 13, 51164], "temperature": 0.0, "avg_logprob": -0.051169032781896454, "compression_ratio": 1.4063492063492065, "no_speech_prob": 0.07027404755353928}, {"id": 187, "seek": 77930, "start": 795.3, "end": 796.3, "text": " Na przyk\u0142ad.", "tokens": [51164, 6056, 23144, 13, 51214], "temperature": 0.0, "avg_logprob": -0.051169032781896454, "compression_ratio": 1.4063492063492065, "no_speech_prob": 0.07027404755353928}, {"id": 188, "seek": 77930, "start": 796.3, "end": 802.3, "text": " Proponuj\u0105 u\u017cycie ich do symulowania debaty mi\u0119dzy dwoma systemami II.", "tokens": [51214, 21944, 266, 13263, 34097, 4260, 1893, 360, 6697, 425, 21308, 3001, 21398, 33964, 27379, 6440, 1185, 4526, 6351, 13, 51514], "temperature": 0.0, "avg_logprob": -0.051169032781896454, "compression_ratio": 1.4063492063492065, "no_speech_prob": 0.07027404755353928}, {"id": 189, "seek": 77930, "start": 802.3, "end": 807.3, "text": " Je\u015bli jeden system ma jaki\u015b pogl\u0105d, drugi mo\u017ce pr\u00f3bowa\u0107 znale\u017a\u0107 w nim luki.", "tokens": [51514, 37086, 12906, 1185, 463, 34721, 32037, 75, 18962, 11, 4110, 72, 12034, 8565, 65, 11445, 15397, 1220, 10659, 2162, 261, 24887, 287, 11788, 13, 51764], "temperature": 0.0, "avg_logprob": -0.051169032781896454, "compression_ratio": 1.4063492063492065, "no_speech_prob": 0.07027404755353928}, {"id": 190, "seek": 80730, "start": 807.3, "end": 810.3, "text": " Taki zautomatyzowany adwokat, diab\u0142a.", "tokens": [50364, 314, 7421, 710, 1375, 298, 21398, 89, 23341, 614, 86, 453, 267, 11, 33227, 5024, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07826392504633689, "compression_ratio": 1.3865248226950355, "no_speech_prob": 0.017820263281464577}, {"id": 191, "seek": 80730, "start": 810.3, "end": 811.3, "text": " Podoba mi si\u0119 to.", "tokens": [50514, 12646, 19481, 2752, 3244, 281, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07826392504633689, "compression_ratio": 1.3865248226950355, "no_speech_prob": 0.017820263281464577}, {"id": 192, "seek": 80730, "start": 811.3, "end": 816.3, "text": " Zamiast ba\u0107 si\u0119 pot\u0119\u017cnej AI u\u017cyjmy jej do wzajemnej kontroli.", "tokens": [50564, 1176, 4526, 525, 4773, 2162, 3244, 1847, 1274, 1427, 11794, 7318, 34097, 73, 2226, 28924, 360, 24809, 1805, 443, 11794, 14373, 340, 2081, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07826392504633689, "compression_ratio": 1.3865248226950355, "no_speech_prob": 0.017820263281464577}, {"id": 193, "seek": 80730, "start": 816.3, "end": 820.3, "text": " Artyku\u0142 ko\u0144czy si\u0119 te\u017c kilkoma bardzo cennymi, technicznymi lekcjami.", "tokens": [50814, 1587, 874, 5279, 1221, 26470, 6522, 3244, 9516, 5128, 74, 6440, 9034, 27900, 31813, 11, 1537, 17946, 31813, 30863, 66, 73, 4526, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07826392504633689, "compression_ratio": 1.3865248226950355, "no_speech_prob": 0.017820263281464577}, {"id": 194, "seek": 80730, "start": 820.3, "end": 823.3, "text": " Tak, z samego procesu treningu.", "tokens": [51014, 9118, 11, 710, 912, 1571, 17565, 84, 2192, 773, 84, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07826392504633689, "compression_ratio": 1.3865248226950355, "no_speech_prob": 0.017820263281464577}, {"id": 195, "seek": 80730, "start": 823.3, "end": 828.3, "text": " Okaza\u0142o si\u0119 na przyk\u0142ad, \u017ce pewne algorytmy, kt\u00f3re \u015bwietnie dzia\u0142a\u0142y w mniejszej skali", "tokens": [51164, 3477, 12257, 5249, 3244, 1667, 23144, 11, 3561, 25889, 716, 3501, 827, 83, 2226, 11, 8864, 8299, 39083, 2766, 37903, 6825, 261, 275, 30295, 16920, 1110, 5103, 51414], "temperature": 0.0, "avg_logprob": -0.07826392504633689, "compression_ratio": 1.3865248226950355, "no_speech_prob": 0.017820263281464577}, {"id": 196, "seek": 80730, "start": 828.3, "end": 831.3, "text": " kompletnie zawodzi\u0142y przy rozmiarze GoFer'a.", "tokens": [51414, 5207, 14657, 2766, 28165, 14543, 6825, 6501, 9544, 3057, 289, 1381, 1037, 37, 260, 6, 64, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07826392504633689, "compression_ratio": 1.3865248226950355, "no_speech_prob": 0.017820263281464577}, {"id": 197, "seek": 80730, "start": 831.3, "end": 832.3, "text": " Jakie na przyk\u0142ad?", "tokens": [51564, 15029, 414, 1667, 23144, 30, 51614], "temperature": 0.0, "avg_logprob": -0.07826392504633689, "compression_ratio": 1.3865248226950355, "no_speech_prob": 0.017820263281464577}, {"id": 198, "seek": 83230, "start": 832.3, "end": 837.3, "text": " Optymalizator Adam okaza\u0142 si\u0119 du\u017co stabilniejszy ni\u017c popularny Adafactor.", "tokens": [50364, 21455, 4199, 304, 590, 1639, 7938, 3133, 12257, 1221, 3244, 26673, 11652, 10402, 7706, 28502, 3743, 1634, 1999, 2792, 15104, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09988663243312462, "compression_ratio": 1.4115755627009647, "no_speech_prob": 0.12627330422401428}, {"id": 199, "seek": 83230, "start": 837.3, "end": 843.3, "text": " Trening z ni\u017csz\u0105 precyzj\u0105 liczbow\u0105, Blowfet 16, powodowa\u0142, \u017ce niekt\u00f3re parametry", "tokens": [50614, 8648, 773, 710, 28502, 82, 8925, 659, 1344, 89, 8555, 6169, 89, 8202, 1611, 11, 46391, 69, 302, 3165, 11, 3388, 378, 30105, 11, 3561, 2838, 43073, 265, 6220, 9889, 50914], "temperature": 0.0, "avg_logprob": -0.09988663243312462, "compression_ratio": 1.4115755627009647, "no_speech_prob": 0.12627330422401428}, {"id": 200, "seek": 83230, "start": 843.3, "end": 846.3, "text": " zamra\u017ca\u0142y si\u0119 i przestawa\u0142y si\u0119 uczy\u0107.", "tokens": [50914, 19876, 424, 35075, 6825, 3244, 741, 44264, 10449, 6825, 3244, 344, 33967, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09988663243312462, "compression_ratio": 1.4115755627009647, "no_speech_prob": 0.12627330422401428}, {"id": 201, "seek": 83230, "start": 846.3, "end": 848.3, "text": " To s\u0105 bezcenne wskaz\u00f3wki dla innych zespo\u0142\u00f3w.", "tokens": [51064, 1407, 9015, 10782, 66, 13295, 261, 5161, 921, 3901, 2984, 12285, 36286, 710, 279, 2259, 1221, 3901, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09988663243312462, "compression_ratio": 1.4115755627009647, "no_speech_prob": 0.12627330422401428}, {"id": 202, "seek": 83230, "start": 848.3, "end": 852.3, "text": " Ale chyba najciekawszy by\u0142 wynik pr\u00f3b kompresji modelu.", "tokens": [51164, 9366, 31532, 11212, 4260, 74, 1607, 7706, 16673, 31936, 1035, 8565, 65, 5207, 14508, 4013, 2316, 84, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09988663243312462, "compression_ratio": 1.4115755627009647, "no_speech_prob": 0.12627330422401428}, {"id": 203, "seek": 83230, "start": 852.3, "end": 854.3, "text": " Pr\u00f3bowali go jako\u015b zmniejszy\u0107?", "tokens": [51364, 2114, 812, 8202, 5103, 352, 17123, 1788, 17020, 10402, 7706, 2162, 30, 51464], "temperature": 0.0, "avg_logprob": -0.09988663243312462, "compression_ratio": 1.4115755627009647, "no_speech_prob": 0.12627330422401428}, {"id": 204, "seek": 83230, "start": 854.3, "end": 860.3, "text": " Tak, u\u017cywaj\u0105c technik, jak pruning, czyli przycinanie najmniej wa\u017cnych po\u0142\u0105cze\u0144.", "tokens": [51464, 9118, 11, 34097, 86, 38757, 1537, 1035, 11, 4207, 582, 37726, 11, 16591, 6501, 20021, 7155, 11212, 47658, 27777, 9399, 714, 15926, 9680, 5248, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09988663243312462, "compression_ratio": 1.4115755627009647, "no_speech_prob": 0.12627330422401428}, {"id": 205, "seek": 86030, "start": 860.3, "end": 866.3, "text": " Czy distillation, czyli pr\u00f3by nauczenia mniejszego modelu przez ten du\u017cy?", "tokens": [50364, 19832, 42923, 399, 11, 16591, 8565, 2322, 49103, 14320, 39513, 15453, 6308, 2316, 84, 14064, 2064, 1581, 7735, 30, 50664], "temperature": 0.0, "avg_logprob": -0.08729892655422813, "compression_ratio": 1.4340836012861737, "no_speech_prob": 0.005670203361660242}, {"id": 206, "seek": 86030, "start": 866.3, "end": 869.3, "text": " Obie metody da\u0142y bardzo skromne rezultaty.", "tokens": [50664, 4075, 414, 1131, 843, 1120, 6825, 9034, 1110, 4397, 716, 48060, 723, 21398, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08729892655422813, "compression_ratio": 1.4340836012861737, "no_speech_prob": 0.005670203361660242}, {"id": 207, "seek": 86030, "start": 869.3, "end": 872.3, "text": " Czyli te gigantyczne modele s\u0105 jak suflet.", "tokens": [50814, 37099, 535, 8741, 394, 17466, 716, 4391, 306, 9015, 4207, 46282, 2631, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08729892655422813, "compression_ratio": 1.4340836012861737, "no_speech_prob": 0.005670203361660242}, {"id": 208, "seek": 86030, "start": 872.3, "end": 875.3, "text": " Pr\u00f3ba zmniejszenia go ko\u0144czy si\u0119 katastrof\u0105.", "tokens": [50964, 2114, 812, 4231, 17020, 30295, 14320, 352, 26470, 6522, 3244, 16536, 525, 340, 69, 1611, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08729892655422813, "compression_ratio": 1.4340836012861737, "no_speech_prob": 0.005670203361660242}, {"id": 209, "seek": 86030, "start": 875.3, "end": 878.3, "text": " Albo jak wie\u017ca d\u017c\u0119ga, wyci\u0105gni\u0119cie nawet kilku,", "tokens": [51114, 967, 1763, 4207, 3355, 35075, 274, 1427, 1274, 3680, 11, 4628, 34381, 70, 35938, 4260, 22696, 5128, 5279, 11, 51264], "temperature": 0.0, "avg_logprob": -0.08729892655422813, "compression_ratio": 1.4340836012861737, "no_speech_prob": 0.005670203361660242}, {"id": 210, "seek": 86030, "start": 878.3, "end": 882.3, "text": " ma\u0142o wa\u017cnych klock\u00f3w mo\u017ce spowodowa\u0107 zawalenie si\u0119 ca\u0142ej struktury.", "tokens": [51264, 463, 5249, 27777, 9399, 350, 4102, 3901, 12034, 637, 305, 378, 11445, 28165, 21745, 414, 3244, 47631, 73, 342, 19977, 2598, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08729892655422813, "compression_ratio": 1.4340836012861737, "no_speech_prob": 0.005670203361660242}, {"id": 211, "seek": 86030, "start": 882.3, "end": 884.3, "text": " To jest \u015bwietna analogia.", "tokens": [51464, 1407, 3492, 8299, 39083, 629, 16660, 654, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08729892655422813, "compression_ratio": 1.4340836012861737, "no_speech_prob": 0.005670203361660242}, {"id": 212, "seek": 86030, "start": 884.3, "end": 889.3, "text": " Wygl\u0105da na to, \u017ce wiedza w tych modelach jest rozproszona w bardzo delikatny", "tokens": [51564, 14458, 7191, 26398, 1667, 281, 11, 3561, 46894, 2394, 261, 15180, 2316, 608, 3492, 9544, 1424, 329, 13383, 261, 9034, 1103, 36300, 1634, 51814], "temperature": 0.0, "avg_logprob": -0.08729892655422813, "compression_ratio": 1.4340836012861737, "no_speech_prob": 0.005670203361660242}, {"id": 213, "seek": 88930, "start": 889.3, "end": 892.3, "text": " i g\u0119sty spos\u00f3b po ca\u0142ej sieci.", "tokens": [50364, 741, 290, 1274, 25134, 22904, 714, 47631, 73, 2804, 537, 13, 50514], "temperature": 0.0, "avg_logprob": -0.05436465106432951, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.027689410373568535}, {"id": 214, "seek": 88930, "start": 892.3, "end": 894.3, "text": " Nie ma tam zb\u0119dnych cz\u0119\u015bci.", "tokens": [50514, 12016, 463, 7677, 710, 65, 6298, 9399, 41314, 13, 50614], "temperature": 0.0, "avg_logprob": -0.05436465106432951, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.027689410373568535}, {"id": 215, "seek": 88930, "start": 894.3, "end": 900.3, "text": " Czyli konkluzja autor\u00f3w jest jasna, skala daje niesamowite rezultaty w zadaniach opartych na wiedzy,", "tokens": [50614, 37099, 21428, 2781, 89, 2938, 19510, 3901, 3492, 361, 296, 629, 11, 1110, 5159, 1120, 2884, 48100, 335, 305, 642, 48060, 723, 21398, 261, 42788, 3782, 608, 999, 446, 16384, 1667, 46894, 1229, 11, 50914], "temperature": 0.0, "avg_logprob": -0.05436465106432951, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.027689410373568535}, {"id": 216, "seek": 88930, "start": 900.3, "end": 902.3, "text": " ale ma swoje granice w rozumowaniu.", "tokens": [50914, 6775, 463, 29489, 9370, 573, 261, 48797, 305, 25849, 13, 51014], "temperature": 0.0, "avg_logprob": -0.05436465106432951, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.027689410373568535}, {"id": 217, "seek": 88930, "start": 902.3, "end": 908.3, "text": " A architektura Transformer to prawdopodobnie tylko etap przej\u015bciowy na drodze do czego\u015b zupe\u0142nie nowego.", "tokens": [51014, 316, 3912, 642, 2320, 2991, 27938, 260, 281, 41175, 46684, 996, 2766, 13219, 47634, 8325, 73, 6199, 10089, 1667, 3789, 67, 1381, 360, 36559, 1788, 49922, 586, 6308, 13, 51314], "temperature": 0.0, "avg_logprob": -0.05436465106432951, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.027689410373568535}, {"id": 218, "seek": 88930, "start": 908.3, "end": 912.3, "text": " Zosta\u0142a mi w g\u0142owie jeszcze jedna ostatnia my\u015bl.", "tokens": [51314, 1176, 8638, 5024, 2752, 261, 18117, 13998, 14168, 5232, 629, 32686, 12679, 452, 19212, 13, 51514], "temperature": 0.0, "avg_logprob": -0.05436465106432951, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.027689410373568535}, {"id": 219, "seek": 88930, "start": 912.3, "end": 918.3, "text": " W dodatkach do artyku\u0142u opisano test, w kt\u00f3rym zwi\u0119kszono d\u0142ugo\u015b\u0107 kontekstu, z kt\u00f3rym pracuje model.", "tokens": [51514, 343, 13886, 33525, 608, 360, 594, 874, 5279, 24066, 45477, 3730, 1500, 11, 261, 30120, 11873, 5034, 1694, 89, 8957, 44042, 20746, 7753, 14373, 916, 372, 84, 11, 710, 30120, 22404, 13008, 2316, 13, 51814], "temperature": 0.0, "avg_logprob": -0.05436465106432951, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.027689410373568535}, {"id": 220, "seek": 91830, "start": 918.3, "end": 922.3, "text": " Z tysi\u0105ca do ponad 2 tysi\u0119cy token\u00f3w.", "tokens": [50364, 1176, 38156, 11404, 496, 360, 9224, 345, 568, 38156, 47303, 14862, 3901, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07473450490873154, "compression_ratio": 1.4035087719298245, "no_speech_prob": 0.0072518629021942616}, {"id": 221, "seek": 91830, "start": 922.3, "end": 927.3, "text": " Okaza\u0142o si\u0119, \u017ce bardzo pomog\u0142o to w analizie kodu i artyku\u0142\u00f3w naukowych.", "tokens": [50564, 3477, 12257, 5249, 3244, 11, 3561, 9034, 12991, 664, 5249, 281, 261, 2624, 590, 414, 350, 34873, 741, 594, 874, 5279, 1221, 3901, 35616, 74, 19605, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07473450490873154, "compression_ratio": 1.4035087719298245, "no_speech_prob": 0.0072518629021942616}, {"id": 222, "seek": 91830, "start": 927.3, "end": 929.3, "text": " Co jest logiczne?", "tokens": [50814, 3066, 3492, 9952, 43077, 30, 50914], "temperature": 0.0, "avg_logprob": -0.07473450490873154, "compression_ratio": 1.4035087719298245, "no_speech_prob": 0.0072518629021942616}, {"id": 223, "seek": 91830, "start": 929.3, "end": 934.3, "text": " Zar\u00f3wno kod, jak i prace naukowe maj\u0105 zale\u017cno\u015bci rozci\u0105gni\u0119te na wiele kapit\u00f3w.", "tokens": [50914, 41580, 812, 20944, 350, 378, 11, 4207, 741, 582, 617, 35616, 74, 6880, 26064, 710, 45494, 16438, 9544, 34381, 70, 35938, 975, 1667, 33137, 13816, 270, 3901, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07473450490873154, "compression_ratio": 1.4035087719298245, "no_speech_prob": 0.0072518629021942616}, {"id": 224, "seek": 91830, "start": 934.3, "end": 936.3, "text": " Wi\u0119kszy kontekst pozwala je lepiej wychwyci\u0107.", "tokens": [51164, 30127, 1694, 1229, 14373, 916, 372, 40557, 5159, 1506, 476, 39699, 4628, 339, 9726, 39162, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07473450490873154, "compression_ratio": 1.4035087719298245, "no_speech_prob": 0.0072518629021942616}, {"id": 225, "seek": 91830, "start": 936.3, "end": 937.3, "text": " No w\u0142a\u015bnie.", "tokens": [51264, 883, 14234, 13, 51314], "temperature": 0.0, "avg_logprob": -0.07473450490873154, "compression_ratio": 1.4035087719298245, "no_speech_prob": 0.0072518629021942616}, {"id": 226, "seek": 91830, "start": 937.3, "end": 939.3, "text": " I tu jest zagadka.", "tokens": [51314, 286, 2604, 3492, 27001, 345, 2330, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07473450490873154, "compression_ratio": 1.4035087719298245, "no_speech_prob": 0.0072518629021942616}, {"id": 227, "seek": 91830, "start": 939.3, "end": 945.3, "text": " Ten sam zabieg da\u0142 znacznie mniejszy zysk w przypadku analizy ksi\u0105\u017cek z klasyki literatury.", "tokens": [51414, 9380, 3247, 24838, 20408, 1120, 1221, 15397, 14875, 2766, 39513, 7706, 710, 749, 74, 261, 41955, 2624, 590, 88, 39311, 916, 710, 9671, 5871, 2984, 2733, 267, 2598, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07473450490873154, "compression_ratio": 1.4035087719298245, "no_speech_prob": 0.0072518629021942616}, {"id": 228, "seek": 94530, "start": 945.3, "end": 951.3, "text": " A przecie\u017c wydawa\u0142oby si\u0119, \u017ce to w\u0142a\u015bnie w powie\u015bciach d\u0142ugi kontekst jest absolutnie kluczowy.", "tokens": [50364, 316, 8325, 40082, 25984, 10449, 1221, 13944, 3244, 11, 3561, 281, 14234, 261, 3388, 414, 6199, 608, 44042, 24780, 14373, 916, 372, 3492, 18757, 2766, 9671, 1311, 89, 10089, 13, 50664], "temperature": 0.0, "avg_logprob": -0.0723808315438284, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.0012479274300858378}, {"id": 229, "seek": 94530, "start": 951.3, "end": 953.3, "text": " To jest rzeczywi\u015bcie frapuj\u0105ce.", "tokens": [50664, 1407, 3492, 44922, 283, 4007, 13263, 384, 13, 50764], "temperature": 0.0, "avg_logprob": -0.0723808315438284, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.0012479274300858378}, {"id": 230, "seek": 94530, "start": 953.3, "end": 956.3, "text": " Autorzy podsuwaj\u0105 dwie r\u00f3wnie intryguj\u0105ce mo\u017cliwo\u015bci.", "tokens": [50764, 6049, 284, 1229, 31925, 36824, 11133, 274, 8699, 11416, 14215, 560, 627, 2794, 8555, 384, 30854, 36476, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0723808315438284, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.0012479274300858378}, {"id": 231, "seek": 94530, "start": 956.3, "end": 960.3, "text": " Pierwsza mo\u017ce te ksi\u0105\u017cki, nasze wielkie dzie\u0142a literackie,", "tokens": [50914, 16676, 14358, 2394, 12034, 535, 39311, 2984, 11, 43394, 20570, 22872, 17953, 5024, 2733, 501, 414, 11, 51114], "temperature": 0.0, "avg_logprob": -0.0723808315438284, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.0012479274300858378}, {"id": 232, "seek": 94530, "start": 960.3, "end": 966.3, "text": " zawieraj\u0105 w rzeczywisto\u015bci mniej formalnych, d\u0142ugodystansowych zale\u017cno\u015bci ni\u017c nam si\u0119 wydaje.", "tokens": [51114, 28165, 811, 11133, 261, 26297, 86, 9334, 6199, 39513, 9860, 9399, 11, 274, 34077, 843, 372, 599, 19605, 710, 45494, 16438, 28502, 8835, 3244, 49165, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0723808315438284, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.0012479274300858378}, {"id": 233, "seek": 94530, "start": 966.3, "end": 969.3, "text": " \u017be ich z\u0142o\u017cono\u015b\u0107 le\u017cy gdzie indziej?", "tokens": [51414, 46864, 1893, 710, 5249, 1427, 8957, 7753, 476, 7735, 18922, 1016, 19554, 30, 51564], "temperature": 0.0, "avg_logprob": -0.0723808315438284, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.0012479274300858378}, {"id": 234, "seek": 94530, "start": 969.3, "end": 972.3, "text": " W emocjach, metaforach, pod tekstach?", "tokens": [51564, 343, 28283, 45059, 11, 1131, 2792, 284, 608, 11, 2497, 16624, 372, 608, 30, 51714], "temperature": 0.0, "avg_logprob": -0.0723808315438284, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.0012479274300858378}, {"id": 235, "seek": 97230, "start": 972.3, "end": 977.3, "text": " Mo\u017ce. A nie w skomplikowanej sk\u0142adni rozci\u0105gni\u0119tej na dziesi\u0105tki stron.", "tokens": [50364, 43774, 13, 316, 2838, 261, 1110, 298, 564, 1035, 23066, 73, 1110, 10358, 3722, 9544, 34381, 70, 35938, 975, 73, 1667, 9758, 530, 11404, 83, 2984, 45766, 13, 50614], "temperature": 0.0, "avg_logprob": -0.06866209374533759, "compression_ratio": 1.4417808219178083, "no_speech_prob": 0.17715740203857422}, {"id": 236, "seek": 97230, "start": 977.3, "end": 979.3, "text": " A druga mo\u017cliwo\u015b\u0107?", "tokens": [50614, 316, 4110, 64, 30854, 48847, 30, 50714], "temperature": 0.0, "avg_logprob": -0.06866209374533759, "compression_ratio": 1.4417808219178083, "no_speech_prob": 0.17715740203857422}, {"id": 237, "seek": 97230, "start": 979.3, "end": 986.3, "text": " Druga jest taka, \u017ce gofer mimo swojej gigantycznej skali po prostu nie jest jeszcze na tyle pot\u0119\u017cny,", "tokens": [50714, 2491, 19364, 3492, 28017, 11, 3561, 352, 612, 275, 6934, 29489, 73, 8741, 394, 17466, 11794, 1110, 5103, 714, 19518, 2838, 3492, 14168, 1667, 39293, 1847, 1274, 1427, 1634, 11, 51064], "temperature": 0.0, "avg_logprob": -0.06866209374533759, "compression_ratio": 1.4417808219178083, "no_speech_prob": 0.17715740203857422}, {"id": 238, "seek": 97230, "start": 986.3, "end": 991.3, "text": " by te subtelne literackie zale\u017cno\u015bci wychwyci\u0107 i wykorzysta\u0107.", "tokens": [51064, 538, 535, 7257, 338, 716, 2733, 501, 414, 710, 45494, 16438, 4628, 339, 9726, 39162, 741, 43606, 49590, 2162, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06866209374533759, "compression_ratio": 1.4417808219178083, "no_speech_prob": 0.17715740203857422}, {"id": 239, "seek": 97230, "start": 991.3, "end": 995.3, "text": " I to pozostawia nas z fundamentalnym pytaniem.", "tokens": [51314, 286, 281, 21281, 555, 34953, 5382, 710, 8088, 12996, 25878, 282, 4907, 13, 51514], "temperature": 0.0, "avg_logprob": -0.06866209374533759, "compression_ratio": 1.4417808219178083, "no_speech_prob": 0.17715740203857422}, {"id": 240, "seek": 97230, "start": 995.3, "end": 996.3, "text": " Mianowicie.", "tokens": [51514, 376, 952, 305, 28434, 13, 51564], "temperature": 0.0, "avg_logprob": -0.06866209374533759, "compression_ratio": 1.4417808219178083, "no_speech_prob": 0.17715740203857422}, {"id": 241, "seek": 97230, "start": 996.3, "end": 1001.3, "text": " Czy nasze najbardziej z\u0142o\u017cone dzie\u0142a kultury s\u0105 w rzeczywisto\u015bci prostsze ni\u017c s\u0105dzimy?", "tokens": [51564, 19832, 43394, 41857, 710, 5249, 1427, 546, 17953, 5024, 350, 723, 2598, 9015, 261, 26297, 86, 9334, 6199, 10293, 82, 1381, 28502, 9015, 28168, 13189, 30, 51814], "temperature": 0.0, "avg_logprob": -0.06866209374533759, "compression_ratio": 1.4417808219178083, "no_speech_prob": 0.17715740203857422}, {"id": 242, "seek": 100130, "start": 1001.3, "end": 1007.3, "text": " Czy te\u017c nasza sztuczna inteligencja jest wci\u0105\u017c \u015blepa na prawdziw\u0105 natur\u0119 ich z\u0142o\u017cono\u015bci?", "tokens": [50364, 19832, 9516, 5382, 2394, 262, 2682, 1311, 35458, 24777, 3213, 34056, 3492, 261, 537, 27242, 8299, 306, 4306, 1667, 41175, 3992, 86, 1611, 26389, 1274, 1893, 710, 5249, 1427, 8957, 6199, 30, 50664], "temperature": 0.0, "avg_logprob": -0.0417076587677002, "compression_ratio": 0.9897959183673469, "no_speech_prob": 0.09710732847452164}], "language": "pl"}