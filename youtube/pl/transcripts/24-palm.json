{"text": " Jakby to by\u0142o, gdyby narz\u0119dzie, z kt\u00f3rego korzystamy na co dzie\u0144, potrafi\u0142o nie tylko, no wiesz, odpowiedzie\u0107 na pytanie, ale te\u017c wyja\u015bni\u0107 jaki\u015b subtelny \u017cart, rozwi\u0105za\u0107 zadanie z matmy pokazuj\u0105c krok po kroku jak my\u015bli, a na koniec napisa\u0107 i nawet poprawi\u0107 kod. Brzmi troch\u0119 jak science fiction, ale dzisiaj bierzemy na warsztat prac\u0119 badawcz\u0105, kt\u00f3ra opisa\u0142a model zbli\u017caj\u0105cy nas w\u0142a\u015bnie do tej wizji. Dok\u0142adnie tak. Przyjrzymy si\u0119 absolutnie kluczowej pracy od Google. Chodzi o model j\u0119zykowy PALM, czyli Pathways Language Model. Nasza misja jest prosta. Chcemy zrozumie\u0107, dlaczego ten artyku\u0142 narobi\u0142 tyle szumu. Co sprawi\u0142o, \u017ce ten konkretny model by\u0142 a\u017c tak du\u017cym krokiem naprz\u00f3d. Skupimy si\u0119 na jego, powiedzmy, bezprecedensowej skali, ale przede wszystkim na technologii, kt\u00f3ra t\u0119 skal\u0119 w og\u00f3le umo\u017cliwi\u0142a, no i na jego naprawd\u0119, naprawd\u0119 zaskakuj\u0105cych zdolno\u015bciach. Dobrze. To roz\u0142\u00f3\u017cmy to na czynniki pierwsze. Pierwsza liczba, kt\u00f3ra rzuca si\u0119 w oczy, kt\u00f3ra trafia do nag\u0142\u00f3wk\u00f3w, to 540 miliard\u00f3w parametr\u00f3w. To jest astronomiczna liczba, ale z tego, co czyta\u0142em w artykule, samo wi\u0119cej to nie jest ca\u0142a historia, prawda? Absolutnie nie. Sama wielko\u015b\u0107 to jedno, ale prawdziwy prze\u0142om tkwi\u0142 w tym, jak t\u0119 skal\u0119 osi\u0105gni\u0119to. I tu na scen\u0119 wkracza system o nazwie Pathways. Czekaj, Pathways. To brzmi troch\u0119 jak nazwa kodowa z jakiego\u015b filmu Szpiegowskiego. O co w tym chodzi\u0142o? W pewnym sensie tak. To by\u0142a wierzch zupe\u0142nie nowatorska architektura. Chodzi\u0142o o trenowanie jednego, ogromnego modelu nad tysi\u0105cach procesor\u00f3w. Jednocze\u015bnie. W tym konkretnym przypadku to by\u0142o ponad 6 tysi\u0119cy chip\u00f3w TPU V4. Pomy\u015bli o tym tak. Starsze metody, jak np. pipeline parallelizm, dzia\u0142a\u0142y troch\u0119 jak linia monta\u017cowa w fabryce. Jeden procesor robi\u0142 swoj\u0105 cz\u0119\u015b\u0107, przekazywa\u0142 dalej i cz\u0119sto musia\u0142 czeka\u0107. To powodowa\u0142o co\u015b, co badacze, w szczerze m\u00f3wi\u0105c do\u015b\u0107 z\u0142o\u015bliwie, nazwali p\u0119cherzykiem bezczynno\u015bci, czyli bubble of idleness. P\u0119cherzyk bezczynno\u015bci. Czyli po prostu marnowa\u0142a si\u0119 cenna moc obliczeniowa. Bo cz\u0119\u015b\u0107 systemu jakby czeka\u0142a na swoj\u0105 kolej. Troch\u0119 jak na linii monta\u017cowej, gdzie jeden pracownik za\u0142o\u017cy ko\u0142o i czeka, a\u017c drugi sko\u0144czy montowa\u0107 drzwi zamiast pracowa\u0107 r\u00f3wnolegle. Idealna analogia. W\u0142a\u015bnie o to chodzi\u0142o, \u017ceby pozby\u0107 si\u0119 tego p\u0119cherzyka. Pathways pozwoli\u0142o na znacznie bardziej efektywn\u0105 r\u00f3wnoleg\u0142\u0105 prac\u0119. Wszystkie czipy by\u0142y obci\u0105\u017cone niemal ca\u0142y czas. I \u017ceby zmierzy\u0107, jak dobrze im to wysz\u0142o, wprowadzi\u0142y now\u0105, bardzo wymown\u0105 miar\u0119. Model Flops Utilization. W skr\u00f3cie MFU. Czyli taki wska\u017anik, kt\u00f3ry m\u00f3wi, jaki procent teoretycznej mocy jest faktycznie wykorzystywany w praktyce. Taki no test wydajno\u015bci dla superkomputer\u00f3w. Dok\u0142adnie. I wyniki by\u0142y naprawd\u0119 imponuj\u0105ce. Palm osi\u0105gn\u0105\u0142 MFU na poziomie 46,26%. Dla por\u00f3wnania wcze\u015bniejsze du\u017ce modele, jak GPT-3, to by\u0142o w okolicach powiedzmy 21%. Czyli ponad dwukrotny skok. Tak. To ponad dwukrotny skok wydajno\u015bci. Dlatego m\u00f3wimy, \u017ce to by\u0142 wyczyn in\u017cynieryjny na r\u00f3wni z naukowym. Nie chodzi\u0142o tylko o zbudowanie wi\u0119kszego modelu, ale o wymy\u015blenie, jak zrobi\u0107 to w spos\u00f3b radykalnie bardziej efektywny. Ok. Czyli mamy wydajniejszy spos\u00f3b na zbudowanie wi\u0119kszego m\u00f3zgu. To teraz kluczowe pytanie. Czym ten m\u00f3zg nakarmiono? Jak wygl\u0105da\u0142 jego zbi\u00f3r danych? By\u0142 gigantyczny. M\u00f3wimy o 780 miliardach token\u00f3w, czyli fragment\u00f3w s\u0142\u00f3w, naprawd\u0119 wysokiej jako\u015bci tekstu. W menu znalaz\u0142y si\u0119 przefiltrowane strony internetowe, ksi\u0105\u017cki, ca\u0142a angielska wikipedia, kot z githaba, a nawet rozmowy z medi\u00f3w spo\u0142eczno\u015bciowych. Jest tu jednak jeden, ale bardzo wa\u017cny szczeg\u00f3\u0142, kt\u00f3ry b\u0119dzie rzutowa\u0142 na p\u00f3\u017aniejsze wyniki. Domy\u015blam si\u0119, j\u0119zyk? W\u0142a\u015bnie. Tylko oko\u0142o 22% tych danych by\u0142o w j\u0119zykach innych ni\u017c angielski i to jest kluczowy kontekst dla zdolno\u015bci, kt\u00f3re za chwil\u0119 om\u00f3wimy. Czyli mamy przepis na giganta. Przejd\u017amy wi\u0119c do najciekawszego. Co ten nowy, super wydajnie zbudowany model, potrafi\u0142 zrobi\u0107, czego inne nie potrafi\u0142y. Artyku\u0142 m\u00f3wi o prze\u0142omowych zdolno\u015bciach, kt\u00f3re pojawi\u0142y si\u0119 wraz z eskal\u0105. Na pierwszy ogie\u0144 idzie rozumowanie. I tu kluczowa okaza\u0142a si\u0119 technika, kt\u00f3ra dzisiaj jest standardem, ale wtedy to by\u0142o objawienie. Chain of Thought Prompting. To jest, wiesz, bardzo prosta w za\u0142o\u017ceniu idea. Zamiast oczekiwa\u0107 od modelu natychmiastowej ostatecznej odpowiedzi na z\u0142o\u017cone pytanie. M\u00f3wi mu si\u0119, chwila, zanim odpowiesz, poka\u017c mi, jak do tego doszed\u0142e\u015b. Troch\u0119 jak nauczyciel w szkole, kt\u00f3ry prosi, \u017ceby pokaza\u0107 obliczenia na marginesie, a nie tylko poda\u0107 wynik. Dok\u0142adnie tak. W pracy podano \u015bwietny przyk\u0142ad zadania matematycznego, co\u015b w stylu. Roger mia\u0142 pi\u0119\u0107 pi\u0142ek, kupi\u0142 dwie kolejne puszki w ka\u017cdej po trzy pi\u0142ki. Ile ma ich teraz? I zamiast od razu odpowiedzie\u0107 11, model z instrukcj\u0105 Chain of Thought najpierw generuje sw\u00f3j tok my\u015blenia. Roger zacz\u0105\u0142 z pi\u0119\u0107 pi\u0142kami, dwie puszki po trzy pi\u0142ki to sze\u015b\u0107 pi\u0142ek, pi\u0119\u0107 plus sze\u015b\u0107, jedyna\u015bcie. Ta prosta zmiana mia\u0142a kolosalne znaczenie. A\u017c tak? Przecie\u017c to tylko zmiana sposob\u00f3w, jaki zadajemy pytanie. Ta zmiana w po\u0142\u0105czeniu z gigantyczn\u0105 skal\u0105 modelu PLM pozwoli\u0142a mu rozwi\u0105zywa\u0107 zadania wymagaj\u0105ce rozumowania na poziomie modeli, kt\u00f3re by\u0142y specjalnie do tego celu dostrajane, czyli przechodzi\u0142y fine tuning. To by\u0142 wielki post\u0119p, bo pokaza\u0142, \u017ce nie trzeba tworzy\u0107 specjalistycznego modelu do matematyki. Wystarczy jakby nauczy\u0107 uniwersalny model, jak ma my\u015ble\u0107 na g\u0142os. To jest znacznie bardziej elastyczne i pot\u0119\u017cne podej\u015bcie. Niesamolite. To pokazuje, jak wa\u017cny jest nie tylko to, co model wie, ale te\u017c jak go odpytujemy. A co z zadaniami, kt\u00f3re wydaj\u0105 si\u0119 jeszcze bardziej ludzkie? Ten przyk\u0142ad z wyja\u015bnianiem \u017cart\u00f3w w artykule jest absolutnie genialny. Tak, to jeden z tych przyk\u0142ad\u00f3w, kt\u00f3re naprawd\u0119 zapadaj\u0105 w pami\u0119\u0107. \u017bart brzmia\u0142 mniej wi\u0119cej tak. S\u0142ysza\u0142e\u015b, \u017ce Google zatrudni\u0142o elokw\u0119tnego wieloryba do zespo\u0142u TPU? Pokaza\u0142 im, jak komunikowa\u0107 si\u0119 mi\u0119dzy dwoma r\u00f3\u017cnymi podz. Przyznam, \u017ce to do\u015b\u0107 niszowy, bardzo techniczny \u017cart. I w\u0142a\u015bnie dlatego wyja\u015bnienie, kt\u00f3re wygenerowa\u0142 palm jest tak zdumiewaj\u0105ce. Model poprawnie identyfikowa\u0142 gr\u0119 s\u0142\u00f3w. Zrozumia\u0142, \u017ce angielskie s\u0142owo pod oznacza zar\u00f3wno grup\u0119 wieloryb\u00f3w, jak i grup\u0119 po\u0142\u0105czonych procesor\u00f3w TPU u\u017cywanych w Google. Zrozumia\u0142 oba konteksty biologiczny i technologiczny i precyzyjnie wyja\u015bni\u0142, na czym polega dowcip. To pokazuje rozumienie na poziomie, kt\u00f3rego wcze\u015bniej w tej skali po prostu nie obserwowano. Ten \u017cart z wielorybem jest genialny, bo pokazuje, \u017ce model rozumie nie tylko s\u0142owa, ale i bardzo niszowy, techniczny kontekst. To prowadzi mnie do pytania o granicy jego mo\u017cliwo\u015bci. Czy on po prostu staje si\u0119 coraz lepszy w miar\u0119, jak go powi\u0119kszamy? Czy dzieje si\u0119 tam co\u015b dziwniejszego? W artykule natkn\u0105\u0142em si\u0119 na termin nieci\u0105g\u0142e ulepszenia. I to jest sedno jednego z najwa\u017cniejszych odkry\u0107 naukowych tej pracy. Big Bench to zbi\u00f3r naprawd\u0119 trudnych, cz\u0119sto abstrakcyjnych zada\u0144 j\u0119zykowych, zaprojektowanych, by testowa\u0107 granice mo\u017cliwo\u015bci modeli. A nieci\u0105g\u0142e ulepszenia oznaczaj\u0105, \u017ce wydajno\u015b\u0107 modelu nie ro\u015bnie liniowo. Czasem po przekroczeniu pewnego progu skali pojawia si\u0119 nag\u0142y skokowy wzrost umiej\u0119tno\u015bci. Czyli to nie jest tak, \u017ce model staje si\u0119 po prostu o 10% lepszy, gdy jest o 10% wi\u0119kszy. Czasem nast\u0119puje gwa\u0142towna zmiana, jakby nagle co\u015b zaskoczy\u0142o. W\u0142a\u015bnie. Wyobra\u017a sobie, \u017ce uczysz kogo\u015b obcego j\u0119zyka. D\u0142ugo, d\u0142ugo myli gramatyk\u0119, buduje proste zdania, a potem nagle, pewnego dnia, zaczyna opowiaga\u0107 do wcipy i rozumie\u0107 sarkazm. To nie jest liniowy post\u0119p. W\u0142a\u015bnie to zaobserwowali badacze. W zadaniu z dopasowywaniem angielskich przys\u0142owim, czyli English Proverbs, mniejszy 62 miliardowy model by\u0142 na porzomie losowego zgadywania. Oko\u0142o 25% skuteczno\u015bci, a najwi\u0119kszy 540 miliardowy prawie 90%. Ta umiej\u0119tno\u015b\u0107 nie uros\u0142a, ona si\u0119 po prostu pojawi\u0142a. To by\u0142o fundamentalne odkrycie, kt\u00f3re pokaza\u0142o, \u017ce sama skala mo\u017ce odblokowywa\u0107 zupe\u0142nie nowe, jako\u015bciowo inne zdolno\u015bci. Dobra, czyli model jest filozofem, matematykiem i komikiem. Ale czy potrafi zrobi\u0107 co\u015b praktycznego? W danych treningowych by\u0142 te\u017c kod. Potrafi by\u0107 programist\u0105? A to i jakim? Badacze wzi\u0119li podstawowy model Palm i zrobili dodatkowy fine-tuning na danych zawieraj\u0105cych wy\u0142\u0105cznie kod. Tak powsta\u0142 Palm Coder. Ju\u017c bazowy model by\u0142 niez\u0142y, ale Palm Coder osi\u0105gn\u0105\u0142 wyniki okre\u015blane jako State of the Art, czyli najlepsze w swojej dziedzinie w tamtym czasie. Znakomicie sobie radzi\u0142 zgenerowaniem kod\u00f3w w Pajtonie na podstawie opis\u00f3w te\u015bcie, humanival, ale potrafi\u0142 te\u017c co\u015b jeszcze bardziej imponuj\u0105cego \u2013 naprawia\u0107 b\u0142\u0119dy w istniej\u0105cym kodzie. Naprawia\u0107 b\u0142\u0119dy. Ok, to ju\u017c brzmi jak asystent programisty, kt\u00f3rego ka\u017cdy by chcia\u0142 mie\u0107. Ale czy to na pewno jest przejaw inteligencji, a mo\u017ce to po prostu statystyczne powielanie najcz\u0119stszych wzorc\u00f3w z GitHub'a bez \u017cadnego zrozumienia, dlaczego dana poprawka jest dobra? To jest \u015bwietne pytanie, kt\u00f3re dotyka sed na debaty o tych modelach. Z jednej strony mo\u017cna argumentowa\u0107, \u017ce to niezwykle zaawansowana mi mikra, ale z drugiej sp\u00f3jrzmy na przyk\u0142ad z artyku\u0142u. Model dosta\u0142 fragment kodu w j\u0119zyku C, kt\u00f3ry mia\u0142 b\u0142\u0119dy uniemo\u017aliwiaj\u0105ce kompilacj\u0119. PLM Coder nie tylko naprawi\u0142 te b\u0142\u0119dy, ale przy okazji poprawi\u0142 te\u017c styl kodu, na przyk\u0142ad po\u0142\u0105czy\u0142 deklaracj\u0119 kilkuzmiennych w jedn\u0105 linijk\u0119, co jest uznawany za dobr\u0105 praktyk\u0119. Czyli nie tylko sprawi\u0142, \u017ce kod zacz\u0105\u0142 dzia\u0142a\u0107, ale te\u017c, \u017ce wygl\u0105da\u0142 lepiej? Tak, to sugeruje, \u017ce uczy\u0142 si\u0119 nie tylko czystej sk\u0142adni j\u0119zyka, ale te\u017c pewnych konwencji, wzorc\u00f3w i stylu programowania, preferowanego przez ludzi. Niezale\u017cnie od tego, czy nazwiemy to zrozumieniem, czy nie, z praktycznego punktu widzenia, jest to niezwykle u\u017cyteczna zdolno\u015b\u0107. Dobrze, to wszystko brzni imponuj\u0105co, ale mam jedno ale. M\u00f3wi\u0142a\u015b na pocz\u0105tku, \u017ce dane by\u0142y w przyt\u0142aczaj\u0105cej wi\u0119kszo\u015bci po angielsku. Ponad 78% ca\u0142o\u015bci. Czy to nie oznacza, \u017ce stworzyli\u015bmy genialnego lingwist\u0119, kt\u00f3ry tak naprawd\u0119 m\u00f3wi p\u0142ynnie tylko w jednym j\u0119zyku, a reszt\u0119 ledwo nowie\u017c duka? Logika podpowiada\u0142aby, \u017ce tak. I tu dochodzimy do kolejnego zaskoczenia z tej pracy. Mimo tak ogromnej dysproporcji w danych, Palm radzi\u0142 sobie zaskakuj\u0105co dobrze w zadaniach wieloj\u0119zycznych. W niekt\u00f3rych przypadkach pokonywa\u0142 nawet modele, kt\u00f3re by\u0142y specjalnie trenowane do t\u0142umacze\u0144, np. w t\u0142umaczeniu z rumu\u0144skiego na angielski. Zaskakuj\u0105ce, ale pewnie jest tu jaki\u015b haczyk. Oczywi\u015bcie, kluczowe zastrze\u017cenie jest takie, \u017ce model by\u0142 znacznie, znacznie lepszy w t\u0142umaczeniu na angielski, ni\u017c z angielskiego na inne j\u0119zyki. I to jest bezpo\u015brednie odbicie sk\u0142adu danych treningowych. Poniewa\u017c angielski by\u0142 tak dominuj\u0105cy w jego, powiedzmy, diecie, model nauczy\u0142 si\u0119 go generowa\u0107 na absolutnie najwy\u017cszym poziomie. Sta\u0142 si\u0119 dla niego j\u0119zykiem docelowym punktem odniesienia. To idealnie pokazuje, jak fundamentalnie sk\u0142ad danych treningowych kszta\u0142tuje ostateczne zdolno\u015bci i dziwactwa modelu. Rozumiem. Czyli mamy ten ogromny model, wytrenowany z prze\u0142omow\u0105 wydajno\u015bci\u0105, kt\u00f3ry potrafi rozumowa\u0107 krok po kroku, opowiada\u0107 do wcipy, pisa\u0107 kod i ca\u0142kiem nie\u017ale t\u0142umaczy\u0107, mimo \u017ce uczy\u0142 si\u0119 g\u0142\u00f3wnie z jednego j\u0119zyka. Wszystko to zdaje si\u0119 krzycze\u0107, bigger is better. Im wi\u0119kszy model, tym lepiej. To chyba by\u0142a g\u0142\u00f3wna konkluzja w tamtym czasie. Tak, to by\u0142 dominuj\u0105cy paradygmat. Skala wydawa\u0142a si\u0119 kr\u00f3low\u0105, ale co fascynuj\u0105ce, sam artyku\u0142 ko\u0144czy si\u0119 wa\u017cnym, otwartym pytaniem, kt\u00f3re chwil\u0119 p\u00f3\u017aniej sta\u0142o si\u0119 centralnym punktem dyskusji w ca\u0142ej bran\u017cy. G\u0142\u00f3wnie za spraw\u0105 publikacji na temat innego modelu, Chinchilla. Jakie to by\u0142o pytanie? Pytanie brzmi. Maj\u0105c do dyspozycji okre\u015blon\u0105, ograniczon\u0105 moc obliczeniow\u0105, a ona zawsze jest ograniczona, co jest lepsz\u0105 strategi\u0105. Czy zbudowanie absolutnie gigantycznego modelu i wytrenowanie go na powiedzmy jednym cyklu danych, tak jak zrobiono z palm, czy mo\u017ce lepiej jest zbudowa\u0107 nieco mniejszy model, ale za to trenowa\u0107 go na znacznie, znacznie wi\u0119ksze ilo\u015bci danych. Czyli co jest wa\u017cniejsze? Rozmiar m\u00f3zgu czy ilo\u015b\u0107 przeczytanych ksi\u0105\u017cek? I jaka jest odpowied\u017a? P\u00f3\u017aniejsze badania, w\u0142a\u015bnie te dotycz\u0105ce modelu Chinchilla, mocno zasugerowa\u0142y, \u017ce wiele ogromnych modeli, w tym palm, mog\u0142o by\u0107 niedotrenowanych. Innymi s\u0142owy by\u0142y zbyt du\u017ce w stosunku do ilo\u015bci danych, kt\u00f3re przetworzy\u0142y w trakcie treningu. Okaza\u0142o si\u0119, \u017ce mniejszy model, ale karmiony danymi przez d\u0142u\u017cszy czas, mo\u017ce osi\u0105gn\u0105\u0107 lepsze wyniki ni\u017c gigant, kt\u00f3ry tych danych tylko skosztowa\u0142. Czyli wy\u015bcig na sam\u0105 wielko\u015b\u0107 modelu, na czyst\u0105 liczb\u0119 parametr\u00f3w, mo\u017ce nie by\u0107 jedyn\u0105, a nawet nie najlepsz\u0105 drog\u0105 naprz\u00f3d. Dok\u0142adnie. To by\u0142 bardzo wa\u017cny moment odsze\u017awienia dla ca\u0142ej dziedziny. Zrozumiano, \u017ce istnieje pewna optymalna r\u00f3wnowaga mi\u0119dzy rozmiarem modelu a ilo\u015bci\u0105 danych treningowych. Nie chodzi tylko o to, by budowa\u0107 najwi\u0119ksze katedry, ale by robi\u0107 to z odpowiedniej ilo\u015bci cegie\u0142. A wi\u0119c nast\u0119pny prze\u0142om mo\u017ce nie polega\u0107 na stworzeniu jeszcze wi\u0119kszego, trillionowego modelu, ale na m\u0105drzejszym jego trenowaniu i znalezieniu tego z\u0142otego \u015brodka. To pozostawia nas z niezwykle ciekawym pytaniem na przysz\u0142o\u015b\u0107. Jakie nowe, jeszcze bardziej nieprzewidywalne zdolno\u015bci wy\u0142oni\u0105 si\u0119, gdy w ko\u0144cu znajdziemy idealn\u0105 r\u00f3wnowag\u0119 mi\u0119dzy rozmiarem sztucznej inteligencji, a ogromem wiedzy, z kt\u00f3rej si\u0119 ona uczy.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.0, "text": " Jakby to by\u0142o, gdyby narz\u0119dzie, z kt\u00f3rego korzystamy na co dzie\u0144, potrafi\u0142o nie tylko, no wiesz, odpowiedzie\u0107 na pytanie,", "tokens": [50364, 15029, 2322, 281, 14811, 11, 28405, 2322, 6714, 89, 42643, 11, 710, 46951, 14784, 36049, 7804, 1667, 598, 47568, 11, 1847, 10437, 72, 5249, 2838, 13219, 11, 572, 261, 15347, 11, 24314, 22078, 1667, 36610, 11, 50714], "temperature": 0.0, "avg_logprob": -0.10907903600622107, "compression_ratio": 1.4758620689655173, "no_speech_prob": 0.0014961546985432506}, {"id": 1, "seek": 0, "start": 7.0, "end": 17.0, "text": " ale te\u017c wyja\u015bni\u0107 jaki\u015b subtelny \u017cart, rozwi\u0105za\u0107 zadanie z matmy pokazuj\u0105c krok po kroku jak my\u015bli, a na koniec napisa\u0107 i nawet poprawi\u0107 kod.", "tokens": [50714, 6775, 9516, 4628, 2938, 1788, 3722, 2162, 34721, 7257, 338, 1634, 19625, 446, 11, 9544, 18234, 35873, 42788, 7155, 710, 3803, 2226, 13010, 921, 44733, 350, 31621, 714, 45909, 5279, 4207, 452, 15350, 11, 257, 1667, 5897, 35733, 9296, 3837, 2162, 741, 22696, 1665, 5131, 12757, 350, 378, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10907903600622107, "compression_ratio": 1.4758620689655173, "no_speech_prob": 0.0014961546985432506}, {"id": 2, "seek": 0, "start": 17.0, "end": 25.0, "text": " Brzmi troch\u0119 jak science fiction, ale dzisiaj bierzemy na warsztat prac\u0119 badawcz\u0105, kt\u00f3ra opisa\u0142a model zbli\u017caj\u0105cy nas w\u0142a\u015bnie do tej wizji.", "tokens": [51214, 1603, 89, 3057, 24926, 4207, 3497, 13266, 11, 6775, 25772, 272, 34602, 3633, 1667, 13718, 2682, 267, 22404, 1274, 272, 1538, 86, 3689, 1611, 11, 19456, 999, 3837, 5024, 2316, 710, 32117, 1427, 11133, 1344, 5382, 14234, 360, 12573, 40808, 4013, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10907903600622107, "compression_ratio": 1.4758620689655173, "no_speech_prob": 0.0014961546985432506}, {"id": 3, "seek": 2500, "start": 25.0, "end": 34.0, "text": " Dok\u0142adnie tak. Przyjrzymy si\u0119 absolutnie kluczowej pracy od Google. Chodzi o model j\u0119zykowy PALM, czyli Pathways Language Model.", "tokens": [50364, 29768, 10358, 2766, 991, 13, 39590, 73, 13047, 2226, 3244, 18757, 2766, 9671, 1311, 89, 21091, 35591, 3611, 3329, 13, 761, 14543, 277, 2316, 49055, 74, 10089, 46390, 44, 11, 16591, 21914, 942, 24445, 17105, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09457379199088888, "compression_ratio": 1.2951541850220265, "no_speech_prob": 0.0821930393576622}, {"id": 4, "seek": 2500, "start": 34.0, "end": 45.0, "text": " Nasza misja jest prosta. Chcemy zrozumie\u0107, dlaczego ten artyku\u0142 narobi\u0142 tyle szumu. Co sprawi\u0142o, \u017ce ten konkretny model by\u0142 a\u017c tak du\u017cym krokiem naprz\u00f3d.", "tokens": [50814, 16151, 2394, 3346, 2938, 3492, 582, 8638, 13, 761, 384, 2226, 710, 27857, 449, 414, 2162, 11, 37873, 39329, 2064, 594, 874, 5279, 1221, 6714, 19293, 1221, 39293, 7870, 30034, 13, 3066, 22734, 72, 5249, 11, 3561, 2064, 36500, 1634, 2316, 16673, 48134, 991, 21783, 4199, 45909, 26116, 9296, 19390, 17081, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09457379199088888, "compression_ratio": 1.2951541850220265, "no_speech_prob": 0.0821930393576622}, {"id": 5, "seek": 4500, "start": 45.0, "end": 56.0, "text": " Skupimy si\u0119 na jego, powiedzmy, bezprecedensowej skali, ale przede wszystkim na technologii, kt\u00f3ra t\u0119 skal\u0119 w og\u00f3le umo\u017cliwi\u0142a, no i na jego naprawd\u0119, naprawd\u0119 zaskakuj\u0105cych zdolno\u015bciach.", "tokens": [50364, 7324, 1010, 13189, 3244, 1667, 26542, 11, 27617, 2226, 11, 10782, 3712, 1232, 694, 21091, 1110, 5103, 11, 6775, 44786, 30481, 1667, 1537, 1132, 5597, 11, 19456, 32489, 16890, 1274, 261, 29229, 1105, 78, 1427, 2081, 6253, 5024, 11, 572, 741, 1667, 26542, 20970, 11, 20970, 710, 3863, 514, 13263, 31306, 16221, 401, 16438, 608, 13, 50914], "temperature": 0.0, "avg_logprob": -0.06397160968265019, "compression_ratio": 1.5113268608414239, "no_speech_prob": 0.4598692059516907}, {"id": 6, "seek": 4500, "start": 56.0, "end": 73.0, "text": " Dobrze. To roz\u0142\u00f3\u017cmy to na czynniki pierwsze. Pierwsza liczba, kt\u00f3ra rzuca si\u0119 w oczy, kt\u00f3ra trafia do nag\u0142\u00f3wk\u00f3w, to 540 miliard\u00f3w parametr\u00f3w. To jest astronomiczna liczba, ale z tego, co czyta\u0142em w artykule, samo wi\u0119cej to nie jest ca\u0142a historia, prawda?", "tokens": [50914, 29679, 13503, 13, 1407, 9544, 1221, 812, 1427, 2226, 281, 1667, 6430, 26384, 9850, 45994, 13, 16676, 14358, 2394, 6169, 89, 4231, 11, 19456, 367, 11728, 496, 3244, 261, 277, 6522, 11, 19456, 944, 22054, 360, 17096, 1221, 3901, 23849, 11, 281, 1025, 5254, 1962, 72, 515, 3901, 6220, 27965, 3901, 13, 1407, 3492, 26302, 17946, 629, 6169, 89, 4231, 11, 6775, 710, 8627, 11, 598, 6430, 1328, 11126, 261, 594, 874, 74, 2271, 11, 36422, 26004, 281, 2838, 3492, 1335, 5024, 18385, 11, 43607, 30, 51764], "temperature": 0.0, "avg_logprob": -0.06397160968265019, "compression_ratio": 1.5113268608414239, "no_speech_prob": 0.4598692059516907}, {"id": 7, "seek": 7300, "start": 73.0, "end": 85.0, "text": " Absolutnie nie. Sama wielko\u015b\u0107 to jedno, ale prawdziwy prze\u0142om tkwi\u0142 w tym, jak t\u0119 skal\u0119 osi\u0105gni\u0119to. I tu na scen\u0119 wkracza system o nazwie Pathways.", "tokens": [50364, 5813, 2308, 2766, 2838, 13, 318, 2404, 20570, 4093, 7753, 281, 5232, 1771, 11, 6775, 41175, 3992, 9726, 8325, 1221, 298, 256, 74, 6253, 1221, 261, 8107, 11, 4207, 32489, 16890, 1274, 3003, 11404, 70, 35938, 1353, 13, 286, 2604, 1667, 4191, 1274, 261, 74, 12080, 2394, 1185, 277, 20151, 8699, 21914, 942, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07616115353771091, "compression_ratio": 1.306930693069307, "no_speech_prob": 0.04295159876346588}, {"id": 8, "seek": 7300, "start": 85.0, "end": 91.0, "text": " Czekaj, Pathways. To brzmi troch\u0119 jak nazwa kodowa z jakiego\u015b filmu Szpiegowskiego. O co w tym chodzi\u0142o?", "tokens": [50964, 383, 19878, 1805, 11, 21914, 942, 13, 1407, 738, 89, 3057, 24926, 4207, 20151, 4151, 350, 378, 5528, 710, 4207, 12200, 1788, 2007, 84, 24699, 9144, 70, 1509, 42349, 13, 422, 598, 261, 8107, 23998, 5249, 30, 51264], "temperature": 0.0, "avg_logprob": -0.07616115353771091, "compression_ratio": 1.306930693069307, "no_speech_prob": 0.04295159876346588}, {"id": 9, "seek": 9100, "start": 92.0, "end": 101.0, "text": " W pewnym sensie tak. To by\u0142a wierzch zupe\u0142nie nowatorska architektura. Chodzi\u0142o o trenowanie jednego, ogromnego modelu nad tysi\u0105cach procesor\u00f3w.", "tokens": [50414, 343, 47160, 4199, 2923, 414, 991, 13, 1407, 23936, 261, 34602, 339, 49922, 586, 3391, 2330, 3912, 642, 2320, 2991, 13, 761, 14543, 5249, 277, 23136, 22028, 5232, 11858, 11, 34416, 298, 11858, 2316, 84, 12617, 38156, 11404, 66, 608, 17565, 284, 3901, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12394517176860087, "compression_ratio": 1.2393617021276595, "no_speech_prob": 0.5870528221130371}, {"id": 10, "seek": 9100, "start": 101.0, "end": 107.0, "text": " Jednocze\u015bnie. W tym konkretnym przypadku to by\u0142o ponad 6 tysi\u0119cy chip\u00f3w TPU V4.", "tokens": [50864, 27076, 26694, 1381, 12221, 13, 343, 8107, 36500, 12996, 41955, 281, 14811, 9224, 345, 1386, 38156, 47303, 11409, 3901, 314, 8115, 691, 19, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12394517176860087, "compression_ratio": 1.2393617021276595, "no_speech_prob": 0.5870528221130371}, {"id": 11, "seek": 10700, "start": 108.0, "end": 116.0, "text": " Pomy\u015bli o tym tak. Starsze metody, jak np. pipeline parallelizm, dzia\u0142a\u0142y troch\u0119 jak linia monta\u017cowa w fabryce.", "tokens": [50414, 430, 8488, 15350, 277, 8107, 991, 13, 20957, 1381, 1131, 843, 11, 4207, 33808, 13, 15517, 8952, 590, 76, 11, 37903, 6825, 24926, 4207, 22896, 654, 8143, 18264, 5528, 261, 5355, 627, 384, 13, 50814], "temperature": 0.0, "avg_logprob": -0.0902516448382035, "compression_ratio": 1.3643724696356276, "no_speech_prob": 0.5091810822486877}, {"id": 12, "seek": 10700, "start": 116.0, "end": 130.0, "text": " Jeden procesor robi\u0142 swoj\u0105 cz\u0119\u015b\u0107, przekazywa\u0142 dalej i cz\u0119sto musia\u0142 czeka\u0107. To powodowa\u0142o co\u015b, co badacze, w szczerze m\u00f3wi\u0105c do\u015b\u0107 z\u0142o\u015bliwie, nazwali p\u0119cherzykiem bezczynno\u015bci, czyli bubble of idleness.", "tokens": [50814, 508, 6876, 17565, 284, 3870, 40622, 49194, 47149, 11, 29785, 33235, 44603, 34257, 741, 34369, 1038, 8908, 6472, 36361, 2162, 13, 1407, 3388, 378, 5528, 5249, 19241, 11, 598, 1578, 326, 1381, 11, 261, 22090, 260, 1381, 46591, 66, 49333, 710, 5249, 15350, 8699, 11, 20151, 40054, 280, 1274, 6759, 1229, 26116, 10782, 6522, 77, 16438, 11, 16591, 12212, 295, 4496, 45887, 13, 51514], "temperature": 0.0, "avg_logprob": -0.0902516448382035, "compression_ratio": 1.3643724696356276, "no_speech_prob": 0.5091810822486877}, {"id": 13, "seek": 13000, "start": 130.0, "end": 139.0, "text": " P\u0119cherzyk bezczynno\u015bci. Czyli po prostu marnowa\u0142a si\u0119 cenna moc obliczeniowa. Bo cz\u0119\u015b\u0107 systemu jakby czeka\u0142a na swoj\u0105 kolej.", "tokens": [50364, 430, 1274, 6759, 1229, 74, 10782, 6522, 77, 16438, 13, 37099, 714, 19518, 275, 1083, 5528, 5024, 3244, 27900, 629, 34962, 1111, 1050, 42124, 5528, 13, 3286, 47149, 1185, 84, 28976, 6472, 36361, 5024, 1667, 49194, 23749, 13, 50814], "temperature": 0.0, "avg_logprob": -0.05696972438267299, "compression_ratio": 1.4466666666666668, "no_speech_prob": 0.026146765798330307}, {"id": 14, "seek": 13000, "start": 139.0, "end": 146.0, "text": " Troch\u0119 jak na linii monta\u017cowej, gdzie jeden pracownik za\u0142o\u017cy ko\u0142o i czeka, a\u017c drugi sko\u0144czy montowa\u0107 drzwi zamiast pracowa\u0107 r\u00f3wnolegle.", "tokens": [50814, 19406, 23006, 4207, 1667, 287, 3812, 72, 8143, 18264, 21091, 11, 18922, 12906, 22404, 44895, 7949, 5249, 7735, 8384, 5249, 741, 6472, 36361, 11, 48134, 4110, 72, 1110, 78, 5248, 6522, 8143, 11445, 1224, 89, 6253, 710, 4526, 525, 22404, 11445, 11416, 895, 4812, 22631, 13, 51164], "temperature": 0.0, "avg_logprob": -0.05696972438267299, "compression_ratio": 1.4466666666666668, "no_speech_prob": 0.026146765798330307}, {"id": 15, "seek": 13000, "start": 146.0, "end": 156.0, "text": " Idealna analogia. W\u0142a\u015bnie o to chodzi\u0142o, \u017ceby pozby\u0107 si\u0119 tego p\u0119cherzyka. Pathways pozwoli\u0142o na znacznie bardziej efektywn\u0105 r\u00f3wnoleg\u0142\u0105 prac\u0119.", "tokens": [51164, 13090, 304, 629, 16660, 654, 13, 343, 5024, 12221, 277, 281, 23998, 5249, 11, 11316, 21281, 2322, 2162, 3244, 8627, 280, 1274, 6759, 40940, 13, 21914, 942, 40557, 9384, 5249, 1667, 15397, 14875, 2766, 27209, 31482, 916, 874, 895, 1611, 11416, 895, 4812, 70, 15926, 22404, 1274, 13, 51664], "temperature": 0.0, "avg_logprob": -0.05696972438267299, "compression_ratio": 1.4466666666666668, "no_speech_prob": 0.026146765798330307}, {"id": 16, "seek": 15600, "start": 156.0, "end": 168.0, "text": " Wszystkie czipy by\u0142y obci\u0105\u017cone niemal ca\u0142y czas. I \u017ceby zmierzy\u0107, jak dobrze im to wysz\u0142o, wprowadzi\u0142y now\u0105, bardzo wymown\u0105 miar\u0119. Model Flops Utilization. W skr\u00f3cie MFU.", "tokens": [50364, 343, 10424, 22872, 6472, 647, 88, 26366, 1111, 537, 27242, 546, 2838, 5579, 35226, 13190, 13, 286, 11316, 17020, 811, 27150, 11, 4207, 28335, 566, 281, 261, 20589, 5249, 11, 46733, 3992, 6825, 586, 1611, 11, 9034, 29764, 648, 1611, 2752, 289, 1274, 13, 17105, 3235, 3370, 12555, 388, 2144, 13, 343, 1110, 11721, 4260, 376, 37, 52, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08140098296844207, "compression_ratio": 1.32421875, "no_speech_prob": 0.01786286011338234}, {"id": 17, "seek": 15600, "start": 168.0, "end": 178.0, "text": " Czyli taki wska\u017anik, kt\u00f3ry m\u00f3wi, jaki procent teoretycznej mocy jest faktycznie wykorzystywany w praktyce. Taki no test wydajno\u015bci dla superkomputer\u00f3w.", "tokens": [50964, 37099, 20065, 261, 20771, 10659, 13123, 11, 9913, 24592, 11, 24492, 38826, 535, 418, 874, 3689, 11794, 705, 1344, 3492, 33647, 45586, 43606, 1229, 25134, 86, 1325, 261, 3206, 74, 874, 384, 13, 314, 7421, 572, 1500, 25984, 1805, 16438, 12285, 1687, 20557, 13849, 3901, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08140098296844207, "compression_ratio": 1.32421875, "no_speech_prob": 0.01786286011338234}, {"id": 18, "seek": 17800, "start": 178.0, "end": 187.0, "text": " Dok\u0142adnie. I wyniki by\u0142y naprawd\u0119 imponuj\u0105ce. Palm osi\u0105gn\u0105\u0142 MFU na poziomie 46,26%.", "tokens": [50364, 29768, 10358, 2766, 13, 286, 31936, 9850, 26366, 20970, 704, 266, 13263, 384, 13, 32668, 3003, 11404, 4568, 1611, 1221, 376, 37, 52, 1667, 38503, 40120, 17835, 11, 10880, 6856, 50814], "temperature": 0.0, "avg_logprob": -0.07771092406974352, "compression_ratio": 1.3292181069958848, "no_speech_prob": 0.5595519542694092}, {"id": 19, "seek": 17800, "start": 187.0, "end": 194.0, "text": " Dla por\u00f3wnania wcze\u015bniejsze du\u017ce modele, jak GPT-3, to by\u0142o w okolicach powiedzmy 21%.", "tokens": [50814, 413, 875, 1515, 812, 895, 5609, 38533, 1788, 44258, 1581, 2875, 4391, 306, 11, 4207, 26039, 51, 12, 18, 11, 281, 14811, 261, 3133, 7940, 608, 27617, 2226, 5080, 6856, 51164], "temperature": 0.0, "avg_logprob": -0.07771092406974352, "compression_ratio": 1.3292181069958848, "no_speech_prob": 0.5595519542694092}, {"id": 20, "seek": 17800, "start": 194.0, "end": 196.0, "text": " Czyli ponad dwukrotny skok.", "tokens": [51164, 37099, 9224, 345, 27379, 2034, 10536, 1634, 1110, 453, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07771092406974352, "compression_ratio": 1.3292181069958848, "no_speech_prob": 0.5595519542694092}, {"id": 21, "seek": 17800, "start": 196.0, "end": 203.0, "text": " Tak. To ponad dwukrotny skok wydajno\u015bci. Dlatego m\u00f3wimy, \u017ce to by\u0142 wyczyn in\u017cynieryjny na r\u00f3wni z naukowym.", "tokens": [51264, 9118, 13, 1407, 9224, 345, 27379, 2034, 10536, 1634, 1110, 453, 25984, 1805, 16438, 13, 47184, 13489, 13189, 11, 3561, 281, 16673, 4628, 6522, 77, 294, 1427, 2534, 811, 88, 73, 1634, 1667, 11416, 895, 72, 710, 35616, 74, 31691, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07771092406974352, "compression_ratio": 1.3292181069958848, "no_speech_prob": 0.5595519542694092}, {"id": 22, "seek": 20300, "start": 204.0, "end": 211.0, "text": " Nie chodzi\u0142o tylko o zbudowanie wi\u0119kszego modelu, ale o wymy\u015blenie, jak zrobi\u0107 to w spos\u00f3b radykalnie bardziej efektywny.", "tokens": [50414, 12016, 23998, 5249, 13219, 277, 710, 18281, 22028, 29968, 27725, 2316, 84, 11, 6775, 277, 4628, 2226, 1788, 6698, 414, 11, 4207, 31785, 281, 261, 22904, 367, 880, 19990, 2766, 27209, 31482, 916, 874, 43682, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06497061721921905, "compression_ratio": 1.4199288256227758, "no_speech_prob": 0.42602548003196716}, {"id": 23, "seek": 20300, "start": 211.0, "end": 221.0, "text": " Ok. Czyli mamy wydajniejszy spos\u00f3b na zbudowanie wi\u0119kszego m\u00f3zgu. To teraz kluczowe pytanie. Czym ten m\u00f3zg nakarmiono? Jak wygl\u0105da\u0142 jego zbi\u00f3r danych?", "tokens": [50764, 3477, 13, 37099, 17335, 25984, 1805, 10402, 7706, 22904, 1667, 710, 18281, 22028, 29968, 27725, 32515, 89, 2794, 13, 1407, 16854, 9671, 1311, 89, 6880, 36610, 13, 19832, 76, 2064, 32515, 89, 70, 20332, 4452, 49020, 30, 15029, 32015, 1221, 26542, 710, 5614, 15614, 274, 34644, 30, 51264], "temperature": 0.0, "avg_logprob": -0.06497061721921905, "compression_ratio": 1.4199288256227758, "no_speech_prob": 0.42602548003196716}, {"id": 24, "seek": 20300, "start": 221.0, "end": 231.0, "text": " By\u0142 gigantyczny. M\u00f3wimy o 780 miliardach token\u00f3w, czyli fragment\u00f3w s\u0142\u00f3w, naprawd\u0119 wysokiej jako\u015bci tekstu.", "tokens": [51264, 3146, 1221, 8741, 394, 17466, 1634, 13, 376, 3901, 13189, 277, 1614, 4702, 1962, 72, 515, 608, 14862, 3901, 11, 16591, 26424, 3901, 15116, 3901, 11, 20970, 27062, 453, 7764, 17123, 6199, 16624, 372, 84, 13, 51764], "temperature": 0.0, "avg_logprob": -0.06497061721921905, "compression_ratio": 1.4199288256227758, "no_speech_prob": 0.42602548003196716}, {"id": 25, "seek": 23100, "start": 231.0, "end": 241.0, "text": " W menu znalaz\u0142y si\u0119 przefiltrowane strony internetowe, ksi\u0105\u017cki, ca\u0142a angielska wikipedia, kot z githaba, a nawet rozmowy z medi\u00f3w spo\u0142eczno\u015bciowych.", "tokens": [50364, 343, 6510, 710, 4660, 921, 6825, 3244, 8325, 69, 2352, 1892, 1929, 32406, 4705, 6880, 11, 39311, 2984, 11, 1335, 5024, 2562, 1187, 20771, 261, 1035, 647, 14212, 11, 43029, 710, 290, 355, 5509, 11, 257, 22696, 35234, 10089, 710, 17269, 3901, 36851, 89, 16438, 19605, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07970221721342881, "compression_ratio": 1.4150326797385622, "no_speech_prob": 0.00967932865023613}, {"id": 26, "seek": 23100, "start": 241.0, "end": 247.0, "text": " Jest tu jednak jeden, ale bardzo wa\u017cny szczeg\u00f3\u0142, kt\u00f3ry b\u0119dzie rzutowa\u0142 na p\u00f3\u017aniejsze wyniki.", "tokens": [50864, 24918, 2604, 25897, 12906, 11, 6775, 9034, 27777, 1634, 22090, 1146, 16181, 11, 9913, 10562, 367, 89, 325, 30105, 1667, 36968, 82, 1381, 31936, 9850, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07970221721342881, "compression_ratio": 1.4150326797385622, "no_speech_prob": 0.00967932865023613}, {"id": 27, "seek": 23100, "start": 247.0, "end": 249.0, "text": " Domy\u015blam si\u0119, j\u0119zyk?", "tokens": [51164, 413, 8488, 1788, 4326, 3244, 11, 49055, 74, 30, 51264], "temperature": 0.0, "avg_logprob": -0.07970221721342881, "compression_ratio": 1.4150326797385622, "no_speech_prob": 0.00967932865023613}, {"id": 28, "seek": 23100, "start": 249.0, "end": 258.0, "text": " W\u0142a\u015bnie. Tylko oko\u0142o 22% tych danych by\u0142o w j\u0119zykach innych ni\u017c angielski i to jest kluczowy kontekst dla zdolno\u015bci, kt\u00f3re za chwil\u0119 om\u00f3wimy.", "tokens": [51264, 343, 5024, 12221, 13, 49286, 4093, 45730, 5249, 5853, 4, 15180, 274, 34644, 14811, 261, 49055, 41326, 36286, 28502, 2562, 1187, 18020, 741, 281, 3492, 9671, 1311, 89, 10089, 14373, 916, 372, 12285, 16221, 401, 16438, 11, 8864, 7949, 41941, 1274, 3406, 3901, 13189, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07970221721342881, "compression_ratio": 1.4150326797385622, "no_speech_prob": 0.00967932865023613}, {"id": 29, "seek": 25800, "start": 259.0, "end": 263.0, "text": " Czyli mamy przepis na giganta. Przejd\u017amy wi\u0119c do najciekawszego.", "tokens": [50414, 37099, 17335, 30829, 271, 1667, 8741, 5983, 13, 2114, 16920, 67, 10659, 2226, 16677, 360, 11212, 4260, 74, 1607, 15453, 6308, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11439490962672878, "compression_ratio": 1.43125, "no_speech_prob": 0.4209015965461731}, {"id": 30, "seek": 25800, "start": 263.0, "end": 269.0, "text": " Co ten nowy, super wydajnie zbudowany model, potrafi\u0142 zrobi\u0107, czego inne nie potrafi\u0142y.", "tokens": [50614, 3066, 2064, 586, 88, 11, 1687, 25984, 1805, 2766, 710, 18281, 23341, 2316, 11, 1847, 10437, 40622, 31785, 11, 36559, 24170, 2838, 1847, 10437, 72, 6825, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11439490962672878, "compression_ratio": 1.43125, "no_speech_prob": 0.4209015965461731}, {"id": 31, "seek": 25800, "start": 269.0, "end": 276.0, "text": " Artyku\u0142 m\u00f3wi o prze\u0142omowych zdolno\u015bciach, kt\u00f3re pojawi\u0142y si\u0119 wraz z eskal\u0105. Na pierwszy ogie\u0144 idzie rozumowanie.", "tokens": [50914, 1587, 874, 5279, 1221, 24592, 277, 8325, 1221, 298, 19605, 16221, 401, 16438, 608, 11, 8864, 30655, 72, 6825, 3244, 7843, 89, 710, 785, 19990, 1611, 13, 6056, 34016, 5360, 414, 5248, 4496, 3283, 48797, 22028, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11439490962672878, "compression_ratio": 1.43125, "no_speech_prob": 0.4209015965461731}, {"id": 32, "seek": 25800, "start": 276.0, "end": 283.0, "text": " I tu kluczowa okaza\u0142a si\u0119 technika, kt\u00f3ra dzisiaj jest standardem, ale wtedy to by\u0142o objawienie.", "tokens": [51264, 286, 2604, 9671, 1311, 89, 5528, 3133, 12257, 5024, 3244, 1537, 5439, 11, 19456, 25772, 3492, 3832, 443, 11, 6775, 26959, 281, 14811, 1111, 22199, 27385, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11439490962672878, "compression_ratio": 1.43125, "no_speech_prob": 0.4209015965461731}, {"id": 33, "seek": 25800, "start": 283.0, "end": 287.0, "text": " Chain of Thought Prompting. To jest, wiesz, bardzo prosta w za\u0142o\u017ceniu idea.", "tokens": [51614, 33252, 295, 23058, 15833, 662, 278, 13, 1407, 3492, 11, 261, 15347, 11, 9034, 582, 8638, 261, 7949, 5249, 24930, 5951, 1558, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11439490962672878, "compression_ratio": 1.43125, "no_speech_prob": 0.4209015965461731}, {"id": 34, "seek": 28700, "start": 287.0, "end": 292.0, "text": " Zamiast oczekiwa\u0107 od modelu natychmiastowej ostatecznej odpowiedzi na z\u0142o\u017cone pytanie.", "tokens": [50364, 1176, 4526, 525, 277, 3689, 14753, 25234, 3611, 2316, 84, 2249, 16384, 3057, 525, 21091, 277, 15406, 3689, 11794, 36574, 3992, 1667, 710, 5249, 1427, 546, 36610, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08669300432558413, "compression_ratio": 1.4668769716088328, "no_speech_prob": 0.06793718039989471}, {"id": 35, "seek": 28700, "start": 292.0, "end": 297.0, "text": " M\u00f3wi mu si\u0119, chwila, zanim odpowiesz, poka\u017c mi, jak do tego doszed\u0142e\u015b.", "tokens": [50614, 376, 3901, 72, 2992, 3244, 11, 26237, 7371, 11, 710, 17869, 3611, 79, 305, 15347, 11, 13010, 18264, 2752, 11, 4207, 360, 8627, 4491, 11312, 19827, 1788, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08669300432558413, "compression_ratio": 1.4668769716088328, "no_speech_prob": 0.06793718039989471}, {"id": 36, "seek": 28700, "start": 297.0, "end": 303.0, "text": " Troch\u0119 jak nauczyciel w szkole, kt\u00f3ry prosi, \u017ceby pokaza\u0107 obliczenia na marginesie, a nie tylko poda\u0107 wynik.", "tokens": [50864, 19406, 23006, 4207, 49103, 1229, 537, 338, 261, 7870, 4093, 306, 11, 9913, 6267, 72, 11, 11316, 13010, 12257, 2162, 1111, 1050, 14320, 1667, 10270, 279, 414, 11, 257, 2838, 13219, 2497, 43379, 31936, 1035, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08669300432558413, "compression_ratio": 1.4668769716088328, "no_speech_prob": 0.06793718039989471}, {"id": 37, "seek": 28700, "start": 303.0, "end": 308.0, "text": " Dok\u0142adnie tak. W pracy podano \u015bwietny przyk\u0142ad zadania matematycznego, co\u015b w stylu.", "tokens": [51164, 29768, 10358, 2766, 991, 13, 343, 35591, 2497, 3730, 8299, 39083, 1634, 23144, 42788, 5609, 3803, 8615, 17466, 11858, 11, 19241, 261, 7952, 2781, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08669300432558413, "compression_ratio": 1.4668769716088328, "no_speech_prob": 0.06793718039989471}, {"id": 38, "seek": 28700, "start": 308.0, "end": 315.0, "text": " Roger mia\u0142 pi\u0119\u0107 pi\u0142ek, kupi\u0142 dwie kolejne puszki w ka\u017cdej po trzy pi\u0142ki. Ile ma ich teraz?", "tokens": [51414, 17666, 27989, 32677, 2162, 3895, 1221, 916, 11, 37534, 40622, 274, 8699, 23749, 716, 280, 22378, 2984, 261, 21912, 1479, 73, 714, 34573, 3895, 1221, 2984, 13, 286, 306, 463, 1893, 16854, 30, 51764], "temperature": 0.0, "avg_logprob": -0.08669300432558413, "compression_ratio": 1.4668769716088328, "no_speech_prob": 0.06793718039989471}, {"id": 39, "seek": 31500, "start": 316.0, "end": 323.0, "text": " I zamiast od razu odpowiedzie\u0107 11, model z instrukcj\u0105 Chain of Thought najpierw generuje sw\u00f3j tok my\u015blenia.", "tokens": [50414, 286, 710, 4526, 525, 3611, 367, 8813, 24314, 22078, 2975, 11, 2316, 710, 1058, 25126, 66, 8555, 33252, 295, 23058, 11212, 45119, 86, 1337, 13008, 1693, 18999, 19164, 48633, 6698, 654, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07785192330678305, "compression_ratio": 1.356, "no_speech_prob": 0.04553883150219917}, {"id": 40, "seek": 31500, "start": 323.0, "end": 330.0, "text": " Roger zacz\u0105\u0142 z pi\u0119\u0107 pi\u0142kami, dwie puszki po trzy pi\u0142ki to sze\u015b\u0107 pi\u0142ek, pi\u0119\u0107 plus sze\u015b\u0107, jedyna\u015bcie.", "tokens": [50764, 17666, 34430, 8925, 1221, 710, 32677, 2162, 3895, 1221, 48737, 11, 274, 8699, 280, 22378, 2984, 714, 34573, 3895, 1221, 2984, 281, 262, 1381, 7753, 3895, 1221, 916, 11, 32677, 2162, 1804, 262, 1381, 7753, 11, 5232, 88, 629, 9815, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07785192330678305, "compression_ratio": 1.356, "no_speech_prob": 0.04553883150219917}, {"id": 41, "seek": 31500, "start": 330.0, "end": 334.0, "text": " Ta prosta zmiana mia\u0142a kolosalne znaczenie.", "tokens": [51114, 6551, 582, 8638, 17020, 8497, 21290, 5024, 17818, 329, 304, 716, 15397, 326, 16778, 13, 51314], "temperature": 0.0, "avg_logprob": -0.07785192330678305, "compression_ratio": 1.356, "no_speech_prob": 0.04553883150219917}, {"id": 42, "seek": 31500, "start": 334.0, "end": 338.0, "text": " A\u017c tak? Przecie\u017c to tylko zmiana sposob\u00f3w, jaki zadajemy pytanie.", "tokens": [51314, 316, 1427, 991, 30, 2114, 1381, 40082, 281, 13219, 17020, 8497, 20443, 996, 3901, 11, 24492, 710, 1538, 73, 3633, 36610, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07785192330678305, "compression_ratio": 1.356, "no_speech_prob": 0.04553883150219917}, {"id": 43, "seek": 33800, "start": 338.0, "end": 347.0, "text": " Ta zmiana w po\u0142\u0105czeniu z gigantyczn\u0105 skal\u0105 modelu PLM pozwoli\u0142a mu rozwi\u0105zywa\u0107 zadania wymagaj\u0105ce rozumowania na poziomie modeli,", "tokens": [50364, 6551, 17020, 8497, 261, 714, 15926, 66, 39651, 710, 8741, 394, 17466, 13113, 16890, 1611, 2316, 84, 6999, 44, 40557, 9384, 5024, 2992, 9544, 18234, 1229, 25234, 42788, 5609, 29764, 559, 11133, 384, 48797, 21308, 1667, 38503, 40120, 2316, 72, 11, 50814], "temperature": 0.0, "avg_logprob": -0.05590477607233061, "compression_ratio": 1.4884488448844884, "no_speech_prob": 0.5045354962348938}, {"id": 44, "seek": 33800, "start": 347.0, "end": 352.0, "text": " kt\u00f3re by\u0142y specjalnie do tego celu dostrajane, czyli przechodzi\u0142y fine tuning.", "tokens": [50814, 8864, 26366, 46433, 2766, 360, 8627, 9277, 84, 20568, 48690, 1929, 11, 16591, 8325, 34616, 6825, 2489, 15164, 13, 51064], "temperature": 0.0, "avg_logprob": -0.05590477607233061, "compression_ratio": 1.4884488448844884, "no_speech_prob": 0.5045354962348938}, {"id": 45, "seek": 33800, "start": 352.0, "end": 358.0, "text": " To by\u0142 wielki post\u0119p, bo pokaza\u0142, \u017ce nie trzeba tworzy\u0107 specjalistycznego modelu do matematyki.", "tokens": [51064, 1407, 16673, 20570, 2984, 2183, 18085, 11, 748, 13010, 12257, 1221, 11, 3561, 2838, 25860, 46288, 27150, 46433, 468, 17466, 11858, 2316, 84, 360, 3803, 8615, 88, 2984, 13, 51364], "temperature": 0.0, "avg_logprob": -0.05590477607233061, "compression_ratio": 1.4884488448844884, "no_speech_prob": 0.5045354962348938}, {"id": 46, "seek": 33800, "start": 358.0, "end": 364.0, "text": " Wystarczy jakby nauczy\u0107 uniwersalny model, jak ma my\u015ble\u0107 na g\u0142os.", "tokens": [51364, 14458, 9710, 6522, 28976, 49103, 27150, 36435, 5364, 304, 1634, 2316, 11, 4207, 463, 48633, 306, 2162, 1667, 43767, 13, 51664], "temperature": 0.0, "avg_logprob": -0.05590477607233061, "compression_ratio": 1.4884488448844884, "no_speech_prob": 0.5045354962348938}, {"id": 47, "seek": 33800, "start": 364.0, "end": 367.0, "text": " To jest znacznie bardziej elastyczne i pot\u0119\u017cne podej\u015bcie.", "tokens": [51664, 1407, 3492, 15397, 14875, 2766, 27209, 806, 9820, 38491, 741, 1847, 1274, 1427, 716, 7468, 73, 9815, 13, 51814], "temperature": 0.0, "avg_logprob": -0.05590477607233061, "compression_ratio": 1.4884488448844884, "no_speech_prob": 0.5045354962348938}, {"id": 48, "seek": 36700, "start": 367.0, "end": 374.0, "text": " Niesamolite. To pokazuje, jak wa\u017cny jest nie tylko to, co model wie, ale te\u017c jak go odpytujemy.", "tokens": [50364, 426, 530, 335, 401, 642, 13, 1407, 13010, 43317, 11, 4207, 27777, 1634, 3492, 2838, 13219, 281, 11, 598, 2316, 3355, 11, 6775, 9516, 4207, 352, 3611, 8200, 83, 21767, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08097991809039048, "compression_ratio": 1.411371237458194, "no_speech_prob": 0.04293416813015938}, {"id": 49, "seek": 36700, "start": 374.0, "end": 378.0, "text": " A co z zadaniami, kt\u00f3re wydaj\u0105 si\u0119 jeszcze bardziej ludzkie?", "tokens": [50714, 316, 598, 710, 710, 11338, 15568, 11, 8864, 25984, 11133, 3244, 14168, 27209, 15946, 89, 22872, 30, 50914], "temperature": 0.0, "avg_logprob": -0.08097991809039048, "compression_ratio": 1.411371237458194, "no_speech_prob": 0.04293416813015938}, {"id": 50, "seek": 36700, "start": 378.0, "end": 382.0, "text": " Ten przyk\u0142ad z wyja\u015bnianiem \u017cart\u00f3w w artykule jest absolutnie genialny.", "tokens": [50914, 9380, 23144, 710, 4628, 2938, 1788, 77, 952, 4907, 19625, 446, 3901, 261, 594, 874, 74, 2271, 3492, 18757, 2766, 48228, 1634, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08097991809039048, "compression_ratio": 1.411371237458194, "no_speech_prob": 0.04293416813015938}, {"id": 51, "seek": 36700, "start": 382.0, "end": 385.0, "text": " Tak, to jeden z tych przyk\u0142ad\u00f3w, kt\u00f3re naprawd\u0119 zapadaj\u0105 w pami\u0119\u0107.", "tokens": [51114, 9118, 11, 281, 12906, 710, 15180, 23144, 3901, 11, 8864, 20970, 14223, 1538, 8555, 261, 31088, 2162, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08097991809039048, "compression_ratio": 1.411371237458194, "no_speech_prob": 0.04293416813015938}, {"id": 52, "seek": 36700, "start": 385.0, "end": 388.0, "text": " \u017bart brzmia\u0142 mniej wi\u0119cej tak.", "tokens": [51264, 29804, 446, 738, 89, 76, 8908, 39513, 26004, 991, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08097991809039048, "compression_ratio": 1.411371237458194, "no_speech_prob": 0.04293416813015938}, {"id": 53, "seek": 36700, "start": 388.0, "end": 393.0, "text": " S\u0142ysza\u0142e\u015b, \u017ce Google zatrudni\u0142o elokw\u0119tnego wieloryba do zespo\u0142u TPU?", "tokens": [51414, 318, 1221, 749, 2394, 19827, 1788, 11, 3561, 3329, 35802, 47130, 3722, 5249, 806, 453, 86, 46788, 11858, 20570, 827, 4231, 360, 710, 279, 2259, 24066, 314, 8115, 30, 51664], "temperature": 0.0, "avg_logprob": -0.08097991809039048, "compression_ratio": 1.411371237458194, "no_speech_prob": 0.04293416813015938}, {"id": 54, "seek": 39300, "start": 393.0, "end": 397.0, "text": " Pokaza\u0142 im, jak komunikowa\u0107 si\u0119 mi\u0119dzy dwoma r\u00f3\u017cnymi podz.", "tokens": [50364, 14958, 12257, 1221, 566, 11, 4207, 45359, 1035, 11445, 3244, 33964, 27379, 6440, 19637, 31813, 2497, 89, 13, 50564], "temperature": 0.0, "avg_logprob": -0.057547241676854724, "compression_ratio": 1.3857142857142857, "no_speech_prob": 0.019988076761364937}, {"id": 55, "seek": 39300, "start": 397.0, "end": 401.0, "text": " Przyznam, \u017ce to do\u015b\u0107 niszowy, bardzo techniczny \u017cart.", "tokens": [50564, 39590, 89, 5378, 11, 3561, 281, 49333, 297, 23848, 10089, 11, 9034, 1537, 17946, 1634, 19625, 446, 13, 50764], "temperature": 0.0, "avg_logprob": -0.057547241676854724, "compression_ratio": 1.3857142857142857, "no_speech_prob": 0.019988076761364937}, {"id": 56, "seek": 39300, "start": 401.0, "end": 406.0, "text": " I w\u0142a\u015bnie dlatego wyja\u015bnienie, kt\u00f3re wygenerowa\u0142 palm jest tak zdumiewaj\u0105ce.", "tokens": [50764, 286, 14234, 32205, 4628, 2938, 1788, 77, 27385, 11, 8864, 4628, 21848, 30105, 17018, 3492, 991, 16221, 449, 1093, 11133, 384, 13, 51014], "temperature": 0.0, "avg_logprob": -0.057547241676854724, "compression_ratio": 1.3857142857142857, "no_speech_prob": 0.019988076761364937}, {"id": 57, "seek": 39300, "start": 406.0, "end": 408.0, "text": " Model poprawnie identyfikowa\u0142 gr\u0119 s\u0142\u00f3w.", "tokens": [51014, 17105, 1665, 424, 14215, 2473, 88, 31230, 30105, 677, 1274, 15116, 3901, 13, 51114], "temperature": 0.0, "avg_logprob": -0.057547241676854724, "compression_ratio": 1.3857142857142857, "no_speech_prob": 0.019988076761364937}, {"id": 58, "seek": 39300, "start": 408.0, "end": 413.0, "text": " Zrozumia\u0142, \u017ce angielskie s\u0142owo pod oznacza zar\u00f3wno grup\u0119 wieloryb\u00f3w,", "tokens": [51114, 1176, 27857, 449, 8908, 11, 3561, 2562, 1187, 5161, 414, 15116, 19941, 2497, 277, 22672, 326, 2394, 22675, 812, 20944, 12740, 1274, 20570, 827, 65, 3901, 11, 51364], "temperature": 0.0, "avg_logprob": -0.057547241676854724, "compression_ratio": 1.3857142857142857, "no_speech_prob": 0.019988076761364937}, {"id": 59, "seek": 39300, "start": 413.0, "end": 417.0, "text": " jak i grup\u0119 po\u0142\u0105czonych procesor\u00f3w TPU u\u017cywanych w Google.", "tokens": [51364, 4207, 741, 12740, 1274, 714, 43558, 2526, 339, 17565, 284, 3901, 314, 8115, 34097, 86, 34644, 261, 3329, 13, 51564], "temperature": 0.0, "avg_logprob": -0.057547241676854724, "compression_ratio": 1.3857142857142857, "no_speech_prob": 0.019988076761364937}, {"id": 60, "seek": 41700, "start": 418.0, "end": 421.0, "text": " Zrozumia\u0142 oba konteksty biologiczny i technologiczny", "tokens": [50414, 1176, 27857, 449, 8908, 1111, 64, 14373, 916, 25134, 3228, 1132, 17946, 1634, 741, 1537, 1132, 17946, 1634, 50564], "temperature": 0.0, "avg_logprob": -0.07704129276505436, "compression_ratio": 1.584664536741214, "no_speech_prob": 0.0821985974907875}, {"id": 61, "seek": 41700, "start": 421.0, "end": 424.0, "text": " i precyzyjnie wyja\u015bni\u0142, na czym polega dowcip.", "tokens": [50564, 741, 659, 1344, 1229, 73, 2766, 4628, 2938, 1788, 3722, 1221, 11, 1667, 31466, 13208, 3680, 9459, 11371, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07704129276505436, "compression_ratio": 1.584664536741214, "no_speech_prob": 0.0821985974907875}, {"id": 62, "seek": 41700, "start": 424.0, "end": 429.0, "text": " To pokazuje rozumienie na poziomie, kt\u00f3rego wcze\u015bniej w tej skali po prostu nie obserwowano.", "tokens": [50714, 1407, 13010, 43317, 48797, 27385, 1667, 38503, 40120, 11, 46951, 40785, 261, 12573, 1110, 5103, 714, 19518, 2838, 12887, 34354, 3730, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07704129276505436, "compression_ratio": 1.584664536741214, "no_speech_prob": 0.0821985974907875}, {"id": 63, "seek": 41700, "start": 429.0, "end": 431.0, "text": " Ten \u017cart z wielorybem jest genialny,", "tokens": [50964, 9380, 19625, 446, 710, 20570, 827, 65, 443, 3492, 48228, 1634, 11, 51064], "temperature": 0.0, "avg_logprob": -0.07704129276505436, "compression_ratio": 1.584664536741214, "no_speech_prob": 0.0821985974907875}, {"id": 64, "seek": 41700, "start": 431.0, "end": 436.0, "text": " bo pokazuje, \u017ce model rozumie nie tylko s\u0142owa, ale i bardzo niszowy, techniczny kontekst.", "tokens": [51064, 748, 13010, 43317, 11, 3561, 2316, 48797, 414, 2838, 13219, 15116, 5528, 11, 6775, 741, 9034, 297, 23848, 10089, 11, 1537, 17946, 1634, 14373, 916, 372, 13, 51314], "temperature": 0.0, "avg_logprob": -0.07704129276505436, "compression_ratio": 1.584664536741214, "no_speech_prob": 0.0821985974907875}, {"id": 65, "seek": 41700, "start": 436.0, "end": 439.0, "text": " To prowadzi mnie do pytania o granicy jego mo\u017cliwo\u015bci.", "tokens": [51314, 1407, 36590, 3992, 17661, 360, 25878, 5609, 277, 9370, 2632, 26542, 30854, 36476, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07704129276505436, "compression_ratio": 1.584664536741214, "no_speech_prob": 0.0821985974907875}, {"id": 66, "seek": 41700, "start": 439.0, "end": 443.0, "text": " Czy on po prostu staje si\u0119 coraz lepszy w miar\u0119, jak go powi\u0119kszamy?", "tokens": [51464, 19832, 322, 714, 19518, 342, 11153, 3244, 25899, 476, 1878, 1229, 261, 2752, 289, 1274, 11, 4207, 352, 3388, 5034, 1694, 89, 7804, 30, 51664], "temperature": 0.0, "avg_logprob": -0.07704129276505436, "compression_ratio": 1.584664536741214, "no_speech_prob": 0.0821985974907875}, {"id": 67, "seek": 41700, "start": 443.0, "end": 446.0, "text": " Czy dzieje si\u0119 tam co\u015b dziwniejszego?", "tokens": [51664, 19832, 17953, 2884, 3244, 7677, 19241, 31981, 895, 7764, 15453, 6308, 30, 51814], "temperature": 0.0, "avg_logprob": -0.07704129276505436, "compression_ratio": 1.584664536741214, "no_speech_prob": 0.0821985974907875}, {"id": 68, "seek": 44600, "start": 446.0, "end": 450.0, "text": " W artykule natkn\u0105\u0142em si\u0119 na termin nieci\u0105g\u0142e ulepszenia.", "tokens": [50364, 343, 594, 874, 74, 2271, 2249, 5457, 1611, 11126, 3244, 1667, 10761, 2838, 34381, 70, 19827, 344, 306, 1878, 14320, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07844077746073405, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.014342649839818478}, {"id": 69, "seek": 44600, "start": 454.0, "end": 458.0, "text": " I to jest sedno jednego z najwa\u017cniejszych odkry\u0107 naukowych tej pracy.", "tokens": [50764, 286, 281, 3492, 9643, 1771, 5232, 11858, 710, 11212, 27111, 10402, 45021, 3611, 43298, 2162, 35616, 74, 19605, 12573, 35591, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07844077746073405, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.014342649839818478}, {"id": 70, "seek": 44600, "start": 458.0, "end": 464.0, "text": " Big Bench to zbi\u00f3r naprawd\u0119 trudnych, cz\u0119sto abstrakcyjnych zada\u0144 j\u0119zykowych,", "tokens": [50964, 5429, 3964, 339, 281, 710, 5614, 15614, 20970, 32007, 9399, 11, 34369, 10823, 11272, 42949, 9399, 710, 1538, 5248, 49055, 74, 19605, 11, 51264], "temperature": 0.0, "avg_logprob": -0.07844077746073405, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.014342649839818478}, {"id": 71, "seek": 44600, "start": 464.0, "end": 468.0, "text": " zaprojektowanych, by testowa\u0107 granice mo\u017cliwo\u015bci modeli.", "tokens": [51264, 14223, 340, 14930, 23341, 339, 11, 538, 1500, 11445, 9370, 573, 30854, 36476, 2316, 72, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07844077746073405, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.014342649839818478}, {"id": 72, "seek": 44600, "start": 468.0, "end": 473.0, "text": " A nieci\u0105g\u0142e ulepszenia oznaczaj\u0105, \u017ce wydajno\u015b\u0107 modelu nie ro\u015bnie liniowo.", "tokens": [51464, 316, 2838, 34381, 70, 19827, 344, 306, 1878, 14320, 277, 22672, 14875, 11133, 11, 3561, 25984, 1805, 23293, 2316, 84, 2838, 744, 12221, 287, 3812, 19941, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07844077746073405, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.014342649839818478}, {"id": 73, "seek": 47300, "start": 473.0, "end": 480.0, "text": " Czasem po przekroczeniu pewnego progu skali pojawia si\u0119 nag\u0142y skokowy wzrost umiej\u0119tno\u015bci.", "tokens": [50364, 383, 24561, 443, 714, 29785, 24174, 39651, 25889, 11858, 447, 2794, 1110, 5103, 30655, 654, 3244, 17096, 6825, 1110, 453, 10089, 24809, 27494, 1105, 7764, 46788, 16438, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07606769882085669, "compression_ratio": 1.4470588235294117, "no_speech_prob": 0.005942473188042641}, {"id": 74, "seek": 47300, "start": 480.0, "end": 486.0, "text": " Czyli to nie jest tak, \u017ce model staje si\u0119 po prostu o 10% lepszy, gdy jest o 10% wi\u0119kszy.", "tokens": [50714, 37099, 281, 2838, 3492, 991, 11, 3561, 2316, 342, 11153, 3244, 714, 19518, 277, 1266, 4, 476, 1878, 1229, 11, 28405, 3492, 277, 1266, 4, 29968, 1229, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07606769882085669, "compression_ratio": 1.4470588235294117, "no_speech_prob": 0.005942473188042641}, {"id": 75, "seek": 47300, "start": 486.0, "end": 491.0, "text": " Czasem nast\u0119puje gwa\u0142towna zmiana, jakby nagle co\u015b zaskoczy\u0142o.", "tokens": [51014, 383, 24561, 443, 39662, 13008, 290, 44603, 83, 305, 629, 17020, 8497, 11, 28976, 297, 15088, 19241, 710, 3863, 905, 1229, 5249, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07606769882085669, "compression_ratio": 1.4470588235294117, "no_speech_prob": 0.005942473188042641}, {"id": 76, "seek": 47300, "start": 491.0, "end": 495.0, "text": " W\u0142a\u015bnie. Wyobra\u017a sobie, \u017ce uczysz kogo\u015b obcego j\u0119zyka.", "tokens": [51264, 343, 5024, 12221, 13, 14458, 24393, 10659, 13652, 11, 3561, 35403, 20589, 350, 23515, 1788, 1111, 384, 1571, 42309, 40940, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07606769882085669, "compression_ratio": 1.4470588235294117, "no_speech_prob": 0.005942473188042641}, {"id": 77, "seek": 47300, "start": 495.0, "end": 499.0, "text": " D\u0142ugo, d\u0142ugo myli gramatyk\u0119, buduje proste zdania,", "tokens": [51464, 413, 1221, 20746, 11, 44042, 20746, 452, 2081, 21353, 21398, 15724, 11, 3265, 13008, 10293, 68, 16221, 5609, 11, 51664], "temperature": 0.0, "avg_logprob": -0.07606769882085669, "compression_ratio": 1.4470588235294117, "no_speech_prob": 0.005942473188042641}, {"id": 78, "seek": 49900, "start": 499.0, "end": 504.0, "text": " a potem nagle, pewnego dnia, zaczyna opowiaga\u0107 do wcipy i rozumie\u0107 sarkazm.", "tokens": [50364, 257, 36513, 297, 15088, 11, 25889, 11858, 274, 12679, 11, 43811, 629, 999, 24503, 9286, 2162, 360, 261, 537, 8200, 741, 48797, 414, 2162, 262, 809, 921, 76, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07915861423198993, "compression_ratio": 1.359073359073359, "no_speech_prob": 0.00606328621506691}, {"id": 79, "seek": 49900, "start": 504.0, "end": 509.0, "text": " To nie jest liniowy post\u0119p. W\u0142a\u015bnie to zaobserwowali badacze.", "tokens": [50614, 1407, 2838, 3492, 287, 3812, 10089, 2183, 18085, 13, 343, 5024, 12221, 281, 7949, 16537, 260, 34354, 5103, 1578, 326, 1381, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07915861423198993, "compression_ratio": 1.359073359073359, "no_speech_prob": 0.00606328621506691}, {"id": 80, "seek": 49900, "start": 509.0, "end": 514.0, "text": " W zadaniu z dopasowywaniem angielskich przys\u0142owim, czyli English Proverbs,", "tokens": [50864, 343, 42788, 25849, 710, 360, 20990, 10089, 7916, 4907, 2562, 1187, 5161, 480, 6541, 39508, 305, 332, 11, 16591, 3669, 1705, 43348, 11, 51114], "temperature": 0.0, "avg_logprob": -0.07915861423198993, "compression_ratio": 1.359073359073359, "no_speech_prob": 0.00606328621506691}, {"id": 81, "seek": 49900, "start": 514.0, "end": 519.0, "text": " mniejszy 62 miliardowy model by\u0142 na porzomie losowego zgadywania.", "tokens": [51114, 39513, 7706, 24536, 1962, 72, 515, 10089, 2316, 16673, 1667, 1515, 89, 40120, 1750, 26576, 40948, 880, 86, 5609, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07915861423198993, "compression_ratio": 1.359073359073359, "no_speech_prob": 0.00606328621506691}, {"id": 82, "seek": 49900, "start": 519.0, "end": 527.0, "text": " Oko\u0142o 25% skuteczno\u015bci, a najwi\u0119kszy 540 miliardowy prawie 90%.", "tokens": [51364, 3477, 78, 5249, 3552, 4, 1110, 1169, 3689, 16438, 11, 257, 48636, 1694, 1229, 1025, 5254, 1962, 72, 515, 10089, 3206, 8699, 4289, 6856, 51764], "temperature": 0.0, "avg_logprob": -0.07915861423198993, "compression_ratio": 1.359073359073359, "no_speech_prob": 0.00606328621506691}, {"id": 83, "seek": 52700, "start": 527.0, "end": 531.0, "text": " Ta umiej\u0119tno\u015b\u0107 nie uros\u0142a, ona si\u0119 po prostu pojawi\u0142a.", "tokens": [50364, 6551, 1105, 7764, 46788, 23293, 2838, 344, 2635, 5024, 11, 20325, 3244, 714, 19518, 30655, 72, 5024, 13, 50564], "temperature": 0.0, "avg_logprob": -0.06714642749113195, "compression_ratio": 1.3993174061433447, "no_speech_prob": 0.031896382570266724}, {"id": 84, "seek": 52700, "start": 531.0, "end": 535.0, "text": " To by\u0142o fundamentalne odkrycie, kt\u00f3re pokaza\u0142o, \u017ce sama skala", "tokens": [50564, 1407, 14811, 8088, 716, 3611, 43298, 4260, 11, 8864, 13010, 12257, 5249, 11, 3561, 17768, 1110, 5159, 50764], "temperature": 0.0, "avg_logprob": -0.06714642749113195, "compression_ratio": 1.3993174061433447, "no_speech_prob": 0.031896382570266724}, {"id": 85, "seek": 52700, "start": 535.0, "end": 540.0, "text": " mo\u017ce odblokowywa\u0107 zupe\u0142nie nowe, jako\u015bciowo inne zdolno\u015bci.", "tokens": [50764, 12034, 3611, 5199, 453, 10089, 25234, 49922, 586, 68, 11, 17123, 6199, 19941, 24170, 16221, 401, 16438, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06714642749113195, "compression_ratio": 1.3993174061433447, "no_speech_prob": 0.031896382570266724}, {"id": 86, "seek": 52700, "start": 540.0, "end": 544.0, "text": " Dobra, czyli model jest filozofem, matematykiem i komikiem.", "tokens": [51014, 413, 24393, 11, 16591, 2316, 3492, 1387, 15151, 2670, 443, 11, 3803, 8615, 88, 26116, 741, 5207, 1035, 4907, 13, 51214], "temperature": 0.0, "avg_logprob": -0.06714642749113195, "compression_ratio": 1.3993174061433447, "no_speech_prob": 0.031896382570266724}, {"id": 87, "seek": 52700, "start": 544.0, "end": 547.0, "text": " Ale czy potrafi zrobi\u0107 co\u015b praktycznego?", "tokens": [51214, 9366, 6430, 1847, 10437, 72, 31785, 19241, 3206, 74, 874, 3689, 11858, 30, 51364], "temperature": 0.0, "avg_logprob": -0.06714642749113195, "compression_ratio": 1.3993174061433447, "no_speech_prob": 0.031896382570266724}, {"id": 88, "seek": 52700, "start": 547.0, "end": 550.0, "text": " W danych treningowych by\u0142 te\u017c kod. Potrafi by\u0107 programist\u0105?", "tokens": [51364, 343, 274, 34644, 2192, 773, 19605, 16673, 9516, 350, 378, 13, 9145, 10437, 72, 15069, 1461, 468, 1611, 30, 51514], "temperature": 0.0, "avg_logprob": -0.06714642749113195, "compression_ratio": 1.3993174061433447, "no_speech_prob": 0.031896382570266724}, {"id": 89, "seek": 52700, "start": 550.0, "end": 555.0, "text": " A to i jakim? Badacze wzi\u0119li podstawowy model Palm", "tokens": [51514, 316, 281, 741, 49410, 30, 11523, 326, 1381, 261, 16706, 2081, 43443, 10089, 2316, 32668, 51764], "temperature": 0.0, "avg_logprob": -0.06714642749113195, "compression_ratio": 1.3993174061433447, "no_speech_prob": 0.031896382570266724}, {"id": 90, "seek": 55500, "start": 555.0, "end": 560.0, "text": " i zrobili dodatkowy fine-tuning na danych zawieraj\u0105cych wy\u0142\u0105cznie kod.", "tokens": [50364, 741, 44399, 2312, 13886, 33525, 10089, 2489, 12, 83, 37726, 1667, 274, 34644, 28165, 811, 11133, 31306, 4628, 15926, 19923, 350, 378, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1086731935158754, "compression_ratio": 1.4018691588785046, "no_speech_prob": 0.04711972177028656}, {"id": 91, "seek": 55500, "start": 560.0, "end": 564.0, "text": " Tak powsta\u0142 Palm Coder. Ju\u017c bazowy model by\u0142 niez\u0142y,", "tokens": [50614, 9118, 3388, 9140, 1221, 32668, 383, 19866, 13, 13582, 1427, 27147, 10089, 2316, 16673, 33511, 6825, 11, 50814], "temperature": 0.0, "avg_logprob": -0.1086731935158754, "compression_ratio": 1.4018691588785046, "no_speech_prob": 0.04711972177028656}, {"id": 92, "seek": 55500, "start": 564.0, "end": 569.0, "text": " ale Palm Coder osi\u0105gn\u0105\u0142 wyniki okre\u015blane jako State of the Art,", "tokens": [50814, 6775, 32668, 383, 19866, 3003, 11404, 4568, 1611, 1221, 31936, 9850, 3133, 265, 19212, 1929, 17123, 4533, 295, 264, 5735, 11, 51064], "temperature": 0.0, "avg_logprob": -0.1086731935158754, "compression_ratio": 1.4018691588785046, "no_speech_prob": 0.04711972177028656}, {"id": 93, "seek": 55500, "start": 569.0, "end": 572.0, "text": " czyli najlepsze w swojej dziedzinie w tamtym czasie.", "tokens": [51064, 16591, 41903, 1878, 1381, 261, 29489, 73, 9758, 15338, 259, 414, 261, 7677, 874, 76, 42667, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1086731935158754, "compression_ratio": 1.4018691588785046, "no_speech_prob": 0.04711972177028656}, {"id": 94, "seek": 55500, "start": 572.0, "end": 576.0, "text": " Znakomicie sobie radzi\u0142 zgenerowaniem kod\u00f3w w Pajtonie", "tokens": [51214, 1176, 16852, 21401, 414, 13652, 2843, 3992, 1221, 710, 21848, 37345, 4907, 350, 378, 3901, 261, 430, 1805, 1756, 414, 51414], "temperature": 0.0, "avg_logprob": -0.1086731935158754, "compression_ratio": 1.4018691588785046, "no_speech_prob": 0.04711972177028656}, {"id": 95, "seek": 55500, "start": 576.0, "end": 580.0, "text": " na podstawie opis\u00f3w te\u015bcie, humanival, ale potrafi\u0142 te\u017c co\u015b jeszcze", "tokens": [51414, 1667, 43443, 414, 45477, 3901, 535, 9815, 11, 1952, 3576, 11, 6775, 1847, 10437, 40622, 9516, 19241, 14168, 51614], "temperature": 0.0, "avg_logprob": -0.1086731935158754, "compression_ratio": 1.4018691588785046, "no_speech_prob": 0.04711972177028656}, {"id": 96, "seek": 55500, "start": 580.0, "end": 584.0, "text": " bardziej imponuj\u0105cego \u2013 naprawia\u0107 b\u0142\u0119dy w istniej\u0105cym kodzie.", "tokens": [51614, 27209, 704, 266, 13263, 384, 1571, 1662, 9296, 5131, 654, 2162, 272, 46564, 3173, 261, 1418, 2766, 8555, 1344, 76, 350, 378, 3283, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1086731935158754, "compression_ratio": 1.4018691588785046, "no_speech_prob": 0.04711972177028656}, {"id": 97, "seek": 58400, "start": 584.0, "end": 586.0, "text": " Naprawia\u0107 b\u0142\u0119dy.", "tokens": [50364, 18287, 5131, 654, 2162, 272, 46564, 3173, 13, 50464], "temperature": 0.0, "avg_logprob": -0.07221334321158272, "compression_ratio": 1.4276315789473684, "no_speech_prob": 0.023633701726794243}, {"id": 98, "seek": 58400, "start": 586.0, "end": 590.0, "text": " Ok, to ju\u017c brzmi jak asystent programisty, kt\u00f3rego ka\u017cdy by chcia\u0142 mie\u0107.", "tokens": [50464, 3477, 11, 281, 10678, 738, 89, 3057, 4207, 382, 38593, 317, 1461, 38618, 11, 46951, 31615, 538, 26497, 1221, 35612, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07221334321158272, "compression_ratio": 1.4276315789473684, "no_speech_prob": 0.023633701726794243}, {"id": 99, "seek": 58400, "start": 590.0, "end": 593.0, "text": " Ale czy to na pewno jest przejaw inteligencji,", "tokens": [50664, 9366, 6430, 281, 1667, 33002, 3492, 8325, 22199, 24777, 3213, 19649, 11, 50814], "temperature": 0.0, "avg_logprob": -0.07221334321158272, "compression_ratio": 1.4276315789473684, "no_speech_prob": 0.023633701726794243}, {"id": 100, "seek": 58400, "start": 593.0, "end": 597.0, "text": " a mo\u017ce to po prostu statystyczne powielanie najcz\u0119stszych wzorc\u00f3w z GitHub'a", "tokens": [50814, 257, 12034, 281, 714, 19518, 2219, 38593, 17466, 716, 3388, 1187, 7155, 11212, 41151, 372, 45021, 24809, 284, 29268, 710, 23331, 6, 64, 51014], "temperature": 0.0, "avg_logprob": -0.07221334321158272, "compression_ratio": 1.4276315789473684, "no_speech_prob": 0.023633701726794243}, {"id": 101, "seek": 58400, "start": 597.0, "end": 600.0, "text": " bez \u017cadnego zrozumienia, dlaczego dana poprawka jest dobra?", "tokens": [51014, 10782, 39628, 11858, 710, 27857, 449, 18811, 11, 37873, 39329, 274, 2095, 1665, 5131, 2330, 3492, 360, 6198, 30, 51164], "temperature": 0.0, "avg_logprob": -0.07221334321158272, "compression_ratio": 1.4276315789473684, "no_speech_prob": 0.023633701726794243}, {"id": 102, "seek": 58400, "start": 600.0, "end": 604.0, "text": " To jest \u015bwietne pytanie, kt\u00f3re dotyka sed na debaty o tych modelach.", "tokens": [51164, 1407, 3492, 8299, 39083, 716, 36610, 11, 8864, 5893, 88, 2330, 9643, 1667, 3001, 21398, 277, 15180, 2316, 608, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07221334321158272, "compression_ratio": 1.4276315789473684, "no_speech_prob": 0.023633701726794243}, {"id": 103, "seek": 58400, "start": 604.0, "end": 610.0, "text": " Z jednej strony mo\u017cna argumentowa\u0107, \u017ce to niezwykle zaawansowana mi mikra,", "tokens": [51364, 1176, 5232, 11794, 32406, 17790, 6770, 11445, 11, 3561, 281, 33511, 9726, 14677, 7949, 1607, 599, 40458, 2752, 23959, 424, 11, 51664], "temperature": 0.0, "avg_logprob": -0.07221334321158272, "compression_ratio": 1.4276315789473684, "no_speech_prob": 0.023633701726794243}, {"id": 104, "seek": 61000, "start": 610.0, "end": 614.0, "text": " ale z drugiej sp\u00f3jrzmy na przyk\u0142ad z artyku\u0142u.", "tokens": [50364, 6775, 710, 47373, 637, 18999, 19390, 2226, 1667, 23144, 710, 594, 874, 5279, 24066, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09721717578452706, "compression_ratio": 1.4842105263157894, "no_speech_prob": 0.08961035311222076}, {"id": 105, "seek": 61000, "start": 614.0, "end": 618.0, "text": " Model dosta\u0142 fragment kodu w j\u0119zyku C, kt\u00f3ry mia\u0142 b\u0142\u0119dy uniemo\u017aliwiaj\u0105ce", "tokens": [50564, 17105, 274, 8638, 1221, 26424, 350, 34873, 261, 49055, 5279, 383, 11, 9913, 27989, 272, 46564, 3173, 517, 414, 3280, 10659, 2081, 86, 48125, 384, 50764], "temperature": 0.0, "avg_logprob": -0.09721717578452706, "compression_ratio": 1.4842105263157894, "no_speech_prob": 0.08961035311222076}, {"id": 106, "seek": 61000, "start": 618.0, "end": 620.0, "text": " kompilacj\u0119.", "tokens": [50764, 5207, 79, 388, 29924, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09721717578452706, "compression_ratio": 1.4842105263157894, "no_speech_prob": 0.08961035311222076}, {"id": 107, "seek": 61000, "start": 620.0, "end": 626.0, "text": " PLM Coder nie tylko naprawi\u0142 te b\u0142\u0119dy, ale przy okazji poprawi\u0142 te\u017c styl kodu,", "tokens": [50864, 6999, 44, 383, 19866, 2838, 13219, 9296, 5131, 40622, 535, 272, 46564, 3173, 11, 6775, 6501, 3133, 921, 4013, 1665, 5131, 40622, 9516, 23736, 350, 34873, 11, 51164], "temperature": 0.0, "avg_logprob": -0.09721717578452706, "compression_ratio": 1.4842105263157894, "no_speech_prob": 0.08961035311222076}, {"id": 108, "seek": 61000, "start": 626.0, "end": 630.0, "text": " na przyk\u0142ad po\u0142\u0105czy\u0142 deklaracj\u0119 kilkuzmiennych w jedn\u0105 linijk\u0119,", "tokens": [51164, 1667, 23144, 714, 15926, 6522, 1221, 368, 74, 2200, 29924, 5128, 74, 3334, 76, 1053, 9399, 261, 5232, 13113, 22896, 1718, 15724, 11, 51364], "temperature": 0.0, "avg_logprob": -0.09721717578452706, "compression_ratio": 1.4842105263157894, "no_speech_prob": 0.08961035311222076}, {"id": 109, "seek": 61000, "start": 630.0, "end": 632.0, "text": " co jest uznawany za dobr\u0105 praktyk\u0119.", "tokens": [51364, 598, 3492, 16851, 629, 86, 1325, 7949, 23067, 1611, 3206, 74, 874, 15724, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09721717578452706, "compression_ratio": 1.4842105263157894, "no_speech_prob": 0.08961035311222076}, {"id": 110, "seek": 61000, "start": 632.0, "end": 637.0, "text": " Czyli nie tylko sprawi\u0142, \u017ce kod zacz\u0105\u0142 dzia\u0142a\u0107, ale te\u017c, \u017ce wygl\u0105da\u0142 lepiej?", "tokens": [51464, 37099, 2838, 13219, 22734, 40622, 11, 3561, 350, 378, 34430, 8925, 1221, 37903, 2162, 11, 6775, 9516, 11, 3561, 32015, 1221, 476, 39699, 30, 51714], "temperature": 0.0, "avg_logprob": -0.09721717578452706, "compression_ratio": 1.4842105263157894, "no_speech_prob": 0.08961035311222076}, {"id": 111, "seek": 63700, "start": 637.0, "end": 641.0, "text": " Tak, to sugeruje, \u017ce uczy\u0142 si\u0119 nie tylko czystej sk\u0142adni j\u0119zyka,", "tokens": [50364, 9118, 11, 281, 459, 1321, 13008, 11, 3561, 344, 6522, 1221, 3244, 2838, 13219, 6430, 2941, 73, 1110, 10358, 3722, 42309, 40940, 11, 50564], "temperature": 0.0, "avg_logprob": -0.06212170295466005, "compression_ratio": 1.45, "no_speech_prob": 0.05799998342990875}, {"id": 112, "seek": 63700, "start": 641.0, "end": 645.0, "text": " ale te\u017c pewnych konwencji, wzorc\u00f3w i stylu programowania,", "tokens": [50564, 6775, 9516, 47160, 16384, 5897, 15615, 19649, 11, 24809, 284, 29268, 741, 7952, 2781, 1461, 21308, 11, 50764], "temperature": 0.0, "avg_logprob": -0.06212170295466005, "compression_ratio": 1.45, "no_speech_prob": 0.05799998342990875}, {"id": 113, "seek": 63700, "start": 645.0, "end": 647.0, "text": " preferowanego przez ludzi.", "tokens": [50764, 4382, 37345, 6308, 14064, 29586, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06212170295466005, "compression_ratio": 1.45, "no_speech_prob": 0.05799998342990875}, {"id": 114, "seek": 63700, "start": 647.0, "end": 650.0, "text": " Niezale\u017cnie od tego, czy nazwiemy to zrozumieniem, czy nie,", "tokens": [50864, 12016, 89, 45494, 2766, 3611, 8627, 11, 6430, 20151, 8699, 2226, 281, 710, 27857, 449, 1053, 4907, 11, 6430, 2838, 11, 51014], "temperature": 0.0, "avg_logprob": -0.06212170295466005, "compression_ratio": 1.45, "no_speech_prob": 0.05799998342990875}, {"id": 115, "seek": 63700, "start": 650.0, "end": 655.0, "text": " z praktycznego punktu widzenia, jest to niezwykle u\u017cyteczna zdolno\u015b\u0107.", "tokens": [51014, 710, 3206, 74, 874, 3689, 11858, 39561, 84, 5274, 14320, 11, 3492, 281, 33511, 9726, 14677, 34097, 975, 3689, 629, 16221, 401, 23293, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06212170295466005, "compression_ratio": 1.45, "no_speech_prob": 0.05799998342990875}, {"id": 116, "seek": 63700, "start": 655.0, "end": 660.0, "text": " Dobrze, to wszystko brzni imponuj\u0105co, ale mam jedno ale.", "tokens": [51264, 29679, 13503, 11, 281, 22607, 738, 89, 3722, 704, 266, 13263, 1291, 11, 6775, 13524, 5232, 1771, 6775, 13, 51514], "temperature": 0.0, "avg_logprob": -0.06212170295466005, "compression_ratio": 1.45, "no_speech_prob": 0.05799998342990875}, {"id": 117, "seek": 63700, "start": 660.0, "end": 664.0, "text": " M\u00f3wi\u0142a\u015b na pocz\u0105tku, \u017ce dane by\u0142y w przyt\u0142aczaj\u0105cej wi\u0119kszo\u015bci po angielsku.", "tokens": [51514, 376, 3901, 72, 5024, 1788, 1667, 43959, 11, 3561, 49206, 26366, 261, 6501, 83, 1221, 14875, 11133, 20811, 29968, 4765, 6199, 714, 2562, 1187, 5161, 84, 13, 51714], "temperature": 0.0, "avg_logprob": -0.06212170295466005, "compression_ratio": 1.45, "no_speech_prob": 0.05799998342990875}, {"id": 118, "seek": 66400, "start": 664.0, "end": 667.0, "text": " Ponad 78% ca\u0142o\u015bci.", "tokens": [50364, 31756, 345, 26369, 4, 1335, 35059, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07769058732425466, "compression_ratio": 1.3529411764705883, "no_speech_prob": 0.026136398315429688}, {"id": 119, "seek": 66400, "start": 667.0, "end": 670.0, "text": " Czy to nie oznacza, \u017ce stworzyli\u015bmy genialnego lingwist\u0119,", "tokens": [50514, 19832, 281, 2838, 277, 22672, 326, 2394, 11, 3561, 342, 28321, 1229, 38452, 48228, 11858, 22949, 86, 468, 1274, 11, 50664], "temperature": 0.0, "avg_logprob": -0.07769058732425466, "compression_ratio": 1.3529411764705883, "no_speech_prob": 0.026136398315429688}, {"id": 120, "seek": 66400, "start": 670.0, "end": 673.0, "text": " kt\u00f3ry tak naprawd\u0119 m\u00f3wi p\u0142ynnie tylko w jednym j\u0119zyku,", "tokens": [50664, 9913, 991, 20970, 24592, 28695, 2534, 2766, 13219, 261, 5232, 12996, 49055, 5279, 11, 50814], "temperature": 0.0, "avg_logprob": -0.07769058732425466, "compression_ratio": 1.3529411764705883, "no_speech_prob": 0.026136398315429688}, {"id": 121, "seek": 66400, "start": 673.0, "end": 676.0, "text": " a reszt\u0119 ledwo nowie\u017c duka?", "tokens": [50814, 257, 725, 2682, 1274, 4684, 6120, 586, 414, 1427, 274, 13599, 30, 50964], "temperature": 0.0, "avg_logprob": -0.07769058732425466, "compression_ratio": 1.3529411764705883, "no_speech_prob": 0.026136398315429688}, {"id": 122, "seek": 66400, "start": 676.0, "end": 679.0, "text": " Logika podpowiada\u0142aby, \u017ce tak.", "tokens": [50964, 10824, 5439, 2497, 14701, 39018, 1221, 2509, 11, 3561, 991, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07769058732425466, "compression_ratio": 1.3529411764705883, "no_speech_prob": 0.026136398315429688}, {"id": 123, "seek": 66400, "start": 679.0, "end": 682.0, "text": " I tu dochodzimy do kolejnego zaskoczenia z tej pracy.", "tokens": [51114, 286, 2604, 9243, 378, 89, 13189, 360, 23749, 11858, 710, 3863, 905, 14320, 710, 12573, 35591, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07769058732425466, "compression_ratio": 1.3529411764705883, "no_speech_prob": 0.026136398315429688}, {"id": 124, "seek": 66400, "start": 682.0, "end": 685.0, "text": " Mimo tak ogromnej dysproporcji w danych,", "tokens": [51264, 376, 6934, 991, 34416, 298, 11794, 15243, 79, 1513, 284, 19649, 261, 274, 34644, 11, 51414], "temperature": 0.0, "avg_logprob": -0.07769058732425466, "compression_ratio": 1.3529411764705883, "no_speech_prob": 0.026136398315429688}, {"id": 125, "seek": 66400, "start": 685.0, "end": 690.0, "text": " Palm radzi\u0142 sobie zaskakuj\u0105co dobrze w zadaniach wieloj\u0119zycznych.", "tokens": [51414, 32668, 2843, 3992, 1221, 13652, 710, 3863, 514, 13263, 1291, 28335, 261, 42788, 3782, 608, 20570, 78, 11115, 1229, 3689, 9399, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07769058732425466, "compression_ratio": 1.3529411764705883, "no_speech_prob": 0.026136398315429688}, {"id": 126, "seek": 69000, "start": 690.0, "end": 693.0, "text": " W niekt\u00f3rych przypadkach pokonywa\u0142 nawet modele,", "tokens": [50364, 343, 2838, 43073, 627, 339, 33100, 41326, 13010, 2526, 44603, 22696, 4391, 306, 11, 50514], "temperature": 0.0, "avg_logprob": -0.055612080580704694, "compression_ratio": 1.5547169811320756, "no_speech_prob": 0.1545431762933731}, {"id": 127, "seek": 69000, "start": 693.0, "end": 696.0, "text": " kt\u00f3re by\u0142y specjalnie trenowane do t\u0142umacze\u0144,", "tokens": [50514, 8864, 26366, 46433, 2766, 23136, 23066, 360, 256, 49166, 326, 49689, 11, 50664], "temperature": 0.0, "avg_logprob": -0.055612080580704694, "compression_ratio": 1.5547169811320756, "no_speech_prob": 0.1545431762933731}, {"id": 128, "seek": 69000, "start": 696.0, "end": 699.0, "text": " np. w t\u0142umaczeniu z rumu\u0144skiego na angielski.", "tokens": [50664, 33808, 13, 261, 256, 49166, 326, 39651, 710, 8347, 84, 27125, 12200, 1667, 2562, 1187, 18020, 13, 50814], "temperature": 0.0, "avg_logprob": -0.055612080580704694, "compression_ratio": 1.5547169811320756, "no_speech_prob": 0.1545431762933731}, {"id": 129, "seek": 69000, "start": 699.0, "end": 703.0, "text": " Zaskakuj\u0105ce, ale pewnie jest tu jaki\u015b haczyk.", "tokens": [50814, 1176, 3863, 514, 13263, 384, 11, 6775, 520, 14215, 3492, 2604, 34721, 324, 6522, 74, 13, 51014], "temperature": 0.0, "avg_logprob": -0.055612080580704694, "compression_ratio": 1.5547169811320756, "no_speech_prob": 0.1545431762933731}, {"id": 130, "seek": 69000, "start": 703.0, "end": 706.0, "text": " Oczywi\u015bcie, kluczowe zastrze\u017cenie jest takie,", "tokens": [51014, 42980, 11, 9671, 1311, 89, 6880, 36746, 13503, 41118, 3492, 15963, 11, 51164], "temperature": 0.0, "avg_logprob": -0.055612080580704694, "compression_ratio": 1.5547169811320756, "no_speech_prob": 0.1545431762933731}, {"id": 131, "seek": 69000, "start": 706.0, "end": 712.0, "text": " \u017ce model by\u0142 znacznie, znacznie lepszy w t\u0142umaczeniu na angielski,", "tokens": [51164, 3561, 2316, 16673, 15397, 14875, 2766, 11, 15397, 14875, 2766, 476, 1878, 1229, 261, 256, 49166, 326, 39651, 1667, 2562, 1187, 18020, 11, 51464], "temperature": 0.0, "avg_logprob": -0.055612080580704694, "compression_ratio": 1.5547169811320756, "no_speech_prob": 0.1545431762933731}, {"id": 132, "seek": 69000, "start": 712.0, "end": 715.0, "text": " ni\u017c z angielskiego na inne j\u0119zyki.", "tokens": [51464, 28502, 710, 2562, 1187, 5161, 12200, 1667, 24170, 49055, 2984, 13, 51614], "temperature": 0.0, "avg_logprob": -0.055612080580704694, "compression_ratio": 1.5547169811320756, "no_speech_prob": 0.1545431762933731}, {"id": 133, "seek": 69000, "start": 715.0, "end": 719.0, "text": " I to jest bezpo\u015brednie odbicie sk\u0142adu danych treningowych.", "tokens": [51614, 286, 281, 3492, 10782, 2259, 1788, 986, 2766, 3611, 65, 28434, 1110, 10358, 84, 274, 34644, 2192, 773, 19605, 13, 51814], "temperature": 0.0, "avg_logprob": -0.055612080580704694, "compression_ratio": 1.5547169811320756, "no_speech_prob": 0.1545431762933731}, {"id": 134, "seek": 71900, "start": 719.0, "end": 723.0, "text": " Poniewa\u017c angielski by\u0142 tak dominuj\u0105cy w jego, powiedzmy, diecie,", "tokens": [50364, 31756, 27806, 2562, 1187, 18020, 16673, 991, 8859, 13263, 1344, 261, 26542, 11, 27617, 2226, 11, 978, 4260, 11, 50564], "temperature": 0.0, "avg_logprob": -0.05797643335456522, "compression_ratio": 1.415686274509804, "no_speech_prob": 0.008779718540608883}, {"id": 135, "seek": 71900, "start": 723.0, "end": 728.0, "text": " model nauczy\u0142 si\u0119 go generowa\u0107 na absolutnie najwy\u017cszym poziomie.", "tokens": [50564, 2316, 49103, 1229, 1221, 3244, 352, 1337, 11445, 1667, 18757, 2766, 11212, 9726, 1427, 7706, 76, 38503, 40120, 13, 50814], "temperature": 0.0, "avg_logprob": -0.05797643335456522, "compression_ratio": 1.415686274509804, "no_speech_prob": 0.008779718540608883}, {"id": 136, "seek": 71900, "start": 728.0, "end": 732.0, "text": " Sta\u0142 si\u0119 dla niego j\u0119zykiem docelowym punktem odniesienia.", "tokens": [50814, 16959, 1221, 3244, 12285, 49615, 49055, 26116, 3211, 338, 31691, 39561, 443, 3611, 40549, 18811, 13, 51014], "temperature": 0.0, "avg_logprob": -0.05797643335456522, "compression_ratio": 1.415686274509804, "no_speech_prob": 0.008779718540608883}, {"id": 137, "seek": 71900, "start": 732.0, "end": 737.0, "text": " To idealnie pokazuje, jak fundamentalnie sk\u0142ad danych treningowych", "tokens": [51014, 1407, 7157, 2766, 13010, 43317, 11, 4207, 8088, 2766, 1110, 10358, 274, 34644, 2192, 773, 19605, 51264], "temperature": 0.0, "avg_logprob": -0.05797643335456522, "compression_ratio": 1.415686274509804, "no_speech_prob": 0.008779718540608883}, {"id": 138, "seek": 71900, "start": 737.0, "end": 742.0, "text": " kszta\u0142tuje ostateczne zdolno\u015bci i dziwactwa modelu.", "tokens": [51264, 350, 15453, 46426, 9179, 2884, 277, 15406, 38491, 16221, 401, 16438, 741, 31981, 86, 578, 4151, 2316, 84, 13, 51514], "temperature": 0.0, "avg_logprob": -0.05797643335456522, "compression_ratio": 1.415686274509804, "no_speech_prob": 0.008779718540608883}, {"id": 139, "seek": 71900, "start": 742.0, "end": 743.0, "text": " Rozumiem.", "tokens": [51514, 43313, 449, 4907, 13, 51564], "temperature": 0.0, "avg_logprob": -0.05797643335456522, "compression_ratio": 1.415686274509804, "no_speech_prob": 0.008779718540608883}, {"id": 140, "seek": 71900, "start": 743.0, "end": 745.0, "text": " Czyli mamy ten ogromny model,", "tokens": [51564, 37099, 17335, 2064, 34416, 298, 1634, 2316, 11, 51664], "temperature": 0.0, "avg_logprob": -0.05797643335456522, "compression_ratio": 1.415686274509804, "no_speech_prob": 0.008779718540608883}, {"id": 141, "seek": 74500, "start": 745.0, "end": 751.0, "text": " wytrenowany z prze\u0142omow\u0105 wydajno\u015bci\u0105, kt\u00f3ry potrafi rozumowa\u0107 krok po kroku,", "tokens": [50364, 261, 4328, 1095, 23341, 710, 8325, 1221, 298, 30297, 25984, 1805, 16438, 1611, 11, 9913, 1847, 10437, 72, 48797, 11445, 350, 31621, 714, 45909, 5279, 11, 50664], "temperature": 0.0, "avg_logprob": -0.06954717960487418, "compression_ratio": 1.4049295774647887, "no_speech_prob": 0.15274518728256226}, {"id": 142, "seek": 74500, "start": 751.0, "end": 755.0, "text": " opowiada\u0107 do wcipy, pisa\u0107 kod i ca\u0142kiem nie\u017ale t\u0142umaczy\u0107,", "tokens": [50664, 999, 24503, 1538, 2162, 360, 261, 537, 8200, 11, 280, 3837, 2162, 350, 378, 741, 35224, 26116, 2838, 10659, 306, 256, 49166, 14691, 2162, 11, 50864], "temperature": 0.0, "avg_logprob": -0.06954717960487418, "compression_ratio": 1.4049295774647887, "no_speech_prob": 0.15274518728256226}, {"id": 143, "seek": 74500, "start": 755.0, "end": 758.0, "text": " mimo \u017ce uczy\u0142 si\u0119 g\u0142\u00f3wnie z jednego j\u0119zyka.", "tokens": [50864, 275, 6934, 3561, 344, 6522, 1221, 3244, 18117, 812, 14215, 710, 5232, 11858, 42309, 40940, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06954717960487418, "compression_ratio": 1.4049295774647887, "no_speech_prob": 0.15274518728256226}, {"id": 144, "seek": 74500, "start": 758.0, "end": 761.0, "text": " Wszystko to zdaje si\u0119 krzycze\u0107, bigger is better.", "tokens": [51014, 343, 10424, 4093, 281, 16221, 11153, 3244, 350, 13047, 9680, 2162, 11, 3801, 307, 1101, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06954717960487418, "compression_ratio": 1.4049295774647887, "no_speech_prob": 0.15274518728256226}, {"id": 145, "seek": 74500, "start": 761.0, "end": 763.0, "text": " Im wi\u0119kszy model, tym lepiej.", "tokens": [51164, 4331, 29968, 1229, 2316, 11, 8107, 476, 39699, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06954717960487418, "compression_ratio": 1.4049295774647887, "no_speech_prob": 0.15274518728256226}, {"id": 146, "seek": 74500, "start": 763.0, "end": 766.0, "text": " To chyba by\u0142a g\u0142\u00f3wna konkluzja w tamtym czasie.", "tokens": [51264, 1407, 31532, 23936, 18117, 3901, 629, 21428, 2781, 89, 2938, 261, 7677, 874, 76, 42667, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06954717960487418, "compression_ratio": 1.4049295774647887, "no_speech_prob": 0.15274518728256226}, {"id": 147, "seek": 74500, "start": 766.0, "end": 769.0, "text": " Tak, to by\u0142 dominuj\u0105cy paradygmat.", "tokens": [51414, 9118, 11, 281, 16673, 8859, 13263, 1344, 13480, 18103, 15677, 13, 51564], "temperature": 0.0, "avg_logprob": -0.06954717960487418, "compression_ratio": 1.4049295774647887, "no_speech_prob": 0.15274518728256226}, {"id": 148, "seek": 74500, "start": 769.0, "end": 771.0, "text": " Skala wydawa\u0142a si\u0119 kr\u00f3low\u0105,", "tokens": [51564, 7324, 5159, 25984, 10449, 5024, 3244, 42366, 14107, 1611, 11, 51664], "temperature": 0.0, "avg_logprob": -0.06954717960487418, "compression_ratio": 1.4049295774647887, "no_speech_prob": 0.15274518728256226}, {"id": 149, "seek": 77100, "start": 771.0, "end": 777.0, "text": " ale co fascynuj\u0105ce, sam artyku\u0142 ko\u0144czy si\u0119 wa\u017cnym, otwartym pytaniem,", "tokens": [50364, 6775, 598, 30632, 1344, 77, 13263, 384, 11, 3247, 594, 874, 5279, 1221, 26470, 6522, 3244, 27777, 12996, 11, 4337, 29587, 4199, 25878, 282, 4907, 11, 50664], "temperature": 0.0, "avg_logprob": -0.10444996130727503, "compression_ratio": 1.3942652329749103, "no_speech_prob": 0.17594261467456818}, {"id": 150, "seek": 77100, "start": 777.0, "end": 781.0, "text": " kt\u00f3re chwil\u0119 p\u00f3\u017aniej sta\u0142o si\u0119 centralnym punktem dyskusji w ca\u0142ej bran\u017cy.", "tokens": [50664, 8864, 41941, 1274, 36968, 11135, 5249, 3244, 5777, 12996, 39561, 443, 15243, 35080, 4013, 261, 47631, 73, 12029, 7735, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10444996130727503, "compression_ratio": 1.3942652329749103, "no_speech_prob": 0.17594261467456818}, {"id": 151, "seek": 77100, "start": 781.0, "end": 786.0, "text": " G\u0142\u00f3wnie za spraw\u0105 publikacji na temat innego modelu, Chinchilla.", "tokens": [50864, 460, 1221, 812, 14215, 7949, 22734, 1611, 11227, 1035, 13152, 1667, 32954, 294, 11858, 2316, 84, 11, 4430, 339, 5291, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10444996130727503, "compression_ratio": 1.3942652329749103, "no_speech_prob": 0.17594261467456818}, {"id": 152, "seek": 77100, "start": 786.0, "end": 787.0, "text": " Jakie to by\u0142o pytanie?", "tokens": [51114, 15029, 414, 281, 14811, 36610, 30, 51164], "temperature": 0.0, "avg_logprob": -0.10444996130727503, "compression_ratio": 1.3942652329749103, "no_speech_prob": 0.17594261467456818}, {"id": 153, "seek": 77100, "start": 787.0, "end": 788.0, "text": " Pytanie brzmi.", "tokens": [51164, 430, 4328, 7155, 738, 89, 3057, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10444996130727503, "compression_ratio": 1.3942652329749103, "no_speech_prob": 0.17594261467456818}, {"id": 154, "seek": 77100, "start": 788.0, "end": 792.0, "text": " Maj\u0105c do dyspozycji okre\u015blon\u0105, ograniczon\u0105 moc obliczeniow\u0105,", "tokens": [51214, 7048, 1611, 66, 360, 15243, 2259, 1229, 19649, 3133, 265, 19212, 266, 1611, 11, 34416, 282, 17946, 266, 1611, 34962, 1111, 1050, 42124, 30297, 11, 51414], "temperature": 0.0, "avg_logprob": -0.10444996130727503, "compression_ratio": 1.3942652329749103, "no_speech_prob": 0.17594261467456818}, {"id": 155, "seek": 77100, "start": 792.0, "end": 796.0, "text": " a ona zawsze jest ograniczona, co jest lepsz\u0105 strategi\u0105.", "tokens": [51414, 257, 20325, 30964, 3492, 34416, 30732, 13383, 11, 598, 3492, 476, 1878, 8925, 5464, 11404, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10444996130727503, "compression_ratio": 1.3942652329749103, "no_speech_prob": 0.17594261467456818}, {"id": 156, "seek": 79600, "start": 796.0, "end": 799.0, "text": " Czy zbudowanie absolutnie gigantycznego modelu", "tokens": [50364, 19832, 710, 18281, 22028, 18757, 2766, 8741, 394, 17466, 11858, 2316, 84, 50514], "temperature": 0.0, "avg_logprob": -0.08412282930003653, "compression_ratio": 1.5036231884057971, "no_speech_prob": 0.05684792995452881}, {"id": 157, "seek": 79600, "start": 799.0, "end": 804.0, "text": " i wytrenowanie go na powiedzmy jednym cyklu danych, tak jak zrobiono z palm,", "tokens": [50514, 741, 261, 4328, 1095, 22028, 352, 1667, 27617, 2226, 5232, 12996, 3185, 74, 2781, 274, 34644, 11, 991, 4207, 44399, 49020, 710, 17018, 11, 50764], "temperature": 0.0, "avg_logprob": -0.08412282930003653, "compression_ratio": 1.5036231884057971, "no_speech_prob": 0.05684792995452881}, {"id": 158, "seek": 79600, "start": 804.0, "end": 807.0, "text": " czy mo\u017ce lepiej jest zbudowa\u0107 nieco mniejszy model,", "tokens": [50764, 6430, 12034, 476, 39699, 3492, 710, 18281, 11445, 2838, 1291, 39513, 7706, 2316, 11, 50914], "temperature": 0.0, "avg_logprob": -0.08412282930003653, "compression_ratio": 1.5036231884057971, "no_speech_prob": 0.05684792995452881}, {"id": 159, "seek": 79600, "start": 807.0, "end": 811.0, "text": " ale za to trenowa\u0107 go na znacznie, znacznie wi\u0119ksze ilo\u015bci danych.", "tokens": [50914, 6775, 7949, 281, 23136, 11445, 352, 1667, 15397, 14875, 2766, 11, 15397, 14875, 2766, 29968, 1381, 1930, 44468, 274, 34644, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08412282930003653, "compression_ratio": 1.5036231884057971, "no_speech_prob": 0.05684792995452881}, {"id": 160, "seek": 79600, "start": 811.0, "end": 813.0, "text": " Czyli co jest wa\u017cniejsze?", "tokens": [51114, 37099, 598, 3492, 27777, 44258, 30, 51214], "temperature": 0.0, "avg_logprob": -0.08412282930003653, "compression_ratio": 1.5036231884057971, "no_speech_prob": 0.05684792995452881}, {"id": 161, "seek": 79600, "start": 813.0, "end": 816.0, "text": " Rozmiar m\u00f3zgu czy ilo\u015b\u0107 przeczytanych ksi\u0105\u017cek?", "tokens": [51214, 43313, 3057, 289, 32515, 89, 2794, 6430, 1930, 78, 7753, 8325, 6522, 83, 34644, 39311, 916, 30, 51364], "temperature": 0.0, "avg_logprob": -0.08412282930003653, "compression_ratio": 1.5036231884057971, "no_speech_prob": 0.05684792995452881}, {"id": 162, "seek": 79600, "start": 816.0, "end": 818.0, "text": " I jaka jest odpowied\u017a?", "tokens": [51364, 286, 4207, 64, 3492, 36574, 10659, 30, 51464], "temperature": 0.0, "avg_logprob": -0.08412282930003653, "compression_ratio": 1.5036231884057971, "no_speech_prob": 0.05684792995452881}, {"id": 163, "seek": 79600, "start": 818.0, "end": 822.0, "text": " P\u00f3\u017aniejsze badania, w\u0142a\u015bnie te dotycz\u0105ce modelu Chinchilla,", "tokens": [51464, 430, 812, 33405, 82, 1381, 1578, 5609, 11, 14234, 535, 5893, 17466, 1611, 384, 2316, 84, 4430, 339, 5291, 11, 51664], "temperature": 0.0, "avg_logprob": -0.08412282930003653, "compression_ratio": 1.5036231884057971, "no_speech_prob": 0.05684792995452881}, {"id": 164, "seek": 82200, "start": 822.0, "end": 826.0, "text": " mocno zasugerowa\u0142y, \u017ce wiele ogromnych modeli, w tym palm,", "tokens": [50364, 34962, 1771, 26530, 44953, 5528, 6825, 11, 3561, 33137, 34416, 298, 9399, 2316, 72, 11, 261, 8107, 17018, 11, 50564], "temperature": 0.0, "avg_logprob": -0.039257791307237416, "compression_ratio": 1.4740484429065743, "no_speech_prob": 0.003470486029982567}, {"id": 165, "seek": 82200, "start": 826.0, "end": 828.0, "text": " mog\u0142o by\u0107 niedotrenowanych.", "tokens": [50564, 13172, 5249, 15069, 32488, 310, 1095, 23341, 339, 13, 50664], "temperature": 0.0, "avg_logprob": -0.039257791307237416, "compression_ratio": 1.4740484429065743, "no_speech_prob": 0.003470486029982567}, {"id": 166, "seek": 82200, "start": 828.0, "end": 832.0, "text": " Innymi s\u0142owy by\u0142y zbyt du\u017ce w stosunku do ilo\u015bci danych,", "tokens": [50664, 682, 31813, 15116, 10089, 26366, 710, 2322, 83, 1581, 2875, 261, 43581, 49910, 360, 1930, 44468, 274, 34644, 11, 50864], "temperature": 0.0, "avg_logprob": -0.039257791307237416, "compression_ratio": 1.4740484429065743, "no_speech_prob": 0.003470486029982567}, {"id": 167, "seek": 82200, "start": 832.0, "end": 834.0, "text": " kt\u00f3re przetworzy\u0142y w trakcie treningu.", "tokens": [50864, 8864, 6541, 302, 28321, 1229, 6825, 261, 944, 74, 4260, 2192, 773, 84, 13, 50964], "temperature": 0.0, "avg_logprob": -0.039257791307237416, "compression_ratio": 1.4740484429065743, "no_speech_prob": 0.003470486029982567}, {"id": 168, "seek": 82200, "start": 834.0, "end": 839.0, "text": " Okaza\u0142o si\u0119, \u017ce mniejszy model, ale karmiony danymi przez d\u0142u\u017cszy czas,", "tokens": [50964, 3477, 12257, 5249, 3244, 11, 3561, 39513, 7706, 2316, 11, 6775, 350, 4452, 46184, 274, 1325, 3057, 14064, 274, 24066, 1427, 7706, 13190, 11, 51214], "temperature": 0.0, "avg_logprob": -0.039257791307237416, "compression_ratio": 1.4740484429065743, "no_speech_prob": 0.003470486029982567}, {"id": 169, "seek": 82200, "start": 839.0, "end": 842.0, "text": " mo\u017ce osi\u0105gn\u0105\u0107 lepsze wyniki ni\u017c gigant,", "tokens": [51214, 12034, 3003, 11404, 4568, 36374, 476, 1878, 1381, 31936, 9850, 28502, 8741, 394, 11, 51364], "temperature": 0.0, "avg_logprob": -0.039257791307237416, "compression_ratio": 1.4740484429065743, "no_speech_prob": 0.003470486029982567}, {"id": 170, "seek": 82200, "start": 842.0, "end": 844.0, "text": " kt\u00f3ry tych danych tylko skosztowa\u0142.", "tokens": [51364, 9913, 15180, 274, 34644, 13219, 1110, 329, 2682, 30105, 13, 51464], "temperature": 0.0, "avg_logprob": -0.039257791307237416, "compression_ratio": 1.4740484429065743, "no_speech_prob": 0.003470486029982567}, {"id": 171, "seek": 82200, "start": 844.0, "end": 849.0, "text": " Czyli wy\u015bcig na sam\u0105 wielko\u015b\u0107 modelu, na czyst\u0105 liczb\u0119 parametr\u00f3w,", "tokens": [51464, 37099, 4628, 1788, 66, 328, 1667, 3247, 1611, 20570, 4093, 7753, 2316, 84, 11, 1667, 6430, 372, 1611, 6169, 89, 65, 1274, 6220, 27965, 3901, 11, 51714], "temperature": 0.0, "avg_logprob": -0.039257791307237416, "compression_ratio": 1.4740484429065743, "no_speech_prob": 0.003470486029982567}, {"id": 172, "seek": 84900, "start": 849.0, "end": 853.0, "text": " mo\u017ce nie by\u0107 jedyn\u0105, a nawet nie najlepsz\u0105 drog\u0105 naprz\u00f3d.", "tokens": [50364, 12034, 2838, 15069, 5232, 2534, 1611, 11, 257, 22696, 2838, 41903, 1878, 8925, 3789, 70, 1611, 9296, 19390, 17081, 13, 50564], "temperature": 0.0, "avg_logprob": -0.0690160526169671, "compression_ratio": 1.430921052631579, "no_speech_prob": 0.007566776592284441}, {"id": 173, "seek": 84900, "start": 853.0, "end": 857.0, "text": " Dok\u0142adnie. To by\u0142 bardzo wa\u017cny moment odsze\u017awienia dla ca\u0142ej dziedziny.", "tokens": [50564, 29768, 10358, 2766, 13, 1407, 16673, 9034, 27777, 1634, 1623, 3611, 82, 1381, 10659, 86, 18811, 12285, 47631, 73, 9758, 15338, 3519, 13, 50764], "temperature": 0.0, "avg_logprob": -0.0690160526169671, "compression_ratio": 1.430921052631579, "no_speech_prob": 0.007566776592284441}, {"id": 174, "seek": 84900, "start": 857.0, "end": 862.0, "text": " Zrozumiano, \u017ce istnieje pewna optymalna r\u00f3wnowaga mi\u0119dzy rozmiarem modelu", "tokens": [50764, 1176, 27857, 449, 6254, 11, 3561, 1418, 2766, 2884, 25889, 629, 2427, 4199, 304, 629, 11416, 895, 305, 9286, 33964, 9544, 3057, 19183, 2316, 84, 51014], "temperature": 0.0, "avg_logprob": -0.0690160526169671, "compression_ratio": 1.430921052631579, "no_speech_prob": 0.007566776592284441}, {"id": 175, "seek": 84900, "start": 862.0, "end": 864.0, "text": " a ilo\u015bci\u0105 danych treningowych.", "tokens": [51014, 257, 1930, 44468, 1611, 274, 34644, 2192, 773, 19605, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0690160526169671, "compression_ratio": 1.430921052631579, "no_speech_prob": 0.007566776592284441}, {"id": 176, "seek": 84900, "start": 864.0, "end": 867.0, "text": " Nie chodzi tylko o to, by budowa\u0107 najwi\u0119ksze katedry,", "tokens": [51114, 12016, 23998, 13219, 277, 281, 11, 538, 3265, 11445, 48636, 1694, 1381, 350, 770, 627, 11, 51264], "temperature": 0.0, "avg_logprob": -0.0690160526169671, "compression_ratio": 1.430921052631579, "no_speech_prob": 0.007566776592284441}, {"id": 177, "seek": 84900, "start": 867.0, "end": 870.0, "text": " ale by robi\u0107 to z odpowiedniej ilo\u015bci cegie\u0142.", "tokens": [51264, 6775, 538, 46900, 281, 710, 36574, 10402, 1930, 44468, 269, 1146, 414, 1221, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0690160526169671, "compression_ratio": 1.430921052631579, "no_speech_prob": 0.007566776592284441}, {"id": 178, "seek": 84900, "start": 870.0, "end": 876.0, "text": " A wi\u0119c nast\u0119pny prze\u0142om mo\u017ce nie polega\u0107 na stworzeniu jeszcze wi\u0119kszego,", "tokens": [51414, 316, 16677, 39662, 1634, 8325, 1221, 298, 12034, 2838, 13208, 3680, 2162, 1667, 342, 28321, 39651, 14168, 29968, 27725, 11, 51714], "temperature": 0.0, "avg_logprob": -0.0690160526169671, "compression_ratio": 1.430921052631579, "no_speech_prob": 0.007566776592284441}, {"id": 179, "seek": 87600, "start": 876.0, "end": 880.0, "text": " trillionowego modelu, ale na m\u0105drzejszym jego trenowaniu", "tokens": [50364, 504, 11836, 26576, 2316, 84, 11, 6775, 1667, 275, 18962, 13503, 73, 7706, 76, 26542, 23136, 305, 25849, 50564], "temperature": 0.0, "avg_logprob": -0.07619936370849609, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.029103519394993782}, {"id": 180, "seek": 87600, "start": 880.0, "end": 883.0, "text": " i znalezieniu tego z\u0142otego \u015brodka.", "tokens": [50564, 741, 15397, 37646, 1053, 5951, 8627, 31614, 310, 6308, 28580, 2330, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07619936370849609, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.029103519394993782}, {"id": 181, "seek": 87600, "start": 883.0, "end": 887.0, "text": " To pozostawia nas z niezwykle ciekawym pytaniem na przysz\u0142o\u015b\u0107.", "tokens": [50714, 1407, 21281, 555, 34953, 5382, 710, 33511, 9726, 14677, 46419, 1607, 4199, 25878, 282, 4907, 1667, 44018, 44742, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07619936370849609, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.029103519394993782}, {"id": 182, "seek": 87600, "start": 887.0, "end": 892.0, "text": " Jakie nowe, jeszcze bardziej nieprzewidywalne zdolno\u015bci wy\u0142oni\u0105 si\u0119,", "tokens": [50914, 15029, 414, 586, 68, 11, 14168, 27209, 2838, 1424, 43551, 327, 27112, 304, 716, 16221, 401, 16438, 4628, 1221, 266, 11404, 3244, 11, 51164], "temperature": 0.0, "avg_logprob": -0.07619936370849609, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.029103519394993782}, {"id": 183, "seek": 87600, "start": 892.0, "end": 895.0, "text": " gdy w ko\u0144cu znajdziemy idealn\u0105 r\u00f3wnowag\u0119", "tokens": [51164, 28405, 261, 26470, 12032, 27318, 13096, 2226, 7157, 13113, 11416, 895, 305, 40748, 51314], "temperature": 0.0, "avg_logprob": -0.07619936370849609, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.029103519394993782}, {"id": 184, "seek": 87600, "start": 895.0, "end": 900.0, "text": " mi\u0119dzy rozmiarem sztucznej inteligencji, a ogromem wiedzy, z kt\u00f3rej si\u0119 ona uczy.", "tokens": [51314, 33964, 9544, 3057, 19183, 262, 2682, 1311, 89, 11794, 24777, 3213, 19649, 11, 257, 34416, 298, 443, 46894, 1229, 11, 710, 36023, 3244, 20325, 344, 6522, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07619936370849609, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.029103519394993782}], "language": "pl"}