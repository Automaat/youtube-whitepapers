{"text": " Mamy dzi\u015b na stole prac\u0119 naukow\u0105, kt\u00f3ra na pierwszy rzut oka, no wiesz, wygl\u0105da jak kolejny komunikat prasowy o wielkich modelach j\u0119zykowych. Mhm. Ale kiedy si\u0119 w ni\u0105 tak naprawd\u0119 wgry\u017a\u0107, okazuje si\u0119, \u017ce jest tam co\u015b znacznie ciekawszego. Zamiast krzycze\u0107, zbudowali\u015bmy co\u015b wi\u0119kszego. Co jest teraz takie modne? Dok\u0142adnie. Autorzy z Meta AI zadali inne, moim zdaniem, wa\u017cniejsze pytanie. Jak zbudowa\u0107 to m\u0105drzej? Co tak naprawd\u0119 decyduje, \u017ce taki surowy model j\u0119zykowy staje si\u0119 precyzyjnym narz\u0119dziem. Takim, kt\u00f3ry rozumie nasze polecenia. W\u0142a\u015bnie. Mamy przed sob\u0105 prac\u0119 OPTIML \u2013 Scaling Language Model Instruction Metal Learning through the Lens of Generalization. Spr\u00f3bujmy to roz\u0142o\u017cy\u0107 na czynniki pierwsze. Dok\u0142adnie tak. Ca\u0142y sens tej pracy to, no, nie jest chwalenie si\u0119 nowym modelem, ale metodyczny zrozumienie samego procesu. Mo\u017cna to sobie wyobrazi\u0107 tak. Mamy pot\u0119\u017cny, ale troch\u0119 chaotyczny, taki nieokrzesany silnik, to nasz wst\u0119pnie wytrenowany model j\u0119zykowy. W tym przypadku OPT. A proces, kt\u00f3ry nazywamy Instruction Tuning, to jest jak precyzyjne strojenie tego silnika, \u017ceby reagowa\u0142 na ka\u017cde naci\u015bni\u0119cie gazu dok\u0142adnie tak, jak chcemy. Czyli ta praca to jest w zasadzie taka instrukcja obs\u0142ugi do strojenia. Powiedzia\u0142bym, \u017ce to pr\u00f3ba stworzenia ostatecznej ksi\u0105\u017cki kucharskiej dla tego procesu. Testowali ka\u017cdy sk\u0142adnik, ka\u017cd\u0105 proporcj\u0119, \u017ceby zobaczy\u0107, co naprawd\u0119 dzia\u0142a, a co jest, no, wiesz, tylko mitem. Ksi\u0105\u017cka kucharska to \u015bwietna metafora, czyli g\u0142\u00f3wna teza nie brzmi, nasz model jest najlepszy na \u015bwiecie. Zdecydowanie nie. Ale raczej odkryli\u015bmy przepis na to, jak sprawi\u0107, by modele stawa\u0142y si\u0119 lepsze w uog\u00f3lnianiu wiedzy na zupe\u0142nie nowe zadania. Co to w\u0142a\u015bciwie oznacza w praktyce? Jaki fundamentalny problem oni tu rozwi\u0105zuj\u0105? Problem jest kluczowy dla ca\u0142ej bran\u017cy, naprawd\u0119. Samo bezmy\u015blne powi\u0119kszanie modelu, czyli dodawanie miliard\u00f3w parametr\u00f3w, no, nie gwarantuje, \u017ce b\u0119dzie on dobrze wykonywa\u0142 konkretne, niewidziane wcze\u015bniej zadania. Czasem mo\u017ce by\u0107 wr\u0119cz korzej. Jasne. Badacze chcieli zrozumie\u0107, jakie konkretne decyzje podczas procesu fine tuning maj\u0105 najwi\u0119kszy wp\u0142yw na zdolno\u015b\u0107 do generalizacji. I jak to zbadali? No w\u0142a\u015bnie. \u017beby to zbada\u0107, musieli najpierw zbudowa\u0107, powiedzmy, poligon do\u015bwiadczalny i stworzyli co\u015b, co nazwali OPTIML Bench. Brzmi powa\u017cnie, co to takiego? To jest gigantyczny zbi\u00f3r, prawie 2000 zada\u0144 z zakresu przetwarzania j\u0119zyka naturalnego. Zebrali je z 8 r\u00f3\u017cnych powszechnie u\u017cywanych benchmark\u00f3w. Czyli to nie jest tylko zbi\u00f3r danych? Nie, nie. To ca\u0142e ustandaryzowane \u015brodowisko testowe, kt\u00f3re pozwoli\u0142o im precyzyjnie mierzy\u0107 post\u0119py. I co wi\u0119cej, zdefiniowali trzy bardzo konkretne poziomy generalizacji, kt\u00f3re chcieli zmierzy\u0107. Zrozumienie ich jest, no, kluczowe, \u017ceby doceni\u0107 wyniki tej pracy. W porz\u0105dku, wchod\u017amy w to. Jaki jest ten pierwszy poziom? Pierwszy i od razu powiem najtrudniejszy to jest generalizacja na ca\u0142kowicie nowe kategorie zada\u0144. Czyli na przyk\u0142ad? Wyobra\u017a sobie, \u017ce uczymy model analizy sentymentu i odpowiadania na pytania z tekstu, a potem bez \u017cadnego dodatkowego treningu dajemy mu do rozwi\u0105zania zadanie logiczne. Albo programistyczne, kt\u00f3rego kategorii nigdy wcze\u015bniej nie widzia\u0142. Ok, czyli to jest test na tak\u0105 prawdziw\u0105 elastyczno\u015b\u0107 my\u015blenia. Dok\u0142adnie. Sprawdzamy, czy model potrafi przenie\u015b\u0107 jak\u0105\u015b no fundamentaln\u0105 umiej\u0119tno\u015b\u0107 rozumienia na zupe\u0142nie obcy dla siebie grunt, a drugi poziom? Drugi to nowe zadania w znanych kategoriach. Czyli? Tutaj przyk\u0142adem mo\u017ce by\u0107 model, kt\u00f3ry by\u0142 trenowany na odpowiadaniu na pytania z og\u00f3lnej wiedzy, powiedzmy z wikipedii. Kategoria zadania jest mu znana odpowiadanie na pytania. Ok. Ale teraz testujemy go na odpowiadaniu na bardzo specjalistyczne pytania z artyku\u0142\u00f3w medycznych. Musi zaadaptowa\u0107 znan\u0105 umiej\u0119tno\u015b\u0107 do nowej, nieznanej domeny. Rozumiem. Ta sama mechanika, ale inne dekoracje, a trzeci poziom zak\u0142adam, \u017ce jest najprostszy. Dok\u0142adnie. Trzeci to nowe przyk\u0142ady znanych zada\u0144. To jest w zasadzie klasyczne uczenie wielozadaniowe, czyli multitask learning. Model by\u0142 ju\u017c trenowany na przyk\u0142ad na zadaniu t\u0142umaczenia z angielskiego na francuski, a teraz dostaje po prostu nowe zdania do przet\u0142umaczenia. Widzia\u0142 ju\u017c to konkretne zadanie, ale mierzy si\u0119 z nowymi, niewidzianymi wcze\u015bniej danymi wej\u015bciowymi. To faktycznie brzmi jak ogromne, bardzo metodyczne przedsi\u0119wzi\u0119cie. Skoro mamy ju\u017c ten poligon i wiemy co mierzyli, przejd\u017amy do mi\u0119sa. Co by\u0142o najciekawsze w ich eksperymentach? Co konkretnie wrzucali do tego garnka ze swojej ksi\u0105\u017cki kucharskiej? Zbadali kilka kluczowych czynnik\u00f3w. Pierwszy i moim zdaniem bardzo wa\u017cny to proporcje danych treningowych. Okej. Okaza\u0142o si\u0119, \u017ce ma ogromne znaczenie, z kt\u00f3rych benchmark\u00f3w, jak na przyk\u0142ad Flan czy Prompt Source, pochodzi wi\u0119cej danych. Ka\u017cdy z tych zbior\u00f3w ma, wiesz, nieco inny styl instrukcji, inny smak, niekt\u00f3re s\u0105 bardziej formalne, inne, bardziej konwersacyjne. Czyli to, w jaki spos\u00f3b formu\u0142owane s\u0105 polecenia, ma znaczenie. To troch\u0119 tak jakby uczy\u0107 si\u0119 od r\u00f3\u017cnych nauczycieli, ka\u017cdy ma inny styl. W\u0142a\u015bnie. I co si\u0119 okaza\u0142o? Odpowiednie zbalansowanie proporcji z tych r\u00f3\u017cnych \u017ar\u00f3de\u0142 znacz\u0105co poprawia\u0142o wyniki. Ale najciekawsze jest to, \u017ce najlepsze wyniki dla danego benchmarku, powiedzmy Flan, osi\u0105gano nie wtedy, gdy 90% danych treningowych pochodzi\u0142o z Flan, A kiedy? ale gdy jego udzia\u0142 by\u0142 dobrze zbilansowany z innymi. To jest twardy dow\u00f3d na to, \u017ce r\u00f3\u017cnorodno\u015b\u0107 instrukcji jest absolutnie kluczowa. Mhm. Model uczy si\u0119 radzi\u0107 sobie z r\u00f3\u017cnymi stylami polece\u0144, co czyni go bardziej odpornym i po prostu wszechstronnym. Czyli nie chodzi o to, \u017ceby zala\u0107 model jednym typem danych, nawet je\u015bli wydaje si\u0119 on docelowy. To w sumie intuicyjne, ale \u015bwietnie, \u017ce maj\u0105 na to twarde dane. No tak. To prowadzi do kolejnego pytania. A co ze skolowaniem? Czy prosta zasada, wi\u0119cej zada\u0144 zawsze oznacza lepiej? Sprawdzi\u0142a si\u0119, ale z niuansami, kt\u00f3re s\u0105 naprawd\u0119 fascynuj\u0105ce i stanowi\u0105 jakby sedno tej pracy. Jak pokazuj\u0105 wykresy, konkretnie rysunek dwa w artykule. Zwi\u0119kszanie liczby zada\u0144 treningowych z kilkunastu do ponad tysi\u0105ca przynosi najwi\u0119ksze korzy\u015bci dla generalizacji na zadania ca\u0142kowicie nowe. Czyli test pierwszego, najtrudniejszego poziomu? Dok\u0142adnie. I test drugiego, cz\u0119\u015bciowo nowe. A co z zadaniami, kt\u00f3re model ju\u017c zna\u0142? Test trzeciego, najmatwiejszego poziomu? I tu jest ca\u0142a magia. Wydajno\u015b\u0107 na zadaniach ju\u017c widzianych, kt\u00f3re w pracy nazywaj\u0105 Fully Supervised, prawie w og\u00f3le si\u0119 nie zmienia\u0142a. Wzrost by\u0142 minimalny. To ciekawe. I to jest pot\u0119\u017cny dow\u00f3d na to, \u017ce Instruction Tuning na wielk\u0105 skal\u0119 nie uczy modelu zapami\u0119tywania konkretnych zada\u0144. A czego uczy? On uczy go fundamentalnej zdolno\u015bci do generalizacji. Uczy si\u0119, jak si\u0119 uczy\u0107 i jak podchodzi\u0107 do problem\u00f3w, a nie tylko co wie na temat konkretnych wyuczonych zada\u0144. To naprawd\u0119 zmienia perspektyw\u0119, bo to oznacza, \u017ce trenujemy nie pami\u0119\u0107, ale w pewnym sensie inteligencj\u0119. Skoro ju\u017c wiemy, \u017ce r\u00f3\u017cnorodno\u015b\u0107 i liczba zada\u0144 maj\u0105 znaczenia, to co z ich jako\u015bci\u0105 i typem? O, to jest \u015bwietne pytanie. S\u0142yszy si\u0119 teraz mn\u00f3stwo o specjalistycznych danych, np. Chain of Thought Reasoning, czyli tych \u0142a\u0144cuchach my\u015blowych, albo o danych konwersacyjnych, kt\u00f3re maj\u0105 sprawi\u0107, \u017ce modele b\u0119d\u0105 bardziej naturalne. I to jest jeden z najbardziej zaskakuj\u0105cych i pouczaj\u0105cych fragment\u00f3w ca\u0142ej pracy. Zacznijmy od danych reasoning. Dobrze. Dodali do swojej mieszanki treningowej dane zawieraj\u0105ce Chain of Thought, czyli przyk\u0142ady, gdzie model pokazuje krok po kroku, jak dochodzi do odpowiedzi. I wystarczy\u0142 zaledwie 1% takich danych, \u017ceby znacz\u0105co poprawi\u0107 wyniki. Oczywi\u015bcie. Na zadaniach wymagaj\u0105cych rozumowania to logiczne. Tak, ale nie tylko. I to jest niesamowite. Poprawi\u0142y si\u0119 te\u017c wyniki na zadaniach pozornie niezwi\u0105zanych, jak wykrywanie stareotyp\u00f3w czy toksyczno\u015bci w tek\u015bcie? Naprawd\u0119? Tak. Wygl\u0105da na to, \u017ce nauka my\u015blenia krok po kroku da\u0142a modelowi jak\u0105\u015b g\u0142\u0119bsz\u0105 zdolno\u015b\u0107 do analizy struktury problemu, kt\u00f3ra przenios\u0142a si\u0119 na inne obszary. Ale pewnie jest jaki\u015b haczyk. Zdecydowanie. Gdy zwi\u0119kszyli udzia\u0142 tych danych do powiedzmy 4%, wyniki w niekt\u00f3rych obszarach zacz\u0119\u0142y si\u0119 pogarsza\u0107. Czyli co za du\u017co to niezdrowo? Dok\u0142adnie. Okaza\u0142o si\u0119, \u017ce jest z\u0142oty \u015brodek. 1% to by\u0142o to. Wi\u0119cej zaczyna\u0142o szkodzi\u0107, by\u0107 mo\u017ce przestrajaj\u0105c model zbyt mocno w kierunku analitycznego, a nie, no, intuicyjnego my\u015blenia. Niesamowite. A co z danymi dialogowymi? Tutaj intuicja podpowiada, \u017ce dodanie rozm\u00f3w z czadbot\u00f3w powinno tylko pom\u00f3c uczyni\u0107 model bardziej pomocnym, ludzkim. I tu czeka\u0142a na badaczy prawdziwa niespodzianka. Dodanie danych z czadbot\u00f3w, nawet w \u015bladowe ilo\u015bci, m\u00f3wimy o zaledwie po\u0142owie procenta, pogorszy\u0142o wyniki na wielu zadaniach. Chwila, jak to pogorszy\u0142o? No w\u0142a\u015bnie. Szczeg\u00f3lnie na tych, kt\u00f3re wymaga\u0142y trzymania si\u0119 \u015bcis\u0142ego, precyzyjnego formatu odpowiedzi. Zaraz, czyli pr\u00f3ba bycia bardziej ludzkim sprawi\u0142a, \u017ce model zapomnia\u0142, \u017ce ma by\u0107 precyzyjnym narz\u0119dziem? To troch\u0119 jak problem z nadgorliwym pracownikiem, kt\u00f3ry zamiast odpowiedzie\u0107 na proste pytanie, zaczyna opowiada\u0107 histori\u0119 swojego \u017cycia. To jest idealna analogia. Model sta\u0142 si\u0119 bardziej gadatliwy, konwersacyjny. Ale przez to straci\u0142 zdolno\u015b\u0107 do \u015bcis\u0142ego pod\u0105\u017cania za instrukcjami. Czyli sta\u0142 si\u0119 mniej u\u017cyteczny. W\u0142a\u015bnie. Pr\u00f3bowa\u0142 by\u0107 bardziej pomocnym, rozmownym asystentem, a w efekcie sta\u0142 si\u0119 mniej u\u017cyteczny jako narz\u0119dzie do wykonywania konkretnych polece\u0144. To pokazuje, jak delikatna i nieoczywista jest ta r\u00f3wnowaga. To jeden z tych wynik\u00f3w, kt\u00f3re ka\u017c\u0105 si\u0119 zatrzyma\u0107 i pomy\u015ble\u0107. A co z jeszcze jedn\u0105 popularn\u0105 technik\u0105, czyli uczeniem na przyk\u0142adach podawanych w pr\u0105bcie, znan\u0105 jako in-context learning? To podej\u015bcie, kt\u00f3re w pracy nazwali meta ICL r\u00f3wnie\u017c przynios\u0142o nieoczekiwane i w wi\u0119kszo\u015bci negatywne rezultaty. Te\u017c? Tak. Trenowanie modelu tak, aby uczy\u0142 si\u0119 z kilku przyk\u0142ad\u00f3w podanych bezpo\u015brednio w zapytaniu, w wielu przypadkach pogarsza\u0142o jego wydajno\u015b\u0107, szczeg\u00f3lnie w zadaniach generacyjnych. Dlaczego? Gubi\u0142 si\u0119 w tych przyk\u0142adach. Mo\u017cna tak powiedzie\u0107, zamiast skupi\u0107 si\u0119 na instrukcji i wymaganym formacie odpowiedzi, zaczyna\u0142 \u015blepo na\u015bladowa\u0107 styl, kt\u00f3ry widzia\u0142 w przyk\u0142adach. Nawet je\u015bli zupe\u0142nie nie pasowa\u0142 on do bie\u017c\u0105cego zadania. Ok. Co gorsza? Okaza\u0142o si\u0119, \u017ce jest ekstremalnie wra\u017cliwy na detale takie jak, nie wiem, znaki separatora u\u017cywane mi\u0119dzy przyk\u0142adami w pr\u0105bcie. To sugeruje g\u0142\u0119bokie przeuczenie, czyli overfitting, do bardzo konkretnej struktury zapytania. Wyczyni go to kruchym i nieelastycznym w praktycznym u\u017cyciu. Dobrze, czyli zebrali ca\u0142\u0105 t\u0119 bezcenn\u0105 wiedz\u0119, co dzia\u0142a, co szkodzi, jakie s\u0105 te z\u0142ote proporcje i co z ni\u0105 zrobili? Stworzyli na jej podstawie swoje ostateczne modele, OPIT i ML. Jak one wypadaj\u0105 w por\u00f3wnaniu z innymi? Bardzo, bardzo dobrze. Modele OPIT, ML 30B i 175B, czyli te o 30 i 175 miliardach parametr\u00f3w, znacz\u0105co przewy\u017cszaj\u0105 swoje bazowe, surowe wersje, czyli modele OPIT. I to na wszystkich testowanych benchmarkach. Prompt Source, Flan i Supernatural Instructions. Czyli ich ksi\u0105\u017cka kucharska po prostu dzia\u0142a? Na to wychodzi. To jest ostateczny dow\u00f3d. Ale czy jest jaki\u015b jeden wynik, kt\u00f3ry naprawd\u0119 wybije si\u0119 ponad reszt\u0119? Co\u015b, co jest ostatecznym, praktycznym dowodem ich tezy o m\u0105drzejszym zamiast wi\u0119kszym? Zdecydowanie. Jest jeden wnosek, kt\u00f3ry powinien by\u0107 wydrukowany wielkimi literami. S\u0142ucham. Optymel 30B, czyli model z 30 miliardami parametr\u00f3w. Po tym inteligentnym strojeniu cz\u0119sto przewy\u017csza\u0142 nietrenowany bazowy model OPTIM 175B, kt\u00f3ry ma 175 miliard\u00f3w parametr\u00f3w. Chwila, zatrzymajmy si\u0119 tutaj, bo to jest sedno. Mniejszy model po inteligentnym treningu opartym na tych wszystkich odkryciach pokonuje model prawie 6 razy wi\u0119ksze? Dok\u0142adnie. To jest najwa\u017cniejszy wniosek z ca\u0142ej tej pracy. Pokazuje czarno na bia\u0142ym, \u017ce m\u0105drze przeprowadzony Instruction Tuning jest o wiele bardziej efektywny ni\u017c samobrutalne skalowanie modelu w g\u00f3r\u0119. A to oznacza? \u017be mo\u017cna uzyska\u0107 lepsze, bardziej u\u017cyteczne i pos\u0142uszne modele, kt\u00f3re s\u0105 jednocze\u015bnie znacznie mniejsze, a przez to ta\u0144sze w u\u017cyciu i bardziej dost\u0119pne. To ma ogromne implikacje. Co to w praktyce oznacza dla firm czy badaczy, kt\u00f3rzy nie s\u0105 gigantami technologicznymi i nie maj\u0105 dost\u0119pu do farm-serwer\u00f3w? Dok\u0142adnie o to chodzi. Czy to otwiera drzwi do budowania w\u0142asnych, wyspecjalizowanych modeli na mniejsz\u0105, bardziej osi\u0105galn\u0105 skal\u0119? W\u0142a\u015bnie. To potencjalnie demokratyzuje dost\u0119p do zaawansowanego AI. Zamiast by\u0107 zdaniem na korzystanie z api kilku najwi\u0119kszych graczy mo\u017cna wzi\u0105\u0107 mniejszy otwarty model i stosuj\u0105c te zasady dostroi\u0107 go do swoich potrzeb. I osi\u0105gn\u0105\u0107 wyniki por\u00f3wnywalne r\u00f3b nawet lepsze ni\u017c oferowane przez znacznie wi\u0119ksze, surowe modele. Oczywi\u015bcie ka\u017cda dobra praca naukowa musi by\u0107 te\u017c szczera co do swoich ogranicze\u0144. Czy te modele s\u0105 ju\u017c najlepsze we wszystkim? Gdzie le\u017c\u0105 ich w \u0142abo\u015bci? I autorzy s\u0105 tutaj bardzo transparentni. Co jest godne pochwa\u0142y? Przyznaj\u0105, \u017ce na najtrudniejszych, najbardziej kompleksowych benchmarkach takich jak MMLu... Kt\u00f3ry mierzy wiedz\u0119 og\u00f3ln\u0105 na poziomie akademickim? W\u0142a\u015bnie. Albo Big Bench Hard, kt\u00f3ry zawiera zadania wymagaj\u0105ce bardzo z\u0142o\u017conego, wielotapowego rozumowania. Modele OPT ML wci\u0105\u017c ust\u0119puj\u0105 modelom takim jak Flan Palm od Google, czy najnowszym zamkni\u0119tym modelom od OpenAI. Dlaczego? Wskazuj\u0105 na jakie\u015b konkretne przyczyny tej luki? Tak. I te przyczyny s\u0105 r\u00f3wnie pouczaj\u0105ce. Po pierwsze, dane pretreningowe. Ok. Bazowy model OPT by\u0142 trenowany na mniejszej ilo\u015bci danych, oko\u0142o 180 miliard\u00f3w token\u00f3w. Dla por\u00f3wnania model Palm od Google widzia\u0142 prawie 800 miliard\u00f3w token\u00f3w. R\u00f3\u017cnica jest ogromna. Jest. I ten surowiec, z kt\u00f3rego startujemy, wci\u0105\u017c ma fundamentalne znaczenie. Niewa\u017cne, jak dobrze go obrobimy, pewnych brak\u00f3w w wiedzy bazowej nie da si\u0119 nadrobi\u0107 samym Instruction Tuning. Rozumiem. Co\u015b jeszcze? Po drugie. Architektura. Autorzy sugeruj\u0105, \u017ce modele typu encoder-decoder jak T5, na kt\u00f3rym bazuje rodzina modeli Flan, mog\u0105 by\u0107 z natury bardziej efektywne w procesie fine tuning. Bardziej ni\u017c decoder-only, takie jak OPT czy GPT? Tak. Czy mogliby\u015bmy na chwil\u0119 si\u0119 tu zatrzyma\u0107? Jaka jest funkcjonalna r\u00f3\u017cnica? Oczywi\u015bcie. M\u00f3wi\u0105c najpro\u015bciej, modele encoder-decoder jak T5 s\u0105 zbudowane do transformacji jednej sekwencji w drug\u0105. Na przyk\u0142ad zdania w jednym j\u0119zyku na zdanie winnym. Co pasuje do zada\u0144 instrukcyjnych? Dok\u0142adnie, gdzie instrukcja plus dane jest transformowana w odpowied\u017a. Z kolei modele decoder-only jak GPT s\u0105 czystymi generatorami, one po prostu kontynuuj\u0105 podany pext. Musz\u0105 w\u0142o\u017cy\u0107 wi\u0119cej wysi\u0142k\u00f3w w nauczenie si\u0119 w formatu zadania, podczas gdy dla tych pierwszych jest to bardziej naturalne. Czyli sama budowa modelu daje mu pewien handicap. Jeszcze zanim zaczniemy to inteligentne strojenie. I jest jeszcze trzeci czynnik, prawda? Tak, inne techniki. Jest niemal pewne, \u017ce modele OpenAI, z kt\u00f3rymi si\u0119 por\u00f3wnuj\u0105, u\u017cywaj\u0105 dodatkowych, pot\u0119\u017cnych technik, o kt\u00f3rych nie m\u00f3wi\u0105 publicznie. Jak na przyk\u0142ad RLHF? Na pewno. RLFF, czyli Uczenie ze wzmocnieniem z ludzk\u0105 informacj\u0105 zwrotn\u0105. Tego w modelu OPTIML nie by\u0142o, a wiemy, \u017ce RLHF ma ogromny wp\u0142yw na jako\u015b\u0107 i bezpiecze\u0144stwo odpowiedzi. Jasne. To wszystko pokazuje, \u017ce Instruction Tuning, nawet tak zaawansowany, jest niezwykle pot\u0119\u017cnym narz\u0119dziem, ale nie jest magiczn\u0105 ruszczk\u0105. To jeden, cho\u0107 bardzo wa\u017cny element tej skomplikowanej uk\u0142adanki. Zatem, zbieraj\u0105c to wszystko razem, co z tego wynika dla kogo\u015b, kto interesuje si\u0119 t\u0105 dziedzin\u0105? Jaka jest ta jedna kluczowa my\u015bl, z kt\u00f3r\u0105 warto zosta\u0107 po przeanalizowaniu tej pracy? Ta praca bardzo wyra\u017anie sygnalizuje, \u017ce wchodzimy w now\u0105 er\u0119 rozwoju AI. Era, w kt\u00f3rej dominowa\u0142o has\u0142o wi\u0119kszy znaczy lepszy, powoli ust\u0119puje miejsca erze, w kt\u00f3rej liczy si\u0119 m\u0105drzejszy znaczy lepszy. Zrozumienie mechanizmu, w kt\u00f3re sprawiaj\u0105, \u017ce modele j\u0119zykowe ucz\u0105 si\u0119 generalizowa\u0107, czyli radzi\u0107 sobie z nowo\u015bci\u0105, jest cenniejsze ni\u017c samobudowanie kolejnych gigant\u00f3w o setkach miliard\u00f3w parametr\u00f3w. To jest krok w kierunku prawdziwej in\u017cynierii modeli j\u0119zykowych, a nie tylko ich bezrefleksyjnego trenowania na coraz wi\u0119kszych zbiorach danych. Przechodzimy od sztuki do nauki, to dobrze powiedziane. Mamy wreszcie przepis, a nie tylko przeczucie. Dok\u0142adnie. I to prowadzi do prowokacyjnego pytania na koniec. Poprosz\u0119. Skoro mamy ju\u017c ca\u0142kiem niez\u0142y sprawdzony przepis na efektywny Instruction Tuning, to czy nast\u0119pnym logicznym krokiem b\u0119dzie tworzenie niejednego uniwersalnego modelu, kt\u00f3ry ma umie\u0107 wszystko, ale raczej wielu mniejszych wyspecjalizowanych? W\u0142a\u015bnie. Modeli, kt\u00f3re dzi\u0119ki tej wiedzy b\u0119d\u0105 niezwykle skuteczne w swoich w\u0105skich dziedzinach, w medycynie, prawie, finansach, a jednocze\u015bnie ze wzgl\u0105du na sw\u00f3j mniejszy rozmiar b\u0119d\u0105 dost\u0119pne dla znacznie szerszego grona odbiorc\u00f3w. By\u0107 mo\u017ce przysz\u0142o\u015b\u0107 AI to nie jeden wszechwiedz\u0105cy olbrzym, ale ca\u0142a armia z winnych, inteligentnych specjalist\u00f3w.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.54, "text": " Mamy dzi\u015b na stole prac\u0119 naukow\u0105, kt\u00f3ra na pierwszy rzut oka, no wiesz,", "tokens": [50364, 376, 7804, 31981, 1788, 1667, 16326, 22404, 1274, 35616, 74, 30297, 11, 19456, 1667, 34016, 367, 89, 325, 277, 2330, 11, 572, 261, 15347, 11, 50591], "temperature": 0.0, "avg_logprob": -0.16498994191487631, "compression_ratio": 1.4323432343234324, "no_speech_prob": 0.013911822810769081}, {"id": 1, "seek": 0, "start": 4.54, "end": 8.700000000000001, "text": " wygl\u0105da jak kolejny komunikat prasowy o wielkich modelach j\u0119zykowych.", "tokens": [50591, 32015, 4207, 23749, 1634, 45359, 36300, 582, 296, 10089, 277, 20570, 48349, 2316, 608, 49055, 74, 19605, 13, 50799], "temperature": 0.0, "avg_logprob": -0.16498994191487631, "compression_ratio": 1.4323432343234324, "no_speech_prob": 0.013911822810769081}, {"id": 2, "seek": 0, "start": 8.700000000000001, "end": 9.5, "text": " Mhm.", "tokens": [50799, 26272, 13, 50839], "temperature": 0.0, "avg_logprob": -0.16498994191487631, "compression_ratio": 1.4323432343234324, "no_speech_prob": 0.013911822810769081}, {"id": 3, "seek": 0, "start": 9.5, "end": 15.9, "text": " Ale kiedy si\u0119 w ni\u0105 tak naprawd\u0119 wgry\u017a\u0107, okazuje si\u0119, \u017ce jest tam co\u015b znacznie ciekawszego.", "tokens": [50839, 9366, 18777, 3244, 261, 3867, 1611, 991, 20970, 261, 70, 627, 10659, 2162, 11, 3133, 43317, 3244, 11, 3561, 3492, 7677, 19241, 15397, 14875, 2766, 46419, 1607, 15453, 6308, 13, 51159], "temperature": 0.0, "avg_logprob": -0.16498994191487631, "compression_ratio": 1.4323432343234324, "no_speech_prob": 0.013911822810769081}, {"id": 4, "seek": 0, "start": 15.9, "end": 19.2, "text": " Zamiast krzycze\u0107, zbudowali\u015bmy co\u015b wi\u0119kszego.", "tokens": [51159, 1176, 4526, 525, 350, 13047, 9680, 2162, 11, 710, 18281, 305, 33955, 19241, 29968, 27725, 13, 51324], "temperature": 0.0, "avg_logprob": -0.16498994191487631, "compression_ratio": 1.4323432343234324, "no_speech_prob": 0.013911822810769081}, {"id": 5, "seek": 0, "start": 19.2, "end": 20.900000000000002, "text": " Co jest teraz takie modne?", "tokens": [51324, 3066, 3492, 16854, 15963, 1072, 716, 30, 51409], "temperature": 0.0, "avg_logprob": -0.16498994191487631, "compression_ratio": 1.4323432343234324, "no_speech_prob": 0.013911822810769081}, {"id": 6, "seek": 0, "start": 20.900000000000002, "end": 26.6, "text": " Dok\u0142adnie. Autorzy z Meta AI zadali inne, moim zdaniem, wa\u017cniejsze pytanie.", "tokens": [51409, 29768, 10358, 2766, 13, 6049, 284, 1229, 710, 6377, 64, 7318, 42788, 5103, 24170, 11, 48569, 710, 10312, 4907, 11, 27777, 44258, 36610, 13, 51694], "temperature": 0.0, "avg_logprob": -0.16498994191487631, "compression_ratio": 1.4323432343234324, "no_speech_prob": 0.013911822810769081}, {"id": 7, "seek": 0, "start": 26.6, "end": 29.0, "text": " Jak zbudowa\u0107 to m\u0105drzej?", "tokens": [51694, 15029, 710, 18281, 11445, 281, 275, 18962, 13503, 73, 30, 51814], "temperature": 0.0, "avg_logprob": -0.16498994191487631, "compression_ratio": 1.4323432343234324, "no_speech_prob": 0.013911822810769081}, {"id": 8, "seek": 2900, "start": 29.1, "end": 35.1, "text": " Co tak naprawd\u0119 decyduje, \u017ce taki surowy model j\u0119zykowy staje si\u0119 precyzyjnym narz\u0119dziem.", "tokens": [50369, 3066, 991, 20970, 979, 88, 769, 2884, 11, 3561, 20065, 1022, 10089, 2316, 49055, 74, 10089, 342, 11153, 3244, 659, 1344, 1229, 73, 12996, 6714, 89, 42643, 76, 13, 50669], "temperature": 0.0, "avg_logprob": -0.12331744602748326, "compression_ratio": 1.375, "no_speech_prob": 0.001362701877951622}, {"id": 9, "seek": 2900, "start": 35.1, "end": 37.2, "text": " Takim, kt\u00f3ry rozumie nasze polecenia.", "tokens": [50669, 9118, 332, 11, 9913, 48797, 414, 43394, 13208, 13037, 654, 13, 50774], "temperature": 0.0, "avg_logprob": -0.12331744602748326, "compression_ratio": 1.375, "no_speech_prob": 0.001362701877951622}, {"id": 10, "seek": 2900, "start": 37.2, "end": 44.3, "text": " W\u0142a\u015bnie. Mamy przed sob\u0105 prac\u0119 OPTIML \u2013 Scaling Language Model Instruction", "tokens": [50774, 343, 5024, 12221, 13, 376, 7804, 18334, 18253, 1611, 22404, 1274, 23324, 5422, 12683, 1662, 2747, 4270, 24445, 17105, 2730, 3826, 51129], "temperature": 0.0, "avg_logprob": -0.12331744602748326, "compression_ratio": 1.375, "no_speech_prob": 0.001362701877951622}, {"id": 11, "seek": 2900, "start": 44.3, "end": 48.3, "text": " Metal Learning through the Lens of Generalization.", "tokens": [51129, 23488, 15205, 807, 264, 441, 694, 295, 6996, 2144, 13, 51329], "temperature": 0.0, "avg_logprob": -0.12331744602748326, "compression_ratio": 1.375, "no_speech_prob": 0.001362701877951622}, {"id": 12, "seek": 2900, "start": 48.3, "end": 51.0, "text": " Spr\u00f3bujmy to roz\u0142o\u017cy\u0107 na czynniki pierwsze.", "tokens": [51329, 7702, 14216, 4579, 2226, 281, 9544, 5249, 39687, 1667, 6430, 26384, 9850, 45994, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12331744602748326, "compression_ratio": 1.375, "no_speech_prob": 0.001362701877951622}, {"id": 13, "seek": 2900, "start": 51.0, "end": 56.3, "text": " Dok\u0142adnie tak. Ca\u0142y sens tej pracy to, no, nie jest chwalenie si\u0119 nowym modelem,", "tokens": [51464, 29768, 10358, 2766, 991, 13, 7544, 6825, 2923, 12573, 35591, 281, 11, 572, 11, 2838, 3492, 26237, 21745, 414, 3244, 586, 4199, 4391, 10386, 11, 51729], "temperature": 0.0, "avg_logprob": -0.12331744602748326, "compression_ratio": 1.375, "no_speech_prob": 0.001362701877951622}, {"id": 14, "seek": 2900, "start": 56.3, "end": 58.8, "text": " ale metodyczny zrozumienie samego procesu.", "tokens": [51729, 6775, 1131, 843, 3689, 1634, 710, 27857, 449, 27385, 912, 1571, 17565, 84, 13, 51854], "temperature": 0.0, "avg_logprob": -0.12331744602748326, "compression_ratio": 1.375, "no_speech_prob": 0.001362701877951622}, {"id": 15, "seek": 5880, "start": 59.4, "end": 61.5, "text": " Mo\u017cna to sobie wyobrazi\u0107 tak.", "tokens": [50394, 44736, 629, 281, 13652, 4628, 24393, 28496, 991, 13, 50499], "temperature": 0.0, "avg_logprob": -0.11090779640305211, "compression_ratio": 1.4326241134751774, "no_speech_prob": 0.0004489542043302208}, {"id": 16, "seek": 5880, "start": 61.5, "end": 66.1, "text": " Mamy pot\u0119\u017cny, ale troch\u0119 chaotyczny, taki nieokrzesany silnik,", "tokens": [50499, 376, 7804, 1847, 1274, 1427, 1634, 11, 6775, 24926, 6294, 6737, 3689, 1634, 11, 20065, 2838, 453, 19390, 279, 1325, 3425, 13123, 11, 50729], "temperature": 0.0, "avg_logprob": -0.11090779640305211, "compression_ratio": 1.4326241134751774, "no_speech_prob": 0.0004489542043302208}, {"id": 17, "seek": 5880, "start": 66.1, "end": 69.0, "text": " to nasz wst\u0119pnie wytrenowany model j\u0119zykowy.", "tokens": [50729, 281, 5382, 89, 261, 372, 18085, 2766, 261, 4328, 1095, 23341, 2316, 49055, 74, 10089, 13, 50874], "temperature": 0.0, "avg_logprob": -0.11090779640305211, "compression_ratio": 1.4326241134751774, "no_speech_prob": 0.0004489542043302208}, {"id": 18, "seek": 5880, "start": 69.0, "end": 71.2, "text": " W tym przypadku OPT.", "tokens": [50874, 343, 8107, 41955, 23324, 51, 13, 50984], "temperature": 0.0, "avg_logprob": -0.11090779640305211, "compression_ratio": 1.4326241134751774, "no_speech_prob": 0.0004489542043302208}, {"id": 19, "seek": 5880, "start": 71.2, "end": 74.39999999999999, "text": " A proces, kt\u00f3ry nazywamy Instruction Tuning,", "tokens": [50984, 316, 17565, 11, 9913, 20151, 27112, 7804, 2730, 3826, 21363, 278, 11, 51144], "temperature": 0.0, "avg_logprob": -0.11090779640305211, "compression_ratio": 1.4326241134751774, "no_speech_prob": 0.0004489542043302208}, {"id": 20, "seek": 5880, "start": 74.39999999999999, "end": 77.2, "text": " to jest jak precyzyjne strojenie tego silnika,", "tokens": [51144, 281, 3492, 4207, 659, 1344, 1229, 73, 716, 8959, 15378, 414, 8627, 3425, 77, 5439, 11, 51284], "temperature": 0.0, "avg_logprob": -0.11090779640305211, "compression_ratio": 1.4326241134751774, "no_speech_prob": 0.0004489542043302208}, {"id": 21, "seek": 5880, "start": 77.2, "end": 82.0, "text": " \u017ceby reagowa\u0142 na ka\u017cde naci\u015bni\u0119cie gazu dok\u0142adnie tak, jak chcemy.", "tokens": [51284, 11316, 26949, 30105, 1667, 21912, 1479, 297, 22086, 1788, 35938, 4260, 290, 8813, 45864, 2766, 991, 11, 4207, 28928, 2226, 13, 51524], "temperature": 0.0, "avg_logprob": -0.11090779640305211, "compression_ratio": 1.4326241134751774, "no_speech_prob": 0.0004489542043302208}, {"id": 22, "seek": 5880, "start": 82.0, "end": 87.6, "text": " Czyli ta praca to jest w zasadzie taka instrukcja obs\u0142ugi do strojenia.", "tokens": [51524, 37099, 1846, 582, 6628, 281, 3492, 261, 44585, 3283, 28017, 1058, 25126, 34056, 3181, 1221, 24780, 360, 8959, 15378, 654, 13, 51804], "temperature": 0.0, "avg_logprob": -0.11090779640305211, "compression_ratio": 1.4326241134751774, "no_speech_prob": 0.0004489542043302208}, {"id": 23, "seek": 8760, "start": 87.6, "end": 93.1, "text": " Powiedzia\u0142bym, \u017ce to pr\u00f3ba stworzenia ostatecznej ksi\u0105\u017cki kucharskiej dla tego procesu.", "tokens": [50364, 14762, 15338, 8908, 2322, 76, 11, 3561, 281, 8565, 4231, 342, 28321, 14320, 277, 15406, 3689, 11794, 39311, 2984, 350, 625, 685, 45145, 12285, 8627, 17565, 84, 13, 50639], "temperature": 0.0, "avg_logprob": -0.08521483625684466, "compression_ratio": 1.4875, "no_speech_prob": 0.000277278246358037}, {"id": 24, "seek": 8760, "start": 93.1, "end": 98.0, "text": " Testowali ka\u017cdy sk\u0142adnik, ka\u017cd\u0105 proporcj\u0119, \u017ceby zobaczy\u0107, co naprawd\u0119 dzia\u0142a,", "tokens": [50639, 9279, 305, 5103, 31615, 1110, 10358, 13123, 11, 21912, 67, 1611, 2365, 36003, 11115, 11, 11316, 37273, 2162, 11, 598, 20970, 37903, 11, 50884], "temperature": 0.0, "avg_logprob": -0.08521483625684466, "compression_ratio": 1.4875, "no_speech_prob": 0.000277278246358037}, {"id": 25, "seek": 8760, "start": 98.0, "end": 100.5, "text": " a co jest, no, wiesz, tylko mitem.", "tokens": [50884, 257, 598, 3492, 11, 572, 11, 261, 15347, 11, 13219, 2194, 443, 13, 51009], "temperature": 0.0, "avg_logprob": -0.08521483625684466, "compression_ratio": 1.4875, "no_speech_prob": 0.000277278246358037}, {"id": 26, "seek": 8760, "start": 100.5, "end": 103.3, "text": " Ksi\u0105\u017cka kucharska to \u015bwietna metafora,", "tokens": [51009, 591, 7691, 27242, 2330, 350, 625, 685, 2330, 281, 8299, 39083, 629, 1131, 2792, 3252, 11, 51149], "temperature": 0.0, "avg_logprob": -0.08521483625684466, "compression_ratio": 1.4875, "no_speech_prob": 0.000277278246358037}, {"id": 27, "seek": 8760, "start": 103.3, "end": 106.89999999999999, "text": " czyli g\u0142\u00f3wna teza nie brzmi, nasz model jest najlepszy na \u015bwiecie.", "tokens": [51149, 16591, 18117, 3901, 629, 535, 2394, 2838, 738, 89, 3057, 11, 5382, 89, 2316, 3492, 41903, 1878, 1229, 1667, 40078, 4260, 13, 51329], "temperature": 0.0, "avg_logprob": -0.08521483625684466, "compression_ratio": 1.4875, "no_speech_prob": 0.000277278246358037}, {"id": 28, "seek": 8760, "start": 106.89999999999999, "end": 108.19999999999999, "text": " Zdecydowanie nie.", "tokens": [51329, 1176, 1479, 1344, 67, 22028, 2838, 13, 51394], "temperature": 0.0, "avg_logprob": -0.08521483625684466, "compression_ratio": 1.4875, "no_speech_prob": 0.000277278246358037}, {"id": 29, "seek": 8760, "start": 108.19999999999999, "end": 112.0, "text": " Ale raczej odkryli\u015bmy przepis na to, jak sprawi\u0107,", "tokens": [51394, 9366, 4129, 16920, 3611, 43298, 38452, 30829, 271, 1667, 281, 11, 4207, 22734, 12757, 11, 51584], "temperature": 0.0, "avg_logprob": -0.08521483625684466, "compression_ratio": 1.4875, "no_speech_prob": 0.000277278246358037}, {"id": 30, "seek": 8760, "start": 112.0, "end": 117.5, "text": " by modele stawa\u0142y si\u0119 lepsze w uog\u00f3lnianiu wiedzy na zupe\u0142nie nowe zadania.", "tokens": [51584, 538, 4391, 306, 342, 10449, 6825, 3244, 476, 1878, 1381, 261, 344, 664, 15741, 77, 952, 5951, 46894, 1229, 1667, 49922, 586, 68, 42788, 5609, 13, 51859], "temperature": 0.0, "avg_logprob": -0.08521483625684466, "compression_ratio": 1.4875, "no_speech_prob": 0.000277278246358037}, {"id": 31, "seek": 11750, "start": 117.5, "end": 119.8, "text": " Co to w\u0142a\u015bciwie oznacza w praktyce?", "tokens": [50364, 3066, 281, 50108, 277, 22672, 326, 2394, 261, 3206, 74, 874, 384, 30, 50479], "temperature": 0.0, "avg_logprob": -0.09146648199379849, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.002094884868711233}, {"id": 32, "seek": 11750, "start": 119.8, "end": 123.5, "text": " Jaki fundamentalny problem oni tu rozwi\u0105zuj\u0105?", "tokens": [50479, 508, 7421, 8088, 1634, 1154, 36317, 2604, 9544, 18234, 89, 13263, 30, 50664], "temperature": 0.0, "avg_logprob": -0.09146648199379849, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.002094884868711233}, {"id": 33, "seek": 11750, "start": 123.5, "end": 126.9, "text": " Problem jest kluczowy dla ca\u0142ej bran\u017cy, naprawd\u0119.", "tokens": [50664, 11676, 3492, 9671, 1311, 89, 10089, 12285, 47631, 73, 12029, 7735, 11, 20970, 13, 50834], "temperature": 0.0, "avg_logprob": -0.09146648199379849, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.002094884868711233}, {"id": 34, "seek": 11750, "start": 126.9, "end": 131.7, "text": " Samo bezmy\u015blne powi\u0119kszanie modelu, czyli dodawanie miliard\u00f3w parametr\u00f3w,", "tokens": [50834, 4832, 78, 10782, 2226, 19212, 716, 3388, 5034, 1694, 89, 7155, 2316, 84, 11, 16591, 13886, 1607, 7155, 1962, 72, 515, 3901, 6220, 27965, 3901, 11, 51074], "temperature": 0.0, "avg_logprob": -0.09146648199379849, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.002094884868711233}, {"id": 35, "seek": 11750, "start": 131.7, "end": 137.8, "text": " no, nie gwarantuje, \u017ce b\u0119dzie on dobrze wykonywa\u0142 konkretne, niewidziane wcze\u015bniej zadania.", "tokens": [51074, 572, 11, 2838, 290, 6925, 394, 13008, 11, 3561, 10562, 322, 28335, 39287, 2526, 44603, 36500, 716, 11, 43622, 327, 89, 21133, 40785, 42788, 5609, 13, 51379], "temperature": 0.0, "avg_logprob": -0.09146648199379849, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.002094884868711233}, {"id": 36, "seek": 11750, "start": 137.8, "end": 139.5, "text": " Czasem mo\u017ce by\u0107 wr\u0119cz korzej.", "tokens": [51379, 383, 24561, 443, 12034, 15069, 928, 1274, 3689, 14784, 16920, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09146648199379849, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.002094884868711233}, {"id": 37, "seek": 11750, "start": 139.5, "end": 140.3, "text": " Jasne.", "tokens": [51464, 34023, 716, 13, 51504], "temperature": 0.0, "avg_logprob": -0.09146648199379849, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.002094884868711233}, {"id": 38, "seek": 11750, "start": 140.3, "end": 145.6, "text": " Badacze chcieli zrozumie\u0107, jakie konkretne decyzje podczas procesu fine tuning", "tokens": [51504, 11523, 326, 1381, 417, 537, 10148, 710, 27857, 449, 414, 2162, 11, 22124, 36500, 716, 979, 37433, 2884, 2497, 30989, 17565, 84, 2489, 15164, 51769], "temperature": 0.0, "avg_logprob": -0.09146648199379849, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.002094884868711233}, {"id": 39, "seek": 14560, "start": 145.7, "end": 149.7, "text": " maj\u0105 najwi\u0119kszy wp\u0142yw na zdolno\u015b\u0107 do generalizacji.", "tokens": [50369, 26064, 48636, 1694, 1229, 32444, 6825, 86, 1667, 16221, 401, 23293, 360, 2674, 590, 13152, 13, 50569], "temperature": 0.0, "avg_logprob": -0.09618781488152998, "compression_ratio": 1.384126984126984, "no_speech_prob": 0.02208898589015007}, {"id": 40, "seek": 14560, "start": 149.7, "end": 150.9, "text": " I jak to zbadali?", "tokens": [50569, 286, 4207, 281, 710, 27580, 5103, 30, 50629], "temperature": 0.0, "avg_logprob": -0.09618781488152998, "compression_ratio": 1.384126984126984, "no_speech_prob": 0.02208898589015007}, {"id": 41, "seek": 14560, "start": 150.9, "end": 151.9, "text": " No w\u0142a\u015bnie.", "tokens": [50629, 883, 14234, 13, 50679], "temperature": 0.0, "avg_logprob": -0.09618781488152998, "compression_ratio": 1.384126984126984, "no_speech_prob": 0.02208898589015007}, {"id": 42, "seek": 14560, "start": 151.9, "end": 157.2, "text": " \u017beby to zbada\u0107, musieli najpierw zbudowa\u0107, powiedzmy, poligon do\u015bwiadczalny", "tokens": [50679, 46864, 2322, 281, 710, 65, 1538, 2162, 11, 1038, 23099, 11212, 45119, 86, 710, 18281, 11445, 11, 27617, 2226, 11, 1180, 328, 266, 46661, 3689, 304, 1634, 50944], "temperature": 0.0, "avg_logprob": -0.09618781488152998, "compression_ratio": 1.384126984126984, "no_speech_prob": 0.02208898589015007}, {"id": 43, "seek": 14560, "start": 157.2, "end": 161.5, "text": " i stworzyli co\u015b, co nazwali OPTIML Bench.", "tokens": [50944, 741, 342, 28321, 1229, 2081, 19241, 11, 598, 20151, 40054, 23324, 5422, 12683, 3964, 339, 13, 51159], "temperature": 0.0, "avg_logprob": -0.09618781488152998, "compression_ratio": 1.384126984126984, "no_speech_prob": 0.02208898589015007}, {"id": 44, "seek": 14560, "start": 161.5, "end": 163.9, "text": " Brzmi powa\u017cnie, co to takiego?", "tokens": [51159, 1603, 89, 3057, 3388, 18264, 2766, 11, 598, 281, 32296, 30, 51279], "temperature": 0.0, "avg_logprob": -0.09618781488152998, "compression_ratio": 1.384126984126984, "no_speech_prob": 0.02208898589015007}, {"id": 45, "seek": 14560, "start": 163.9, "end": 169.6, "text": " To jest gigantyczny zbi\u00f3r, prawie 2000 zada\u0144 z zakresu przetwarzania j\u0119zyka naturalnego.", "tokens": [51279, 1407, 3492, 8741, 394, 17466, 1634, 710, 5614, 15614, 11, 3206, 8699, 8132, 710, 1538, 5248, 710, 23810, 495, 84, 6541, 302, 31991, 5609, 42309, 40940, 3303, 11858, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09618781488152998, "compression_ratio": 1.384126984126984, "no_speech_prob": 0.02208898589015007}, {"id": 46, "seek": 14560, "start": 169.6, "end": 173.5, "text": " Zebrali je z 8 r\u00f3\u017cnych powszechnie u\u017cywanych benchmark\u00f3w.", "tokens": [51564, 4853, 1443, 5103, 1506, 710, 1649, 42602, 280, 1509, 1381, 1377, 414, 34097, 86, 34644, 18927, 3901, 13, 51759], "temperature": 0.0, "avg_logprob": -0.09618781488152998, "compression_ratio": 1.384126984126984, "no_speech_prob": 0.02208898589015007}, {"id": 47, "seek": 14560, "start": 173.5, "end": 175.4, "text": " Czyli to nie jest tylko zbi\u00f3r danych?", "tokens": [51759, 37099, 281, 2838, 3492, 13219, 710, 5614, 15614, 274, 34644, 30, 51854], "temperature": 0.0, "avg_logprob": -0.09618781488152998, "compression_ratio": 1.384126984126984, "no_speech_prob": 0.02208898589015007}, {"id": 48, "seek": 17540, "start": 175.4, "end": 183.0, "text": " Nie, nie. To ca\u0142e ustandaryzowane \u015brodowisko testowe, kt\u00f3re pozwoli\u0142o im precyzyjnie mierzy\u0107 post\u0119py.", "tokens": [50364, 12016, 11, 2838, 13, 1407, 47631, 26189, 474, 822, 89, 23066, 28580, 305, 43442, 1500, 6880, 11, 8864, 40557, 9384, 5249, 566, 659, 1344, 1229, 73, 2766, 47448, 27150, 2183, 1274, 8200, 13, 50744], "temperature": 0.0, "avg_logprob": -0.08353043217812815, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.0007436667219735682}, {"id": 49, "seek": 17540, "start": 183.0, "end": 189.20000000000002, "text": " I co wi\u0119cej, zdefiniowali trzy bardzo konkretne poziomy generalizacji, kt\u00f3re chcieli zmierzy\u0107.", "tokens": [50744, 286, 598, 26004, 11, 710, 20595, 3812, 305, 5103, 34573, 9034, 36500, 716, 38503, 8488, 2674, 590, 13152, 11, 8864, 417, 537, 10148, 17020, 811, 27150, 13, 51054], "temperature": 0.0, "avg_logprob": -0.08353043217812815, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.0007436667219735682}, {"id": 50, "seek": 17540, "start": 189.20000000000002, "end": 193.3, "text": " Zrozumienie ich jest, no, kluczowe, \u017ceby doceni\u0107 wyniki tej pracy.", "tokens": [51054, 1176, 27857, 449, 27385, 1893, 3492, 11, 572, 11, 9671, 1311, 89, 6880, 11, 11316, 3211, 268, 12757, 31936, 9850, 12573, 35591, 13, 51259], "temperature": 0.0, "avg_logprob": -0.08353043217812815, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.0007436667219735682}, {"id": 51, "seek": 17540, "start": 193.3, "end": 195.0, "text": " W porz\u0105dku, wchod\u017amy w to.", "tokens": [51259, 343, 1515, 23876, 5279, 11, 261, 29914, 10659, 2226, 261, 281, 13, 51344], "temperature": 0.0, "avg_logprob": -0.08353043217812815, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.0007436667219735682}, {"id": 52, "seek": 17540, "start": 195.0, "end": 196.9, "text": " Jaki jest ten pierwszy poziom?", "tokens": [51344, 508, 7421, 3492, 2064, 34016, 38503, 298, 30, 51439], "temperature": 0.0, "avg_logprob": -0.08353043217812815, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.0007436667219735682}, {"id": 53, "seek": 17540, "start": 196.9, "end": 203.70000000000002, "text": " Pierwszy i od razu powiem najtrudniejszy to jest generalizacja na ca\u0142kowicie nowe kategorie zada\u0144.", "tokens": [51439, 16676, 30012, 741, 3611, 367, 8813, 3388, 4907, 11212, 6903, 532, 10402, 7706, 281, 3492, 2674, 590, 23395, 1667, 35224, 74, 305, 28434, 586, 68, 350, 2968, 17473, 710, 1538, 5248, 13, 51779], "temperature": 0.0, "avg_logprob": -0.08353043217812815, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.0007436667219735682}, {"id": 54, "seek": 17540, "start": 203.70000000000002, "end": 204.70000000000002, "text": " Czyli na przyk\u0142ad?", "tokens": [51779, 37099, 1667, 23144, 30, 51829], "temperature": 0.0, "avg_logprob": -0.08353043217812815, "compression_ratio": 1.467741935483871, "no_speech_prob": 0.0007436667219735682}, {"id": 55, "seek": 20470, "start": 204.7, "end": 209.2, "text": " Wyobra\u017a sobie, \u017ce uczymy model analizy sentymentu i odpowiadania na pytania z tekstu,", "tokens": [50364, 14458, 24393, 10659, 13652, 11, 3561, 344, 6522, 2226, 2316, 2624, 590, 88, 2279, 88, 518, 84, 741, 24314, 38069, 5609, 1667, 25878, 5609, 710, 16624, 372, 84, 11, 50589], "temperature": 0.0, "avg_logprob": -0.09262738744896579, "compression_ratio": 1.5072886297376094, "no_speech_prob": 0.00038418799522332847}, {"id": 56, "seek": 20470, "start": 209.2, "end": 214.39999999999998, "text": " a potem bez \u017cadnego dodatkowego treningu dajemy mu do rozwi\u0105zania zadanie logiczne.", "tokens": [50589, 257, 36513, 10782, 39628, 11858, 13886, 33525, 26576, 2192, 773, 84, 1120, 73, 3633, 2992, 360, 9544, 22620, 5609, 42788, 7155, 9952, 43077, 13, 50849], "temperature": 0.0, "avg_logprob": -0.09262738744896579, "compression_ratio": 1.5072886297376094, "no_speech_prob": 0.00038418799522332847}, {"id": 57, "seek": 20470, "start": 214.39999999999998, "end": 218.2, "text": " Albo programistyczne, kt\u00f3rego kategorii nigdy wcze\u015bniej nie widzia\u0142.", "tokens": [50849, 967, 1763, 1461, 468, 17466, 716, 11, 46951, 350, 2968, 284, 5597, 26996, 3173, 40785, 2838, 27486, 8908, 13, 51039], "temperature": 0.0, "avg_logprob": -0.09262738744896579, "compression_ratio": 1.5072886297376094, "no_speech_prob": 0.00038418799522332847}, {"id": 58, "seek": 20470, "start": 218.2, "end": 222.0, "text": " Ok, czyli to jest test na tak\u0105 prawdziw\u0105 elastyczno\u015b\u0107 my\u015blenia.", "tokens": [51039, 3477, 11, 16591, 281, 3492, 1500, 1667, 31069, 41175, 3992, 86, 1611, 806, 9820, 3689, 23293, 48633, 6698, 654, 13, 51229], "temperature": 0.0, "avg_logprob": -0.09262738744896579, "compression_ratio": 1.5072886297376094, "no_speech_prob": 0.00038418799522332847}, {"id": 59, "seek": 20470, "start": 222.0, "end": 223.1, "text": " Dok\u0142adnie.", "tokens": [51229, 29768, 10358, 2766, 13, 51284], "temperature": 0.0, "avg_logprob": -0.09262738744896579, "compression_ratio": 1.5072886297376094, "no_speech_prob": 0.00038418799522332847}, {"id": 60, "seek": 20470, "start": 223.1, "end": 231.2, "text": " Sprawdzamy, czy model potrafi przenie\u015b\u0107 jak\u0105\u015b no fundamentaln\u0105 umiej\u0119tno\u015b\u0107 rozumienia na zupe\u0142nie obcy dla siebie grunt, a drugi poziom?", "tokens": [51284, 1738, 15889, 89, 7804, 11, 6430, 2316, 1847, 10437, 72, 582, 16778, 7753, 46719, 1788, 572, 8088, 13113, 1105, 7764, 46788, 23293, 48797, 18811, 1667, 49922, 1111, 1344, 12285, 39137, 677, 2760, 11, 257, 4110, 72, 38503, 298, 30, 51689], "temperature": 0.0, "avg_logprob": -0.09262738744896579, "compression_ratio": 1.5072886297376094, "no_speech_prob": 0.00038418799522332847}, {"id": 61, "seek": 20470, "start": 231.2, "end": 234.6, "text": " Drugi to nowe zadania w znanych kategoriach.", "tokens": [51689, 2491, 24780, 281, 586, 68, 42788, 5609, 261, 15397, 34644, 350, 2968, 7386, 608, 13, 51859], "temperature": 0.0, "avg_logprob": -0.09262738744896579, "compression_ratio": 1.5072886297376094, "no_speech_prob": 0.00038418799522332847}, {"id": 62, "seek": 23460, "start": 234.6, "end": 235.29999999999998, "text": " Czyli?", "tokens": [50364, 37099, 30, 50399], "temperature": 0.0, "avg_logprob": -0.09478587853281122, "compression_ratio": 1.5098684210526316, "no_speech_prob": 0.0005298873293213546}, {"id": 63, "seek": 23460, "start": 235.29999999999998, "end": 242.6, "text": " Tutaj przyk\u0142adem mo\u017ce by\u0107 model, kt\u00f3ry by\u0142 trenowany na odpowiadaniu na pytania z og\u00f3lnej wiedzy, powiedzmy z wikipedii.", "tokens": [50399, 41819, 23144, 443, 12034, 15069, 2316, 11, 9913, 16673, 23136, 23341, 1667, 24314, 38069, 25849, 1667, 25878, 5609, 710, 5360, 15741, 11794, 46894, 1229, 11, 27617, 2226, 710, 261, 1035, 647, 292, 5597, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09478587853281122, "compression_ratio": 1.5098684210526316, "no_speech_prob": 0.0005298873293213546}, {"id": 64, "seek": 23460, "start": 242.6, "end": 245.79999999999998, "text": " Kategoria zadania jest mu znana odpowiadanie na pytania.", "tokens": [50764, 591, 2968, 8172, 42788, 5609, 3492, 2992, 15397, 2095, 24314, 38069, 7155, 1667, 25878, 5609, 13, 50924], "temperature": 0.0, "avg_logprob": -0.09478587853281122, "compression_ratio": 1.5098684210526316, "no_speech_prob": 0.0005298873293213546}, {"id": 65, "seek": 23460, "start": 245.79999999999998, "end": 246.4, "text": " Ok.", "tokens": [50924, 3477, 13, 50954], "temperature": 0.0, "avg_logprob": -0.09478587853281122, "compression_ratio": 1.5098684210526316, "no_speech_prob": 0.0005298873293213546}, {"id": 66, "seek": 23460, "start": 246.4, "end": 251.79999999999998, "text": " Ale teraz testujemy go na odpowiadaniu na bardzo specjalistyczne pytania z artyku\u0142\u00f3w medycznych.", "tokens": [50954, 9366, 16854, 1500, 21767, 352, 1667, 24314, 38069, 25849, 1667, 9034, 46433, 468, 17466, 716, 25878, 5609, 710, 594, 874, 5279, 1221, 3901, 1205, 17466, 9399, 13, 51224], "temperature": 0.0, "avg_logprob": -0.09478587853281122, "compression_ratio": 1.5098684210526316, "no_speech_prob": 0.0005298873293213546}, {"id": 67, "seek": 23460, "start": 251.79999999999998, "end": 256.1, "text": " Musi zaadaptowa\u0107 znan\u0105 umiej\u0119tno\u015b\u0107 do nowej, nieznanej domeny.", "tokens": [51224, 3569, 72, 7949, 345, 2796, 11445, 15397, 282, 1611, 1105, 7764, 46788, 23293, 360, 586, 40779, 11, 2838, 22672, 1929, 73, 3285, 43100, 13, 51439], "temperature": 0.0, "avg_logprob": -0.09478587853281122, "compression_ratio": 1.5098684210526316, "no_speech_prob": 0.0005298873293213546}, {"id": 68, "seek": 23460, "start": 256.1, "end": 263.0, "text": " Rozumiem. Ta sama mechanika, ale inne dekoracje, a trzeci poziom zak\u0142adam, \u017ce jest najprostszy.", "tokens": [51439, 43313, 449, 4907, 13, 6551, 17768, 4236, 5439, 11, 6775, 24170, 368, 19339, 29293, 11, 257, 22266, 537, 38503, 298, 23810, 10358, 335, 11, 3561, 3492, 11212, 1424, 555, 7706, 13, 51784], "temperature": 0.0, "avg_logprob": -0.09478587853281122, "compression_ratio": 1.5098684210526316, "no_speech_prob": 0.0005298873293213546}, {"id": 69, "seek": 26300, "start": 263.0, "end": 263.7, "text": " Dok\u0142adnie.", "tokens": [50364, 29768, 10358, 2766, 13, 50399], "temperature": 0.0, "avg_logprob": -0.07987124668924432, "compression_ratio": 1.5787671232876712, "no_speech_prob": 0.05551910772919655}, {"id": 70, "seek": 26300, "start": 263.7, "end": 266.4, "text": " Trzeci to nowe przyk\u0142ady znanych zada\u0144.", "tokens": [50399, 1765, 1381, 537, 281, 586, 68, 6501, 74, 1221, 880, 15397, 34644, 710, 1538, 5248, 13, 50534], "temperature": 0.0, "avg_logprob": -0.07987124668924432, "compression_ratio": 1.5787671232876712, "no_speech_prob": 0.05551910772919655}, {"id": 71, "seek": 26300, "start": 266.4, "end": 270.8, "text": " To jest w zasadzie klasyczne uczenie wielozadaniowe, czyli multitask learning.", "tokens": [50534, 1407, 3492, 261, 44585, 3283, 9671, 5871, 38491, 344, 39043, 20570, 15151, 345, 3782, 6880, 11, 16591, 42338, 3863, 2539, 13, 50754], "temperature": 0.0, "avg_logprob": -0.07987124668924432, "compression_ratio": 1.5787671232876712, "no_speech_prob": 0.05551910772919655}, {"id": 72, "seek": 26300, "start": 270.8, "end": 278.8, "text": " Model by\u0142 ju\u017c trenowany na przyk\u0142ad na zadaniu t\u0142umaczenia z angielskiego na francuski, a teraz dostaje po prostu nowe zdania do przet\u0142umaczenia.", "tokens": [50754, 17105, 16673, 10678, 23136, 23341, 1667, 23144, 1667, 42788, 25849, 256, 49166, 326, 14320, 710, 2562, 1187, 5161, 12200, 1667, 431, 282, 1149, 2984, 11, 257, 16854, 20568, 11153, 714, 19518, 586, 68, 16221, 5609, 360, 6541, 302, 49166, 326, 14320, 13, 51154], "temperature": 0.0, "avg_logprob": -0.07987124668924432, "compression_ratio": 1.5787671232876712, "no_speech_prob": 0.05551910772919655}, {"id": 73, "seek": 26300, "start": 278.8, "end": 284.6, "text": " Widzia\u0142 ju\u017c to konkretne zadanie, ale mierzy si\u0119 z nowymi, niewidzianymi wcze\u015bniej danymi wej\u015bciowymi.", "tokens": [51154, 28331, 43070, 10678, 281, 36500, 716, 42788, 7155, 11, 6775, 47448, 1229, 3244, 710, 586, 88, 3057, 11, 43622, 327, 89, 952, 88, 3057, 40785, 274, 1325, 3057, 321, 73, 6199, 10089, 3057, 13, 51444], "temperature": 0.0, "avg_logprob": -0.07987124668924432, "compression_ratio": 1.5787671232876712, "no_speech_prob": 0.05551910772919655}, {"id": 74, "seek": 26300, "start": 284.6, "end": 289.0, "text": " To faktycznie brzmi jak ogromne, bardzo metodyczne przedsi\u0119wzi\u0119cie.", "tokens": [51444, 1407, 33647, 45586, 738, 89, 3057, 4207, 34416, 298, 716, 11, 9034, 1131, 843, 38491, 6541, 38395, 86, 16706, 4260, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07987124668924432, "compression_ratio": 1.5787671232876712, "no_speech_prob": 0.05551910772919655}, {"id": 75, "seek": 28900, "start": 289.0, "end": 293.7, "text": " Skoro mamy ju\u017c ten poligon i wiemy co mierzyli, przejd\u017amy do mi\u0119sa.", "tokens": [50364, 7324, 10780, 17335, 10678, 2064, 1180, 328, 266, 741, 3355, 2226, 598, 47448, 1229, 2081, 11, 8325, 37109, 10659, 2226, 360, 2752, 1274, 5790, 13, 50599], "temperature": 0.0, "avg_logprob": -0.0976679553724315, "compression_ratio": 1.4025974025974026, "no_speech_prob": 0.01003809180110693}, {"id": 76, "seek": 28900, "start": 293.7, "end": 296.6, "text": " Co by\u0142o najciekawsze w ich eksperymentach?", "tokens": [50599, 3066, 14811, 11212, 4260, 74, 28354, 261, 1893, 30724, 610, 88, 518, 608, 30, 50744], "temperature": 0.0, "avg_logprob": -0.0976679553724315, "compression_ratio": 1.4025974025974026, "no_speech_prob": 0.01003809180110693}, {"id": 77, "seek": 28900, "start": 296.6, "end": 301.6, "text": " Co konkretnie wrzucali do tego garnka ze swojej ksi\u0105\u017cki kucharskiej?", "tokens": [50744, 3066, 36500, 2766, 928, 89, 1311, 5103, 360, 8627, 25067, 2330, 5277, 29489, 73, 39311, 2984, 350, 625, 685, 45145, 30, 50994], "temperature": 0.0, "avg_logprob": -0.0976679553724315, "compression_ratio": 1.4025974025974026, "no_speech_prob": 0.01003809180110693}, {"id": 78, "seek": 28900, "start": 301.6, "end": 303.9, "text": " Zbadali kilka kluczowych czynnik\u00f3w.", "tokens": [50994, 1176, 27580, 5103, 36466, 9671, 1311, 89, 19605, 6430, 77, 47447, 13, 51109], "temperature": 0.0, "avg_logprob": -0.0976679553724315, "compression_ratio": 1.4025974025974026, "no_speech_prob": 0.01003809180110693}, {"id": 79, "seek": 28900, "start": 303.9, "end": 308.3, "text": " Pierwszy i moim zdaniem bardzo wa\u017cny to proporcje danych treningowych.", "tokens": [51109, 16676, 30012, 741, 48569, 710, 10312, 4907, 9034, 27777, 1634, 281, 2365, 36003, 2884, 274, 34644, 2192, 773, 19605, 13, 51329], "temperature": 0.0, "avg_logprob": -0.0976679553724315, "compression_ratio": 1.4025974025974026, "no_speech_prob": 0.01003809180110693}, {"id": 80, "seek": 28900, "start": 308.3, "end": 309.3, "text": " Okej.", "tokens": [51329, 29094, 73, 13, 51379], "temperature": 0.0, "avg_logprob": -0.0976679553724315, "compression_ratio": 1.4025974025974026, "no_speech_prob": 0.01003809180110693}, {"id": 81, "seek": 28900, "start": 309.3, "end": 316.2, "text": " Okaza\u0142o si\u0119, \u017ce ma ogromne znaczenie, z kt\u00f3rych benchmark\u00f3w, jak na przyk\u0142ad Flan czy Prompt Source, pochodzi wi\u0119cej danych.", "tokens": [51379, 3477, 12257, 5249, 3244, 11, 3561, 463, 34416, 298, 716, 15397, 326, 16778, 11, 710, 30382, 18927, 3901, 11, 4207, 1667, 23144, 3235, 282, 6430, 15833, 662, 29629, 11, 714, 34616, 26004, 274, 34644, 13, 51724], "temperature": 0.0, "avg_logprob": -0.0976679553724315, "compression_ratio": 1.4025974025974026, "no_speech_prob": 0.01003809180110693}, {"id": 82, "seek": 31620, "start": 316.2, "end": 323.59999999999997, "text": " Ka\u017cdy z tych zbior\u00f3w ma, wiesz, nieco inny styl instrukcji, inny smak, niekt\u00f3re s\u0105 bardziej formalne, inne, bardziej konwersacyjne.", "tokens": [50364, 10988, 1427, 3173, 710, 15180, 710, 33362, 3901, 463, 11, 261, 15347, 11, 2838, 1291, 294, 1634, 23736, 1058, 25126, 19649, 11, 294, 1634, 899, 514, 11, 2838, 43073, 265, 9015, 27209, 9860, 716, 11, 24170, 11, 27209, 5897, 5364, 31285, 716, 13, 50734], "temperature": 0.0, "avg_logprob": -0.0918656278539587, "compression_ratio": 1.5138461538461538, "no_speech_prob": 0.002901227679103613}, {"id": 83, "seek": 31620, "start": 323.59999999999997, "end": 327.3, "text": " Czyli to, w jaki spos\u00f3b formu\u0142owane s\u0105 polecenia, ma znaczenie.", "tokens": [50734, 37099, 281, 11, 261, 24492, 22904, 1254, 84, 1221, 23066, 9015, 13208, 13037, 654, 11, 463, 15397, 326, 16778, 13, 50919], "temperature": 0.0, "avg_logprob": -0.0918656278539587, "compression_ratio": 1.5138461538461538, "no_speech_prob": 0.002901227679103613}, {"id": 84, "seek": 31620, "start": 327.3, "end": 331.3, "text": " To troch\u0119 tak jakby uczy\u0107 si\u0119 od r\u00f3\u017cnych nauczycieli, ka\u017cdy ma inny styl.", "tokens": [50919, 1407, 24926, 991, 28976, 344, 33967, 3244, 3611, 42602, 49103, 1229, 537, 10148, 11, 31615, 463, 294, 1634, 23736, 13, 51119], "temperature": 0.0, "avg_logprob": -0.0918656278539587, "compression_ratio": 1.5138461538461538, "no_speech_prob": 0.002901227679103613}, {"id": 85, "seek": 31620, "start": 331.3, "end": 333.3, "text": " W\u0142a\u015bnie. I co si\u0119 okaza\u0142o?", "tokens": [51119, 343, 5024, 12221, 13, 286, 598, 3244, 3133, 12257, 5249, 30, 51219], "temperature": 0.0, "avg_logprob": -0.0918656278539587, "compression_ratio": 1.5138461538461538, "no_speech_prob": 0.002901227679103613}, {"id": 86, "seek": 31620, "start": 333.3, "end": 338.0, "text": " Odpowiednie zbalansowanie proporcji z tych r\u00f3\u017cnych \u017ar\u00f3de\u0142 znacz\u0105co poprawia\u0142o wyniki.", "tokens": [51219, 12210, 14701, 1091, 2766, 710, 2645, 599, 22028, 2365, 36003, 4013, 710, 15180, 42602, 50212, 11721, 1479, 1221, 15397, 326, 8925, 1291, 1665, 5131, 654, 5249, 31936, 9850, 13, 51454], "temperature": 0.0, "avg_logprob": -0.0918656278539587, "compression_ratio": 1.5138461538461538, "no_speech_prob": 0.002901227679103613}, {"id": 87, "seek": 31620, "start": 338.0, "end": 345.09999999999997, "text": " Ale najciekawsze jest to, \u017ce najlepsze wyniki dla danego benchmarku, powiedzmy Flan,", "tokens": [51454, 9366, 11212, 4260, 74, 28354, 3492, 281, 11, 3561, 41903, 1878, 1381, 31936, 9850, 12285, 3277, 6308, 18927, 84, 11, 27617, 2226, 3235, 282, 11, 51809], "temperature": 0.0, "avg_logprob": -0.0918656278539587, "compression_ratio": 1.5138461538461538, "no_speech_prob": 0.002901227679103613}, {"id": 88, "seek": 34510, "start": 345.1, "end": 350.20000000000005, "text": " osi\u0105gano nie wtedy, gdy 90% danych treningowych pochodzi\u0142o z Flan,", "tokens": [50364, 3003, 11404, 35255, 2838, 26959, 11, 28405, 4289, 4, 274, 34644, 2192, 773, 19605, 714, 34616, 5249, 710, 3235, 282, 11, 50619], "temperature": 0.0, "avg_logprob": -0.10024991735711798, "compression_ratio": 1.4462540716612378, "no_speech_prob": 0.009827268309891224}, {"id": 89, "seek": 34510, "start": 350.20000000000005, "end": 351.3, "text": " A kiedy?", "tokens": [50619, 316, 18777, 30, 50674], "temperature": 0.0, "avg_logprob": -0.10024991735711798, "compression_ratio": 1.4462540716612378, "no_speech_prob": 0.009827268309891224}, {"id": 90, "seek": 34510, "start": 351.3, "end": 354.90000000000003, "text": " ale gdy jego udzia\u0142 by\u0142 dobrze zbilansowany z innymi.", "tokens": [50674, 6775, 28405, 26542, 11727, 43070, 16673, 28335, 710, 15384, 599, 23341, 710, 294, 31813, 13, 50854], "temperature": 0.0, "avg_logprob": -0.10024991735711798, "compression_ratio": 1.4462540716612378, "no_speech_prob": 0.009827268309891224}, {"id": 91, "seek": 34510, "start": 354.90000000000003, "end": 359.5, "text": " To jest twardy dow\u00f3d na to, \u017ce r\u00f3\u017cnorodno\u015b\u0107 instrukcji jest absolutnie kluczowa.", "tokens": [50854, 1407, 3492, 683, 515, 88, 9459, 17081, 1667, 281, 11, 3561, 19637, 19048, 378, 23293, 1058, 25126, 19649, 3492, 18757, 2766, 9671, 1311, 89, 5528, 13, 51084], "temperature": 0.0, "avg_logprob": -0.10024991735711798, "compression_ratio": 1.4462540716612378, "no_speech_prob": 0.009827268309891224}, {"id": 92, "seek": 34510, "start": 359.5, "end": 360.20000000000005, "text": " Mhm.", "tokens": [51084, 26272, 13, 51119], "temperature": 0.0, "avg_logprob": -0.10024991735711798, "compression_ratio": 1.4462540716612378, "no_speech_prob": 0.009827268309891224}, {"id": 93, "seek": 34510, "start": 360.20000000000005, "end": 367.70000000000005, "text": " Model uczy si\u0119 radzi\u0107 sobie z r\u00f3\u017cnymi stylami polece\u0144, co czyni go bardziej odpornym i po prostu wszechstronnym.", "tokens": [51119, 17105, 344, 6522, 3244, 2843, 28496, 13652, 710, 19637, 31813, 23736, 4526, 13208, 384, 5248, 11, 598, 6430, 3722, 352, 27209, 3611, 2816, 12996, 741, 714, 19518, 37647, 19439, 372, 2044, 12996, 13, 51494], "temperature": 0.0, "avg_logprob": -0.10024991735711798, "compression_ratio": 1.4462540716612378, "no_speech_prob": 0.009827268309891224}, {"id": 94, "seek": 34510, "start": 367.70000000000005, "end": 374.1, "text": " Czyli nie chodzi o to, \u017ceby zala\u0107 model jednym typem danych, nawet je\u015bli wydaje si\u0119 on docelowy.", "tokens": [51494, 37099, 2838, 23998, 277, 281, 11, 11316, 710, 5159, 2162, 2316, 5232, 12996, 2125, 443, 274, 34644, 11, 22696, 25630, 49165, 3244, 322, 3211, 338, 10089, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10024991735711798, "compression_ratio": 1.4462540716612378, "no_speech_prob": 0.009827268309891224}, {"id": 95, "seek": 37410, "start": 374.1, "end": 377.5, "text": " To w sumie intuicyjne, ale \u015bwietnie, \u017ce maj\u0105 na to twarde dane.", "tokens": [50364, 1407, 261, 2408, 414, 560, 84, 2632, 73, 716, 11, 6775, 8299, 39083, 2766, 11, 3561, 26064, 1667, 281, 683, 10866, 49206, 13, 50534], "temperature": 0.0, "avg_logprob": -0.09705206655686902, "compression_ratio": 1.380952380952381, "no_speech_prob": 0.008764155209064484}, {"id": 96, "seek": 37410, "start": 377.5, "end": 378.3, "text": " No tak.", "tokens": [50534, 883, 991, 13, 50574], "temperature": 0.0, "avg_logprob": -0.09705206655686902, "compression_ratio": 1.380952380952381, "no_speech_prob": 0.008764155209064484}, {"id": 97, "seek": 37410, "start": 378.3, "end": 385.90000000000003, "text": " To prowadzi do kolejnego pytania. A co ze skolowaniem? Czy prosta zasada, wi\u0119cej zada\u0144 zawsze oznacza lepiej?", "tokens": [50574, 1407, 36590, 3992, 360, 23749, 11858, 25878, 5609, 13, 316, 598, 5277, 1110, 401, 37345, 4907, 30, 19832, 582, 8638, 26530, 1538, 11, 26004, 710, 1538, 5248, 30964, 277, 22672, 326, 2394, 476, 39699, 30, 50954], "temperature": 0.0, "avg_logprob": -0.09705206655686902, "compression_ratio": 1.380952380952381, "no_speech_prob": 0.008764155209064484}, {"id": 98, "seek": 37410, "start": 385.90000000000003, "end": 392.70000000000005, "text": " Sprawdzi\u0142a si\u0119, ale z niuansami, kt\u00f3re s\u0105 naprawd\u0119 fascynuj\u0105ce i stanowi\u0105 jakby sedno tej pracy.", "tokens": [50954, 1738, 15889, 3992, 5024, 3244, 11, 6775, 710, 3867, 84, 599, 4526, 11, 8864, 9015, 20970, 30632, 1344, 77, 13263, 384, 741, 27984, 47886, 28976, 9643, 1771, 12573, 35591, 13, 51294], "temperature": 0.0, "avg_logprob": -0.09705206655686902, "compression_ratio": 1.380952380952381, "no_speech_prob": 0.008764155209064484}, {"id": 99, "seek": 37410, "start": 392.70000000000005, "end": 396.90000000000003, "text": " Jak pokazuj\u0105 wykresy, konkretnie rysunek dwa w artykule.", "tokens": [51294, 15029, 13010, 921, 13263, 39287, 495, 88, 11, 36500, 2766, 367, 749, 409, 916, 35045, 261, 594, 874, 74, 2271, 13, 51504], "temperature": 0.0, "avg_logprob": -0.09705206655686902, "compression_ratio": 1.380952380952381, "no_speech_prob": 0.008764155209064484}, {"id": 100, "seek": 39690, "start": 397.09999999999997, "end": 406.5, "text": " Zwi\u0119kszanie liczby zada\u0144 treningowych z kilkunastu do ponad tysi\u0105ca przynosi najwi\u0119ksze korzy\u015bci dla generalizacji na zadania ca\u0142kowicie nowe.", "tokens": [50374, 1176, 22423, 1694, 89, 7155, 6169, 89, 2322, 710, 1538, 5248, 2192, 773, 19605, 710, 5128, 22531, 525, 84, 360, 9224, 345, 38156, 11404, 496, 6501, 16751, 72, 48636, 1694, 1381, 14784, 1229, 6199, 12285, 2674, 590, 13152, 1667, 42788, 5609, 35224, 74, 305, 28434, 586, 68, 13, 50844], "temperature": 0.0, "avg_logprob": -0.0746403390711004, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.15340548753738403}, {"id": 101, "seek": 39690, "start": 406.5, "end": 409.09999999999997, "text": " Czyli test pierwszego, najtrudniejszego poziomu?", "tokens": [50844, 37099, 1500, 27623, 27725, 11, 11212, 6903, 532, 10402, 15453, 6308, 38503, 298, 84, 30, 50974], "temperature": 0.0, "avg_logprob": -0.0746403390711004, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.15340548753738403}, {"id": 102, "seek": 39690, "start": 409.09999999999997, "end": 411.7, "text": " Dok\u0142adnie. I test drugiego, cz\u0119\u015bciowo nowe.", "tokens": [50974, 29768, 10358, 2766, 13, 286, 1500, 4110, 12200, 11, 41314, 19941, 586, 68, 13, 51104], "temperature": 0.0, "avg_logprob": -0.0746403390711004, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.15340548753738403}, {"id": 103, "seek": 39690, "start": 411.7, "end": 416.09999999999997, "text": " A co z zadaniami, kt\u00f3re model ju\u017c zna\u0142? Test trzeciego, najmatwiejszego poziomu?", "tokens": [51104, 316, 598, 710, 710, 11338, 15568, 11, 8864, 2316, 10678, 710, 629, 1221, 30, 9279, 22266, 4260, 1571, 11, 11212, 15677, 86, 7764, 15453, 6308, 38503, 298, 84, 30, 51324], "temperature": 0.0, "avg_logprob": -0.0746403390711004, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.15340548753738403}, {"id": 104, "seek": 39690, "start": 416.09999999999997, "end": 423.7, "text": " I tu jest ca\u0142a magia. Wydajno\u015b\u0107 na zadaniach ju\u017c widzianych, kt\u00f3re w pracy nazywaj\u0105 Fully Supervised, prawie w og\u00f3le si\u0119 nie zmienia\u0142a.", "tokens": [51324, 286, 2604, 3492, 1335, 5024, 2258, 654, 13, 343, 6655, 1805, 23293, 1667, 42788, 3782, 608, 10678, 27486, 952, 16384, 11, 8864, 261, 35591, 20151, 27112, 11133, 479, 2150, 4548, 24420, 11, 3206, 8699, 261, 29229, 3244, 2838, 17020, 18811, 5024, 13, 51704], "temperature": 0.0, "avg_logprob": -0.0746403390711004, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.15340548753738403}, {"id": 105, "seek": 39690, "start": 423.7, "end": 425.2, "text": " Wzrost by\u0142 minimalny.", "tokens": [51704, 343, 89, 27494, 16673, 13206, 1634, 13, 51779], "temperature": 0.0, "avg_logprob": -0.0746403390711004, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.15340548753738403}, {"id": 106, "seek": 39690, "start": 425.2, "end": 426.0, "text": " To ciekawe.", "tokens": [51779, 1407, 30596, 2330, 826, 13, 51819], "temperature": 0.0, "avg_logprob": -0.0746403390711004, "compression_ratio": 1.5119047619047619, "no_speech_prob": 0.15340548753738403}, {"id": 107, "seek": 42600, "start": 426.0, "end": 433.3, "text": " I to jest pot\u0119\u017cny dow\u00f3d na to, \u017ce Instruction Tuning na wielk\u0105 skal\u0119 nie uczy modelu zapami\u0119tywania konkretnych zada\u0144.", "tokens": [50364, 286, 281, 3492, 1847, 1274, 1427, 1634, 9459, 17081, 1667, 281, 11, 3561, 2730, 3826, 21363, 278, 1667, 20570, 26304, 16890, 1274, 2838, 344, 6522, 2316, 84, 14223, 23806, 874, 86, 5609, 36500, 9399, 710, 1538, 5248, 13, 50729], "temperature": 0.0, "avg_logprob": -0.07063784912554887, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.005916772410273552}, {"id": 108, "seek": 42600, "start": 433.3, "end": 434.7, "text": " A czego uczy?", "tokens": [50729, 316, 36559, 344, 6522, 30, 50799], "temperature": 0.0, "avg_logprob": -0.07063784912554887, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.005916772410273552}, {"id": 109, "seek": 42600, "start": 434.7, "end": 438.4, "text": " On uczy go fundamentalnej zdolno\u015bci do generalizacji.", "tokens": [50799, 1282, 344, 6522, 352, 8088, 11794, 16221, 401, 16438, 360, 2674, 590, 13152, 13, 50984], "temperature": 0.0, "avg_logprob": -0.07063784912554887, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.005916772410273552}, {"id": 110, "seek": 42600, "start": 438.4, "end": 445.8, "text": " Uczy si\u0119, jak si\u0119 uczy\u0107 i jak podchodzi\u0107 do problem\u00f3w, a nie tylko co wie na temat konkretnych wyuczonych zada\u0144.", "tokens": [50984, 624, 6522, 3244, 11, 4207, 3244, 344, 33967, 741, 4207, 2497, 34616, 2162, 360, 1154, 3901, 11, 257, 2838, 13219, 598, 3355, 1667, 32954, 36500, 9399, 4628, 1311, 44479, 339, 710, 1538, 5248, 13, 51354], "temperature": 0.0, "avg_logprob": -0.07063784912554887, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.005916772410273552}, {"id": 111, "seek": 42600, "start": 445.8, "end": 453.2, "text": " To naprawd\u0119 zmienia perspektyw\u0119, bo to oznacza, \u017ce trenujemy nie pami\u0119\u0107, ale w pewnym sensie inteligencj\u0119.", "tokens": [51354, 1407, 20970, 17020, 18811, 868, 32659, 874, 86, 1274, 11, 748, 281, 277, 22672, 326, 2394, 11, 3561, 23136, 21767, 2838, 31088, 2162, 11, 6775, 261, 47160, 4199, 2923, 414, 24777, 3213, 41960, 13, 51724], "temperature": 0.0, "avg_logprob": -0.07063784912554887, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.005916772410273552}, {"id": 112, "seek": 45320, "start": 453.2, "end": 460.0, "text": " Skoro ju\u017c wiemy, \u017ce r\u00f3\u017cnorodno\u015b\u0107 i liczba zada\u0144 maj\u0105 znaczenia, to co z ich jako\u015bci\u0105 i typem?", "tokens": [50364, 7324, 10780, 10678, 3355, 2226, 11, 3561, 19637, 19048, 378, 23293, 741, 6169, 89, 4231, 710, 1538, 5248, 26064, 15397, 326, 14320, 11, 281, 598, 710, 1893, 17123, 50227, 741, 2125, 443, 30, 50704], "temperature": 0.0, "avg_logprob": -0.0846360313971311, "compression_ratio": 1.462962962962963, "no_speech_prob": 0.005562497302889824}, {"id": 113, "seek": 45320, "start": 460.0, "end": 462.09999999999997, "text": " O, to jest \u015bwietne pytanie.", "tokens": [50704, 422, 11, 281, 3492, 8299, 39083, 716, 36610, 13, 50809], "temperature": 0.0, "avg_logprob": -0.0846360313971311, "compression_ratio": 1.462962962962963, "no_speech_prob": 0.005562497302889824}, {"id": 114, "seek": 45320, "start": 462.09999999999997, "end": 469.3, "text": " S\u0142yszy si\u0119 teraz mn\u00f3stwo o specjalistycznych danych, np. Chain of Thought Reasoning, czyli tych \u0142a\u0144cuchach my\u015blowych,", "tokens": [50809, 318, 1221, 749, 1229, 3244, 16854, 275, 77, 45052, 6120, 277, 46433, 468, 17466, 9399, 274, 34644, 11, 33808, 13, 33252, 295, 23058, 39693, 278, 11, 16591, 15180, 220, 5024, 5248, 66, 625, 608, 452, 19212, 19605, 11, 51169], "temperature": 0.0, "avg_logprob": -0.0846360313971311, "compression_ratio": 1.462962962962963, "no_speech_prob": 0.005562497302889824}, {"id": 115, "seek": 45320, "start": 469.3, "end": 474.09999999999997, "text": " albo o danych konwersacyjnych, kt\u00f3re maj\u0105 sprawi\u0107, \u017ce modele b\u0119d\u0105 bardziej naturalne.", "tokens": [51169, 22622, 277, 274, 34644, 5897, 5364, 31285, 9399, 11, 8864, 26064, 22734, 12757, 11, 3561, 4391, 306, 26239, 27209, 3303, 716, 13, 51409], "temperature": 0.0, "avg_logprob": -0.0846360313971311, "compression_ratio": 1.462962962962963, "no_speech_prob": 0.005562497302889824}, {"id": 116, "seek": 45320, "start": 474.09999999999997, "end": 480.59999999999997, "text": " I to jest jeden z najbardziej zaskakuj\u0105cych i pouczaj\u0105cych fragment\u00f3w ca\u0142ej pracy. Zacznijmy od danych reasoning.", "tokens": [51409, 286, 281, 3492, 12906, 710, 41857, 710, 3863, 514, 13263, 31306, 741, 5043, 3689, 11133, 31306, 26424, 3901, 47631, 73, 35591, 13, 1176, 14875, 77, 1718, 2226, 3611, 274, 34644, 21577, 13, 51734], "temperature": 0.0, "avg_logprob": -0.0846360313971311, "compression_ratio": 1.462962962962963, "no_speech_prob": 0.005562497302889824}, {"id": 117, "seek": 45320, "start": 480.59999999999997, "end": 481.4, "text": " Dobrze.", "tokens": [51734, 29679, 13503, 13, 51774], "temperature": 0.0, "avg_logprob": -0.0846360313971311, "compression_ratio": 1.462962962962963, "no_speech_prob": 0.005562497302889824}, {"id": 118, "seek": 48140, "start": 481.4, "end": 490.9, "text": " Dodali do swojej mieszanki treningowej dane zawieraj\u0105ce Chain of Thought, czyli przyk\u0142ady, gdzie model pokazuje krok po kroku, jak dochodzi do odpowiedzi.", "tokens": [50364, 26904, 5103, 360, 29489, 73, 33039, 27203, 2192, 773, 21091, 49206, 28165, 811, 11133, 384, 33252, 295, 23058, 11, 16591, 6501, 74, 1221, 880, 11, 18922, 2316, 13010, 43317, 350, 31621, 714, 45909, 5279, 11, 4207, 9243, 14543, 360, 36574, 3992, 13, 50839], "temperature": 0.0, "avg_logprob": -0.09144799063138873, "compression_ratio": 1.3724696356275303, "no_speech_prob": 0.014091883786022663}, {"id": 119, "seek": 48140, "start": 490.9, "end": 496.09999999999997, "text": " I wystarczy\u0142 zaledwie 1% takich danych, \u017ceby znacz\u0105co poprawi\u0107 wyniki.", "tokens": [50839, 286, 4628, 9710, 6522, 1221, 710, 5573, 8699, 502, 4, 29607, 274, 34644, 11, 11316, 15397, 326, 8925, 1291, 1665, 5131, 12757, 31936, 9850, 13, 51099], "temperature": 0.0, "avg_logprob": -0.09144799063138873, "compression_ratio": 1.3724696356275303, "no_speech_prob": 0.014091883786022663}, {"id": 120, "seek": 48140, "start": 496.09999999999997, "end": 500.4, "text": " Oczywi\u015bcie. Na zadaniach wymagaj\u0105cych rozumowania to logiczne.", "tokens": [51099, 42980, 13, 6056, 42788, 3782, 608, 29764, 559, 11133, 31306, 48797, 21308, 281, 9952, 43077, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09144799063138873, "compression_ratio": 1.3724696356275303, "no_speech_prob": 0.014091883786022663}, {"id": 121, "seek": 48140, "start": 500.4, "end": 504.4, "text": " Tak, ale nie tylko. I to jest niesamowite.", "tokens": [51314, 9118, 11, 6775, 2838, 13219, 13, 286, 281, 3492, 48100, 335, 305, 642, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09144799063138873, "compression_ratio": 1.3724696356275303, "no_speech_prob": 0.014091883786022663}, {"id": 122, "seek": 50440, "start": 504.4, "end": 512.1, "text": " Poprawi\u0142y si\u0119 te\u017c wyniki na zadaniach pozornie niezwi\u0105zanych, jak wykrywanie stareotyp\u00f3w czy toksyczno\u015bci w tek\u015bcie?", "tokens": [50364, 10215, 5131, 72, 6825, 3244, 9516, 31936, 9850, 1667, 42788, 3782, 608, 21281, 1865, 414, 33511, 22620, 34644, 11, 4207, 39287, 47705, 7155, 22432, 6737, 79, 3901, 6430, 281, 1694, 17466, 16438, 261, 16624, 9815, 30, 50749], "temperature": 0.0, "avg_logprob": -0.07257510934557233, "compression_ratio": 1.3373015873015872, "no_speech_prob": 0.3022926449775696}, {"id": 123, "seek": 50440, "start": 512.1, "end": 513.1, "text": " Naprawd\u0119?", "tokens": [50749, 18287, 20098, 30, 50799], "temperature": 0.0, "avg_logprob": -0.07257510934557233, "compression_ratio": 1.3373015873015872, "no_speech_prob": 0.3022926449775696}, {"id": 124, "seek": 50440, "start": 513.1, "end": 525.4, "text": " Tak. Wygl\u0105da na to, \u017ce nauka my\u015blenia krok po kroku da\u0142a modelowi jak\u0105\u015b g\u0142\u0119bsz\u0105 zdolno\u015b\u0107 do analizy struktury problemu, kt\u00f3ra przenios\u0142a si\u0119 na inne obszary.", "tokens": [50799, 9118, 13, 14458, 7191, 26398, 1667, 281, 11, 3561, 35616, 2330, 48633, 6698, 654, 350, 31621, 714, 45909, 5279, 1120, 5024, 2316, 24503, 46719, 1788, 18117, 1274, 929, 8925, 16221, 401, 23293, 360, 2624, 590, 88, 342, 19977, 2598, 1154, 84, 11, 19456, 582, 2904, 2717, 5024, 3244, 1667, 24170, 3181, 89, 822, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07257510934557233, "compression_ratio": 1.3373015873015872, "no_speech_prob": 0.3022926449775696}, {"id": 125, "seek": 50440, "start": 525.4, "end": 527.1999999999999, "text": " Ale pewnie jest jaki\u015b haczyk.", "tokens": [51414, 9366, 520, 14215, 3492, 34721, 324, 6522, 74, 13, 51504], "temperature": 0.0, "avg_logprob": -0.07257510934557233, "compression_ratio": 1.3373015873015872, "no_speech_prob": 0.3022926449775696}, {"id": 126, "seek": 52720, "start": 527.2, "end": 535.7, "text": " Zdecydowanie. Gdy zwi\u0119kszyli udzia\u0142 tych danych do powiedzmy 4%, wyniki w niekt\u00f3rych obszarach zacz\u0119\u0142y si\u0119 pogarsza\u0107.", "tokens": [50364, 1176, 1479, 1344, 67, 22028, 13, 460, 3173, 11873, 5034, 1694, 1229, 2081, 11727, 43070, 15180, 274, 34644, 360, 27617, 2226, 1017, 8923, 31936, 9850, 261, 2838, 43073, 627, 339, 3181, 26236, 608, 34430, 11052, 6825, 3244, 32037, 685, 35873, 13, 50789], "temperature": 0.0, "avg_logprob": -0.0895527998606364, "compression_ratio": 1.3819444444444444, "no_speech_prob": 0.026485353708267212}, {"id": 127, "seek": 52720, "start": 535.7, "end": 537.5, "text": " Czyli co za du\u017co to niezdrowo?", "tokens": [50789, 37099, 598, 7949, 26673, 281, 2838, 31278, 1892, 78, 30, 50879], "temperature": 0.0, "avg_logprob": -0.0895527998606364, "compression_ratio": 1.3819444444444444, "no_speech_prob": 0.026485353708267212}, {"id": 128, "seek": 52720, "start": 537.5, "end": 543.4000000000001, "text": " Dok\u0142adnie. Okaza\u0142o si\u0119, \u017ce jest z\u0142oty \u015brodek. 1% to by\u0142o to.", "tokens": [50879, 29768, 10358, 2766, 13, 3477, 12257, 5249, 3244, 11, 3561, 3492, 31614, 6737, 28580, 916, 13, 502, 4, 281, 14811, 281, 13, 51174], "temperature": 0.0, "avg_logprob": -0.0895527998606364, "compression_ratio": 1.3819444444444444, "no_speech_prob": 0.026485353708267212}, {"id": 129, "seek": 52720, "start": 543.4000000000001, "end": 551.6, "text": " Wi\u0119cej zaczyna\u0142o szkodzi\u0107, by\u0107 mo\u017ce przestrajaj\u0105c model zbyt mocno w kierunku analitycznego, a nie, no, intuicyjnego my\u015blenia.", "tokens": [51174, 30127, 20811, 43811, 629, 5249, 7870, 74, 14543, 2162, 11, 15069, 12034, 44264, 48690, 38757, 2316, 710, 2322, 83, 34962, 1771, 261, 38767, 49910, 364, 1860, 3689, 11858, 11, 257, 2838, 11, 572, 11, 560, 84, 2632, 73, 11858, 48633, 6698, 654, 13, 51584], "temperature": 0.0, "avg_logprob": -0.0895527998606364, "compression_ratio": 1.3819444444444444, "no_speech_prob": 0.026485353708267212}, {"id": 130, "seek": 52720, "start": 551.6, "end": 554.6, "text": " Niesamowite. A co z danymi dialogowymi?", "tokens": [51584, 426, 530, 335, 305, 642, 13, 316, 598, 710, 274, 1325, 3057, 19308, 10089, 3057, 30, 51734], "temperature": 0.0, "avg_logprob": -0.0895527998606364, "compression_ratio": 1.3819444444444444, "no_speech_prob": 0.026485353708267212}, {"id": 131, "seek": 55460, "start": 554.6, "end": 562.0, "text": " Tutaj intuicja podpowiada, \u017ce dodanie rozm\u00f3w z czadbot\u00f3w powinno tylko pom\u00f3c uczyni\u0107 model bardziej pomocnym, ludzkim.", "tokens": [50364, 41819, 560, 84, 299, 2938, 2497, 14701, 39018, 11, 3561, 13886, 7155, 35234, 3901, 710, 6472, 345, 18870, 3901, 27310, 1771, 13219, 12991, 40993, 344, 6522, 3722, 2162, 2316, 27209, 48962, 12996, 11, 15946, 89, 25112, 13, 50734], "temperature": 0.0, "avg_logprob": -0.08223094940185546, "compression_ratio": 1.4768211920529801, "no_speech_prob": 0.046431660652160645}, {"id": 132, "seek": 55460, "start": 562.0, "end": 565.4, "text": " I tu czeka\u0142a na badaczy prawdziwa niespodzianka.", "tokens": [50734, 286, 2604, 6472, 36361, 5024, 1667, 1578, 14691, 41175, 3992, 4151, 48100, 79, 14543, 21729, 13, 50904], "temperature": 0.0, "avg_logprob": -0.08223094940185546, "compression_ratio": 1.4768211920529801, "no_speech_prob": 0.046431660652160645}, {"id": 133, "seek": 55460, "start": 565.4, "end": 575.0, "text": " Dodanie danych z czadbot\u00f3w, nawet w \u015bladowe ilo\u015bci, m\u00f3wimy o zaledwie po\u0142owie procenta, pogorszy\u0142o wyniki na wielu zadaniach.", "tokens": [50904, 26904, 7155, 274, 34644, 710, 6472, 345, 18870, 3901, 11, 22696, 261, 8299, 9290, 6880, 1930, 44468, 11, 13489, 13189, 277, 710, 5573, 8699, 714, 1221, 13998, 38826, 64, 11, 32037, 830, 1229, 5249, 31936, 9850, 1667, 40437, 42788, 3782, 608, 13, 51384], "temperature": 0.0, "avg_logprob": -0.08223094940185546, "compression_ratio": 1.4768211920529801, "no_speech_prob": 0.046431660652160645}, {"id": 134, "seek": 55460, "start": 575.0, "end": 577.3000000000001, "text": " Chwila, jak to pogorszy\u0142o?", "tokens": [51384, 761, 86, 7371, 11, 4207, 281, 32037, 830, 1229, 5249, 30, 51499], "temperature": 0.0, "avg_logprob": -0.08223094940185546, "compression_ratio": 1.4768211920529801, "no_speech_prob": 0.046431660652160645}, {"id": 135, "seek": 55460, "start": 577.3000000000001, "end": 583.5, "text": " No w\u0142a\u015bnie. Szczeg\u00f3lnie na tych, kt\u00f3re wymaga\u0142y trzymania si\u0119 \u015bcis\u0142ego, precyzyjnego formatu odpowiedzi.", "tokens": [51499, 883, 14234, 13, 24699, 3689, 38079, 2766, 1667, 15180, 11, 8864, 29764, 9286, 6825, 34573, 37268, 3244, 8299, 26720, 1221, 6308, 11, 659, 1344, 1229, 73, 11858, 7877, 84, 36574, 3992, 13, 51809], "temperature": 0.0, "avg_logprob": -0.08223094940185546, "compression_ratio": 1.4768211920529801, "no_speech_prob": 0.046431660652160645}, {"id": 136, "seek": 58350, "start": 583.5, "end": 591.7, "text": " Zaraz, czyli pr\u00f3ba bycia bardziej ludzkim sprawi\u0142a, \u017ce model zapomnia\u0142, \u017ce ma by\u0107 precyzyjnym narz\u0119dziem?", "tokens": [50364, 41580, 921, 11, 16591, 8565, 4231, 538, 2755, 27209, 15946, 89, 25112, 22734, 72, 5024, 11, 3561, 2316, 14223, 38131, 8908, 11, 3561, 463, 15069, 659, 1344, 1229, 73, 12996, 6714, 89, 42643, 76, 30, 50774], "temperature": 0.0, "avg_logprob": -0.06555722781590052, "compression_ratio": 1.4686468646864685, "no_speech_prob": 0.0013743418967351317}, {"id": 137, "seek": 58350, "start": 591.7, "end": 599.9, "text": " To troch\u0119 jak problem z nadgorliwym pracownikiem, kt\u00f3ry zamiast odpowiedzie\u0107 na proste pytanie, zaczyna opowiada\u0107 histori\u0119 swojego \u017cycia.", "tokens": [50774, 1407, 24926, 4207, 1154, 710, 12617, 26465, 2081, 86, 4199, 22404, 44895, 4907, 11, 9913, 710, 4526, 525, 24314, 22078, 1667, 10293, 68, 36610, 11, 43811, 629, 999, 24503, 1538, 2162, 4058, 5034, 13291, 39738, 44343, 13, 51184], "temperature": 0.0, "avg_logprob": -0.06555722781590052, "compression_ratio": 1.4686468646864685, "no_speech_prob": 0.0013743418967351317}, {"id": 138, "seek": 58350, "start": 599.9, "end": 607.1, "text": " To jest idealna analogia. Model sta\u0142 si\u0119 bardziej gadatliwy, konwersacyjny.", "tokens": [51184, 1407, 3492, 7157, 629, 16660, 654, 13, 17105, 11135, 1221, 3244, 27209, 21318, 267, 2081, 9726, 11, 5897, 5364, 31285, 1634, 13, 51544], "temperature": 0.0, "avg_logprob": -0.06555722781590052, "compression_ratio": 1.4686468646864685, "no_speech_prob": 0.0013743418967351317}, {"id": 139, "seek": 58350, "start": 607.1, "end": 611.3, "text": " Ale przez to straci\u0142 zdolno\u015b\u0107 do \u015bcis\u0142ego pod\u0105\u017cania za instrukcjami.", "tokens": [51544, 9366, 14064, 281, 1056, 22086, 1221, 16221, 401, 23293, 360, 8299, 26720, 1221, 6308, 2497, 27242, 5609, 7949, 1058, 25126, 66, 73, 4526, 13, 51754], "temperature": 0.0, "avg_logprob": -0.06555722781590052, "compression_ratio": 1.4686468646864685, "no_speech_prob": 0.0013743418967351317}, {"id": 140, "seek": 58350, "start": 611.3, "end": 613.3, "text": " Czyli sta\u0142 si\u0119 mniej u\u017cyteczny.", "tokens": [51754, 37099, 11135, 1221, 3244, 39513, 34097, 975, 3689, 1634, 13, 51854], "temperature": 0.0, "avg_logprob": -0.06555722781590052, "compression_ratio": 1.4686468646864685, "no_speech_prob": 0.0013743418967351317}, {"id": 141, "seek": 61330, "start": 613.3, "end": 623.0999999999999, "text": " W\u0142a\u015bnie. Pr\u00f3bowa\u0142 by\u0107 bardziej pomocnym, rozmownym asystentem, a w efekcie sta\u0142 si\u0119 mniej u\u017cyteczny jako narz\u0119dzie do wykonywania konkretnych polece\u0144.", "tokens": [50364, 343, 5024, 12221, 13, 2114, 14216, 30105, 15069, 27209, 48962, 12996, 11, 35234, 648, 4199, 382, 38593, 317, 443, 11, 257, 261, 31482, 916, 4260, 11135, 1221, 3244, 39513, 34097, 975, 3689, 1634, 17123, 6714, 89, 42643, 360, 39287, 2526, 86, 5609, 36500, 9399, 13208, 384, 5248, 13, 50854], "temperature": 0.0, "avg_logprob": -0.08663376284317231, "compression_ratio": 1.419463087248322, "no_speech_prob": 0.004294401500374079}, {"id": 142, "seek": 61330, "start": 623.0999999999999, "end": 627.6999999999999, "text": " To pokazuje, jak delikatna i nieoczywista jest ta r\u00f3wnowaga.", "tokens": [50854, 1407, 13010, 43317, 11, 4207, 1103, 36300, 629, 741, 2838, 905, 1229, 86, 5236, 3492, 1846, 11416, 895, 305, 9286, 13, 51084], "temperature": 0.0, "avg_logprob": -0.08663376284317231, "compression_ratio": 1.419463087248322, "no_speech_prob": 0.004294401500374079}, {"id": 143, "seek": 61330, "start": 627.6999999999999, "end": 632.3, "text": " To jeden z tych wynik\u00f3w, kt\u00f3re ka\u017c\u0105 si\u0119 zatrzyma\u0107 i pomy\u015ble\u0107.", "tokens": [51084, 1407, 12906, 710, 15180, 31936, 1035, 3901, 11, 8864, 6799, 1427, 1611, 3244, 35802, 13047, 1696, 2162, 741, 280, 8488, 1788, 306, 2162, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08663376284317231, "compression_ratio": 1.419463087248322, "no_speech_prob": 0.004294401500374079}, {"id": 144, "seek": 61330, "start": 632.3, "end": 642.0999999999999, "text": " A co z jeszcze jedn\u0105 popularn\u0105 technik\u0105, czyli uczeniem na przyk\u0142adach podawanych w pr\u0105bcie, znan\u0105 jako in-context learning?", "tokens": [51314, 316, 598, 710, 14168, 5232, 13113, 3743, 13113, 1537, 1035, 1611, 11, 16591, 344, 66, 2904, 4907, 1667, 23144, 608, 2497, 1607, 34644, 261, 582, 1611, 65, 4260, 11, 15397, 282, 1611, 17123, 294, 12, 9000, 3828, 2539, 30, 51804], "temperature": 0.0, "avg_logprob": -0.08663376284317231, "compression_ratio": 1.419463087248322, "no_speech_prob": 0.004294401500374079}, {"id": 145, "seek": 64210, "start": 642.1, "end": 650.3000000000001, "text": " To podej\u015bcie, kt\u00f3re w pracy nazwali meta ICL r\u00f3wnie\u017c przynios\u0142o nieoczekiwane i w wi\u0119kszo\u015bci negatywne rezultaty.", "tokens": [50364, 1407, 7468, 73, 9815, 11, 8864, 261, 35591, 20151, 40054, 19616, 286, 22458, 20532, 6501, 77, 2717, 5249, 2838, 905, 89, 14753, 86, 1929, 741, 261, 29968, 4765, 6199, 2485, 21398, 86, 716, 48060, 723, 21398, 13, 50774], "temperature": 0.0, "avg_logprob": -0.09874166761125837, "compression_ratio": 1.40625, "no_speech_prob": 0.1303320825099945}, {"id": 146, "seek": 64210, "start": 650.3000000000001, "end": 650.8000000000001, "text": " Te\u017c?", "tokens": [50774, 1989, 1427, 30, 50799], "temperature": 0.0, "avg_logprob": -0.09874166761125837, "compression_ratio": 1.40625, "no_speech_prob": 0.1303320825099945}, {"id": 147, "seek": 64210, "start": 650.8000000000001, "end": 661.5, "text": " Tak. Trenowanie modelu tak, aby uczy\u0142 si\u0119 z kilku przyk\u0142ad\u00f3w podanych bezpo\u015brednio w zapytaniu, w wielu przypadkach pogarsza\u0142o jego wydajno\u015b\u0107, szczeg\u00f3lnie w zadaniach generacyjnych.", "tokens": [50799, 9118, 13, 314, 1095, 22028, 2316, 84, 991, 11, 24457, 344, 6522, 1221, 3244, 710, 5128, 5279, 23144, 3901, 2497, 34644, 10782, 2259, 1788, 986, 41084, 261, 14223, 4328, 25849, 11, 261, 40437, 33100, 41326, 32037, 685, 2394, 5249, 26542, 25984, 1805, 23293, 11, 49624, 2766, 261, 42788, 3782, 608, 1337, 31285, 9399, 13, 51334], "temperature": 0.0, "avg_logprob": -0.09874166761125837, "compression_ratio": 1.40625, "no_speech_prob": 0.1303320825099945}, {"id": 148, "seek": 64210, "start": 661.5, "end": 662.6, "text": " Dlaczego?", "tokens": [51334, 413, 75, 39329, 30, 51389], "temperature": 0.0, "avg_logprob": -0.09874166761125837, "compression_ratio": 1.40625, "no_speech_prob": 0.1303320825099945}, {"id": 149, "seek": 64210, "start": 662.6, "end": 664.3000000000001, "text": " Gubi\u0142 si\u0119 w tych przyk\u0142adach.", "tokens": [51389, 460, 836, 40622, 3244, 261, 15180, 23144, 608, 13, 51474], "temperature": 0.0, "avg_logprob": -0.09874166761125837, "compression_ratio": 1.40625, "no_speech_prob": 0.1303320825099945}, {"id": 150, "seek": 66430, "start": 664.3, "end": 673.8, "text": " Mo\u017cna tak powiedzie\u0107, zamiast skupi\u0107 si\u0119 na instrukcji i wymaganym formacie odpowiedzi, zaczyna\u0142 \u015blepo na\u015bladowa\u0107 styl, kt\u00f3ry widzia\u0142 w przyk\u0142adach.", "tokens": [50364, 44736, 629, 991, 27886, 11, 710, 4526, 525, 1110, 1010, 12757, 3244, 1667, 1058, 25126, 19649, 741, 29764, 559, 1325, 76, 1254, 30805, 36574, 3992, 11, 43811, 629, 1221, 8299, 306, 2259, 1667, 1788, 9290, 11445, 23736, 11, 9913, 27486, 8908, 261, 23144, 608, 13, 50839], "temperature": 0.0, "avg_logprob": -0.07018411159515381, "compression_ratio": 1.4537037037037037, "no_speech_prob": 0.1947024017572403}, {"id": 151, "seek": 66430, "start": 673.8, "end": 676.9, "text": " Nawet je\u015bli zupe\u0142nie nie pasowa\u0142 on do bie\u017c\u0105cego zadania.", "tokens": [50839, 40315, 302, 25630, 49922, 2838, 1736, 30105, 322, 360, 272, 414, 1427, 1611, 384, 1571, 42788, 5609, 13, 50994], "temperature": 0.0, "avg_logprob": -0.07018411159515381, "compression_ratio": 1.4537037037037037, "no_speech_prob": 0.1947024017572403}, {"id": 152, "seek": 66430, "start": 676.9, "end": 679.1999999999999, "text": " Ok. Co gorsza?", "tokens": [50994, 3477, 13, 3066, 290, 830, 2394, 30, 51109], "temperature": 0.0, "avg_logprob": -0.07018411159515381, "compression_ratio": 1.4537037037037037, "no_speech_prob": 0.1947024017572403}, {"id": 153, "seek": 66430, "start": 679.1999999999999, "end": 687.3, "text": " Okaza\u0142o si\u0119, \u017ce jest ekstremalnie wra\u017cliwy na detale takie jak, nie wiem, znaki separatora u\u017cywane mi\u0119dzy przyk\u0142adami w pr\u0105bcie.", "tokens": [51109, 3477, 12257, 5249, 3244, 11, 3561, 3492, 13359, 372, 2579, 304, 2766, 7843, 1427, 2081, 9726, 1667, 1141, 1220, 15963, 4207, 11, 2838, 26522, 11, 15397, 7421, 3128, 1639, 64, 34097, 86, 1929, 33964, 23144, 4526, 261, 582, 1611, 65, 4260, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07018411159515381, "compression_ratio": 1.4537037037037037, "no_speech_prob": 0.1947024017572403}, {"id": 154, "seek": 66430, "start": 687.3, "end": 694.0, "text": " To sugeruje g\u0142\u0119bokie przeuczenie, czyli overfitting, do bardzo konkretnej struktury zapytania.", "tokens": [51514, 1407, 459, 1321, 13008, 18117, 1274, 21666, 414, 8325, 1311, 16778, 11, 16591, 670, 69, 2414, 11, 360, 9034, 36500, 11794, 342, 19977, 2598, 14223, 4328, 5609, 13, 51849], "temperature": 0.0, "avg_logprob": -0.07018411159515381, "compression_ratio": 1.4537037037037037, "no_speech_prob": 0.1947024017572403}, {"id": 155, "seek": 69400, "start": 694.0, "end": 698.0, "text": " Wyczyni go to kruchym i nieelastycznym w praktycznym u\u017cyciu.", "tokens": [50364, 14458, 6522, 3722, 352, 281, 15913, 625, 4199, 741, 2838, 338, 9820, 3689, 12996, 261, 3206, 74, 874, 3689, 12996, 34097, 30795, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10712891533261254, "compression_ratio": 1.3257918552036199, "no_speech_prob": 0.014896458014845848}, {"id": 156, "seek": 69400, "start": 698.0, "end": 706.5, "text": " Dobrze, czyli zebrali ca\u0142\u0105 t\u0119 bezcenn\u0105 wiedz\u0119, co dzia\u0142a, co szkodzi, jakie s\u0105 te z\u0142ote proporcje i co z ni\u0105 zrobili?", "tokens": [50564, 29679, 13503, 11, 16591, 5277, 1443, 5103, 1335, 15926, 32489, 10782, 13037, 13113, 46894, 11052, 11, 598, 37903, 11, 598, 7870, 74, 14543, 11, 22124, 9015, 535, 31614, 1370, 2365, 36003, 2884, 741, 598, 710, 3867, 1611, 44399, 2312, 30, 50989], "temperature": 0.0, "avg_logprob": -0.10712891533261254, "compression_ratio": 1.3257918552036199, "no_speech_prob": 0.014896458014845848}, {"id": 157, "seek": 69400, "start": 706.5, "end": 714.0, "text": " Stworzyli na jej podstawie swoje ostateczne modele, OPIT i ML. Jak one wypadaj\u0105 w por\u00f3wnaniu z innymi?", "tokens": [50989, 745, 28321, 1229, 2081, 1667, 28924, 43443, 414, 29489, 277, 15406, 38491, 4391, 306, 11, 23324, 3927, 741, 21601, 13, 15029, 472, 46392, 1538, 8555, 261, 1515, 812, 895, 25849, 710, 294, 31813, 30, 51364], "temperature": 0.0, "avg_logprob": -0.10712891533261254, "compression_ratio": 1.3257918552036199, "no_speech_prob": 0.014896458014845848}, {"id": 158, "seek": 71400, "start": 715.0, "end": 731.0, "text": " Bardzo, bardzo dobrze. Modele OPIT, ML 30B i 175B, czyli te o 30 i 175 miliardach parametr\u00f3w, znacz\u0105co przewy\u017cszaj\u0105 swoje bazowe, surowe wersje, czyli modele OPIT.", "tokens": [50414, 38559, 11, 9034, 28335, 13, 20500, 306, 23324, 3927, 11, 21601, 2217, 33, 741, 41165, 33, 11, 16591, 535, 277, 2217, 741, 41165, 1962, 72, 515, 608, 6220, 27965, 3901, 11, 15397, 326, 8925, 1291, 39758, 88, 1427, 15453, 11133, 29489, 27147, 6880, 11, 1022, 6880, 261, 433, 2884, 11, 16591, 4391, 306, 23324, 3927, 13, 51214], "temperature": 0.0, "avg_logprob": -0.16077668739087653, "compression_ratio": 1.3080168776371308, "no_speech_prob": 0.8027404546737671}, {"id": 159, "seek": 71400, "start": 731.0, "end": 737.0, "text": " I to na wszystkich testowanych benchmarkach. Prompt Source, Flan i Supernatural Instructions.", "tokens": [51214, 286, 281, 1667, 34234, 1500, 23341, 339, 18927, 608, 13, 15833, 662, 29629, 11, 3235, 282, 741, 4548, 16296, 2730, 1757, 626, 13, 51514], "temperature": 0.0, "avg_logprob": -0.16077668739087653, "compression_ratio": 1.3080168776371308, "no_speech_prob": 0.8027404546737671}, {"id": 160, "seek": 71400, "start": 737.0, "end": 740.0, "text": " Czyli ich ksi\u0105\u017cka kucharska po prostu dzia\u0142a?", "tokens": [51514, 37099, 1893, 39311, 2330, 350, 625, 685, 2330, 714, 19518, 37903, 30, 51664], "temperature": 0.0, "avg_logprob": -0.16077668739087653, "compression_ratio": 1.3080168776371308, "no_speech_prob": 0.8027404546737671}, {"id": 161, "seek": 74000, "start": 740.0, "end": 748.0, "text": " Na to wychodzi. To jest ostateczny dow\u00f3d. Ale czy jest jaki\u015b jeden wynik, kt\u00f3ry naprawd\u0119 wybije si\u0119 ponad reszt\u0119?", "tokens": [50364, 6056, 281, 4628, 34616, 13, 1407, 3492, 277, 15406, 3689, 1634, 9459, 17081, 13, 9366, 6430, 3492, 34721, 12906, 31936, 1035, 11, 9913, 20970, 4628, 30418, 68, 3244, 9224, 345, 725, 2682, 1274, 30, 50764], "temperature": 0.0, "avg_logprob": -0.08218785382192069, "compression_ratio": 1.3693693693693694, "no_speech_prob": 0.05920594930648804}, {"id": 162, "seek": 74000, "start": 748.0, "end": 754.0, "text": " Co\u015b, co jest ostatecznym, praktycznym dowodem ich tezy o m\u0105drzejszym zamiast wi\u0119kszym?", "tokens": [50764, 3066, 1788, 11, 598, 3492, 277, 15406, 3689, 12996, 11, 3206, 74, 874, 3689, 12996, 9459, 378, 443, 1893, 535, 1229, 277, 275, 18962, 13503, 73, 7706, 76, 710, 4526, 525, 29968, 26681, 30, 51064], "temperature": 0.0, "avg_logprob": -0.08218785382192069, "compression_ratio": 1.3693693693693694, "no_speech_prob": 0.05920594930648804}, {"id": 163, "seek": 74000, "start": 754.0, "end": 759.0, "text": " Zdecydowanie. Jest jeden wnosek, kt\u00f3ry powinien by\u0107 wydrukowany wielkimi literami.", "tokens": [51064, 1176, 1479, 1344, 67, 22028, 13, 24918, 12906, 261, 77, 541, 74, 11, 9913, 27310, 1053, 15069, 25984, 25126, 23341, 20570, 74, 10121, 2733, 4526, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08218785382192069, "compression_ratio": 1.3693693693693694, "no_speech_prob": 0.05920594930648804}, {"id": 164, "seek": 74000, "start": 759.0, "end": 760.0, "text": " S\u0142ucham.", "tokens": [51314, 318, 1221, 625, 335, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08218785382192069, "compression_ratio": 1.3693693693693694, "no_speech_prob": 0.05920594930648804}, {"id": 165, "seek": 76000, "start": 760.0, "end": 775.0, "text": " Optymel 30B, czyli model z 30 miliardami parametr\u00f3w. Po tym inteligentnym strojeniu cz\u0119sto przewy\u017csza\u0142 nietrenowany bazowy model OPTIM 175B, kt\u00f3ry ma 175 miliard\u00f3w parametr\u00f3w.", "tokens": [50364, 21455, 4199, 338, 2217, 33, 11, 16591, 2316, 710, 2217, 1962, 72, 515, 4526, 6220, 27965, 3901, 13, 6165, 8107, 24777, 25002, 12996, 8959, 15378, 5951, 34369, 39758, 88, 1427, 82, 2394, 1221, 6899, 1095, 23341, 27147, 10089, 2316, 23324, 5422, 44, 41165, 33, 11, 9913, 463, 41165, 1962, 72, 515, 3901, 6220, 27965, 3901, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0772894462652966, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.034461863338947296}, {"id": 166, "seek": 76000, "start": 775.0, "end": 786.0, "text": " Chwila, zatrzymajmy si\u0119 tutaj, bo to jest sedno. Mniejszy model po inteligentnym treningu opartym na tych wszystkich odkryciach pokonuje model prawie 6 razy wi\u0119ksze?", "tokens": [51114, 761, 86, 7371, 11, 35802, 13047, 1696, 73, 2226, 3244, 12749, 11, 748, 281, 3492, 9643, 1771, 13, 376, 10402, 7706, 2316, 714, 24777, 25002, 12996, 2192, 773, 84, 999, 446, 4199, 1667, 15180, 34234, 3611, 43298, 537, 608, 13010, 266, 13008, 2316, 3206, 8699, 1386, 9639, 88, 29968, 1381, 30, 51664], "temperature": 0.0, "avg_logprob": -0.0772894462652966, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.034461863338947296}, {"id": 167, "seek": 78600, "start": 786.0, "end": 798.0, "text": " Dok\u0142adnie. To jest najwa\u017cniejszy wniosek z ca\u0142ej tej pracy. Pokazuje czarno na bia\u0142ym, \u017ce m\u0105drze przeprowadzony Instruction Tuning jest o wiele bardziej efektywny ni\u017c samobrutalne skalowanie modelu w g\u00f3r\u0119.", "tokens": [50364, 29768, 10358, 2766, 13, 1407, 3492, 11212, 27111, 10402, 7706, 261, 3722, 541, 74, 710, 47631, 73, 12573, 35591, 13, 14958, 43317, 6472, 1083, 78, 1667, 272, 8908, 4199, 11, 3561, 275, 18962, 13503, 30829, 1892, 345, 44479, 2730, 3826, 21363, 278, 3492, 277, 33137, 27209, 31482, 916, 874, 43682, 28502, 3247, 996, 24316, 304, 716, 16890, 22028, 2316, 84, 261, 290, 15614, 1274, 13, 50964], "temperature": 0.0, "avg_logprob": -0.054262553074563194, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.07654151320457458}, {"id": 168, "seek": 78600, "start": 798.0, "end": 799.0, "text": " A to oznacza?", "tokens": [50964, 316, 281, 277, 22672, 326, 2394, 30, 51014], "temperature": 0.0, "avg_logprob": -0.054262553074563194, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.07654151320457458}, {"id": 169, "seek": 78600, "start": 799.0, "end": 810.0, "text": " \u017be mo\u017cna uzyska\u0107 lepsze, bardziej u\u017cyteczne i pos\u0142uszne modele, kt\u00f3re s\u0105 jednocze\u015bnie znacznie mniejsze, a przez to ta\u0144sze w u\u017cyciu i bardziej dost\u0119pne.", "tokens": [51014, 46864, 17790, 16851, 749, 2330, 2162, 476, 1878, 1381, 11, 27209, 34097, 975, 38491, 741, 1366, 1221, 22378, 716, 4391, 306, 11, 8864, 9015, 5232, 26694, 1381, 12221, 15397, 14875, 2766, 275, 44258, 11, 257, 14064, 281, 1846, 5248, 82, 1381, 261, 34097, 30795, 741, 27209, 48209, 716, 13, 51564], "temperature": 0.0, "avg_logprob": -0.054262553074563194, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.07654151320457458}, {"id": 170, "seek": 81000, "start": 810.0, "end": 821.0, "text": " To ma ogromne implikacje. Co to w praktyce oznacza dla firm czy badaczy, kt\u00f3rzy nie s\u0105 gigantami technologicznymi i nie maj\u0105 dost\u0119pu do farm-serwer\u00f3w?", "tokens": [50364, 1407, 463, 34416, 298, 716, 8484, 1035, 29293, 13, 3066, 281, 261, 3206, 74, 874, 384, 277, 22672, 326, 2394, 12285, 6174, 6430, 1578, 14691, 11, 25382, 2838, 9015, 8741, 394, 4526, 1537, 1132, 17946, 31813, 741, 2838, 26064, 48209, 84, 360, 5421, 12, 12484, 1554, 3901, 30, 50914], "temperature": 0.0, "avg_logprob": -0.06364707272462171, "compression_ratio": 1.3257918552036199, "no_speech_prob": 0.16006465256214142}, {"id": 171, "seek": 81000, "start": 821.0, "end": 822.0, "text": " Dok\u0142adnie o to chodzi.", "tokens": [50914, 29768, 10358, 2766, 277, 281, 23998, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06364707272462171, "compression_ratio": 1.3257918552036199, "no_speech_prob": 0.16006465256214142}, {"id": 172, "seek": 81000, "start": 822.0, "end": 829.0, "text": " Czy to otwiera drzwi do budowania w\u0142asnych, wyspecjalizowanych modeli na mniejsz\u0105, bardziej osi\u0105galn\u0105 skal\u0119?", "tokens": [50964, 19832, 281, 4337, 86, 10609, 1224, 89, 6253, 360, 3265, 21308, 43572, 9399, 11, 27062, 494, 66, 22600, 590, 23341, 339, 2316, 72, 1667, 275, 30295, 8925, 11, 27209, 3003, 11404, 9800, 13113, 16890, 1274, 30, 51314], "temperature": 0.0, "avg_logprob": -0.06364707272462171, "compression_ratio": 1.3257918552036199, "no_speech_prob": 0.16006465256214142}, {"id": 173, "seek": 82900, "start": 829.0, "end": 845.0, "text": " W\u0142a\u015bnie. To potencjalnie demokratyzuje dost\u0119p do zaawansowanego AI. Zamiast by\u0107 zdaniem na korzystanie z api kilku najwi\u0119kszych graczy mo\u017cna wzi\u0105\u0107 mniejszy otwarty model i stosuj\u0105c te zasady dostroi\u0107 go do swoich potrzeb.", "tokens": [50364, 343, 5024, 12221, 13, 1407, 1847, 22660, 22600, 2766, 49432, 37433, 13008, 48209, 360, 7949, 1607, 599, 37345, 6308, 7318, 13, 1176, 4526, 525, 15069, 710, 10312, 4907, 1667, 14784, 36049, 7155, 710, 1882, 72, 5128, 5279, 48636, 1694, 28051, 11625, 1229, 17790, 261, 3992, 36374, 275, 10402, 7706, 4337, 29587, 88, 2316, 741, 43581, 44733, 535, 26530, 880, 20568, 340, 12757, 352, 360, 13291, 480, 37595, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11445743665782683, "compression_ratio": 1.3780487804878048, "no_speech_prob": 0.4323253929615021}, {"id": 174, "seek": 82900, "start": 845.0, "end": 851.0, "text": " I osi\u0105gn\u0105\u0107 wyniki por\u00f3wnywalne r\u00f3b nawet lepsze ni\u017c oferowane przez znacznie wi\u0119ksze, surowe modele.", "tokens": [51164, 286, 3003, 11404, 4568, 36374, 31936, 9850, 1515, 812, 895, 27112, 304, 716, 11416, 65, 22696, 476, 1878, 1381, 28502, 295, 260, 23066, 14064, 15397, 14875, 2766, 29968, 1381, 11, 1022, 6880, 4391, 306, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11445743665782683, "compression_ratio": 1.3780487804878048, "no_speech_prob": 0.4323253929615021}, {"id": 175, "seek": 85100, "start": 852.0, "end": 860.0, "text": " Oczywi\u015bcie ka\u017cda dobra praca naukowa musi by\u0107 te\u017c szczera co do swoich ogranicze\u0144. Czy te modele s\u0105 ju\u017c najlepsze we wszystkim? Gdzie le\u017c\u0105 ich w \u0142abo\u015bci?", "tokens": [50414, 42980, 21912, 2675, 360, 6198, 582, 6628, 35616, 74, 5528, 37587, 15069, 9516, 22090, 1663, 598, 360, 13291, 480, 34416, 30732, 49689, 13, 19832, 535, 4391, 306, 9015, 10678, 41903, 1878, 1381, 321, 30481, 30, 460, 13096, 476, 1427, 1611, 1893, 261, 25387, 41265, 6199, 30, 50814], "temperature": 0.0, "avg_logprob": -0.09255410899286685, "compression_ratio": 1.3546099290780143, "no_speech_prob": 0.8116099238395691}, {"id": 176, "seek": 85100, "start": 860.0, "end": 870.0, "text": " I autorzy s\u0105 tutaj bardzo transparentni. Co jest godne pochwa\u0142y? Przyznaj\u0105, \u017ce na najtrudniejszych, najbardziej kompleksowych benchmarkach takich jak MMLu...", "tokens": [50814, 286, 19510, 1229, 9015, 12749, 9034, 12737, 3722, 13, 3066, 3492, 3044, 716, 714, 339, 4151, 6825, 30, 39590, 35458, 8555, 11, 3561, 1667, 11212, 6903, 532, 10402, 45021, 11, 41857, 5207, 781, 1694, 19605, 18927, 608, 29607, 4207, 376, 12683, 84, 485, 51314], "temperature": 0.0, "avg_logprob": -0.09255410899286685, "compression_ratio": 1.3546099290780143, "no_speech_prob": 0.8116099238395691}, {"id": 177, "seek": 85100, "start": 870.0, "end": 874.0, "text": " Kt\u00f3ry mierzy wiedz\u0119 og\u00f3ln\u0105 na poziomie akademickim?", "tokens": [51314, 591, 4547, 627, 47448, 1229, 46894, 11052, 5360, 15741, 13113, 1667, 38503, 40120, 9308, 49290, 618, 332, 30, 51514], "temperature": 0.0, "avg_logprob": -0.09255410899286685, "compression_ratio": 1.3546099290780143, "no_speech_prob": 0.8116099238395691}, {"id": 178, "seek": 87400, "start": 874.0, "end": 882.0, "text": " W\u0142a\u015bnie. Albo Big Bench Hard, kt\u00f3ry zawiera zadania wymagaj\u0105ce bardzo z\u0142o\u017conego, wielotapowego rozumowania.", "tokens": [50364, 343, 5024, 12221, 13, 967, 1763, 5429, 3964, 339, 11817, 11, 9913, 28165, 10609, 42788, 5609, 29764, 559, 11133, 384, 9034, 710, 5249, 1427, 546, 1571, 11, 20570, 310, 569, 26576, 48797, 21308, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13213837146759033, "compression_ratio": 1.3405017921146953, "no_speech_prob": 0.46265077590942383}, {"id": 179, "seek": 87400, "start": 882.0, "end": 890.0, "text": " Modele OPT ML wci\u0105\u017c ust\u0119puj\u0105 modelom takim jak Flan Palm od Google, czy najnowszym zamkni\u0119tym modelom od OpenAI.", "tokens": [50764, 20500, 306, 23324, 51, 21601, 261, 537, 27242, 26189, 18085, 13263, 2316, 298, 31732, 4207, 3235, 282, 32668, 3611, 3329, 11, 6430, 11212, 77, 1509, 26681, 19876, 74, 35938, 874, 76, 2316, 298, 3611, 7238, 48698, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13213837146759033, "compression_ratio": 1.3405017921146953, "no_speech_prob": 0.46265077590942383}, {"id": 180, "seek": 87400, "start": 890.0, "end": 893.0, "text": " Dlaczego? Wskazuj\u0105 na jakie\u015b konkretne przyczyny tej luki?", "tokens": [51164, 413, 75, 39329, 30, 343, 5161, 921, 13263, 1667, 31163, 36500, 716, 6501, 6522, 1634, 12573, 287, 11788, 30, 51314], "temperature": 0.0, "avg_logprob": -0.13213837146759033, "compression_ratio": 1.3405017921146953, "no_speech_prob": 0.46265077590942383}, {"id": 181, "seek": 87400, "start": 893.0, "end": 898.0, "text": " Tak. I te przyczyny s\u0105 r\u00f3wnie pouczaj\u0105ce. Po pierwsze, dane pretreningowe.", "tokens": [51314, 9118, 13, 286, 535, 6501, 6522, 1634, 9015, 11416, 14215, 5043, 3689, 11133, 384, 13, 6165, 45994, 11, 49206, 1162, 1095, 278, 6880, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13213837146759033, "compression_ratio": 1.3405017921146953, "no_speech_prob": 0.46265077590942383}, {"id": 182, "seek": 87400, "start": 898.0, "end": 899.0, "text": " Ok.", "tokens": [51564, 3477, 13, 51614], "temperature": 0.0, "avg_logprob": -0.13213837146759033, "compression_ratio": 1.3405017921146953, "no_speech_prob": 0.46265077590942383}, {"id": 183, "seek": 89900, "start": 899.0, "end": 904.0, "text": " Bazowy model OPT by\u0142 trenowany na mniejszej ilo\u015bci danych, oko\u0142o 180 miliard\u00f3w token\u00f3w.", "tokens": [50364, 42220, 10089, 2316, 23324, 51, 16673, 23136, 23341, 1667, 275, 30295, 16920, 1930, 44468, 274, 34644, 11, 45730, 5249, 11971, 1962, 72, 515, 3901, 14862, 3901, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08085200900123232, "compression_ratio": 1.3968253968253967, "no_speech_prob": 0.05706275999546051}, {"id": 184, "seek": 89900, "start": 904.0, "end": 908.0, "text": " Dla por\u00f3wnania model Palm od Google widzia\u0142 prawie 800 miliard\u00f3w token\u00f3w.", "tokens": [50614, 413, 875, 1515, 812, 895, 5609, 2316, 32668, 3611, 3329, 27486, 8908, 3206, 8699, 13083, 1962, 72, 515, 3901, 14862, 3901, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08085200900123232, "compression_ratio": 1.3968253968253967, "no_speech_prob": 0.05706275999546051}, {"id": 185, "seek": 89900, "start": 908.0, "end": 910.0, "text": " R\u00f3\u017cnica jest ogromna.", "tokens": [50814, 497, 812, 1427, 32687, 3492, 34416, 298, 629, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08085200900123232, "compression_ratio": 1.3968253968253967, "no_speech_prob": 0.05706275999546051}, {"id": 186, "seek": 89900, "start": 910.0, "end": 916.0, "text": " Jest. I ten surowiec, z kt\u00f3rego startujemy, wci\u0105\u017c ma fundamentalne znaczenie.", "tokens": [50914, 24918, 13, 286, 2064, 1022, 13998, 66, 11, 710, 46951, 722, 21767, 11, 261, 537, 27242, 463, 8088, 716, 15397, 326, 16778, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08085200900123232, "compression_ratio": 1.3968253968253967, "no_speech_prob": 0.05706275999546051}, {"id": 187, "seek": 89900, "start": 916.0, "end": 924.0, "text": " Niewa\u017cne, jak dobrze go obrobimy, pewnych brak\u00f3w w wiedzy bazowej nie da si\u0119 nadrobi\u0107 samym Instruction Tuning.", "tokens": [51214, 426, 27806, 716, 11, 4207, 28335, 352, 1111, 16614, 13189, 11, 47160, 16384, 1548, 23849, 261, 46894, 1229, 27147, 21091, 2838, 1120, 3244, 12617, 20027, 2162, 3247, 4199, 2730, 3826, 21363, 278, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08085200900123232, "compression_ratio": 1.3968253968253967, "no_speech_prob": 0.05706275999546051}, {"id": 188, "seek": 89900, "start": 924.0, "end": 926.0, "text": " Rozumiem. Co\u015b jeszcze?", "tokens": [51614, 43313, 449, 4907, 13, 3066, 1788, 14168, 30, 51714], "temperature": 0.0, "avg_logprob": -0.08085200900123232, "compression_ratio": 1.3968253968253967, "no_speech_prob": 0.05706275999546051}, {"id": 189, "seek": 89900, "start": 926.0, "end": 928.0, "text": " Po drugie. Architektura.", "tokens": [51714, 6165, 4110, 414, 13, 10984, 642, 2320, 2991, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08085200900123232, "compression_ratio": 1.3968253968253967, "no_speech_prob": 0.05706275999546051}, {"id": 190, "seek": 92800, "start": 928.0, "end": 935.0, "text": " Autorzy sugeruj\u0105, \u017ce modele typu encoder-decoder jak T5, na kt\u00f3rym bazuje rodzina modeli Flan,", "tokens": [50364, 6049, 284, 1229, 459, 1321, 13263, 11, 3561, 4391, 306, 2125, 84, 2058, 19866, 12, 42821, 19866, 4207, 314, 20, 11, 1667, 30120, 27147, 13008, 28607, 1426, 2316, 72, 3235, 282, 11, 50714], "temperature": 0.0, "avg_logprob": -0.07873054426543567, "compression_ratio": 1.4222972972972974, "no_speech_prob": 0.028048425912857056}, {"id": 191, "seek": 92800, "start": 935.0, "end": 939.0, "text": " mog\u0105 by\u0107 z natury bardziej efektywne w procesie fine tuning.", "tokens": [50714, 34123, 15069, 710, 2249, 2598, 27209, 31482, 916, 874, 86, 716, 261, 17565, 414, 2489, 15164, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07873054426543567, "compression_ratio": 1.4222972972972974, "no_speech_prob": 0.028048425912857056}, {"id": 192, "seek": 92800, "start": 939.0, "end": 943.0, "text": " Bardziej ni\u017c decoder-only, takie jak OPT czy GPT?", "tokens": [50914, 26841, 19554, 28502, 979, 19866, 12, 25202, 11, 15963, 4207, 23324, 51, 6430, 26039, 51, 30, 51114], "temperature": 0.0, "avg_logprob": -0.07873054426543567, "compression_ratio": 1.4222972972972974, "no_speech_prob": 0.028048425912857056}, {"id": 193, "seek": 92800, "start": 943.0, "end": 944.0, "text": " Tak.", "tokens": [51114, 9118, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07873054426543567, "compression_ratio": 1.4222972972972974, "no_speech_prob": 0.028048425912857056}, {"id": 194, "seek": 92800, "start": 944.0, "end": 946.0, "text": " Czy mogliby\u015bmy na chwil\u0119 si\u0119 tu zatrzyma\u0107?", "tokens": [51164, 19832, 13172, 38270, 88, 10513, 1667, 41941, 1274, 3244, 2604, 35802, 13047, 1696, 2162, 30, 51264], "temperature": 0.0, "avg_logprob": -0.07873054426543567, "compression_ratio": 1.4222972972972974, "no_speech_prob": 0.028048425912857056}, {"id": 195, "seek": 92800, "start": 946.0, "end": 948.0, "text": " Jaka jest funkcjonalna r\u00f3\u017cnica?", "tokens": [51264, 508, 7849, 3492, 26476, 45677, 304, 629, 19637, 32687, 30, 51364], "temperature": 0.0, "avg_logprob": -0.07873054426543567, "compression_ratio": 1.4222972972972974, "no_speech_prob": 0.028048425912857056}, {"id": 196, "seek": 92800, "start": 948.0, "end": 949.0, "text": " Oczywi\u015bcie.", "tokens": [51364, 42980, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07873054426543567, "compression_ratio": 1.4222972972972974, "no_speech_prob": 0.028048425912857056}, {"id": 197, "seek": 92800, "start": 949.0, "end": 957.0, "text": " M\u00f3wi\u0105c najpro\u015bciej, modele encoder-decoder jak T5 s\u0105 zbudowane do transformacji jednej sekwencji w drug\u0105.", "tokens": [51414, 376, 3901, 11404, 66, 11212, 4318, 9815, 73, 11, 4391, 306, 2058, 19866, 12, 42821, 19866, 4207, 314, 20, 9015, 710, 18281, 23066, 360, 4088, 13152, 5232, 11794, 17215, 15615, 19649, 261, 4110, 1611, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07873054426543567, "compression_ratio": 1.4222972972972974, "no_speech_prob": 0.028048425912857056}, {"id": 198, "seek": 95700, "start": 957.0, "end": 960.0, "text": " Na przyk\u0142ad zdania w jednym j\u0119zyku na zdanie winnym.", "tokens": [50364, 6056, 23144, 16221, 5609, 261, 5232, 12996, 49055, 5279, 1667, 16221, 7155, 1942, 12996, 13, 50514], "temperature": 0.0, "avg_logprob": -0.06655515141847755, "compression_ratio": 1.397887323943662, "no_speech_prob": 0.009017407894134521}, {"id": 199, "seek": 95700, "start": 960.0, "end": 962.0, "text": " Co pasuje do zada\u0144 instrukcyjnych?", "tokens": [50514, 3066, 1736, 13008, 360, 710, 1538, 5248, 1058, 25126, 42949, 9399, 30, 50614], "temperature": 0.0, "avg_logprob": -0.06655515141847755, "compression_ratio": 1.397887323943662, "no_speech_prob": 0.009017407894134521}, {"id": 200, "seek": 95700, "start": 962.0, "end": 968.0, "text": " Dok\u0142adnie, gdzie instrukcja plus dane jest transformowana w odpowied\u017a.", "tokens": [50614, 29768, 10358, 2766, 11, 18922, 1058, 25126, 34056, 1804, 49206, 3492, 4088, 40458, 261, 36574, 10659, 13, 50914], "temperature": 0.0, "avg_logprob": -0.06655515141847755, "compression_ratio": 1.397887323943662, "no_speech_prob": 0.009017407894134521}, {"id": 201, "seek": 95700, "start": 968.0, "end": 976.0, "text": " Z kolei modele decoder-only jak GPT s\u0105 czystymi generatorami, one po prostu kontynuuj\u0105 podany pext.", "tokens": [50914, 1176, 18303, 72, 4391, 306, 979, 19866, 12, 25202, 4207, 26039, 51, 9015, 6430, 25134, 3057, 19265, 4526, 11, 472, 714, 19518, 5897, 874, 16241, 13263, 2497, 1325, 520, 734, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06655515141847755, "compression_ratio": 1.397887323943662, "no_speech_prob": 0.009017407894134521}, {"id": 202, "seek": 95700, "start": 976.0, "end": 984.0, "text": " Musz\u0105 w\u0142o\u017cy\u0107 wi\u0119cej wysi\u0142k\u00f3w w nauczenie si\u0119 w formatu zadania, podczas gdy dla tych pierwszych jest to bardziej naturalne.", "tokens": [51314, 3569, 8925, 261, 5249, 39687, 26004, 27062, 40622, 23849, 261, 49103, 16778, 3244, 261, 7877, 84, 42788, 5609, 11, 2497, 30989, 28405, 12285, 15180, 34016, 339, 3492, 281, 27209, 3303, 716, 13, 51714], "temperature": 0.0, "avg_logprob": -0.06655515141847755, "compression_ratio": 1.397887323943662, "no_speech_prob": 0.009017407894134521}, {"id": 203, "seek": 98400, "start": 984.0, "end": 987.0, "text": " Czyli sama budowa modelu daje mu pewien handicap.", "tokens": [50364, 37099, 17768, 3265, 5528, 2316, 84, 1120, 2884, 2992, 25889, 1053, 45975, 13, 50514], "temperature": 0.0, "avg_logprob": -0.06353670358657837, "compression_ratio": 1.395189003436426, "no_speech_prob": 0.05212395638227463}, {"id": 204, "seek": 98400, "start": 987.0, "end": 990.0, "text": " Jeszcze zanim zaczniemy to inteligentne strojenie.", "tokens": [50514, 2547, 89, 9680, 710, 17869, 710, 14875, 2766, 2226, 281, 24777, 25002, 716, 8959, 15378, 414, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06353670358657837, "compression_ratio": 1.395189003436426, "no_speech_prob": 0.05212395638227463}, {"id": 205, "seek": 98400, "start": 990.0, "end": 992.0, "text": " I jest jeszcze trzeci czynnik, prawda?", "tokens": [50664, 286, 3492, 14168, 22266, 537, 6430, 77, 13123, 11, 43607, 30, 50764], "temperature": 0.0, "avg_logprob": -0.06353670358657837, "compression_ratio": 1.395189003436426, "no_speech_prob": 0.05212395638227463}, {"id": 206, "seek": 98400, "start": 992.0, "end": 994.0, "text": " Tak, inne techniki.", "tokens": [50764, 9118, 11, 24170, 1537, 9850, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06353670358657837, "compression_ratio": 1.395189003436426, "no_speech_prob": 0.05212395638227463}, {"id": 207, "seek": 98400, "start": 994.0, "end": 1003.0, "text": " Jest niemal pewne, \u017ce modele OpenAI, z kt\u00f3rymi si\u0119 por\u00f3wnuj\u0105, u\u017cywaj\u0105 dodatkowych, pot\u0119\u017cnych technik, o kt\u00f3rych nie m\u00f3wi\u0105 publicznie.", "tokens": [50864, 24918, 2838, 5579, 25889, 716, 11, 3561, 4391, 306, 7238, 48698, 11, 710, 9913, 3057, 3244, 1515, 812, 895, 13263, 11, 34097, 86, 11133, 13886, 33525, 19605, 11, 1847, 1274, 1427, 9399, 1537, 1035, 11, 277, 30382, 2838, 46591, 1908, 89, 2766, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06353670358657837, "compression_ratio": 1.395189003436426, "no_speech_prob": 0.05212395638227463}, {"id": 208, "seek": 98400, "start": 1003.0, "end": 1005.0, "text": " Jak na przyk\u0142ad RLHF?", "tokens": [51314, 15029, 1667, 23144, 497, 43, 39, 37, 30, 51414], "temperature": 0.0, "avg_logprob": -0.06353670358657837, "compression_ratio": 1.395189003436426, "no_speech_prob": 0.05212395638227463}, {"id": 209, "seek": 98400, "start": 1005.0, "end": 1006.0, "text": " Na pewno.", "tokens": [51414, 6056, 33002, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06353670358657837, "compression_ratio": 1.395189003436426, "no_speech_prob": 0.05212395638227463}, {"id": 210, "seek": 98400, "start": 1006.0, "end": 1011.0, "text": " RLFF, czyli Uczenie ze wzmocnieniem z ludzk\u0105 informacj\u0105 zwrotn\u0105.", "tokens": [51464, 497, 43, 6345, 11, 16591, 624, 39043, 5277, 24809, 76, 905, 77, 1053, 4907, 710, 15946, 89, 26304, 1356, 326, 8555, 49111, 310, 13113, 13, 51714], "temperature": 0.0, "avg_logprob": -0.06353670358657837, "compression_ratio": 1.395189003436426, "no_speech_prob": 0.05212395638227463}, {"id": 211, "seek": 101100, "start": 1011.0, "end": 1018.0, "text": " Tego w modelu OPTIML nie by\u0142o, a wiemy, \u017ce RLHF ma ogromny wp\u0142yw na jako\u015b\u0107 i bezpiecze\u0144stwo odpowiedzi.", "tokens": [50364, 314, 6308, 261, 2316, 84, 23324, 5422, 12683, 2838, 14811, 11, 257, 3355, 2226, 11, 3561, 497, 43, 39, 37, 463, 34416, 298, 1634, 32444, 6825, 86, 1667, 17123, 7753, 741, 47153, 9680, 12229, 6120, 36574, 3992, 13, 50714], "temperature": 0.0, "avg_logprob": -0.058694970117856377, "compression_ratio": 1.3915857605177993, "no_speech_prob": 0.007222571410238743}, {"id": 212, "seek": 101100, "start": 1018.0, "end": 1019.0, "text": " Jasne.", "tokens": [50714, 34023, 716, 13, 50764], "temperature": 0.0, "avg_logprob": -0.058694970117856377, "compression_ratio": 1.3915857605177993, "no_speech_prob": 0.007222571410238743}, {"id": 213, "seek": 101100, "start": 1019.0, "end": 1029.0, "text": " To wszystko pokazuje, \u017ce Instruction Tuning, nawet tak zaawansowany, jest niezwykle pot\u0119\u017cnym narz\u0119dziem, ale nie jest magiczn\u0105 ruszczk\u0105.", "tokens": [50764, 1407, 22607, 13010, 43317, 11, 3561, 2730, 3826, 21363, 278, 11, 22696, 991, 7949, 1607, 599, 23341, 11, 3492, 33511, 9726, 14677, 1847, 1274, 1427, 12996, 6714, 89, 42643, 76, 11, 6775, 2838, 3492, 5585, 89, 13113, 367, 22378, 3689, 26304, 13, 51264], "temperature": 0.0, "avg_logprob": -0.058694970117856377, "compression_ratio": 1.3915857605177993, "no_speech_prob": 0.007222571410238743}, {"id": 214, "seek": 101100, "start": 1029.0, "end": 1033.0, "text": " To jeden, cho\u0107 bardzo wa\u017cny element tej skomplikowanej uk\u0142adanki.", "tokens": [51264, 1407, 12906, 11, 1586, 2162, 9034, 27777, 1634, 4478, 12573, 1110, 298, 564, 1035, 23066, 73, 344, 15317, 27203, 13, 51464], "temperature": 0.0, "avg_logprob": -0.058694970117856377, "compression_ratio": 1.3915857605177993, "no_speech_prob": 0.007222571410238743}, {"id": 215, "seek": 101100, "start": 1033.0, "end": 1039.0, "text": " Zatem, zbieraj\u0105c to wszystko razem, co z tego wynika dla kogo\u015b, kto interesuje si\u0119 t\u0105 dziedzin\u0105?", "tokens": [51464, 1176, 26851, 11, 710, 65, 811, 38757, 281, 22607, 40225, 11, 598, 710, 8627, 31936, 5439, 12285, 350, 23515, 1788, 11, 23780, 20157, 13008, 3244, 32294, 9758, 15338, 259, 1611, 30, 51764], "temperature": 0.0, "avg_logprob": -0.058694970117856377, "compression_ratio": 1.3915857605177993, "no_speech_prob": 0.007222571410238743}, {"id": 216, "seek": 103900, "start": 1039.0, "end": 1045.0, "text": " Jaka jest ta jedna kluczowa my\u015bl, z kt\u00f3r\u0105 warto zosta\u0107 po przeanalizowaniu tej pracy?", "tokens": [50364, 508, 7849, 3492, 1846, 5232, 629, 9671, 1311, 89, 5528, 452, 19212, 11, 710, 37415, 31830, 23154, 2162, 714, 8325, 29702, 590, 305, 25849, 12573, 35591, 30, 50664], "temperature": 0.0, "avg_logprob": -0.051426350230902015, "compression_ratio": 1.4965277777777777, "no_speech_prob": 0.01505883689969778}, {"id": 217, "seek": 103900, "start": 1045.0, "end": 1050.0, "text": " Ta praca bardzo wyra\u017anie sygnalizuje, \u017ce wchodzimy w now\u0105 er\u0119 rozwoju AI.", "tokens": [50664, 6551, 582, 6628, 9034, 4628, 424, 10659, 2766, 943, 4568, 304, 590, 13008, 11, 3561, 261, 29914, 89, 13189, 261, 586, 1611, 1189, 1274, 9544, 6120, 8954, 7318, 13, 50914], "temperature": 0.0, "avg_logprob": -0.051426350230902015, "compression_ratio": 1.4965277777777777, "no_speech_prob": 0.01505883689969778}, {"id": 218, "seek": 103900, "start": 1050.0, "end": 1058.0, "text": " Era, w kt\u00f3rej dominowa\u0142o has\u0142o wi\u0119kszy znaczy lepszy, powoli ust\u0119puje miejsca erze, w kt\u00f3rej liczy si\u0119 m\u0105drzejszy znaczy lepszy.", "tokens": [50914, 23071, 11, 261, 36023, 8859, 5528, 5249, 575, 5249, 29968, 1229, 36584, 476, 1878, 1229, 11, 3388, 9384, 26189, 18085, 13008, 18522, 44239, 1189, 1381, 11, 261, 36023, 6169, 1229, 3244, 275, 18962, 13503, 73, 7706, 36584, 476, 1878, 1229, 13, 51314], "temperature": 0.0, "avg_logprob": -0.051426350230902015, "compression_ratio": 1.4965277777777777, "no_speech_prob": 0.01505883689969778}, {"id": 219, "seek": 103900, "start": 1058.0, "end": 1065.0, "text": " Zrozumienie mechanizmu, w kt\u00f3re sprawiaj\u0105, \u017ce modele j\u0119zykowe ucz\u0105 si\u0119 generalizowa\u0107, czyli radzi\u0107 sobie z nowo\u015bci\u0105,", "tokens": [51314, 1176, 27857, 449, 27385, 4236, 590, 20140, 11, 261, 8864, 22734, 48125, 11, 3561, 4391, 306, 49055, 74, 6880, 35403, 1611, 3244, 2674, 590, 11445, 11, 16591, 2843, 28496, 13652, 710, 586, 44468, 1611, 11, 51664], "temperature": 0.0, "avg_logprob": -0.051426350230902015, "compression_ratio": 1.4965277777777777, "no_speech_prob": 0.01505883689969778}, {"id": 220, "seek": 106500, "start": 1065.0, "end": 1070.0, "text": " jest cenniejsze ni\u017c samobudowanie kolejnych gigant\u00f3w o setkach miliard\u00f3w parametr\u00f3w.", "tokens": [50364, 3492, 27900, 44258, 28502, 3247, 996, 532, 22028, 23749, 9399, 8741, 394, 3901, 277, 992, 41326, 1962, 72, 515, 3901, 6220, 27965, 3901, 13, 50614], "temperature": 0.0, "avg_logprob": -0.06834921390890218, "compression_ratio": 1.4909747292418774, "no_speech_prob": 0.35957613587379456}, {"id": 221, "seek": 106500, "start": 1070.0, "end": 1079.0, "text": " To jest krok w kierunku prawdziwej in\u017cynierii modeli j\u0119zykowych, a nie tylko ich bezrefleksyjnego trenowania na coraz wi\u0119kszych zbiorach danych.", "tokens": [50614, 1407, 3492, 350, 31621, 261, 38767, 49910, 41175, 3992, 826, 73, 294, 1427, 2534, 811, 5597, 2316, 72, 49055, 74, 19605, 11, 257, 2838, 13219, 1893, 10782, 33115, 306, 1694, 88, 73, 11858, 23136, 21308, 1667, 25899, 29968, 28051, 710, 33362, 608, 274, 34644, 13, 51064], "temperature": 0.0, "avg_logprob": -0.06834921390890218, "compression_ratio": 1.4909747292418774, "no_speech_prob": 0.35957613587379456}, {"id": 222, "seek": 106500, "start": 1079.0, "end": 1086.0, "text": " Przechodzimy od sztuki do nauki, to dobrze powiedziane. Mamy wreszcie przepis, a nie tylko przeczucie.", "tokens": [51064, 2114, 19439, 378, 89, 13189, 3611, 262, 2682, 11788, 360, 35616, 2984, 11, 281, 28335, 27617, 21133, 13, 376, 7804, 261, 495, 89, 4260, 30829, 271, 11, 257, 2838, 13219, 8325, 3689, 1311, 414, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06834921390890218, "compression_ratio": 1.4909747292418774, "no_speech_prob": 0.35957613587379456}, {"id": 223, "seek": 106500, "start": 1086.0, "end": 1090.0, "text": " Dok\u0142adnie. I to prowadzi do prowokacyjnego pytania na koniec.", "tokens": [51414, 29768, 10358, 2766, 13, 286, 281, 36590, 3992, 360, 45553, 453, 31285, 11858, 25878, 5609, 1667, 5897, 35733, 13, 51614], "temperature": 0.0, "avg_logprob": -0.06834921390890218, "compression_ratio": 1.4909747292418774, "no_speech_prob": 0.35957613587379456}, {"id": 224, "seek": 106500, "start": 1090.0, "end": 1091.0, "text": " Poprosz\u0119.", "tokens": [51614, 10215, 2635, 11052, 13, 51664], "temperature": 0.0, "avg_logprob": -0.06834921390890218, "compression_ratio": 1.4909747292418774, "no_speech_prob": 0.35957613587379456}, {"id": 225, "seek": 109100, "start": 1092.0, "end": 1099.0, "text": " Skoro mamy ju\u017c ca\u0142kiem niez\u0142y sprawdzony przepis na efektywny Instruction Tuning, to czy nast\u0119pnym logicznym krokiem", "tokens": [50414, 7324, 10780, 17335, 10678, 35224, 26116, 33511, 6825, 46192, 44479, 30829, 271, 1667, 31482, 916, 874, 43682, 2730, 3826, 21363, 278, 11, 281, 6430, 39662, 12996, 9952, 89, 12996, 45909, 26116, 50764], "temperature": 0.0, "avg_logprob": -0.08519162970074153, "compression_ratio": 1.4059040590405905, "no_speech_prob": 0.09622905403375626}, {"id": 226, "seek": 109100, "start": 1099.0, "end": 1108.0, "text": " b\u0119dzie tworzenie niejednego uniwersalnego modelu, kt\u00f3ry ma umie\u0107 wszystko, ale raczej wielu mniejszych wyspecjalizowanych?", "tokens": [50764, 10562, 46288, 16778, 2838, 40543, 11858, 36435, 5364, 304, 11858, 2316, 84, 11, 9913, 463, 1105, 414, 2162, 22607, 11, 6775, 4129, 16920, 40437, 39513, 45021, 27062, 494, 66, 22600, 590, 23341, 339, 30, 51214], "temperature": 0.0, "avg_logprob": -0.08519162970074153, "compression_ratio": 1.4059040590405905, "no_speech_prob": 0.09622905403375626}, {"id": 227, "seek": 109100, "start": 1108.0, "end": 1109.0, "text": " W\u0142a\u015bnie.", "tokens": [51214, 343, 5024, 12221, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08519162970074153, "compression_ratio": 1.4059040590405905, "no_speech_prob": 0.09622905403375626}, {"id": 228, "seek": 109100, "start": 1109.0, "end": 1116.0, "text": " Modeli, kt\u00f3re dzi\u0119ki tej wiedzy b\u0119d\u0105 niezwykle skuteczne w swoich w\u0105skich dziedzinach, w medycynie, prawie, finansach,", "tokens": [51264, 17105, 72, 11, 8864, 45003, 12573, 46894, 1229, 26239, 33511, 9726, 14677, 1110, 1169, 38491, 261, 13291, 480, 261, 1611, 5161, 480, 9758, 15338, 259, 608, 11, 261, 1205, 88, 1344, 2766, 11, 3206, 8699, 11, 38843, 608, 11, 51614], "temperature": 0.0, "avg_logprob": -0.08519162970074153, "compression_ratio": 1.4059040590405905, "no_speech_prob": 0.09622905403375626}, {"id": 229, "seek": 111600, "start": 1116.0, "end": 1122.0, "text": " a jednocze\u015bnie ze wzgl\u0105du na sw\u00f3j mniejszy rozmiar b\u0119d\u0105 dost\u0119pne dla znacznie szerszego grona odbiorc\u00f3w.", "tokens": [50364, 257, 5232, 26694, 1381, 12221, 5277, 48538, 1611, 769, 1667, 1693, 18999, 39513, 7706, 9544, 3057, 289, 26239, 48209, 716, 12285, 15397, 14875, 2766, 7870, 433, 27725, 677, 4037, 3611, 33362, 29268, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08101535170045618, "compression_ratio": 1.2380952380952381, "no_speech_prob": 0.1704295426607132}, {"id": 230, "seek": 111600, "start": 1122.0, "end": 1129.0, "text": " By\u0107 mo\u017ce przysz\u0142o\u015b\u0107 AI to nie jeden wszechwiedz\u0105cy olbrzym, ale ca\u0142a armia z winnych, inteligentnych specjalist\u00f3w.", "tokens": [50664, 3146, 2162, 12034, 44018, 44742, 7318, 281, 2838, 12906, 37647, 19439, 86, 1091, 8925, 1344, 2545, 1443, 26681, 11, 6775, 1335, 5024, 3726, 654, 710, 1942, 9399, 11, 24777, 25002, 9399, 46433, 468, 3901, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08101535170045618, "compression_ratio": 1.2380952380952381, "no_speech_prob": 0.1704295426607132}], "language": "pl"}