{"text": " Dobrze, zaczynamy. Dzisiaj na warsztat bierzemy materia\u0142, kt\u00f3ry, no, narobi\u0142 sporo szumu. To raport techniczny na temat modelu DeepSync V3. I na pierwszy rzut oka mo\u017cna pomy\u015ble\u0107. OK, kolejny du\u017cy model j\u0119zykowy, ale wystarczy zajrze\u0107 pod mask\u0119 i od razu wida\u0107, \u017ce to co\u015b wi\u0119cej. Zdecydowanie. Mamy tu model, kt\u00f3ry nie tylko, wiesz, rzuca wyzwanie najwi\u0119kszym graczom, takim jak GPT-4O. Ale robi to w spos\u00f3b, kt\u00f3ry wydawa\u0142 si\u0119 niemo\u017cliwy, przynajmniej pod wzgl\u0119dem efektywno\u015bci. Dok\u0142adnie. Gdyby ten raport mia\u0142 jedno zdanie podsumowania, to nie by\u0142oby... zbudowali\u015bmy wi\u0119kszy model. Nie, to by by\u0142o zbyt banalne. Raczej zbudowali\u015bmy niezwykle pot\u0119\u017cny model, dokonuj\u0105c, no, brutalnej optymalizacji ka\u017cdego elementu. Od samej architektury, przez oprogramowanie, a\u017c po sugestie jak przeprojektowa\u0107 sprz\u0119t. Czyli to jest opowie\u015b\u0107 o inteligentnej in\u017cynierii na wielu poziomach, a nie tylko o dorzucaniu kolejnych tysi\u0119cy kart graficznych do klastra. W\u0142a\u015bnie o to chodzi. Zatem nasza misja jest jasna. Rozebra\u0107 na cz\u0119\u015bci pierwsze jak DeepSync V3 osi\u0105ga t\u0119, no, niewiarykodn\u0105 wydajno\u015b\u0107? Chcemy zrozumie\u0107, co tam siedzi w architekturze, jakie innowacje w treningu pozwoli\u0142y tak obni\u017cy\u0107 koszty. I co to wszystko tak naprawd\u0119 oznacza dla przysz\u0142o\u015bci AI? Dobrze, to zacznijmy od samej konstrukcji. Raport od razu podkre\u015bla, \u017ce sercem modelu jest architektura Mixture of Experts, czyli MO. Tak, to ju\u017c znamy, ale jak oni to zinterpretowali? Tak, to jest fundament. DeepSync V3 ma \u0142\u0105cznie 671 miliard\u00f3w parametr\u00f3w, ale w danym momencie dla ka\u017cdego pojedynczego tokena aktywuje tylko 37 miliard\u00f3w. To architektura, kt\u00f3r\u0105 oddziedziczy\u0142 po poprzedniku DeepSync V2, podobnie jak mechanizm MultiHead Latent Attention, czyli MLA. Czyli podstawa jest znana i sprawdzona? Tak, te elementy od pocz\u0105tku mia\u0142y zapewni\u0107 wydajne wnioskowanie, ale prawdziwa magia zaczyna si\u0119 w nowo\u015bciach, kt\u00f3re na tej podstawie zbudowano. No i tu dochodzimy do Sedna. W \u015bwiecie, gdzie wszyscy, powiedzmy, u\u017cywaj\u0105 podobnych klock\u00f3w, DeepSync znalaz\u0142 nowe sposoby ich uk\u0142adania. Jaka jest ta pierwsza przy\u0142omowa innowacja? To co\u015b, co nazywaj\u0105 strategi\u0105 r\u00f3wnowa\u017cenia obci\u0105\u017cenia bez dodatkowej funkcji straty, czyli Auxiliary Loss Free. Brzmi skomplikowanie. Ale zasada jest prosta. W modelach MOEI mamy wielu ekspert\u00f3w, czyli ma\u0142e sieci neuronowe. System musi decydowa\u0107, kt\u00f3rego z nich zapyta\u0107 o zdanie. Problem w tym, \u017ce model naturalnie mo\u017ce faworyzowa\u0107 kilku najlepszych. Reszt\u0119 zaniedbywa\u0107. Co prowadzi do tego, \u017ce kilku ekspert\u00f3w jest przepracowanych, a reszta sie\u0107 bezczynnie. Marnotrawstwo. W\u0142a\u015bnie. Tradycyjnie rozwi\u0105zuje si\u0119 to dodaj\u0105cy do treningu tak\u0105 matematyczn\u0105 kar\u0119, czyli Auxiliary Loss. Ona zmusza model do bardziej r\u00f3wnomiernego rozdzielania zada\u0144. Ale ta kara ma swoj\u0105 cen\u0119, prawda? Oczywi\u015bcie. Pogarsza og\u00f3ln\u0105 wydajno\u015b\u0107 modelu, bo zmusza go do kompromis\u00f3w. To troch\u0119 jakby m\u00f3wi\u0107 gwie\u017adzie dru\u017cyny koszykarskiej, \u017ce musi podawa\u0107 pi\u0142k\u0119 do s\u0142abszych graczy. Nawet je\u015bli sam ma czyst\u0105 pozycj\u0119 do rzutu. Mo\u017ce to i sprawie div\u0119, ale wynik meczu b\u0119dzie gorszy. Idealna analogia. Dipsy Qv3 jako pierwszy na tak\u0105 skal\u0119 pozbywa si\u0119 tej kary. Zamiast tego wprowadza mechanizm, kt\u00f3ry dynamicznie, za pomoc\u0105 tak zwanego bias term, dostosowuje preferencj\u0119 rotingu. Czyli nie zmusza, a delikatnie zach\u0119ca. Tak. I to pozwala ekspertom na znacznie g\u0142\u0119bsz\u0105 specjalizacj\u0119. Jak pokazuj\u0105 wykresy w raporcie, niekt\u00f3rzy staj\u0105 si\u0119 wybitni w kodowaniu, inni w matematyce, a jeszcze inni w rozumieniu j\u0119zyka. OK. Czyli zamiast armii jednakowo wyszkolonych \u017co\u0142nierzy, mamy zesp\u00f3\u0142 wyspecjalizowanych komandos\u00f3w. To za\u0142atwia kwestie jako\u015bci my\u015blenia, ale w raporcie jest te\u017c du\u017co o szybko\u015bci. Jak do tego podeszli? I tu dochodzimy do drugiej kluczowej innowacji. Multi-Token Prediction, w skr\u00f3cie MTP. Standardowo model j\u0119zykowy podczas treningu uczy si\u0119 przewidziwa\u0107 jedno nast\u0119pne s\u0142owo. Deep-Sig V3 uczy si\u0119 przewidywa\u0107 kilka kolejnych, w tym przypadku dwa. Czyli nie tylko my\u015bli jaki jest nast\u0119pny krok, ale jakie s\u0105 dwa nast\u0119pne kroki. To brzmi jakby mia\u0142o znacznie zwi\u0119kszy\u0107 obci\u0105\u017cenie. Intuicyjnie tak, ale w praktyce to jakby to uj\u0105\u0107 za g\u0119szcza sygna\u0142y treningowe. Za jednym zamachem model dostaje wi\u0119cej informacji zwrotnej, co poprawia wydajno\u015b\u0107 nauki, a jest te\u017c genialny efekt uboczny. Jaki? Te dodatkowe przewidywania mo\u017cna wykorzysta\u0107 do drastycznego przyspieszenia generowania odpowiedzi. W technice zwanej Speculative Decoding. Raport podaje, \u017ce daje to a\u017c jeden przecinek osiemkrotny wzrost szybko\u015bci. To jest ju\u017c odczuwalne. W porz\u0105dku, czyli mamy sprytniejsz\u0105, bardziej wyspecjalizowan\u0105 architektur\u0119 i model, kt\u00f3ry my\u015bli kilka krok\u00f3w do przodu. Ale to wci\u0105\u017c nie wyja\u015bnia najwi\u0119kszej zagadki. Koszt\u00f3w. Podali, \u017ce pe\u0142ny trening kosztowa\u0142 zaledwie dwa przecinek 788 miliona godziny na GPU H800. Co przek\u0142ada si\u0119 na oko\u0142o pi\u0119\u0107 przecinek sze\u015b\u0107 miliona dolar\u00f3w? To s\u0105 pieni\u0105dze. No niewyobra\u017calnie ma\u0142e jak na model tej klasy. Jak to jest w og\u00f3le mo\u017cliwe? To jest absolutnie centralny punkt tej pracy. Osi\u0105gn\u0119li to dzi\u0119ki trzem filarom ekstremalnej optymalacji. Filar pierwszy to trening w precyzji FP8. Chwila FP8. Wi\u0119kszo\u015b\u0107 os\u00f3b, kt\u00f3ra si\u0119 tym interesuje, kojarzy BF16 jako standard. O jakiej skali oszcz\u0119dno\u015bci tu m\u00f3wimy? To jest teoretycznie dwa razy szybciej i dwa razy mniej pami\u0119ci? Dok\u0142adnie tak. Ale to nie jest takie proste. Przej\u015bcie na o\u015bmiobitow\u0105 precyzj\u0119 FP8 zwi\u0105\u017cy si\u0119 z gigantycznym ryzykiem utraty informacji. To jak pr\u00f3ba narysowania fotorealistycznego portretu bardzo grubym o\u0142\u00f3wkiem. \u0141atwo zgubi\u0107 detale. Wi\u0119c jakiego artystycznego triku u\u017cyli, \u017ceby ten gruby o\u0142\u00f3wek rysowa\u0142 z precyzj\u0105 cienkopisu? Wprowadzili co\u015b, co nazywaj\u0105 fine grained quantization. Zamiast traktowa\u0107 ca\u0142\u0105 wielk\u0105 macie\u017c wak jako jeden byt i skalowa\u0107 j\u0105 do FP8. Oni dziel\u0105 j\u0105 na ma\u0142e fragmenty. Na mniejsze kawa\u0142ki? Tak, na bloki o wymiarach 128 na 128 i skaluj\u0105 ka\u017cdy z nich osobno. To pozwala im znacznie lepiej zarz\u0105dza\u0107 warto\u015bciami odstaj\u0105cymi, tak zwanymi outliersy, kt\u00f3re s\u0105 z mor\u0105 treningu w niskiej precyzji. Aha, czyli zamiast u\u017cywa\u0107 jednej du\u017cej miarki dla ca\u0142ego worka m\u0105ki u\u017cyli precyzyjnej \u0142y\u017ceczki do ka\u017cdej szklanki. To pozwala zachowa\u0107 dok\u0142adno\u015b\u0107 tam, gdzie jest ona krytyczna. W\u0142a\u015bnie, ale poszli jeszcze g\u0142\u0119biej. Zmodyfikowali spos\u00f3b, w jaki rdzenia tensor cores w procesorach NVIDI akumuluj\u0105 wyniki. Przenie\u015bli cz\u0119\u015b\u0107 tych oblicze\u0144 do bardziej precyzyjnych rdzeni CUDA cores. Czyli zeszli ju\u017c na poziom samego sprz\u0119tu? To pokazuje, \u017ce nie optymalizowali tylko kodu. Weszli w interakcj\u0119 z architektur\u0105 sprz\u0119tu, \u017ceby wycisn\u0105\u0107 z niego ostatnie soki. To jest niesamowite. W\u0142a\u015bciwie odwracaj\u0105 paradygmat. M\u00f3wi\u0105, \u017ce ich kod jest tak zoptymalizowany, \u017ce teraz to sprz\u0119t musi si\u0119 dostosowa\u0107. Dobrze, a co z drugim filarem? Filar drugi to ich autorski algorytm Dual Pipe. On rozwi\u0105zuje najwi\u0119ksze w\u0105skie gard\u0142o w treningu modeli MOE. Czyli komunikacje. Tak, komunikacje mi\u0119dzy w\u0119z\u0142ami w klasstrze. Gdy masz tysi\u0105ce GPU wymiana danych staje si\u0119 ogromnym problemem. Dual Pipe to niezwykle inteligentny harmonogram, kt\u00f3ry przeplata obliczenia z komunikacj\u0105. Czyli to jest troch\u0119 jak kuchnia w restauracji z gwiazg\u0105 Michelin, gdzie jeden kuchar zaczyna kroi\u0107 warzywa podczas gdy drugi ju\u017c sma\u017cy mi\u0119so, \u017ceby nie by\u0142o ani sekundy przestoju. Dok\u0142adnie tak. Chodzi o to, \u017ceby GPU nigdy nie czeka\u0142o bezczynnie na dane. W momencie, gdy jeden zestaw danych jest wysy\u0142any przez sie\u0107, procesor ju\u017c pracuje nad nast\u0119pnym. Dual Pipe sprawia, \u017ce komunikacja jest niemal ca\u0142kowicie ukryta. Ukryta w czasie, kiedy i tak trwaj\u0105 obliczenia. Tak, dzi\u0119ki temu mog\u0105 skalowa\u0107 model na wi\u0119cej maszyn, bez martwienia si\u0119 o narzut komunikacyjny. A trzeci filar to, jak si\u0119 domy\u015blam, r\u00f3wnie ekstremalne podej\u015bcie do pami\u0119ci. Tak, ekstremalna oszcz\u0119dno\u015b\u0107 pami\u0119ci stosuj\u0105 ca\u0142\u0105 gam\u0119 technik. Na przyk\u0142ad rekomputacja pewnych operacji zamiast przechowywania ich wynik\u00f3w w drogocennej pami\u0119ci VRAM. Do ma sens. Albo trzymanie parametr\u00f3w Exponential Moving Everage w skr\u00f3cie EMA w znacznie wolniejszej, ale pojemniejszej pami\u0119ci serwera, a nie na karcie. Ka\u017cdy zaoszcz\u0119dzony megabyte pozwala\u0142 im trenowa\u0107 lepszy model przy tym samym bud\u017cecie. Mamy wi\u0119c innowacyjn\u0105 architektur\u0119, super wydajny trening. Przejd\u017amy do wynik\u00f3w. Czy ten model faktycznie jest tak dobry, jak sugeruj\u0105 te optymalizacje? Jak wypada w starciu z tytanami? Wyniki s\u0105 szczerze m\u00f3wi\u0105c osza\u0142amiaj\u0105ce. Wersja bazowa DeepSeek V3 Base jest w momencie publikacji najsilniejszym modelem open source na \u015bwiecie. OK. Ale to co naprawd\u0119 szokuje to por\u00f3wnanie z LMA3145B. W wielu kluczowych benchmarkach DeepSeek V3 go przewy\u017csza. Chwileczka to jest absolutnie kluczowe. Powiedzmy to g\u0142o\u015bno. DeepSeek V3 osi\u0105ga lepsze wyniki u\u017cywaj\u0105c 11 razy mniej aktywnych parametr\u00f3w. Tak, 11 razy mniej. To nie jest drobna optymalizacja. To fundamentalna zmiana. To tak jakby samoch\u00f3d silnikiem od kosiarki wygra\u0142 wy\u015bcig z bolidem Formu\u0142y 1. Co to tak naprawd\u0119 oznacza? To oznacza, \u017ce era wi\u0119ksze znaczy lepszy mo\u017ce dobiega\u0107 ko\u0144ca. Teraz liczy si\u0119 efektywno\u015b\u0107 i ile inteligencji potrafisz wycisn\u0105\u0107 z ka\u017cdego parametru i ka\u017cdego wata energii. A DeepSeek V3 pokazuje, \u017ce jest tu jeszcze ogromne pole do popisu. A gdzie ta jego si\u0142a jest najbardziej widoczna? Raport wspomina o specjalizacjach. Tak, zw\u0142aszcza w kodowaniu i matematyce. Sp\u00f3jrzmy na liczby. W te\u015bcie Meth 500, kt\u00f3ry sprawdza rozwi\u0105zywanie problem\u00f3w matematycznych, osi\u0105ga wynik 92 i 20. W AM 2024. To ten trudny presti\u017cowy konkurs. Tak, tam osi\u0105ga wynik 39 i 20. Deklasuj\u0105c konkurencj\u0119. Ale to kodowanie jest chyba najbardziej spektakularne. W benchmarku Code Forces jego wyniki plasuj\u0105 go w 51,6 procentylu. A jak to si\u0119 ma do innych? Gdzie jest na przyk\u0142ad GPT-4O? W tym samym te\u015bcie GPT-4O osi\u0105ga 23,6 procentylu. To jest przepa\u015b\u0107. Ogromna. To oznacza, \u017ce DeepSync V3 generuje kod na poziomie, powiedzmy, przeci\u0119tnego, dobrego, ludzkiego programisty konkursowego. Inne modele s\u0105 daleko w tyle. Co wi\u0119cej, wersja Chat po fine tuningu jest pe\u0142nie por\u00f3wnywalna z zamkni\u0119tymi modelami. A\u017c tak? W te\u015bcie Arena Hard, kt\u00f3re mierzy jak model radzi sobie ze z\u0142o\u017conymi zadaniami, DeepSync V3 jako pierwszy model OpenSource przekroczy\u0142 pr\u00f3g 85 procent, zr\u00f3wnuj\u0105c si\u0119 z Cloud 3.5 Sonet. Zr\u00f3wnuj\u0105c si\u0119 z modelem, kt\u00f3ry jest uwa\u017cany za absolutny top, to brzmi, no, prawie jak marketingowa rewelacja. Czy te benchmarki na pewno s\u0105 wiarygodne? Czy nie ma tu jakiego\u015b uczenia pod testy? To jest bardzo s\u0142uszne pytanie i zdrowy sceptyczyzm. Autorze raportu zdaj\u0105 sobie z tego spraw\u0119. I cz\u0119\u015bciowo odkowiadaj\u0105 na to pytanie, wyja\u015bniaj\u0105c sk\u0105d bior\u0105 si\u0119 te niezwyk\u0142e zdolno\u015bci do rozumowania. Czyli jest jeszcze jaki\u015b sekretny sk\u0142adnik co\u015b wi\u0119cej? Tak. Bardzo ciekawa technika, kt\u00f3r\u0105 zastosowali w fazie post-training. U\u017cyli destylacji wiedzy z innego, wyspecjalizowanego modelu DeepSync R1. Ten model jest ekspertem w rozumowaniu krok po kroku, czyli w generowaniu d\u0142ugich Chain of Thought. W uproszczeniu j\u0105, czyli DeepSync V3, wzorc\u00f3w, weryfikacji i refleksji od swojego starszego, m\u0105drzejszego brata. To w\u0142a\u015bnie ta technika, to douczanie od specjalisty, znacz\u0105co podbi\u0142a jego wyniki w rozumowaniu. Czyli sprawi\u0142a, \u017ce s\u0105 one bardziej solidne i mniej podatne na zwyk\u0142e wykucie odpowiedzi? Dok\u0142adnie tak. Brzmi to wszystko niemal zbyt dobrze, \u017ceby by\u0142o prawdziwe. Ka\u017cdy taki prze\u0142om ma swoje kompromisy. Czy ten model ma jakie\u015b wady? Jakie ograniczenia wymieniaj\u0105 sami autorzy? I tu dochodzimy do wa\u017cnej, praktycznej cz\u0119\u015bci. Autorzy s\u0105 bardzo szczerzy. Po pierwsze, \u017ceby wydajnie wdro\u017cy\u0107 ten model, potrzeba relatywnie du\u017cej infrastruktury. Czyli jakiej? Raport m\u00f3wi o minimum czterech w\u0119z\u0142ach z 32 magii pi\u0142u dla etapu pre-feeling. To nie jest model, kt\u00f3ry mo\u017cna komfortowo uruchomi\u0107 na jednym serwerze w piwnice. Wspomnia\u0142e\u015b o barierze wej\u015bcia minimum 32 gpu. Kogo to w praktyce wyklucza? Czy to oznacza, \u017ce era gara\u017cowych innowacji w Open Source AI na tym poziomie si\u0119 ko\u0144czy? To jest bardzo trafne pytanie. Na pewno utrudnia to zadanie uniwersytetom i mniejszym startupom. Ale z drugiej strony, poniewa\u017c model jest Open Source, du\u017ce centra obliczeniowe mog\u0105 go wdro\u017cy\u0107 i udost\u0119pni\u0107 spo\u0142eczno\u015bci przez API. Czyli zmienia si\u0119 model dost\u0119pu? Tak, to raczej zmienia model dost\u0119pu i wsp\u00f3\u0142pracy, a nie koniecznie zabija innowacje. Ale tak, samodzielne fine-tune'owanie takiego giganta staje si\u0119 wyzwaniem. A co z innymi ograniczeniami? M\u00f3wili\u015bmy o szybko\u015bci. Tak, to drugie ograniczenie. Chocia\u017c pr\u0119dko\u015b\u0107 generowania jest dobra. Wspominali\u015bmy o tym 1.8X przy spieszeniu dzi\u0119ki MTP. Wci\u0105\u017c jest pole do poprawy w por\u00f3wnaniu z niekt\u00f3rymi mniejszymi modelami. Klasyczny kompromis mi\u0119dzy moc\u0105 a surow\u0105 pr\u0119dko\u015bci\u0105. Dok\u0142adnie. Do prostych zada\u0144 mniejszy, szybszy model mo\u017ce by\u0107 wci\u0105\u017c lepszym wyborem. Wi\u0119c co to wszystko oznacza i jaka jest przysz\u0142o\u015b\u0107 wed\u0142ug tw\u00f3rc\u00f3w? Gdzie widz\u0105 kolejne kroki? Planuj\u0105 dalsze prace nad sam\u0105 architektur\u0105. Wspominaj\u0105 o d\u0105\u017ceniu do obs\u0142ugi niesko\u0144czonego kontekstu i by\u0107 mo\u017ce o statecznym prze\u0142amaniu ogranicze\u0144 architektury Transformer. Chc\u0105 te\u017c dalej skalowa\u0107 dane i bada\u0107 zdolno\u015bci modeli do jak to uj\u0119li g\u0142\u0119bokiego my\u015blenia. Cokolwiek to dok\u0142adnie znaczy. Cokolwiek to znaczy. Ale jest co\u015b jeszcze. Co\u015b co wykracza poza sam model. W raporcie zawarli ca\u0142y rozdzia\u0142 sugestii dla producent\u00f3w sprz\u0119tu AI. O, to ciekawe. Proponuj\u0105 konkretne zmiany w budowie przysz\u0142ych chip\u00f3w. Na przyk\u0142ad wbudowany na poziomie krzemu wsparcie dla ich techniki fine grained quantization albo lepsze mechanizmy sprz\u0119towe do ukrywania komunikacji. Czyli to ju\u017c nie jest jednokierunkowa ulica, gdzie tw\u00f3rcy oprogramowania dostosowuj\u0105 si\u0119 do sprz\u0119tu. Zdecydowanie nie. Oni doszli do \u015bciany i m\u00f3wi\u0105. Zrobili\u015bmy co si\u0119 da\u0142o w kodzie. Teraz wy producenci chip\u00f3w. Musicie nam pom\u00f3c p\u00f3j\u015b\u0107 dalej. Dok\u0142adnie. To pokazuje, \u017ce dalsza optymalizacja wymaga fundamentalnych zmien nie tylko w algorytmach, ale i w samym krzemie. To jest sygna\u0142 dla ca\u0142ej bran\u017cy. Podsumowuj\u0105c, DeepSig V3 to nie jest po prostu kolejny, pot\u0119\u017cny model na szczycie tabeli. To jest manifest. Tak, dobre s\u0142owo. Dow\u00f3d na to, \u017ce kluczem do post\u0119pu VI jest teraz inteligentna, holistyczna optymalizacja. Na ka\u017cdym poziomie od algorytmu przez framework, a\u017c po projekt sprz\u0119tu. To model, kt\u00f3ry jest jednocze\u015bnie pot\u0119\u017cniejsze od konkurencji i zaskakuj\u0105co ekonomiczny. I to prowadzi do bardzo wa\u017cnego pytania na przysz\u0142o\u015b\u0107. Te sugestie dla producent\u00f3w sprz\u0119tu, o kt\u00f3rych m\u00f3wi\u0142y\u015bmy, to nie jest tylko ciekawostka. To mo\u017ce by\u0107 zwiastun nowego trendu. Jakiego? Trendu, w kt\u00f3rym oprogramowanie i sprz\u0119t do AI b\u0119d\u0105 projektowane w \u015bcis\u0142ej, nierozerwalnej synergi. I to jest my\u015bl, z kt\u00f3r\u0105 warto zosta\u0107. Czy w miar\u0119, jak modele staj\u0105 si\u0119 coraz inteligentniejsze, a ich wymagania bardziej specyficzne, to prawdziwe w\u0105skie gard\u0142o przenosi si\u0119 z projektowania algorytm\u00f3w na \u015bcis\u0142\u0105 niemal symbiotyczn\u0105 wsp\u00f3\u0142prac\u0119. Wsp\u00f3\u0142prac\u0119 przy projektowaniu oprogramowania i dedykowanego mu krzemu. To jest fundamentalne pytanie o przysz\u0142o\u015b\u0107. Czy przysz\u0142o\u015b\u0107 AI to ju\u017c nie tylko kod, ale nierozerwalny duet kodu i sprz\u0119tu, stworzonych dla siebie nawzajem?", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.12, "text": " Dobrze, zaczynamy. Dzisiaj na warsztat bierzemy materia\u0142, kt\u00f3ry, no, narobi\u0142 sporo szumu.", "tokens": [50364, 29679, 13503, 11, 43811, 5378, 88, 13, 39448, 22356, 1667, 13718, 2682, 267, 272, 34602, 3633, 2389, 8908, 11, 9913, 11, 572, 11, 6714, 19293, 1221, 637, 10780, 7870, 30034, 13, 50670], "temperature": 0.0, "avg_logprob": -0.210248542158571, "compression_ratio": 1.3501683501683501, "no_speech_prob": 0.01265857182443142}, {"id": 1, "seek": 0, "start": 6.72, "end": 9.88, "text": " To raport techniczny na temat modelu DeepSync V3.", "tokens": [50700, 1407, 5099, 477, 1537, 17946, 1634, 1667, 32954, 2316, 84, 14895, 50, 34015, 691, 18, 13, 50858], "temperature": 0.0, "avg_logprob": -0.210248542158571, "compression_ratio": 1.3501683501683501, "no_speech_prob": 0.01265857182443142}, {"id": 2, "seek": 0, "start": 10.64, "end": 12.88, "text": " I na pierwszy rzut oka mo\u017cna pomy\u015ble\u0107.", "tokens": [50896, 286, 1667, 34016, 367, 89, 325, 277, 2330, 17790, 280, 8488, 1788, 306, 2162, 13, 51008], "temperature": 0.0, "avg_logprob": -0.210248542158571, "compression_ratio": 1.3501683501683501, "no_speech_prob": 0.01265857182443142}, {"id": 3, "seek": 0, "start": 13.08, "end": 20.88, "text": " OK, kolejny du\u017cy model j\u0119zykowy, ale wystarczy zajrze\u0107 pod mask\u0119 i od razu wida\u0107, \u017ce to co\u015b wi\u0119cej.", "tokens": [51018, 2264, 11, 23749, 1634, 1581, 7735, 2316, 49055, 74, 10089, 11, 6775, 4628, 9710, 6522, 33729, 13503, 2162, 2497, 6094, 1274, 741, 3611, 367, 8813, 261, 46898, 11, 3561, 281, 19241, 26004, 13, 51408], "temperature": 0.0, "avg_logprob": -0.210248542158571, "compression_ratio": 1.3501683501683501, "no_speech_prob": 0.01265857182443142}, {"id": 4, "seek": 0, "start": 21.240000000000002, "end": 27.68, "text": " Zdecydowanie. Mamy tu model, kt\u00f3ry nie tylko, wiesz, rzuca wyzwanie najwi\u0119kszym graczom, takim jak GPT-4O.", "tokens": [51426, 1176, 1479, 1344, 67, 22028, 13, 376, 7804, 2604, 2316, 11, 9913, 2838, 13219, 11, 261, 15347, 11, 367, 11728, 496, 4628, 14406, 7155, 48636, 1694, 26681, 11625, 89, 298, 11, 31732, 4207, 26039, 51, 12, 19, 46, 13, 51748], "temperature": 0.0, "avg_logprob": -0.210248542158571, "compression_ratio": 1.3501683501683501, "no_speech_prob": 0.01265857182443142}, {"id": 5, "seek": 2768, "start": 27.72, "end": 33.16, "text": " Ale robi to w spos\u00f3b, kt\u00f3ry wydawa\u0142 si\u0119 niemo\u017cliwy, przynajmniej pod wzgl\u0119dem efektywno\u015bci.", "tokens": [50366, 9366, 47380, 281, 261, 22904, 11, 9913, 25984, 10449, 1221, 3244, 2838, 3280, 1427, 2081, 9726, 11, 6501, 20981, 47658, 2497, 48538, 6298, 443, 31482, 916, 874, 20944, 6199, 13, 50638], "temperature": 0.0, "avg_logprob": -0.10520591606964937, "compression_ratio": 1.4375, "no_speech_prob": 0.009557303041219711}, {"id": 6, "seek": 2768, "start": 33.24, "end": 40.16, "text": " Dok\u0142adnie. Gdyby ten raport mia\u0142 jedno zdanie podsumowania, to nie by\u0142oby... zbudowali\u015bmy wi\u0119kszy model.", "tokens": [50642, 29768, 10358, 2766, 13, 460, 3173, 2322, 2064, 5099, 477, 27989, 5232, 1771, 16221, 7155, 31925, 449, 21308, 11, 281, 2838, 16673, 13944, 485, 710, 18281, 305, 33955, 29968, 1229, 2316, 13, 50988], "temperature": 0.0, "avg_logprob": -0.10520591606964937, "compression_ratio": 1.4375, "no_speech_prob": 0.009557303041219711}, {"id": 7, "seek": 2768, "start": 40.2, "end": 41.76, "text": " Nie, to by by\u0142o zbyt banalne.", "tokens": [50990, 12016, 11, 281, 538, 14811, 710, 2322, 83, 5643, 304, 716, 13, 51068], "temperature": 0.0, "avg_logprob": -0.10520591606964937, "compression_ratio": 1.4375, "no_speech_prob": 0.009557303041219711}, {"id": 8, "seek": 2768, "start": 41.88, "end": 50.16, "text": " Raczej zbudowali\u015bmy niezwykle pot\u0119\u017cny model, dokonuj\u0105c, no, brutalnej optymalizacji ka\u017cdego elementu.", "tokens": [51074, 42033, 16920, 710, 18281, 305, 33955, 33511, 9726, 14677, 1847, 1274, 1427, 1634, 2316, 11, 360, 18295, 44733, 11, 572, 11, 17878, 11794, 2427, 4199, 304, 590, 13152, 21912, 67, 6308, 4478, 84, 13, 51488], "temperature": 0.0, "avg_logprob": -0.10520591606964937, "compression_ratio": 1.4375, "no_speech_prob": 0.009557303041219711}, {"id": 9, "seek": 2768, "start": 50.56, "end": 55.56, "text": " Od samej architektury, przez oprogramowanie, a\u017c po sugestie jak przeprojektowa\u0107 sprz\u0119t.", "tokens": [51508, 12210, 912, 73, 3912, 642, 2320, 2598, 11, 14064, 999, 340, 1342, 22028, 11, 48134, 714, 459, 2629, 414, 4207, 30829, 340, 14930, 11445, 6103, 11052, 83, 13, 51758], "temperature": 0.0, "avg_logprob": -0.10520591606964937, "compression_ratio": 1.4375, "no_speech_prob": 0.009557303041219711}, {"id": 10, "seek": 5556, "start": 55.800000000000004, "end": 64.48, "text": " Czyli to jest opowie\u015b\u0107 o inteligentnej in\u017cynierii na wielu poziomach, a nie tylko o dorzucaniu kolejnych tysi\u0119cy kart graficznych do klastra.", "tokens": [50376, 37099, 281, 3492, 999, 13998, 7753, 277, 24777, 25002, 11794, 294, 1427, 2534, 811, 5597, 1667, 40437, 38503, 298, 608, 11, 257, 2838, 13219, 277, 26313, 89, 1311, 25849, 23749, 9399, 38156, 47303, 29120, 1295, 1786, 89, 9399, 360, 9671, 525, 424, 13, 50810], "temperature": 0.0, "avg_logprob": -0.15478285937242106, "compression_ratio": 1.3793103448275863, "no_speech_prob": 0.002482015173882246}, {"id": 11, "seek": 5556, "start": 64.60000000000001, "end": 65.96000000000001, "text": " W\u0142a\u015bnie o to chodzi.", "tokens": [50816, 343, 5024, 12221, 277, 281, 23998, 13, 50884], "temperature": 0.0, "avg_logprob": -0.15478285937242106, "compression_ratio": 1.3793103448275863, "no_speech_prob": 0.002482015173882246}, {"id": 12, "seek": 5556, "start": 66.28, "end": 68.36, "text": " Zatem nasza misja jest jasna.", "tokens": [50900, 1176, 26851, 5382, 2394, 3346, 2938, 3492, 361, 296, 629, 13, 51004], "temperature": 0.0, "avg_logprob": -0.15478285937242106, "compression_ratio": 1.3793103448275863, "no_speech_prob": 0.002482015173882246}, {"id": 13, "seek": 5556, "start": 68.64, "end": 75.6, "text": " Rozebra\u0107 na cz\u0119\u015bci pierwsze jak DeepSync V3 osi\u0105ga t\u0119, no, niewiarykodn\u0105 wydajno\u015b\u0107?", "tokens": [51018, 3101, 1381, 6198, 2162, 1667, 41314, 45994, 4207, 14895, 50, 34015, 691, 18, 3003, 11404, 3680, 32489, 11, 572, 11, 43622, 29104, 74, 378, 13113, 25984, 1805, 23293, 30, 51366], "temperature": 0.0, "avg_logprob": -0.15478285937242106, "compression_ratio": 1.3793103448275863, "no_speech_prob": 0.002482015173882246}, {"id": 14, "seek": 5556, "start": 75.92, "end": 82.24000000000001, "text": " Chcemy zrozumie\u0107, co tam siedzi w architekturze, jakie innowacje w treningu pozwoli\u0142y tak obni\u017cy\u0107 koszty.", "tokens": [51382, 761, 384, 2226, 710, 27857, 449, 414, 2162, 11, 598, 7677, 262, 1091, 3992, 261, 3912, 642, 2320, 374, 1381, 11, 22124, 294, 3785, 29293, 261, 2192, 773, 84, 40557, 9384, 6825, 991, 1111, 3722, 39687, 19532, 89, 874, 13, 51698], "temperature": 0.0, "avg_logprob": -0.15478285937242106, "compression_ratio": 1.3793103448275863, "no_speech_prob": 0.002482015173882246}, {"id": 15, "seek": 8224, "start": 82.32, "end": 85.67999999999999, "text": " I co to wszystko tak naprawd\u0119 oznacza dla przysz\u0142o\u015bci AI?", "tokens": [50368, 286, 598, 281, 22607, 991, 20970, 277, 22672, 326, 2394, 12285, 44018, 35059, 7318, 30, 50536], "temperature": 0.0, "avg_logprob": -0.14192697338591842, "compression_ratio": 1.3439716312056738, "no_speech_prob": 0.00639799190685153}, {"id": 16, "seek": 8224, "start": 86.03999999999999, "end": 88.67999999999999, "text": " Dobrze, to zacznijmy od samej konstrukcji.", "tokens": [50554, 29679, 13503, 11, 281, 710, 14875, 77, 1718, 2226, 3611, 912, 73, 34208, 25126, 19649, 13, 50686], "temperature": 0.0, "avg_logprob": -0.14192697338591842, "compression_ratio": 1.3439716312056738, "no_speech_prob": 0.00639799190685153}, {"id": 17, "seek": 8224, "start": 89.08, "end": 95.28, "text": " Raport od razu podkre\u015bla, \u017ce sercem modelu jest architektura Mixture of Experts, czyli MO.", "tokens": [50706, 16184, 477, 3611, 367, 8813, 2497, 27885, 1788, 875, 11, 3561, 816, 26422, 2316, 84, 3492, 3912, 642, 2320, 2991, 10204, 8890, 295, 12522, 1373, 11, 16591, 19290, 13, 51016], "temperature": 0.0, "avg_logprob": -0.14192697338591842, "compression_ratio": 1.3439716312056738, "no_speech_prob": 0.00639799190685153}, {"id": 18, "seek": 8224, "start": 95.6, "end": 99.39999999999999, "text": " Tak, to ju\u017c znamy, ale jak oni to zinterpretowali?", "tokens": [51032, 9118, 11, 281, 10678, 710, 5378, 88, 11, 6775, 4207, 36317, 281, 710, 41935, 305, 5103, 30, 51222], "temperature": 0.0, "avg_logprob": -0.14192697338591842, "compression_ratio": 1.3439716312056738, "no_speech_prob": 0.00639799190685153}, {"id": 19, "seek": 8224, "start": 99.6, "end": 101.28, "text": " Tak, to jest fundament.", "tokens": [51232, 9118, 11, 281, 3492, 6073, 13, 51316], "temperature": 0.0, "avg_logprob": -0.14192697338591842, "compression_ratio": 1.3439716312056738, "no_speech_prob": 0.00639799190685153}, {"id": 20, "seek": 8224, "start": 101.72, "end": 111.47999999999999, "text": " DeepSync V3 ma \u0142\u0105cznie 671 miliard\u00f3w parametr\u00f3w, ale w danym momencie dla ka\u017cdego pojedynczego tokena", "tokens": [51338, 14895, 50, 34015, 691, 18, 463, 220, 15926, 19923, 23879, 16, 1962, 72, 515, 3901, 6220, 27965, 3901, 11, 6775, 261, 274, 1325, 76, 40883, 12285, 21912, 67, 6308, 714, 40543, 2534, 3689, 6308, 14862, 64, 51826], "temperature": 0.0, "avg_logprob": -0.14192697338591842, "compression_ratio": 1.3439716312056738, "no_speech_prob": 0.00639799190685153}, {"id": 21, "seek": 11148, "start": 111.52000000000001, "end": 114.24000000000001, "text": " aktywuje tylko 37 miliard\u00f3w.", "tokens": [50366, 9308, 874, 86, 13008, 13219, 13435, 1962, 72, 515, 3901, 13, 50502], "temperature": 0.0, "avg_logprob": -0.1399430361661044, "compression_ratio": 1.3475177304964538, "no_speech_prob": 0.01276802271604538}, {"id": 22, "seek": 11148, "start": 114.76, "end": 123.08, "text": " To architektura, kt\u00f3r\u0105 oddziedziczy\u0142 po poprzedniku DeepSync V2, podobnie jak mechanizm MultiHead Latent Attention, czyli MLA.", "tokens": [50528, 1407, 3912, 642, 2320, 2991, 11, 37415, 7401, 89, 15338, 299, 1229, 1221, 714, 1665, 81, 11312, 13123, 84, 14895, 50, 34015, 691, 17, 11, 43024, 2766, 4207, 4236, 590, 76, 29238, 39, 2056, 7354, 317, 31858, 11, 16591, 376, 11435, 13, 50944], "temperature": 0.0, "avg_logprob": -0.1399430361661044, "compression_ratio": 1.3475177304964538, "no_speech_prob": 0.01276802271604538}, {"id": 23, "seek": 11148, "start": 123.4, "end": 125.76, "text": " Czyli podstawa jest znana i sprawdzona?", "tokens": [50960, 37099, 2497, 372, 10449, 3492, 15397, 2095, 741, 46192, 13383, 30, 51078], "temperature": 0.0, "avg_logprob": -0.1399430361661044, "compression_ratio": 1.3475177304964538, "no_speech_prob": 0.01276802271604538}, {"id": 24, "seek": 11148, "start": 125.84, "end": 134.44, "text": " Tak, te elementy od pocz\u0105tku mia\u0142y zapewni\u0107 wydajne wnioskowanie, ale prawdziwa magia zaczyna si\u0119 w nowo\u015bciach, kt\u00f3re na tej podstawie zbudowano.", "tokens": [51082, 9118, 11, 535, 4478, 88, 3611, 43959, 21290, 6825, 7949, 494, 895, 12757, 25984, 1805, 716, 45368, 2717, 74, 22028, 11, 6775, 41175, 3992, 4151, 2258, 654, 43811, 629, 3244, 261, 586, 44468, 608, 11, 8864, 1667, 12573, 43443, 414, 710, 18281, 305, 3730, 13, 51512], "temperature": 0.0, "avg_logprob": -0.1399430361661044, "compression_ratio": 1.3475177304964538, "no_speech_prob": 0.01276802271604538}, {"id": 25, "seek": 11148, "start": 134.68, "end": 136.0, "text": " No i tu dochodzimy do Sedna.", "tokens": [51524, 883, 741, 2604, 9243, 378, 89, 13189, 360, 31213, 629, 13, 51590], "temperature": 0.0, "avg_logprob": -0.1399430361661044, "compression_ratio": 1.3475177304964538, "no_speech_prob": 0.01276802271604538}, {"id": 26, "seek": 13600, "start": 136.4, "end": 143.56, "text": " W \u015bwiecie, gdzie wszyscy, powiedzmy, u\u017cywaj\u0105 podobnych klock\u00f3w, DeepSync znalaz\u0142 nowe sposoby ich uk\u0142adania.", "tokens": [50384, 343, 40078, 4260, 11, 18922, 44232, 11, 27617, 2226, 11, 34097, 86, 11133, 43024, 9399, 350, 4102, 3901, 11, 14895, 50, 34015, 710, 4660, 921, 1221, 586, 68, 20443, 13944, 1893, 344, 15317, 5609, 13, 50742], "temperature": 0.0, "avg_logprob": -0.14148993925614792, "compression_ratio": 1.3851590106007068, "no_speech_prob": 0.36868470907211304}, {"id": 27, "seek": 13600, "start": 144.0, "end": 147.16, "text": " Jaka jest ta pierwsza przy\u0142omowa innowacja?", "tokens": [50764, 508, 7849, 3492, 1846, 27623, 2394, 6501, 1221, 298, 5528, 294, 3785, 23395, 30, 50922], "temperature": 0.0, "avg_logprob": -0.14148993925614792, "compression_ratio": 1.3851590106007068, "no_speech_prob": 0.36868470907211304}, {"id": 28, "seek": 13600, "start": 147.28, "end": 154.48, "text": " To co\u015b, co nazywaj\u0105 strategi\u0105 r\u00f3wnowa\u017cenia obci\u0105\u017cenia bez dodatkowej funkcji straty, czyli Auxiliary Loss Free.", "tokens": [50928, 1407, 19241, 11, 598, 20151, 27112, 11133, 5464, 11404, 11416, 895, 5528, 48830, 1111, 34381, 48830, 10782, 13886, 33525, 21091, 26476, 19649, 1056, 21398, 11, 16591, 316, 2449, 38550, 441, 772, 11551, 13, 51288], "temperature": 0.0, "avg_logprob": -0.14148993925614792, "compression_ratio": 1.3851590106007068, "no_speech_prob": 0.36868470907211304}, {"id": 29, "seek": 13600, "start": 154.6, "end": 156.0, "text": " Brzmi skomplikowanie.", "tokens": [51294, 1603, 89, 3057, 1110, 298, 564, 1035, 22028, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14148993925614792, "compression_ratio": 1.3851590106007068, "no_speech_prob": 0.36868470907211304}, {"id": 30, "seek": 13600, "start": 156.12, "end": 157.4, "text": " Ale zasada jest prosta.", "tokens": [51370, 9366, 26530, 1538, 3492, 582, 8638, 13, 51434], "temperature": 0.0, "avg_logprob": -0.14148993925614792, "compression_ratio": 1.3851590106007068, "no_speech_prob": 0.36868470907211304}, {"id": 31, "seek": 13600, "start": 157.68, "end": 162.12, "text": " W modelach MOEI mamy wielu ekspert\u00f3w, czyli ma\u0142e sieci neuronowe.", "tokens": [51448, 343, 2316, 608, 19290, 36, 40, 17335, 40437, 30724, 15346, 3901, 11, 16591, 463, 19827, 2804, 537, 34090, 6880, 13, 51670], "temperature": 0.0, "avg_logprob": -0.14148993925614792, "compression_ratio": 1.3851590106007068, "no_speech_prob": 0.36868470907211304}, {"id": 32, "seek": 16212, "start": 162.6, "end": 166.04, "text": " System musi decydowa\u0107, kt\u00f3rego z nich zapyta\u0107 o zdanie.", "tokens": [50388, 8910, 37587, 979, 6655, 11445, 11, 46951, 710, 25570, 14223, 88, 42931, 277, 16221, 7155, 13, 50560], "temperature": 0.0, "avg_logprob": -0.13855135286009157, "compression_ratio": 1.4253246753246753, "no_speech_prob": 0.027397075667977333}, {"id": 33, "seek": 16212, "start": 166.44, "end": 170.32, "text": " Problem w tym, \u017ce model naturalnie mo\u017ce faworyzowa\u0107 kilku najlepszych.", "tokens": [50580, 11676, 261, 8107, 11, 3561, 2316, 3303, 2766, 12034, 283, 1607, 827, 89, 11445, 5128, 5279, 41903, 1878, 28051, 13, 50774], "temperature": 0.0, "avg_logprob": -0.13855135286009157, "compression_ratio": 1.4253246753246753, "no_speech_prob": 0.027397075667977333}, {"id": 34, "seek": 16212, "start": 170.44, "end": 171.68, "text": " Reszt\u0119 zaniedbywa\u0107.", "tokens": [50780, 5015, 2682, 1274, 710, 282, 1091, 2322, 25234, 13, 50842], "temperature": 0.0, "avg_logprob": -0.13855135286009157, "compression_ratio": 1.4253246753246753, "no_speech_prob": 0.027397075667977333}, {"id": 35, "seek": 16212, "start": 172.08, "end": 177.36, "text": " Co prowadzi do tego, \u017ce kilku ekspert\u00f3w jest przepracowanych, a reszta sie\u0107 bezczynnie.", "tokens": [50862, 3066, 36590, 3992, 360, 8627, 11, 3561, 5128, 5279, 30724, 15346, 3901, 3492, 30829, 12080, 23341, 339, 11, 257, 725, 89, 1328, 2804, 2162, 10782, 6522, 77, 2766, 13, 51126], "temperature": 0.0, "avg_logprob": -0.13855135286009157, "compression_ratio": 1.4253246753246753, "no_speech_prob": 0.027397075667977333}, {"id": 36, "seek": 16212, "start": 177.6, "end": 178.48000000000002, "text": " Marnotrawstwo.", "tokens": [51138, 376, 1083, 310, 5131, 372, 6120, 13, 51182], "temperature": 0.0, "avg_logprob": -0.13855135286009157, "compression_ratio": 1.4253246753246753, "no_speech_prob": 0.027397075667977333}, {"id": 37, "seek": 16212, "start": 179.0, "end": 179.68, "text": " W\u0142a\u015bnie.", "tokens": [51208, 343, 5024, 12221, 13, 51242], "temperature": 0.0, "avg_logprob": -0.13855135286009157, "compression_ratio": 1.4253246753246753, "no_speech_prob": 0.027397075667977333}, {"id": 38, "seek": 16212, "start": 180.36, "end": 187.88, "text": " Tradycyjnie rozwi\u0105zuje si\u0119 to dodaj\u0105cy do treningu tak\u0105 matematyczn\u0105 kar\u0119, czyli Auxiliary Loss.", "tokens": [51276, 1765, 880, 42949, 2766, 9544, 18234, 11728, 2884, 3244, 281, 13886, 11133, 1344, 360, 2192, 773, 84, 31069, 3803, 8615, 17466, 13113, 7917, 1274, 11, 16591, 316, 2449, 38550, 441, 772, 13, 51652], "temperature": 0.0, "avg_logprob": -0.13855135286009157, "compression_ratio": 1.4253246753246753, "no_speech_prob": 0.027397075667977333}, {"id": 39, "seek": 16212, "start": 188.24, "end": 192.04000000000002, "text": " Ona zmusza model do bardziej r\u00f3wnomiernego rozdzielania zada\u0144.", "tokens": [51670, 49793, 17020, 301, 2394, 2316, 360, 27209, 11416, 895, 298, 811, 11858, 9544, 28168, 1187, 5609, 710, 1538, 5248, 13, 51860], "temperature": 0.0, "avg_logprob": -0.13855135286009157, "compression_ratio": 1.4253246753246753, "no_speech_prob": 0.027397075667977333}, {"id": 40, "seek": 19212, "start": 192.24, "end": 194.68, "text": " Ale ta kara ma swoj\u0105 cen\u0119, prawda?", "tokens": [50370, 9366, 1846, 29555, 463, 49194, 27900, 1274, 11, 43607, 30, 50492], "temperature": 0.0, "avg_logprob": -0.142890672425966, "compression_ratio": 1.3519736842105263, "no_speech_prob": 0.000786031479947269}, {"id": 41, "seek": 19212, "start": 194.8, "end": 195.44, "text": " Oczywi\u015bcie.", "tokens": [50498, 42980, 13, 50530], "temperature": 0.0, "avg_logprob": -0.142890672425966, "compression_ratio": 1.3519736842105263, "no_speech_prob": 0.000786031479947269}, {"id": 42, "seek": 19212, "start": 195.68, "end": 199.68, "text": " Pogarsza og\u00f3ln\u0105 wydajno\u015b\u0107 modelu, bo zmusza go do kompromis\u00f3w.", "tokens": [50542, 430, 664, 685, 2394, 5360, 15741, 13113, 25984, 1805, 23293, 2316, 84, 11, 748, 17020, 301, 2394, 352, 360, 5207, 28722, 271, 3901, 13, 50742], "temperature": 0.0, "avg_logprob": -0.142890672425966, "compression_ratio": 1.3519736842105263, "no_speech_prob": 0.000786031479947269}, {"id": 43, "seek": 19212, "start": 199.8, "end": 205.56, "text": " To troch\u0119 jakby m\u00f3wi\u0107 gwie\u017adzie dru\u017cyny koszykarskiej, \u017ce musi podawa\u0107 pi\u0142k\u0119 do s\u0142abszych graczy.", "tokens": [50748, 1407, 24926, 28976, 13489, 12757, 290, 8699, 10659, 13096, 38864, 7735, 1634, 19532, 1229, 74, 685, 45145, 11, 3561, 37587, 2497, 10449, 2162, 3895, 1221, 15724, 360, 15116, 455, 45021, 11625, 1229, 13, 51036], "temperature": 0.0, "avg_logprob": -0.142890672425966, "compression_ratio": 1.3519736842105263, "no_speech_prob": 0.000786031479947269}, {"id": 44, "seek": 19212, "start": 206.04, "end": 208.56, "text": " Nawet je\u015bli sam ma czyst\u0105 pozycj\u0119 do rzutu.", "tokens": [51060, 40315, 302, 25630, 3247, 463, 6430, 372, 1611, 49358, 41960, 360, 367, 89, 325, 84, 13, 51186], "temperature": 0.0, "avg_logprob": -0.142890672425966, "compression_ratio": 1.3519736842105263, "no_speech_prob": 0.000786031479947269}, {"id": 45, "seek": 19212, "start": 209.16, "end": 212.16, "text": " Mo\u017ce to i sprawie div\u0119, ale wynik meczu b\u0119dzie gorszy.", "tokens": [51216, 43774, 281, 741, 22734, 414, 3414, 1274, 11, 6775, 31936, 1035, 385, 3689, 84, 10562, 290, 830, 1229, 13, 51366], "temperature": 0.0, "avg_logprob": -0.142890672425966, "compression_ratio": 1.3519736842105263, "no_speech_prob": 0.000786031479947269}, {"id": 46, "seek": 19212, "start": 212.36, "end": 213.48000000000002, "text": " Idealna analogia.", "tokens": [51376, 13090, 304, 629, 16660, 654, 13, 51432], "temperature": 0.0, "avg_logprob": -0.142890672425966, "compression_ratio": 1.3519736842105263, "no_speech_prob": 0.000786031479947269}, {"id": 47, "seek": 19212, "start": 213.96, "end": 218.12, "text": " Dipsy Qv3 jako pierwszy na tak\u0105 skal\u0119 pozbywa si\u0119 tej kary.", "tokens": [51456, 413, 2600, 88, 1249, 85, 18, 17123, 34016, 1667, 31069, 16890, 1274, 21281, 2322, 4151, 3244, 12573, 350, 822, 13, 51664], "temperature": 0.0, "avg_logprob": -0.142890672425966, "compression_ratio": 1.3519736842105263, "no_speech_prob": 0.000786031479947269}, {"id": 48, "seek": 21812, "start": 218.52, "end": 226.68, "text": " Zamiast tego wprowadza mechanizm, kt\u00f3ry dynamicznie, za pomoc\u0105 tak zwanego bias term, dostosowuje preferencj\u0119 rotingu.", "tokens": [50384, 1176, 4526, 525, 8627, 46733, 2394, 4236, 590, 76, 11, 9913, 8546, 89, 2766, 11, 7949, 48962, 1611, 991, 710, 7916, 6308, 12577, 1433, 11, 20568, 329, 305, 13008, 4382, 22660, 11115, 4297, 7050, 13, 50792], "temperature": 0.0, "avg_logprob": -0.14639884233474731, "compression_ratio": 1.41796875, "no_speech_prob": 0.07225412875413895}, {"id": 49, "seek": 21812, "start": 226.84, "end": 229.4, "text": " Czyli nie zmusza, a delikatnie zach\u0119ca.", "tokens": [50800, 37099, 2838, 17020, 301, 2394, 11, 257, 1103, 36300, 2766, 29303, 1274, 496, 13, 50928], "temperature": 0.0, "avg_logprob": -0.14639884233474731, "compression_ratio": 1.41796875, "no_speech_prob": 0.07225412875413895}, {"id": 50, "seek": 21812, "start": 229.6, "end": 229.96, "text": " Tak.", "tokens": [50938, 9118, 13, 50956], "temperature": 0.0, "avg_logprob": -0.14639884233474731, "compression_ratio": 1.41796875, "no_speech_prob": 0.07225412875413895}, {"id": 51, "seek": 21812, "start": 230.48000000000002, "end": 234.4, "text": " I to pozwala ekspertom na znacznie g\u0142\u0119bsz\u0105 specjalizacj\u0119.", "tokens": [50982, 286, 281, 40557, 5159, 30724, 15346, 298, 1667, 15397, 14875, 2766, 18117, 1274, 929, 8925, 46433, 590, 29924, 13, 51178], "temperature": 0.0, "avg_logprob": -0.14639884233474731, "compression_ratio": 1.41796875, "no_speech_prob": 0.07225412875413895}, {"id": 52, "seek": 21812, "start": 234.6, "end": 242.72, "text": " Jak pokazuj\u0105 wykresy w raporcie, niekt\u00f3rzy staj\u0105 si\u0119 wybitni w kodowaniu, inni w matematyce, a jeszcze inni w rozumieniu j\u0119zyka.", "tokens": [51188, 15029, 13010, 921, 13263, 39287, 495, 88, 261, 5099, 284, 4260, 11, 2838, 43073, 13047, 342, 11133, 3244, 4628, 5260, 3722, 261, 350, 378, 305, 25849, 11, 294, 3722, 261, 3803, 8615, 88, 384, 11, 257, 14168, 294, 3722, 261, 48797, 1053, 5951, 42309, 40940, 13, 51594], "temperature": 0.0, "avg_logprob": -0.14639884233474731, "compression_ratio": 1.41796875, "no_speech_prob": 0.07225412875413895}, {"id": 53, "seek": 24272, "start": 243.2, "end": 249.64, "text": " OK. Czyli zamiast armii jednakowo wyszkolonych \u017co\u0142nierzy, mamy zesp\u00f3\u0142 wyspecjalizowanych komandos\u00f3w.", "tokens": [50388, 2264, 13, 37099, 710, 4526, 525, 594, 3057, 72, 25897, 19941, 261, 20589, 36620, 2526, 339, 19625, 78, 1221, 19165, 1229, 11, 17335, 710, 13361, 16181, 27062, 494, 66, 22600, 590, 23341, 339, 5207, 474, 329, 3901, 13, 50710], "temperature": 0.0, "avg_logprob": -0.1455753001760929, "compression_ratio": 1.3493150684931507, "no_speech_prob": 0.04102621227502823}, {"id": 54, "seek": 24272, "start": 250.2, "end": 255.12, "text": " To za\u0142atwia kwestie jako\u015bci my\u015blenia, ale w raporcie jest te\u017c du\u017co o szybko\u015bci.", "tokens": [50738, 1407, 7949, 1221, 267, 86, 654, 42035, 414, 17123, 6199, 48633, 6698, 654, 11, 6775, 261, 5099, 284, 4260, 3492, 9516, 26673, 277, 36456, 4093, 6199, 13, 50984], "temperature": 0.0, "avg_logprob": -0.1455753001760929, "compression_ratio": 1.3493150684931507, "no_speech_prob": 0.04102621227502823}, {"id": 55, "seek": 24272, "start": 255.56, "end": 256.64, "text": " Jak do tego podeszli?", "tokens": [51006, 15029, 360, 8627, 2497, 10430, 2081, 30, 51060], "temperature": 0.0, "avg_logprob": -0.1455753001760929, "compression_ratio": 1.3493150684931507, "no_speech_prob": 0.04102621227502823}, {"id": 56, "seek": 24272, "start": 256.96, "end": 259.92, "text": " I tu dochodzimy do drugiej kluczowej innowacji.", "tokens": [51076, 286, 2604, 9243, 378, 89, 13189, 360, 47373, 9671, 1311, 89, 21091, 294, 3785, 13152, 13, 51224], "temperature": 0.0, "avg_logprob": -0.1455753001760929, "compression_ratio": 1.3493150684931507, "no_speech_prob": 0.04102621227502823}, {"id": 57, "seek": 24272, "start": 260.16, "end": 263.32, "text": " Multi-Token Prediction, w skr\u00f3cie MTP.", "tokens": [51236, 29238, 12, 51, 8406, 32969, 4105, 11, 261, 1110, 11721, 4260, 376, 16804, 13, 51394], "temperature": 0.0, "avg_logprob": -0.1455753001760929, "compression_ratio": 1.3493150684931507, "no_speech_prob": 0.04102621227502823}, {"id": 58, "seek": 24272, "start": 264.48, "end": 270.6, "text": " Standardowo model j\u0119zykowy podczas treningu uczy si\u0119 przewidziwa\u0107 jedno nast\u0119pne s\u0142owo.", "tokens": [51452, 21298, 19941, 2316, 49055, 74, 10089, 2497, 30989, 2192, 773, 84, 344, 6522, 3244, 39758, 327, 3992, 25234, 5232, 1771, 39662, 716, 15116, 19941, 13, 51758], "temperature": 0.0, "avg_logprob": -0.1455753001760929, "compression_ratio": 1.3493150684931507, "no_speech_prob": 0.04102621227502823}, {"id": 59, "seek": 27060, "start": 271.08000000000004, "end": 276.24, "text": " Deep-Sig V3 uczy si\u0119 przewidywa\u0107 kilka kolejnych, w tym przypadku dwa.", "tokens": [50388, 14895, 12, 50, 328, 691, 18, 344, 6522, 3244, 39758, 38836, 25234, 36466, 23749, 9399, 11, 261, 8107, 41955, 35045, 13, 50646], "temperature": 0.0, "avg_logprob": -0.15058796222393328, "compression_ratio": 1.4130434782608696, "no_speech_prob": 0.007116564549505711}, {"id": 60, "seek": 27060, "start": 276.6, "end": 282.16, "text": " Czyli nie tylko my\u015bli jaki jest nast\u0119pny krok, ale jakie s\u0105 dwa nast\u0119pne kroki.", "tokens": [50664, 37099, 2838, 13219, 452, 15350, 24492, 3492, 39662, 1634, 350, 31621, 11, 6775, 22124, 9015, 35045, 39662, 716, 45909, 2984, 13, 50942], "temperature": 0.0, "avg_logprob": -0.15058796222393328, "compression_ratio": 1.4130434782608696, "no_speech_prob": 0.007116564549505711}, {"id": 61, "seek": 27060, "start": 282.72, "end": 285.52000000000004, "text": " To brzmi jakby mia\u0142o znacznie zwi\u0119kszy\u0107 obci\u0105\u017cenie.", "tokens": [50970, 1407, 738, 89, 3057, 28976, 21290, 5249, 15397, 14875, 2766, 11873, 5034, 1694, 27150, 1111, 34381, 41118, 13, 51110], "temperature": 0.0, "avg_logprob": -0.15058796222393328, "compression_ratio": 1.4130434782608696, "no_speech_prob": 0.007116564549505711}, {"id": 62, "seek": 27060, "start": 285.84000000000003, "end": 292.6, "text": " Intuicyjnie tak, ale w praktyce to jakby to uj\u0105\u0107 za g\u0119szcza sygna\u0142y treningowe.", "tokens": [51126, 5681, 84, 2632, 73, 2766, 991, 11, 6775, 261, 3206, 74, 874, 384, 281, 28976, 281, 344, 8555, 2162, 7949, 290, 1274, 15453, 41524, 943, 70, 629, 6825, 2192, 773, 6880, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15058796222393328, "compression_ratio": 1.4130434782608696, "no_speech_prob": 0.007116564549505711}, {"id": 63, "seek": 27060, "start": 293.28000000000003, "end": 299.40000000000003, "text": " Za jednym zamachem model dostaje wi\u0119cej informacji zwrotnej, co poprawia wydajno\u015b\u0107 nauki,", "tokens": [51498, 31440, 5232, 12996, 19876, 608, 443, 2316, 20568, 11153, 26004, 1356, 13152, 49111, 310, 11794, 11, 598, 1665, 5131, 654, 25984, 1805, 23293, 35616, 2984, 11, 51804], "temperature": 0.0, "avg_logprob": -0.15058796222393328, "compression_ratio": 1.4130434782608696, "no_speech_prob": 0.007116564549505711}, {"id": 64, "seek": 29940, "start": 299.67999999999995, "end": 302.12, "text": " a jest te\u017c genialny efekt uboczny.", "tokens": [50378, 257, 3492, 9516, 48228, 1634, 31482, 8192, 26709, 905, 89, 1634, 13, 50500], "temperature": 0.0, "avg_logprob": -0.14735770519868827, "compression_ratio": 1.4176829268292683, "no_speech_prob": 0.004009610041975975}, {"id": 65, "seek": 29940, "start": 302.32, "end": 302.84, "text": " Jaki?", "tokens": [50510, 508, 7421, 30, 50536], "temperature": 0.0, "avg_logprob": -0.14735770519868827, "compression_ratio": 1.4176829268292683, "no_speech_prob": 0.004009610041975975}, {"id": 66, "seek": 29940, "start": 303.0, "end": 308.2, "text": " Te dodatkowe przewidywania mo\u017cna wykorzysta\u0107 do drastycznego przyspieszenia generowania odpowiedzi.", "tokens": [50544, 1989, 13886, 33525, 6880, 39758, 38836, 86, 5609, 17790, 43606, 49590, 2162, 360, 1224, 9820, 3689, 11858, 6541, 749, 79, 530, 14320, 1337, 21308, 36574, 3992, 13, 50804], "temperature": 0.0, "avg_logprob": -0.14735770519868827, "compression_ratio": 1.4176829268292683, "no_speech_prob": 0.004009610041975975}, {"id": 67, "seek": 29940, "start": 308.52, "end": 310.88, "text": " W technice zwanej Speculative Decoding.", "tokens": [50820, 343, 1537, 573, 11873, 1929, 73, 3550, 2444, 1166, 12427, 8616, 13, 50938], "temperature": 0.0, "avg_logprob": -0.14735770519868827, "compression_ratio": 1.4176829268292683, "no_speech_prob": 0.004009610041975975}, {"id": 68, "seek": 29940, "start": 311.23999999999995, "end": 315.23999999999995, "text": " Raport podaje, \u017ce daje to a\u017c jeden przecinek osiemkrotny wzrost szybko\u015bci.", "tokens": [50956, 16184, 477, 2497, 11153, 11, 3561, 1120, 2884, 281, 48134, 12906, 8325, 66, 48421, 3003, 4907, 74, 10536, 1634, 24809, 27494, 36456, 4093, 6199, 13, 51156], "temperature": 0.0, "avg_logprob": -0.14735770519868827, "compression_ratio": 1.4176829268292683, "no_speech_prob": 0.004009610041975975}, {"id": 69, "seek": 29940, "start": 315.44, "end": 316.52, "text": " To jest ju\u017c odczuwalne.", "tokens": [51166, 1407, 3492, 10678, 3611, 3689, 84, 29530, 716, 13, 51220], "temperature": 0.0, "avg_logprob": -0.14735770519868827, "compression_ratio": 1.4176829268292683, "no_speech_prob": 0.004009610041975975}, {"id": 70, "seek": 29940, "start": 316.71999999999997, "end": 323.03999999999996, "text": " W porz\u0105dku, czyli mamy sprytniejsz\u0105, bardziej wyspecjalizowan\u0105 architektur\u0119 i model, kt\u00f3ry my\u015bli kilka krok\u00f3w do przodu.", "tokens": [51230, 343, 1515, 23876, 5279, 11, 16591, 17335, 637, 627, 83, 30295, 8925, 11, 27209, 27062, 494, 66, 22600, 590, 37345, 1611, 3912, 642, 2320, 374, 1274, 741, 2316, 11, 9913, 452, 15350, 36466, 45909, 23849, 360, 6541, 34873, 13, 51546], "temperature": 0.0, "avg_logprob": -0.14735770519868827, "compression_ratio": 1.4176829268292683, "no_speech_prob": 0.004009610041975975}, {"id": 71, "seek": 29940, "start": 323.56, "end": 326.03999999999996, "text": " Ale to wci\u0105\u017c nie wyja\u015bnia najwi\u0119kszej zagadki.", "tokens": [51572, 9366, 281, 261, 537, 27242, 2838, 4628, 2938, 1788, 12679, 48636, 1694, 16920, 27001, 345, 2984, 13, 51696], "temperature": 0.0, "avg_logprob": -0.14735770519868827, "compression_ratio": 1.4176829268292683, "no_speech_prob": 0.004009610041975975}, {"id": 72, "seek": 32604, "start": 326.52000000000004, "end": 327.24, "text": " Koszt\u00f3w.", "tokens": [50388, 36909, 2682, 3901, 13, 50424], "temperature": 0.0, "avg_logprob": -0.1796768355543596, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.07474030554294586}, {"id": 73, "seek": 32604, "start": 327.44, "end": 334.12, "text": " Podali, \u017ce pe\u0142ny trening kosztowa\u0142 zaledwie dwa przecinek 788 miliona godziny na GPU H800.", "tokens": [50434, 12646, 5103, 11, 3561, 43205, 1634, 2192, 773, 19532, 2682, 30105, 710, 5573, 8699, 35045, 8325, 66, 48421, 1614, 16919, 1962, 21758, 3044, 89, 3519, 1667, 18407, 389, 14423, 13, 50768], "temperature": 0.0, "avg_logprob": -0.1796768355543596, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.07474030554294586}, {"id": 74, "seek": 32604, "start": 334.32, "end": 338.16, "text": " Co przek\u0142ada si\u0119 na oko\u0142o pi\u0119\u0107 przecinek sze\u015b\u0107 miliona dolar\u00f3w?", "tokens": [50778, 3066, 29785, 46217, 3244, 1667, 45730, 5249, 32677, 2162, 8325, 66, 48421, 262, 1381, 7753, 1962, 21758, 360, 2200, 3901, 30, 50970], "temperature": 0.0, "avg_logprob": -0.1796768355543596, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.07474030554294586}, {"id": 75, "seek": 32604, "start": 338.36, "end": 339.24, "text": " To s\u0105 pieni\u0105dze.", "tokens": [50980, 1407, 9015, 26274, 11404, 67, 1381, 13, 51024], "temperature": 0.0, "avg_logprob": -0.1796768355543596, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.07474030554294586}, {"id": 76, "seek": 32604, "start": 339.44, "end": 342.04, "text": " No niewyobra\u017calnie ma\u0142e jak na model tej klasy.", "tokens": [51034, 883, 43622, 88, 24393, 1427, 304, 2766, 463, 19827, 4207, 1667, 2316, 12573, 9671, 5871, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1796768355543596, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.07474030554294586}, {"id": 77, "seek": 32604, "start": 342.24, "end": 343.44, "text": " Jak to jest w og\u00f3le mo\u017cliwe?", "tokens": [51174, 15029, 281, 3492, 261, 29229, 30854, 826, 30, 51234], "temperature": 0.0, "avg_logprob": -0.1796768355543596, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.07474030554294586}, {"id": 78, "seek": 32604, "start": 343.64000000000004, "end": 347.20000000000005, "text": " To jest absolutnie centralny punkt tej pracy.", "tokens": [51244, 1407, 3492, 18757, 2766, 5777, 1634, 39561, 12573, 35591, 13, 51422], "temperature": 0.0, "avg_logprob": -0.1796768355543596, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.07474030554294586}, {"id": 79, "seek": 32604, "start": 347.40000000000003, "end": 351.64000000000004, "text": " Osi\u0105gn\u0119li to dzi\u0119ki trzem filarom ekstremalnej optymalacji.", "tokens": [51432, 422, 7691, 1611, 4568, 1274, 2081, 281, 45003, 504, 24313, 1387, 289, 298, 13359, 372, 2579, 304, 11794, 2427, 4199, 304, 13152, 13, 51644], "temperature": 0.0, "avg_logprob": -0.1796768355543596, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.07474030554294586}, {"id": 80, "seek": 35164, "start": 351.88, "end": 356.0, "text": " Filar pierwszy to trening w precyzji FP8.", "tokens": [50376, 7905, 289, 34016, 281, 2192, 773, 261, 659, 1344, 89, 4013, 36655, 23, 13, 50582], "temperature": 0.0, "avg_logprob": -0.13673696656158005, "compression_ratio": 1.3790613718411553, "no_speech_prob": 0.023439859971404076}, {"id": 81, "seek": 35164, "start": 356.2, "end": 357.76, "text": " Chwila FP8.", "tokens": [50592, 761, 86, 7371, 36655, 23, 13, 50670], "temperature": 0.0, "avg_logprob": -0.13673696656158005, "compression_ratio": 1.3790613718411553, "no_speech_prob": 0.023439859971404076}, {"id": 82, "seek": 35164, "start": 357.96, "end": 362.28, "text": " Wi\u0119kszo\u015b\u0107 os\u00f3b, kt\u00f3ra si\u0119 tym interesuje, kojarzy BF16 jako standard.", "tokens": [50680, 30127, 1694, 4765, 7753, 32089, 11, 19456, 3244, 8107, 20157, 13008, 11, 8384, 10150, 1229, 363, 37, 6866, 17123, 3832, 13, 50896], "temperature": 0.0, "avg_logprob": -0.13673696656158005, "compression_ratio": 1.3790613718411553, "no_speech_prob": 0.023439859971404076}, {"id": 83, "seek": 35164, "start": 362.47999999999996, "end": 364.47999999999996, "text": " O jakiej skali oszcz\u0119dno\u015bci tu m\u00f3wimy?", "tokens": [50906, 422, 4207, 7764, 1110, 5103, 3003, 43771, 6298, 16438, 2604, 13489, 13189, 30, 51006], "temperature": 0.0, "avg_logprob": -0.13673696656158005, "compression_ratio": 1.3790613718411553, "no_speech_prob": 0.023439859971404076}, {"id": 84, "seek": 35164, "start": 364.68, "end": 367.8, "text": " To jest teoretycznie dwa razy szybciej i dwa razy mniej pami\u0119ci?", "tokens": [51016, 1407, 3492, 535, 418, 45586, 35045, 9639, 88, 36456, 4260, 73, 741, 35045, 9639, 88, 39513, 31088, 537, 30, 51172], "temperature": 0.0, "avg_logprob": -0.13673696656158005, "compression_ratio": 1.3790613718411553, "no_speech_prob": 0.023439859971404076}, {"id": 85, "seek": 35164, "start": 368.0, "end": 369.15999999999997, "text": " Dok\u0142adnie tak.", "tokens": [51182, 29768, 10358, 2766, 991, 13, 51240], "temperature": 0.0, "avg_logprob": -0.13673696656158005, "compression_ratio": 1.3790613718411553, "no_speech_prob": 0.023439859971404076}, {"id": 86, "seek": 35164, "start": 369.36, "end": 371.15999999999997, "text": " Ale to nie jest takie proste.", "tokens": [51250, 9366, 281, 2838, 3492, 15963, 10293, 68, 13, 51340], "temperature": 0.0, "avg_logprob": -0.13673696656158005, "compression_ratio": 1.3790613718411553, "no_speech_prob": 0.023439859971404076}, {"id": 87, "seek": 35164, "start": 371.36, "end": 378.15999999999997, "text": " Przej\u015bcie na o\u015bmiobitow\u0105 precyzj\u0119 FP8 zwi\u0105\u017cy si\u0119 z gigantycznym ryzykiem utraty informacji.", "tokens": [51350, 2114, 16920, 9815, 1667, 277, 1788, 3057, 996, 270, 30297, 659, 1344, 89, 11115, 36655, 23, 710, 18234, 7735, 3244, 710, 8741, 394, 17466, 12996, 20791, 1229, 26116, 2839, 4481, 88, 1356, 13152, 13, 51690], "temperature": 0.0, "avg_logprob": -0.13673696656158005, "compression_ratio": 1.3790613718411553, "no_speech_prob": 0.023439859971404076}, {"id": 88, "seek": 37816, "start": 378.36, "end": 380.92, "text": " To jak pr\u00f3ba narysowania", "tokens": [50374, 1407, 4207, 8565, 4231, 6714, 749, 21308, 50502], "temperature": 0.0, "avg_logprob": -0.14657245408620265, "compression_ratio": 1.3944636678200693, "no_speech_prob": 0.011623709462583065}, {"id": 89, "seek": 37816, "start": 381.12, "end": 384.96000000000004, "text": " fotorealistycznego portretu bardzo grubym o\u0142\u00f3wkiem.", "tokens": [50512, 15418, 418, 304, 468, 17466, 11858, 2436, 1505, 84, 9034, 677, 836, 4199, 277, 1221, 3901, 26116, 13, 50704], "temperature": 0.0, "avg_logprob": -0.14657245408620265, "compression_ratio": 1.3944636678200693, "no_speech_prob": 0.011623709462583065}, {"id": 90, "seek": 37816, "start": 385.16, "end": 386.48, "text": " \u0141atwo zgubi\u0107 detale.", "tokens": [50714, 36901, 267, 6120, 40948, 836, 12757, 1141, 1220, 13, 50780], "temperature": 0.0, "avg_logprob": -0.14657245408620265, "compression_ratio": 1.3944636678200693, "no_speech_prob": 0.011623709462583065}, {"id": 91, "seek": 37816, "start": 386.68, "end": 392.12, "text": " Wi\u0119c jakiego artystycznego triku u\u017cyli, \u017ceby ten gruby o\u0142\u00f3wek rysowa\u0142 z precyzj\u0105 cienkopisu?", "tokens": [50790, 32508, 4207, 12200, 594, 874, 372, 17466, 11858, 1376, 5279, 34097, 2081, 11, 11316, 2064, 677, 836, 88, 277, 1221, 812, 826, 74, 367, 749, 30105, 710, 659, 1344, 89, 8555, 269, 1053, 43855, 25871, 30, 51062], "temperature": 0.0, "avg_logprob": -0.14657245408620265, "compression_ratio": 1.3944636678200693, "no_speech_prob": 0.011623709462583065}, {"id": 92, "seek": 37816, "start": 392.32000000000005, "end": 396.32000000000005, "text": " Wprowadzili co\u015b, co nazywaj\u0105 fine grained quantization.", "tokens": [51072, 343, 35019, 89, 2312, 19241, 11, 598, 20151, 27112, 11133, 2489, 1295, 2001, 4426, 2144, 13, 51272], "temperature": 0.0, "avg_logprob": -0.14657245408620265, "compression_ratio": 1.3944636678200693, "no_speech_prob": 0.011623709462583065}, {"id": 93, "seek": 37816, "start": 396.52000000000004, "end": 402.16, "text": " Zamiast traktowa\u0107 ca\u0142\u0105 wielk\u0105 macie\u017c wak jako jeden byt i skalowa\u0107 j\u0105 do FP8.", "tokens": [51282, 1176, 4526, 525, 944, 2320, 11445, 1335, 15926, 20570, 26304, 7912, 414, 1427, 261, 514, 17123, 12906, 538, 83, 741, 16890, 11445, 35692, 360, 36655, 23, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14657245408620265, "compression_ratio": 1.3944636678200693, "no_speech_prob": 0.011623709462583065}, {"id": 94, "seek": 37816, "start": 402.36, "end": 404.64000000000004, "text": " Oni dziel\u0105 j\u0105 na ma\u0142e fragmenty.", "tokens": [51574, 1282, 72, 9758, 1187, 1611, 35692, 1667, 463, 19827, 26424, 88, 13, 51688], "temperature": 0.0, "avg_logprob": -0.14657245408620265, "compression_ratio": 1.3944636678200693, "no_speech_prob": 0.011623709462583065}, {"id": 95, "seek": 37816, "start": 404.84000000000003, "end": 405.72, "text": " Na mniejsze kawa\u0142ki?", "tokens": [51698, 6056, 275, 44258, 350, 10449, 1221, 2984, 30, 51742], "temperature": 0.0, "avg_logprob": -0.14657245408620265, "compression_ratio": 1.3944636678200693, "no_speech_prob": 0.011623709462583065}, {"id": 96, "seek": 40572, "start": 405.8, "end": 412.16, "text": " Tak, na bloki o wymiarach 128 na 128 i skaluj\u0105 ka\u017cdy z nich osobno.", "tokens": [50368, 9118, 11, 1667, 888, 17056, 277, 4628, 3057, 289, 608, 29810, 1667, 29810, 741, 16890, 13263, 31615, 710, 25570, 41518, 1771, 13, 50686], "temperature": 0.0, "avg_logprob": -0.1387631236643031, "compression_ratio": 1.4456521739130435, "no_speech_prob": 0.003613624721765518}, {"id": 97, "seek": 40572, "start": 412.36, "end": 420.12, "text": " To pozwala im znacznie lepiej zarz\u0105dza\u0107 warto\u015bciami odstaj\u0105cymi, tak zwanymi outliersy, kt\u00f3re s\u0105 z mor\u0105 treningu w niskiej precyzji.", "tokens": [50696, 1407, 40557, 5159, 566, 15397, 14875, 2766, 476, 39699, 22675, 23876, 35873, 31830, 6199, 4526, 3611, 372, 11133, 1344, 3057, 11, 991, 11873, 1325, 3057, 484, 23646, 88, 11, 8864, 9015, 710, 1896, 1611, 2192, 773, 84, 261, 297, 7797, 7764, 659, 1344, 89, 4013, 13, 51084], "temperature": 0.0, "avg_logprob": -0.1387631236643031, "compression_ratio": 1.4456521739130435, "no_speech_prob": 0.003613624721765518}, {"id": 98, "seek": 40572, "start": 420.32000000000005, "end": 428.52000000000004, "text": " Aha, czyli zamiast u\u017cywa\u0107 jednej du\u017cej miarki dla ca\u0142ego worka m\u0105ki u\u017cyli precyzyjnej \u0142y\u017ceczki do ka\u017cdej szklanki.", "tokens": [51094, 27448, 11, 16591, 710, 4526, 525, 34097, 25234, 5232, 11794, 1581, 38493, 2752, 809, 72, 12285, 35224, 6308, 589, 64, 275, 1611, 2984, 34097, 2081, 659, 1344, 1229, 73, 11794, 220, 6825, 2875, 3689, 2984, 360, 21912, 1479, 73, 7870, 7837, 27203, 13, 51504], "temperature": 0.0, "avg_logprob": -0.1387631236643031, "compression_ratio": 1.4456521739130435, "no_speech_prob": 0.003613624721765518}, {"id": 99, "seek": 40572, "start": 428.72, "end": 431.76000000000005, "text": " To pozwala zachowa\u0107 dok\u0142adno\u015b\u0107 tam, gdzie jest ona krytyczna.", "tokens": [51514, 1407, 40557, 5159, 29303, 11445, 45864, 23293, 7677, 11, 18922, 3492, 20325, 34847, 874, 3689, 629, 13, 51666], "temperature": 0.0, "avg_logprob": -0.1387631236643031, "compression_ratio": 1.4456521739130435, "no_speech_prob": 0.003613624721765518}, {"id": 100, "seek": 43176, "start": 432.0, "end": 434.32, "text": " W\u0142a\u015bnie, ale poszli jeszcze g\u0142\u0119biej.", "tokens": [50376, 343, 5024, 12221, 11, 6775, 1366, 89, 2081, 14168, 18117, 1274, 7392, 73, 13, 50492], "temperature": 0.0, "avg_logprob": -0.13265156908100154, "compression_ratio": 1.3885135135135136, "no_speech_prob": 0.10357385873794556}, {"id": 101, "seek": 43176, "start": 434.52, "end": 440.4, "text": " Zmodyfikowali spos\u00f3b, w jaki rdzenia tensor cores w procesorach NVIDI akumuluj\u0105 wyniki.", "tokens": [50502, 1176, 76, 843, 31230, 305, 5103, 22904, 11, 261, 24492, 367, 67, 14320, 40863, 24826, 261, 17565, 284, 608, 426, 3958, 40, 9308, 449, 425, 13263, 31936, 9850, 13, 50796], "temperature": 0.0, "avg_logprob": -0.13265156908100154, "compression_ratio": 1.3885135135135136, "no_speech_prob": 0.10357385873794556}, {"id": 102, "seek": 43176, "start": 440.59999999999997, "end": 445.28, "text": " Przenie\u015bli cz\u0119\u015b\u0107 tych oblicze\u0144 do bardziej precyzyjnych rdzeni CUDA cores.", "tokens": [50806, 2114, 16778, 15350, 47149, 15180, 1111, 1050, 49689, 360, 27209, 659, 1344, 1229, 73, 9399, 367, 67, 42124, 29777, 7509, 24826, 13, 51040], "temperature": 0.0, "avg_logprob": -0.13265156908100154, "compression_ratio": 1.3885135135135136, "no_speech_prob": 0.10357385873794556}, {"id": 103, "seek": 43176, "start": 445.48, "end": 448.0, "text": " Czyli zeszli ju\u017c na poziom samego sprz\u0119tu?", "tokens": [51050, 37099, 710, 10430, 2081, 10678, 1667, 38503, 298, 912, 1571, 6103, 11052, 9179, 30, 51176], "temperature": 0.0, "avg_logprob": -0.13265156908100154, "compression_ratio": 1.3885135135135136, "no_speech_prob": 0.10357385873794556}, {"id": 104, "seek": 43176, "start": 448.2, "end": 450.76, "text": " To pokazuje, \u017ce nie optymalizowali tylko kodu.", "tokens": [51186, 1407, 13010, 43317, 11, 3561, 2838, 2427, 4199, 304, 590, 305, 5103, 13219, 350, 34873, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13265156908100154, "compression_ratio": 1.3885135135135136, "no_speech_prob": 0.10357385873794556}, {"id": 105, "seek": 43176, "start": 450.96, "end": 455.56, "text": " Weszli w interakcj\u0119 z architektur\u0105 sprz\u0119tu, \u017ceby wycisn\u0105\u0107 z niego ostatnie soki.", "tokens": [51324, 343, 10430, 2081, 261, 728, 514, 41960, 710, 3912, 642, 2320, 374, 1611, 6103, 11052, 9179, 11, 11316, 4628, 26720, 13113, 2162, 710, 49615, 32686, 2766, 370, 2984, 13, 51554], "temperature": 0.0, "avg_logprob": -0.13265156908100154, "compression_ratio": 1.3885135135135136, "no_speech_prob": 0.10357385873794556}, {"id": 106, "seek": 43176, "start": 455.76, "end": 457.03999999999996, "text": " To jest niesamowite.", "tokens": [51564, 1407, 3492, 48100, 335, 305, 642, 13, 51628], "temperature": 0.0, "avg_logprob": -0.13265156908100154, "compression_ratio": 1.3885135135135136, "no_speech_prob": 0.10357385873794556}, {"id": 107, "seek": 45704, "start": 457.28000000000003, "end": 459.16, "text": " W\u0142a\u015bciwie odwracaj\u0105 paradygmat.", "tokens": [50376, 343, 5024, 6199, 8699, 3611, 7449, 326, 11133, 13480, 18103, 15677, 13, 50470], "temperature": 0.0, "avg_logprob": -0.15705232059254365, "compression_ratio": 1.3682432432432432, "no_speech_prob": 0.04730736091732979}, {"id": 108, "seek": 45704, "start": 459.36, "end": 464.12, "text": " M\u00f3wi\u0105, \u017ce ich kod jest tak zoptymalizowany, \u017ce teraz to sprz\u0119t musi si\u0119 dostosowa\u0107.", "tokens": [50480, 376, 3901, 11404, 11, 3561, 1893, 350, 378, 3492, 991, 710, 5747, 4199, 304, 590, 23341, 11, 3561, 16854, 281, 6103, 11052, 83, 37587, 3244, 20568, 329, 11445, 13, 50718], "temperature": 0.0, "avg_logprob": -0.15705232059254365, "compression_ratio": 1.3682432432432432, "no_speech_prob": 0.04730736091732979}, {"id": 109, "seek": 45704, "start": 464.32, "end": 466.04, "text": " Dobrze, a co z drugim filarem?", "tokens": [50728, 29679, 13503, 11, 257, 598, 710, 4110, 332, 1387, 19183, 30, 50814], "temperature": 0.0, "avg_logprob": -0.15705232059254365, "compression_ratio": 1.3682432432432432, "no_speech_prob": 0.04730736091732979}, {"id": 110, "seek": 45704, "start": 466.24, "end": 470.32000000000005, "text": " Filar drugi to ich autorski algorytm Dual Pipe.", "tokens": [50824, 7905, 289, 4110, 72, 281, 1893, 1476, 830, 2984, 3501, 827, 83, 76, 37625, 430, 6527, 13, 51028], "temperature": 0.0, "avg_logprob": -0.15705232059254365, "compression_ratio": 1.3682432432432432, "no_speech_prob": 0.04730736091732979}, {"id": 111, "seek": 45704, "start": 470.52000000000004, "end": 473.92, "text": " On rozwi\u0105zuje najwi\u0119ksze w\u0105skie gard\u0142o w treningu modeli MOE.", "tokens": [51038, 1282, 9544, 18234, 11728, 2884, 48636, 1694, 1381, 261, 1611, 5161, 414, 5628, 5249, 261, 2192, 773, 84, 2316, 72, 19290, 36, 13, 51208], "temperature": 0.0, "avg_logprob": -0.15705232059254365, "compression_ratio": 1.3682432432432432, "no_speech_prob": 0.04730736091732979}, {"id": 112, "seek": 45704, "start": 474.12, "end": 475.36, "text": " Czyli komunikacje.", "tokens": [51218, 37099, 45359, 1035, 29293, 13, 51280], "temperature": 0.0, "avg_logprob": -0.15705232059254365, "compression_ratio": 1.3682432432432432, "no_speech_prob": 0.04730736091732979}, {"id": 113, "seek": 45704, "start": 475.56, "end": 478.28000000000003, "text": " Tak, komunikacje mi\u0119dzy w\u0119z\u0142ami w klasstrze.", "tokens": [51290, 9118, 11, 45359, 1035, 29293, 33964, 261, 1274, 89, 1221, 4526, 261, 350, 7743, 9733, 1381, 13, 51426], "temperature": 0.0, "avg_logprob": -0.15705232059254365, "compression_ratio": 1.3682432432432432, "no_speech_prob": 0.04730736091732979}, {"id": 114, "seek": 45704, "start": 478.48, "end": 482.68, "text": " Gdy masz tysi\u0105ce GPU wymiana danych staje si\u0119 ogromnym problemem.", "tokens": [51436, 460, 3173, 2300, 89, 38156, 11404, 384, 18407, 29764, 8497, 274, 34644, 342, 11153, 3244, 34416, 298, 12996, 1154, 443, 13, 51646], "temperature": 0.0, "avg_logprob": -0.15705232059254365, "compression_ratio": 1.3682432432432432, "no_speech_prob": 0.04730736091732979}, {"id": 115, "seek": 48268, "start": 482.88, "end": 488.36, "text": " Dual Pipe to niezwykle inteligentny harmonogram, kt\u00f3ry przeplata obliczenia z komunikacj\u0105.", "tokens": [50374, 37625, 430, 6527, 281, 33511, 9726, 14677, 24777, 25002, 1634, 14750, 12820, 11, 9913, 8325, 564, 3274, 1111, 1050, 14320, 710, 45359, 1035, 326, 8555, 13, 50648], "temperature": 0.0, "avg_logprob": -0.13752052881946303, "compression_ratio": 1.4511041009463723, "no_speech_prob": 0.00894654169678688}, {"id": 116, "seek": 48268, "start": 488.56, "end": 494.12, "text": " Czyli to jest troch\u0119 jak kuchnia w restauracji z gwiazg\u0105 Michelin, gdzie jeden kuchar zaczyna kroi\u0107", "tokens": [50658, 37099, 281, 3492, 24926, 4207, 350, 625, 12679, 261, 4793, 13152, 710, 29255, 654, 89, 70, 1611, 23709, 259, 11, 18922, 12906, 350, 31084, 43811, 629, 45909, 12757, 50936], "temperature": 0.0, "avg_logprob": -0.13752052881946303, "compression_ratio": 1.4511041009463723, "no_speech_prob": 0.00894654169678688}, {"id": 117, "seek": 48268, "start": 494.32, "end": 498.8, "text": " warzywa podczas gdy drugi ju\u017c sma\u017cy mi\u0119so, \u017ceby nie by\u0142o ani sekundy przestoju.", "tokens": [50946, 1516, 1229, 4151, 2497, 30989, 28405, 4110, 72, 10678, 899, 64, 7735, 2752, 1274, 539, 11, 11316, 2838, 14811, 40477, 17215, 49996, 6541, 18465, 8954, 13, 51170], "temperature": 0.0, "avg_logprob": -0.13752052881946303, "compression_ratio": 1.4511041009463723, "no_speech_prob": 0.00894654169678688}, {"id": 118, "seek": 48268, "start": 499.0, "end": 499.92, "text": " Dok\u0142adnie tak.", "tokens": [51180, 29768, 10358, 2766, 991, 13, 51226], "temperature": 0.0, "avg_logprob": -0.13752052881946303, "compression_ratio": 1.4511041009463723, "no_speech_prob": 0.00894654169678688}, {"id": 119, "seek": 48268, "start": 500.12, "end": 503.64, "text": " Chodzi o to, \u017ceby GPU nigdy nie czeka\u0142o bezczynnie na dane.", "tokens": [51236, 761, 14543, 277, 281, 11, 11316, 18407, 26996, 3173, 2838, 6472, 36361, 5249, 10782, 6522, 77, 2766, 1667, 49206, 13, 51412], "temperature": 0.0, "avg_logprob": -0.13752052881946303, "compression_ratio": 1.4511041009463723, "no_speech_prob": 0.00894654169678688}, {"id": 120, "seek": 48268, "start": 503.84000000000003, "end": 509.52, "text": " W momencie, gdy jeden zestaw danych jest wysy\u0142any przez sie\u0107, procesor ju\u017c pracuje nad nast\u0119pnym.", "tokens": [51422, 343, 40883, 11, 28405, 12906, 37889, 1607, 274, 34644, 3492, 27062, 88, 1221, 1325, 14064, 2804, 2162, 11, 17565, 284, 10678, 22404, 13008, 12617, 39662, 12996, 13, 51706], "temperature": 0.0, "avg_logprob": -0.13752052881946303, "compression_ratio": 1.4511041009463723, "no_speech_prob": 0.00894654169678688}, {"id": 121, "seek": 50952, "start": 509.68, "end": 513.52, "text": " Dual Pipe sprawia, \u017ce komunikacja jest niemal ca\u0142kowicie ukryta.", "tokens": [50372, 37625, 430, 6527, 22734, 654, 11, 3561, 45359, 1035, 23395, 3492, 2838, 5579, 35224, 74, 305, 28434, 26769, 627, 1328, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12289982337456246, "compression_ratio": 1.478827361563518, "no_speech_prob": 0.006598290521651506}, {"id": 122, "seek": 50952, "start": 513.72, "end": 516.4, "text": " Ukryta w czasie, kiedy i tak trwaj\u0105 obliczenia.", "tokens": [50574, 9816, 627, 1328, 261, 42667, 11, 18777, 741, 991, 504, 86, 11133, 1111, 1050, 14320, 13, 50708], "temperature": 0.0, "avg_logprob": -0.12289982337456246, "compression_ratio": 1.478827361563518, "no_speech_prob": 0.006598290521651506}, {"id": 123, "seek": 50952, "start": 516.6, "end": 520.96, "text": " Tak, dzi\u0119ki temu mog\u0105 skalowa\u0107 model na wi\u0119cej maszyn, bez martwienia si\u0119 o narzut", "tokens": [50718, 9118, 11, 45003, 33346, 34123, 16890, 11445, 2316, 1667, 26004, 2300, 1229, 77, 11, 10782, 12396, 86, 18811, 3244, 277, 6714, 89, 325, 50936], "temperature": 0.0, "avg_logprob": -0.12289982337456246, "compression_ratio": 1.478827361563518, "no_speech_prob": 0.006598290521651506}, {"id": 124, "seek": 50952, "start": 521.16, "end": 524.56, "text": " komunikacyjny. A trzeci filar to, jak si\u0119 domy\u015blam,", "tokens": [50946, 45359, 1035, 31285, 1634, 13, 316, 22266, 537, 1387, 289, 281, 11, 4207, 3244, 3285, 88, 1788, 4326, 11, 51116], "temperature": 0.0, "avg_logprob": -0.12289982337456246, "compression_ratio": 1.478827361563518, "no_speech_prob": 0.006598290521651506}, {"id": 125, "seek": 50952, "start": 524.76, "end": 527.28, "text": " r\u00f3wnie ekstremalne podej\u015bcie do pami\u0119ci.", "tokens": [51126, 11416, 14215, 13359, 372, 2579, 304, 716, 7468, 73, 9815, 360, 31088, 537, 13, 51252], "temperature": 0.0, "avg_logprob": -0.12289982337456246, "compression_ratio": 1.478827361563518, "no_speech_prob": 0.006598290521651506}, {"id": 126, "seek": 50952, "start": 527.48, "end": 532.36, "text": " Tak, ekstremalna oszcz\u0119dno\u015b\u0107 pami\u0119ci stosuj\u0105 ca\u0142\u0105 gam\u0119 technik.", "tokens": [51262, 9118, 11, 13359, 372, 2579, 304, 629, 3003, 43771, 6298, 23293, 31088, 537, 43581, 13263, 1335, 15926, 8019, 1274, 1537, 1035, 13, 51506], "temperature": 0.0, "avg_logprob": -0.12289982337456246, "compression_ratio": 1.478827361563518, "no_speech_prob": 0.006598290521651506}, {"id": 127, "seek": 50952, "start": 532.56, "end": 537.3199999999999, "text": " Na przyk\u0142ad rekomputacja pewnych operacji zamiast przechowywania ich wynik\u00f3w w", "tokens": [51516, 6056, 23144, 33881, 298, 2582, 23395, 47160, 16384, 2208, 13152, 710, 4526, 525, 8325, 339, 10089, 86, 5609, 1893, 31936, 1035, 3901, 261, 51754], "temperature": 0.0, "avg_logprob": -0.12289982337456246, "compression_ratio": 1.478827361563518, "no_speech_prob": 0.006598290521651506}, {"id": 128, "seek": 53732, "start": 537.32, "end": 538.7600000000001, "text": " drogocennej pami\u0119ci VRAM.", "tokens": [50364, 3789, 70, 905, 268, 11794, 31088, 537, 13722, 2865, 13, 50436], "temperature": 0.0, "avg_logprob": -0.1684166697047701, "compression_ratio": 1.4080267558528428, "no_speech_prob": 0.02976684458553791}, {"id": 129, "seek": 53732, "start": 538.96, "end": 540.0, "text": " Do ma sens.", "tokens": [50446, 1144, 463, 2923, 13, 50498], "temperature": 0.0, "avg_logprob": -0.1684166697047701, "compression_ratio": 1.4080267558528428, "no_speech_prob": 0.02976684458553791}, {"id": 130, "seek": 53732, "start": 540.2, "end": 545.6400000000001, "text": " Albo trzymanie parametr\u00f3w Exponential Moving Everage w skr\u00f3cie EMA w znacznie", "tokens": [50508, 967, 1763, 34573, 1601, 414, 6220, 27965, 3901, 21391, 266, 2549, 14242, 12123, 609, 261, 1110, 11721, 4260, 462, 9998, 261, 15397, 14875, 2766, 50780], "temperature": 0.0, "avg_logprob": -0.1684166697047701, "compression_ratio": 1.4080267558528428, "no_speech_prob": 0.02976684458553791}, {"id": 131, "seek": 53732, "start": 545.84, "end": 550.2, "text": " wolniejszej, ale pojemniejszej pami\u0119ci serwera, a nie na karcie.", "tokens": [50790, 20960, 30295, 16920, 11, 6775, 714, 30833, 30295, 16920, 31088, 537, 816, 1554, 64, 11, 257, 2838, 1667, 7917, 4260, 13, 51008], "temperature": 0.0, "avg_logprob": -0.1684166697047701, "compression_ratio": 1.4080267558528428, "no_speech_prob": 0.02976684458553791}, {"id": 132, "seek": 53732, "start": 550.4000000000001, "end": 555.5200000000001, "text": " Ka\u017cdy zaoszcz\u0119dzony megabyte pozwala\u0142 im trenowa\u0107 lepszy model przy tym samym bud\u017cecie.", "tokens": [51018, 10988, 1427, 3173, 7949, 329, 43771, 6298, 44479, 10816, 34529, 40557, 5159, 1221, 566, 23136, 11445, 476, 1878, 1229, 2316, 6501, 8107, 3247, 4199, 3265, 2875, 4260, 13, 51274], "temperature": 0.0, "avg_logprob": -0.1684166697047701, "compression_ratio": 1.4080267558528428, "no_speech_prob": 0.02976684458553791}, {"id": 133, "seek": 53732, "start": 555.72, "end": 560.08, "text": " Mamy wi\u0119c innowacyjn\u0105 architektur\u0119, super wydajny trening.", "tokens": [51284, 376, 7804, 16677, 294, 3785, 31285, 13113, 3912, 642, 2320, 374, 1274, 11, 1687, 25984, 1805, 1634, 2192, 773, 13, 51502], "temperature": 0.0, "avg_logprob": -0.1684166697047701, "compression_ratio": 1.4080267558528428, "no_speech_prob": 0.02976684458553791}, {"id": 134, "seek": 53732, "start": 560.2800000000001, "end": 561.44, "text": " Przejd\u017amy do wynik\u00f3w.", "tokens": [51512, 2114, 16920, 67, 10659, 2226, 360, 31936, 1035, 3901, 13, 51570], "temperature": 0.0, "avg_logprob": -0.1684166697047701, "compression_ratio": 1.4080267558528428, "no_speech_prob": 0.02976684458553791}, {"id": 135, "seek": 53732, "start": 561.6400000000001, "end": 564.2800000000001, "text": " Czy ten model faktycznie jest tak dobry, jak sugeruj\u0105 te", "tokens": [51580, 19832, 2064, 2316, 33647, 45586, 3492, 991, 35884, 11, 4207, 459, 1321, 13263, 535, 51712], "temperature": 0.0, "avg_logprob": -0.1684166697047701, "compression_ratio": 1.4080267558528428, "no_speech_prob": 0.02976684458553791}, {"id": 136, "seek": 56428, "start": 564.4, "end": 567.8399999999999, "text": " optymalizacje? Jak wypada w starciu z tytanami?", "tokens": [50370, 2427, 4199, 304, 590, 29293, 30, 15029, 46392, 1538, 261, 3543, 30795, 710, 1104, 20356, 4526, 30, 50542], "temperature": 0.0, "avg_logprob": -0.2104766845703125, "compression_ratio": 1.2979591836734694, "no_speech_prob": 0.08383555710315704}, {"id": 137, "seek": 56428, "start": 568.04, "end": 571.0, "text": " Wyniki s\u0105 szczerze m\u00f3wi\u0105c osza\u0142amiaj\u0105ce.", "tokens": [50552, 343, 2534, 9850, 9015, 22090, 260, 1381, 46591, 66, 3003, 2394, 20177, 48125, 384, 13, 50700], "temperature": 0.0, "avg_logprob": -0.2104766845703125, "compression_ratio": 1.2979591836734694, "no_speech_prob": 0.08383555710315704}, {"id": 138, "seek": 56428, "start": 571.1999999999999, "end": 577.4, "text": " Wersja bazowa DeepSeek V3 Base jest w momencie publikacji najsilniejszym modelem", "tokens": [50710, 343, 433, 2938, 27147, 5528, 14895, 10637, 916, 691, 18, 21054, 3492, 261, 40883, 11227, 1035, 13152, 11212, 30605, 10402, 7706, 76, 4391, 10386, 51020], "temperature": 0.0, "avg_logprob": -0.2104766845703125, "compression_ratio": 1.2979591836734694, "no_speech_prob": 0.08383555710315704}, {"id": 139, "seek": 56428, "start": 577.6, "end": 578.68, "text": " open source na \u015bwiecie.", "tokens": [51030, 1269, 4009, 1667, 40078, 4260, 13, 51084], "temperature": 0.0, "avg_logprob": -0.2104766845703125, "compression_ratio": 1.2979591836734694, "no_speech_prob": 0.08383555710315704}, {"id": 140, "seek": 56428, "start": 578.88, "end": 579.3199999999999, "text": " OK.", "tokens": [51094, 2264, 13, 51116], "temperature": 0.0, "avg_logprob": -0.2104766845703125, "compression_ratio": 1.2979591836734694, "no_speech_prob": 0.08383555710315704}, {"id": 141, "seek": 56428, "start": 579.52, "end": 586.0799999999999, "text": " Ale to co naprawd\u0119 szokuje to por\u00f3wnanie z LMA3145B.", "tokens": [51126, 9366, 281, 598, 20970, 7870, 453, 13008, 281, 1515, 812, 895, 7155, 710, 441, 9998, 18, 7271, 20, 33, 13, 51454], "temperature": 0.0, "avg_logprob": -0.2104766845703125, "compression_ratio": 1.2979591836734694, "no_speech_prob": 0.08383555710315704}, {"id": 142, "seek": 56428, "start": 586.28, "end": 590.24, "text": " W wielu kluczowych benchmarkach DeepSeek V3 go przewy\u017csza.", "tokens": [51464, 343, 40437, 9671, 1311, 89, 19605, 18927, 608, 14895, 10637, 916, 691, 18, 352, 39758, 88, 1427, 82, 2394, 13, 51662], "temperature": 0.0, "avg_logprob": -0.2104766845703125, "compression_ratio": 1.2979591836734694, "no_speech_prob": 0.08383555710315704}, {"id": 143, "seek": 59024, "start": 590.4, "end": 592.5600000000001, "text": " Chwileczka to jest absolutnie kluczowe.", "tokens": [50372, 761, 86, 794, 3689, 2330, 281, 3492, 18757, 2766, 9671, 1311, 89, 6880, 13, 50480], "temperature": 0.0, "avg_logprob": -0.14118605646593818, "compression_ratio": 1.4169278996865204, "no_speech_prob": 0.0017824206734076142}, {"id": 144, "seek": 59024, "start": 592.76, "end": 593.92, "text": " Powiedzmy to g\u0142o\u015bno.", "tokens": [50490, 14762, 15338, 2226, 281, 290, 5249, 1788, 1771, 13, 50548], "temperature": 0.0, "avg_logprob": -0.14118605646593818, "compression_ratio": 1.4169278996865204, "no_speech_prob": 0.0017824206734076142}, {"id": 145, "seek": 59024, "start": 594.12, "end": 599.76, "text": " DeepSeek V3 osi\u0105ga lepsze wyniki u\u017cywaj\u0105c 11 razy mniej aktywnych parametr\u00f3w.", "tokens": [50558, 14895, 10637, 916, 691, 18, 3003, 11404, 3680, 476, 1878, 1381, 31936, 9850, 34097, 86, 38757, 2975, 9639, 88, 39513, 9308, 874, 895, 16384, 6220, 27965, 3901, 13, 50840], "temperature": 0.0, "avg_logprob": -0.14118605646593818, "compression_ratio": 1.4169278996865204, "no_speech_prob": 0.0017824206734076142}, {"id": 146, "seek": 59024, "start": 599.96, "end": 601.8, "text": " Tak, 11 razy mniej.", "tokens": [50850, 9118, 11, 2975, 9639, 88, 39513, 13, 50942], "temperature": 0.0, "avg_logprob": -0.14118605646593818, "compression_ratio": 1.4169278996865204, "no_speech_prob": 0.0017824206734076142}, {"id": 147, "seek": 59024, "start": 602.0, "end": 604.08, "text": " To nie jest drobna optymalizacja.", "tokens": [50952, 1407, 2838, 3492, 3789, 65, 629, 2427, 4199, 304, 590, 23395, 13, 51056], "temperature": 0.0, "avg_logprob": -0.14118605646593818, "compression_ratio": 1.4169278996865204, "no_speech_prob": 0.0017824206734076142}, {"id": 148, "seek": 59024, "start": 604.28, "end": 605.88, "text": " To fundamentalna zmiana.", "tokens": [51066, 1407, 8088, 629, 17020, 8497, 13, 51146], "temperature": 0.0, "avg_logprob": -0.14118605646593818, "compression_ratio": 1.4169278996865204, "no_speech_prob": 0.0017824206734076142}, {"id": 149, "seek": 59024, "start": 606.08, "end": 608.0, "text": " To tak jakby samoch\u00f3d silnikiem od", "tokens": [51156, 1407, 991, 28976, 3247, 8997, 17081, 3425, 13123, 4907, 3611, 51252], "temperature": 0.0, "avg_logprob": -0.14118605646593818, "compression_ratio": 1.4169278996865204, "no_speech_prob": 0.0017824206734076142}, {"id": 150, "seek": 59024, "start": 608.2, "end": 611.12, "text": " kosiarki wygra\u0142 wy\u015bcig z bolidem Formu\u0142y 1.", "tokens": [51262, 19532, 72, 809, 72, 4628, 20735, 1221, 4628, 1788, 66, 328, 710, 8986, 327, 443, 10126, 84, 6825, 502, 13, 51408], "temperature": 0.0, "avg_logprob": -0.14118605646593818, "compression_ratio": 1.4169278996865204, "no_speech_prob": 0.0017824206734076142}, {"id": 151, "seek": 59024, "start": 611.32, "end": 612.72, "text": " Co to tak naprawd\u0119 oznacza?", "tokens": [51418, 3066, 281, 991, 20970, 277, 22672, 326, 2394, 30, 51488], "temperature": 0.0, "avg_logprob": -0.14118605646593818, "compression_ratio": 1.4169278996865204, "no_speech_prob": 0.0017824206734076142}, {"id": 152, "seek": 59024, "start": 612.92, "end": 617.04, "text": " To oznacza, \u017ce era wi\u0119ksze znaczy lepszy mo\u017ce dobiega\u0107 ko\u0144ca.", "tokens": [51498, 1407, 277, 22672, 326, 2394, 11, 3561, 4249, 29968, 1381, 36584, 476, 1878, 1229, 12034, 360, 7392, 3680, 2162, 26470, 496, 13, 51704], "temperature": 0.0, "avg_logprob": -0.14118605646593818, "compression_ratio": 1.4169278996865204, "no_speech_prob": 0.0017824206734076142}, {"id": 153, "seek": 59024, "start": 617.24, "end": 620.12, "text": " Teraz liczy si\u0119 efektywno\u015b\u0107 i ile inteligencji", "tokens": [51714, 41810, 6169, 1229, 3244, 31482, 916, 874, 20944, 7753, 741, 15465, 24777, 3213, 19649, 51858], "temperature": 0.0, "avg_logprob": -0.14118605646593818, "compression_ratio": 1.4169278996865204, "no_speech_prob": 0.0017824206734076142}, {"id": 154, "seek": 62024, "start": 620.28, "end": 623.72, "text": " potrafisz wycisn\u0105\u0107 z ka\u017cdego parametru i ka\u017cdego wata energii.", "tokens": [50366, 1847, 10437, 23848, 4628, 26720, 13113, 2162, 710, 21912, 67, 6308, 6220, 302, 894, 741, 21912, 67, 6308, 261, 3274, 10575, 5597, 13, 50538], "temperature": 0.0, "avg_logprob": -0.15681072157256457, "compression_ratio": 1.3598615916955017, "no_speech_prob": 0.013916120864450932}, {"id": 155, "seek": 62024, "start": 623.92, "end": 628.32, "text": " A DeepSeek V3 pokazuje, \u017ce jest tu jeszcze ogromne pole do popisu.", "tokens": [50548, 316, 14895, 10637, 916, 691, 18, 13010, 43317, 11, 3561, 3492, 2604, 14168, 34416, 298, 716, 13208, 360, 1665, 25871, 13, 50768], "temperature": 0.0, "avg_logprob": -0.15681072157256457, "compression_ratio": 1.3598615916955017, "no_speech_prob": 0.013916120864450932}, {"id": 156, "seek": 62024, "start": 628.52, "end": 630.96, "text": " A gdzie ta jego si\u0142a jest najbardziej widoczna?", "tokens": [50778, 316, 18922, 1846, 26542, 1511, 5024, 3492, 41857, 5274, 905, 35458, 30, 50900], "temperature": 0.0, "avg_logprob": -0.15681072157256457, "compression_ratio": 1.3598615916955017, "no_speech_prob": 0.013916120864450932}, {"id": 157, "seek": 62024, "start": 631.16, "end": 633.04, "text": " Raport wspomina o specjalizacjach.", "tokens": [50910, 16184, 477, 17757, 49217, 277, 46433, 590, 326, 45059, 13, 51004], "temperature": 0.0, "avg_logprob": -0.15681072157256457, "compression_ratio": 1.3598615916955017, "no_speech_prob": 0.013916120864450932}, {"id": 158, "seek": 62024, "start": 633.24, "end": 635.84, "text": " Tak, zw\u0142aszcza w kodowaniu i matematyce.", "tokens": [51014, 9118, 11, 11873, 1221, 19601, 41524, 261, 350, 378, 305, 25849, 741, 3803, 8615, 88, 384, 13, 51144], "temperature": 0.0, "avg_logprob": -0.15681072157256457, "compression_ratio": 1.3598615916955017, "no_speech_prob": 0.013916120864450932}, {"id": 159, "seek": 62024, "start": 636.04, "end": 637.64, "text": " Sp\u00f3jrzmy na liczby.", "tokens": [51154, 1738, 18999, 19390, 2226, 1667, 6169, 89, 2322, 13, 51234], "temperature": 0.0, "avg_logprob": -0.15681072157256457, "compression_ratio": 1.3598615916955017, "no_speech_prob": 0.013916120864450932}, {"id": 160, "seek": 62024, "start": 637.84, "end": 643.2, "text": " W te\u015bcie Meth 500, kt\u00f3ry sprawdza rozwi\u0105zywanie problem\u00f3w matematycznych,", "tokens": [51244, 343, 535, 9815, 376, 3293, 5923, 11, 9913, 46192, 2394, 9544, 18234, 1229, 86, 7155, 1154, 3901, 3803, 8615, 17466, 9399, 11, 51512], "temperature": 0.0, "avg_logprob": -0.15681072157256457, "compression_ratio": 1.3598615916955017, "no_speech_prob": 0.013916120864450932}, {"id": 161, "seek": 62024, "start": 643.4, "end": 646.88, "text": " osi\u0105ga wynik 92 i 20.", "tokens": [51522, 3003, 11404, 3680, 31936, 1035, 28225, 741, 945, 13, 51696], "temperature": 0.0, "avg_logprob": -0.15681072157256457, "compression_ratio": 1.3598615916955017, "no_speech_prob": 0.013916120864450932}, {"id": 162, "seek": 62024, "start": 647.08, "end": 649.36, "text": " W AM 2024.", "tokens": [51706, 343, 6475, 45237, 13, 51820], "temperature": 0.0, "avg_logprob": -0.15681072157256457, "compression_ratio": 1.3598615916955017, "no_speech_prob": 0.013916120864450932}, {"id": 163, "seek": 64936, "start": 649.48, "end": 651.8000000000001, "text": " To ten trudny presti\u017cowy konkurs.", "tokens": [50370, 1407, 2064, 32007, 1634, 16305, 72, 1427, 10089, 21428, 2156, 13, 50486], "temperature": 0.0, "avg_logprob": -0.19228712956708177, "compression_ratio": 1.3661417322834646, "no_speech_prob": 0.016141774132847786}, {"id": 164, "seek": 64936, "start": 652.0, "end": 655.6800000000001, "text": " Tak, tam osi\u0105ga wynik 39 i 20.", "tokens": [50496, 9118, 11, 7677, 3003, 11404, 3680, 31936, 1035, 15238, 741, 945, 13, 50680], "temperature": 0.0, "avg_logprob": -0.19228712956708177, "compression_ratio": 1.3661417322834646, "no_speech_prob": 0.016141774132847786}, {"id": 165, "seek": 64936, "start": 655.88, "end": 657.6, "text": " Deklasuj\u0105c konkurencj\u0119.", "tokens": [50690, 1346, 74, 7743, 44733, 21428, 9873, 41960, 13, 50776], "temperature": 0.0, "avg_logprob": -0.19228712956708177, "compression_ratio": 1.3661417322834646, "no_speech_prob": 0.016141774132847786}, {"id": 166, "seek": 64936, "start": 657.8000000000001, "end": 661.08, "text": " Ale to kodowanie jest chyba najbardziej spektakularne.", "tokens": [50786, 9366, 281, 350, 378, 22028, 3492, 31532, 41857, 768, 2320, 514, 1040, 716, 13, 50950], "temperature": 0.0, "avg_logprob": -0.19228712956708177, "compression_ratio": 1.3661417322834646, "no_speech_prob": 0.016141774132847786}, {"id": 167, "seek": 64936, "start": 661.28, "end": 664.36, "text": " W benchmarku Code Forces jego wyniki", "tokens": [50960, 343, 18927, 84, 15549, 27445, 26542, 31936, 9850, 51114], "temperature": 0.0, "avg_logprob": -0.19228712956708177, "compression_ratio": 1.3661417322834646, "no_speech_prob": 0.016141774132847786}, {"id": 168, "seek": 64936, "start": 664.5600000000001, "end": 667.8000000000001, "text": " plasuj\u0105 go w 51,6 procentylu.", "tokens": [51124, 499, 296, 13263, 352, 261, 18485, 11, 21, 38826, 88, 2781, 13, 51286], "temperature": 0.0, "avg_logprob": -0.19228712956708177, "compression_ratio": 1.3661417322834646, "no_speech_prob": 0.016141774132847786}, {"id": 169, "seek": 64936, "start": 668.0, "end": 669.5600000000001, "text": " A jak to si\u0119 ma do innych?", "tokens": [51296, 316, 4207, 281, 3244, 463, 360, 36286, 30, 51374], "temperature": 0.0, "avg_logprob": -0.19228712956708177, "compression_ratio": 1.3661417322834646, "no_speech_prob": 0.016141774132847786}, {"id": 170, "seek": 64936, "start": 669.76, "end": 671.84, "text": " Gdzie jest na przyk\u0142ad GPT-4O?", "tokens": [51384, 460, 13096, 3492, 1667, 23144, 26039, 51, 12, 19, 46, 30, 51488], "temperature": 0.0, "avg_logprob": -0.19228712956708177, "compression_ratio": 1.3661417322834646, "no_speech_prob": 0.016141774132847786}, {"id": 171, "seek": 64936, "start": 672.04, "end": 677.16, "text": " W tym samym te\u015bcie GPT-4O osi\u0105ga 23,6 procentylu.", "tokens": [51498, 343, 8107, 3247, 4199, 535, 9815, 26039, 51, 12, 19, 46, 3003, 11404, 3680, 6673, 11, 21, 38826, 88, 2781, 13, 51754], "temperature": 0.0, "avg_logprob": -0.19228712956708177, "compression_ratio": 1.3661417322834646, "no_speech_prob": 0.016141774132847786}, {"id": 172, "seek": 64936, "start": 677.36, "end": 678.2, "text": " To jest przepa\u015b\u0107.", "tokens": [51764, 1407, 3492, 30829, 64, 7753, 13, 51806], "temperature": 0.0, "avg_logprob": -0.19228712956708177, "compression_ratio": 1.3661417322834646, "no_speech_prob": 0.016141774132847786}, {"id": 173, "seek": 67820, "start": 678.32, "end": 679.48, "text": " Ogromna.", "tokens": [50370, 422, 861, 298, 629, 13, 50428], "temperature": 0.0, "avg_logprob": -0.22518104230853872, "compression_ratio": 1.3900709219858156, "no_speech_prob": 0.06904087960720062}, {"id": 174, "seek": 67820, "start": 679.6800000000001, "end": 684.12, "text": " To oznacza, \u017ce DeepSync V3 generuje kod na poziomie, powiedzmy,", "tokens": [50438, 1407, 277, 22672, 326, 2394, 11, 3561, 14895, 50, 34015, 691, 18, 1337, 13008, 350, 378, 1667, 38503, 40120, 11, 27617, 2226, 11, 50660], "temperature": 0.0, "avg_logprob": -0.22518104230853872, "compression_ratio": 1.3900709219858156, "no_speech_prob": 0.06904087960720062}, {"id": 175, "seek": 67820, "start": 684.32, "end": 688.0, "text": " przeci\u0119tnego, dobrego, ludzkiego programisty konkursowego.", "tokens": [50670, 39622, 46788, 11858, 11, 41959, 1571, 11, 15946, 30154, 12200, 1461, 38618, 21428, 2156, 26576, 13, 50854], "temperature": 0.0, "avg_logprob": -0.22518104230853872, "compression_ratio": 1.3900709219858156, "no_speech_prob": 0.06904087960720062}, {"id": 176, "seek": 67820, "start": 688.2, "end": 690.36, "text": " Inne modele s\u0105 daleko w tyle.", "tokens": [50864, 682, 716, 4391, 306, 9015, 11702, 34241, 261, 39293, 13, 50972], "temperature": 0.0, "avg_logprob": -0.22518104230853872, "compression_ratio": 1.3900709219858156, "no_speech_prob": 0.06904087960720062}, {"id": 177, "seek": 67820, "start": 690.5600000000001, "end": 693.76, "text": " Co wi\u0119cej, wersja Chat po fine tuningu jest", "tokens": [50982, 3066, 26004, 11, 261, 433, 2938, 27503, 714, 2489, 15164, 84, 3492, 51142], "temperature": 0.0, "avg_logprob": -0.22518104230853872, "compression_ratio": 1.3900709219858156, "no_speech_prob": 0.06904087960720062}, {"id": 178, "seek": 67820, "start": 693.96, "end": 696.2, "text": " pe\u0142nie por\u00f3wnywalna z zamkni\u0119tymi modelami.", "tokens": [51152, 43205, 2766, 1515, 812, 895, 27112, 304, 629, 710, 19876, 74, 35938, 874, 3057, 2316, 4526, 13, 51264], "temperature": 0.0, "avg_logprob": -0.22518104230853872, "compression_ratio": 1.3900709219858156, "no_speech_prob": 0.06904087960720062}, {"id": 179, "seek": 67820, "start": 696.4000000000001, "end": 697.0400000000001, "text": " A\u017c tak?", "tokens": [51274, 316, 1427, 991, 30, 51306], "temperature": 0.0, "avg_logprob": -0.22518104230853872, "compression_ratio": 1.3900709219858156, "no_speech_prob": 0.06904087960720062}, {"id": 180, "seek": 67820, "start": 697.24, "end": 702.5200000000001, "text": " W te\u015bcie Arena Hard, kt\u00f3re mierzy jak model radzi sobie ze z\u0142o\u017conymi zadaniami,", "tokens": [51316, 343, 535, 9815, 34290, 11817, 11, 8864, 47448, 1229, 4207, 2316, 2843, 3992, 13652, 5277, 710, 5249, 1427, 2526, 3057, 710, 11338, 15568, 11, 51580], "temperature": 0.0, "avg_logprob": -0.22518104230853872, "compression_ratio": 1.3900709219858156, "no_speech_prob": 0.06904087960720062}, {"id": 181, "seek": 67820, "start": 702.72, "end": 706.12, "text": " DeepSync V3 jako pierwszy model OpenSource", "tokens": [51590, 14895, 50, 34015, 691, 18, 17123, 34016, 2316, 7238, 50, 2948, 51760], "temperature": 0.0, "avg_logprob": -0.22518104230853872, "compression_ratio": 1.3900709219858156, "no_speech_prob": 0.06904087960720062}, {"id": 182, "seek": 70612, "start": 706.16, "end": 708.68, "text": " przekroczy\u0142 pr\u00f3g 85 procent,", "tokens": [50366, 29785, 340, 6522, 1221, 8565, 70, 14695, 38826, 11, 50492], "temperature": 0.0, "avg_logprob": -0.1709414928943127, "compression_ratio": 1.4054982817869415, "no_speech_prob": 0.013236376456916332}, {"id": 183, "seek": 70612, "start": 708.88, "end": 711.52, "text": " zr\u00f3wnuj\u0105c si\u0119 z Cloud 3.5 Sonet.", "tokens": [50502, 710, 11721, 895, 44733, 3244, 710, 8061, 805, 13, 20, 5185, 302, 13, 50634], "temperature": 0.0, "avg_logprob": -0.1709414928943127, "compression_ratio": 1.4054982817869415, "no_speech_prob": 0.013236376456916332}, {"id": 184, "seek": 70612, "start": 711.72, "end": 715.5600000000001, "text": " Zr\u00f3wnuj\u0105c si\u0119 z modelem, kt\u00f3ry jest uwa\u017cany za absolutny top,", "tokens": [50644, 1176, 11721, 895, 44733, 3244, 710, 4391, 10386, 11, 9913, 3492, 48089, 1325, 7949, 18757, 1634, 1192, 11, 50836], "temperature": 0.0, "avg_logprob": -0.1709414928943127, "compression_ratio": 1.4054982817869415, "no_speech_prob": 0.013236376456916332}, {"id": 185, "seek": 70612, "start": 715.76, "end": 720.24, "text": " to brzmi, no, prawie jak marketingowa rewelacja.", "tokens": [50846, 281, 738, 89, 3057, 11, 572, 11, 3206, 8699, 4207, 6370, 5528, 319, 45512, 23395, 13, 51070], "temperature": 0.0, "avg_logprob": -0.1709414928943127, "compression_ratio": 1.4054982817869415, "no_speech_prob": 0.013236376456916332}, {"id": 186, "seek": 70612, "start": 720.44, "end": 723.08, "text": " Czy te benchmarki na pewno s\u0105 wiarygodne?", "tokens": [51080, 19832, 535, 18927, 72, 1667, 33002, 9015, 26393, 822, 21787, 716, 30, 51212], "temperature": 0.0, "avg_logprob": -0.1709414928943127, "compression_ratio": 1.4054982817869415, "no_speech_prob": 0.013236376456916332}, {"id": 187, "seek": 70612, "start": 723.28, "end": 725.96, "text": " Czy nie ma tu jakiego\u015b uczenia pod testy?", "tokens": [51222, 19832, 2838, 463, 2604, 4207, 12200, 1788, 344, 38517, 2497, 1500, 88, 30, 51356], "temperature": 0.0, "avg_logprob": -0.1709414928943127, "compression_ratio": 1.4054982817869415, "no_speech_prob": 0.013236376456916332}, {"id": 188, "seek": 70612, "start": 726.16, "end": 728.92, "text": " To jest bardzo s\u0142uszne pytanie i zdrowy sceptyczyzm.", "tokens": [51366, 1407, 3492, 9034, 15116, 22378, 716, 36610, 741, 49745, 88, 262, 1336, 17466, 37433, 76, 13, 51504], "temperature": 0.0, "avg_logprob": -0.1709414928943127, "compression_ratio": 1.4054982817869415, "no_speech_prob": 0.013236376456916332}, {"id": 189, "seek": 70612, "start": 729.12, "end": 731.92, "text": " Autorze raportu zdaj\u0105 sobie z tego spraw\u0119.", "tokens": [51514, 6049, 284, 1381, 5099, 477, 84, 16221, 11133, 13652, 710, 8627, 22734, 1274, 13, 51654], "temperature": 0.0, "avg_logprob": -0.1709414928943127, "compression_ratio": 1.4054982817869415, "no_speech_prob": 0.013236376456916332}, {"id": 190, "seek": 70612, "start": 732.12, "end": 734.24, "text": " I cz\u0119\u015bciowo odkowiadaj\u0105 na to pytanie,", "tokens": [51664, 286, 41314, 19941, 3611, 74, 24503, 1538, 8555, 1667, 281, 36610, 11, 51770], "temperature": 0.0, "avg_logprob": -0.1709414928943127, "compression_ratio": 1.4054982817869415, "no_speech_prob": 0.013236376456916332}, {"id": 191, "seek": 73424, "start": 734.36, "end": 738.0, "text": " wyja\u015bniaj\u0105c sk\u0105d bior\u0105 si\u0119 te niezwyk\u0142e zdolno\u015bci do rozumowania.", "tokens": [50370, 4628, 2938, 1788, 12679, 8555, 66, 1110, 18962, 272, 1973, 1611, 3244, 535, 33511, 9726, 74, 19827, 16221, 401, 16438, 360, 48797, 21308, 13, 50552], "temperature": 0.0, "avg_logprob": -0.16441707483074008, "compression_ratio": 1.416949152542373, "no_speech_prob": 0.0095268739387393}, {"id": 192, "seek": 73424, "start": 738.2, "end": 741.6800000000001, "text": " Czyli jest jeszcze jaki\u015b sekretny sk\u0142adnik co\u015b wi\u0119cej?", "tokens": [50562, 37099, 3492, 14168, 34721, 17215, 1505, 1634, 1110, 10358, 13123, 19241, 26004, 30, 50736], "temperature": 0.0, "avg_logprob": -0.16441707483074008, "compression_ratio": 1.416949152542373, "no_speech_prob": 0.0095268739387393}, {"id": 193, "seek": 73424, "start": 741.88, "end": 742.48, "text": " Tak.", "tokens": [50746, 9118, 13, 50776], "temperature": 0.0, "avg_logprob": -0.16441707483074008, "compression_ratio": 1.416949152542373, "no_speech_prob": 0.0095268739387393}, {"id": 194, "seek": 73424, "start": 742.6800000000001, "end": 746.4, "text": " Bardzo ciekawa technika, kt\u00f3r\u0105 zastosowali w fazie post-training.", "tokens": [50786, 38559, 46419, 10449, 1537, 5439, 11, 37415, 36746, 329, 305, 5103, 261, 4375, 414, 2183, 12, 17227, 1760, 13, 50972], "temperature": 0.0, "avg_logprob": -0.16441707483074008, "compression_ratio": 1.416949152542373, "no_speech_prob": 0.0095268739387393}, {"id": 195, "seek": 73424, "start": 746.6, "end": 749.0, "text": " U\u017cyli destylacji wiedzy z innego,", "tokens": [50982, 624, 7735, 2081, 2677, 5088, 13152, 46894, 1229, 710, 294, 11858, 11, 51102], "temperature": 0.0, "avg_logprob": -0.16441707483074008, "compression_ratio": 1.416949152542373, "no_speech_prob": 0.0095268739387393}, {"id": 196, "seek": 73424, "start": 749.2, "end": 752.64, "text": " wyspecjalizowanego modelu DeepSync R1.", "tokens": [51112, 261, 749, 494, 66, 22600, 590, 37345, 6308, 2316, 84, 14895, 50, 34015, 497, 16, 13, 51284], "temperature": 0.0, "avg_logprob": -0.16441707483074008, "compression_ratio": 1.416949152542373, "no_speech_prob": 0.0095268739387393}, {"id": 197, "seek": 73424, "start": 752.84, "end": 755.5600000000001, "text": " Ten model jest ekspertem w rozumowaniu krok po kroku,", "tokens": [51294, 9380, 2316, 3492, 30724, 15346, 443, 261, 48797, 305, 25849, 350, 31621, 714, 45909, 5279, 11, 51430], "temperature": 0.0, "avg_logprob": -0.16441707483074008, "compression_ratio": 1.416949152542373, "no_speech_prob": 0.0095268739387393}, {"id": 198, "seek": 73424, "start": 755.76, "end": 758.5600000000001, "text": " czyli w generowaniu d\u0142ugich Chain of Thought.", "tokens": [51440, 16591, 261, 1337, 305, 25849, 274, 34077, 480, 33252, 295, 23058, 13, 51580], "temperature": 0.0, "avg_logprob": -0.16441707483074008, "compression_ratio": 1.416949152542373, "no_speech_prob": 0.0095268739387393}, {"id": 199, "seek": 73424, "start": 758.76, "end": 761.6800000000001, "text": " W uproszczeniu j\u0105, czyli DeepSync V3,", "tokens": [51590, 343, 493, 2635, 89, 66, 39651, 35692, 11, 16591, 14895, 50, 34015, 691, 18, 11, 51736], "temperature": 0.0, "avg_logprob": -0.16441707483074008, "compression_ratio": 1.416949152542373, "no_speech_prob": 0.0095268739387393}, {"id": 200, "seek": 76168, "start": 761.68, "end": 767.2399999999999, "text": " wzorc\u00f3w, weryfikacji i refleksji od swojego starszego, m\u0105drzejszego brata.", "tokens": [50364, 24809, 284, 29268, 11, 261, 2109, 31230, 13152, 741, 36549, 1694, 4013, 3611, 13291, 39738, 6105, 27725, 11, 275, 18962, 13503, 73, 15453, 6308, 738, 3274, 13, 50642], "temperature": 0.0, "avg_logprob": -0.11052039573932516, "compression_ratio": 1.453287197231834, "no_speech_prob": 0.01680641435086727}, {"id": 201, "seek": 76168, "start": 767.4399999999999, "end": 771.12, "text": " To w\u0142a\u015bnie ta technika, to douczanie od specjalisty,", "tokens": [50652, 1407, 14234, 1846, 1537, 5439, 11, 281, 360, 1311, 89, 7155, 3611, 46433, 38618, 11, 50836], "temperature": 0.0, "avg_logprob": -0.11052039573932516, "compression_ratio": 1.453287197231834, "no_speech_prob": 0.01680641435086727}, {"id": 202, "seek": 76168, "start": 771.3199999999999, "end": 774.3199999999999, "text": " znacz\u0105co podbi\u0142a jego wyniki w rozumowaniu.", "tokens": [50846, 15397, 326, 8925, 1291, 2497, 5614, 5024, 26542, 31936, 9850, 261, 48797, 305, 25849, 13, 50996], "temperature": 0.0, "avg_logprob": -0.11052039573932516, "compression_ratio": 1.453287197231834, "no_speech_prob": 0.01680641435086727}, {"id": 203, "seek": 76168, "start": 774.52, "end": 780.92, "text": " Czyli sprawi\u0142a, \u017ce s\u0105 one bardziej solidne i mniej podatne na zwyk\u0142e wykucie odpowiedzi?", "tokens": [51006, 37099, 22734, 72, 5024, 11, 3561, 9015, 472, 27209, 5100, 716, 741, 39513, 2497, 267, 716, 1667, 43436, 74, 19827, 4628, 5279, 4260, 36574, 3992, 30, 51326], "temperature": 0.0, "avg_logprob": -0.11052039573932516, "compression_ratio": 1.453287197231834, "no_speech_prob": 0.01680641435086727}, {"id": 204, "seek": 76168, "start": 781.12, "end": 782.28, "text": " Dok\u0142adnie tak.", "tokens": [51336, 29768, 10358, 2766, 991, 13, 51394], "temperature": 0.0, "avg_logprob": -0.11052039573932516, "compression_ratio": 1.453287197231834, "no_speech_prob": 0.01680641435086727}, {"id": 205, "seek": 76168, "start": 782.4799999999999, "end": 785.68, "text": " Brzmi to wszystko niemal zbyt dobrze, \u017ceby by\u0142o prawdziwe.", "tokens": [51404, 1603, 89, 3057, 281, 22607, 2838, 5579, 710, 2322, 83, 28335, 11, 11316, 14811, 41175, 3992, 826, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11052039573932516, "compression_ratio": 1.453287197231834, "no_speech_prob": 0.01680641435086727}, {"id": 206, "seek": 76168, "start": 785.88, "end": 788.24, "text": " Ka\u017cdy taki prze\u0142om ma swoje kompromisy.", "tokens": [51574, 10988, 1427, 3173, 20065, 8325, 1221, 298, 463, 29489, 5207, 28722, 14169, 13, 51692], "temperature": 0.0, "avg_logprob": -0.11052039573932516, "compression_ratio": 1.453287197231834, "no_speech_prob": 0.01680641435086727}, {"id": 207, "seek": 76168, "start": 788.4399999999999, "end": 790.28, "text": " Czy ten model ma jakie\u015b wady?", "tokens": [51702, 19832, 2064, 2316, 463, 31163, 261, 880, 30, 51794], "temperature": 0.0, "avg_logprob": -0.11052039573932516, "compression_ratio": 1.453287197231834, "no_speech_prob": 0.01680641435086727}, {"id": 208, "seek": 79028, "start": 790.52, "end": 793.04, "text": " Jakie ograniczenia wymieniaj\u0105 sami autorzy?", "tokens": [50376, 15029, 414, 34416, 30732, 14320, 29764, 18811, 8555, 3247, 72, 19510, 1229, 30, 50502], "temperature": 0.0, "avg_logprob": -0.13862577165876117, "compression_ratio": 1.356164383561644, "no_speech_prob": 0.009629184380173683}, {"id": 209, "seek": 79028, "start": 793.24, "end": 796.0799999999999, "text": " I tu dochodzimy do wa\u017cnej, praktycznej cz\u0119\u015bci.", "tokens": [50512, 286, 2604, 9243, 378, 89, 13189, 360, 27777, 11794, 11, 3206, 74, 874, 3689, 11794, 41314, 13, 50654], "temperature": 0.0, "avg_logprob": -0.13862577165876117, "compression_ratio": 1.356164383561644, "no_speech_prob": 0.009629184380173683}, {"id": 210, "seek": 79028, "start": 796.28, "end": 798.36, "text": " Autorzy s\u0105 bardzo szczerzy.", "tokens": [50664, 6049, 284, 1229, 9015, 9034, 22090, 260, 1229, 13, 50768], "temperature": 0.0, "avg_logprob": -0.13862577165876117, "compression_ratio": 1.356164383561644, "no_speech_prob": 0.009629184380173683}, {"id": 211, "seek": 79028, "start": 798.56, "end": 801.48, "text": " Po pierwsze, \u017ceby wydajnie wdro\u017cy\u0107 ten model,", "tokens": [50778, 6165, 45994, 11, 11316, 25984, 1805, 2766, 261, 45869, 39687, 2064, 2316, 11, 50924], "temperature": 0.0, "avg_logprob": -0.13862577165876117, "compression_ratio": 1.356164383561644, "no_speech_prob": 0.009629184380173683}, {"id": 212, "seek": 79028, "start": 801.68, "end": 804.4, "text": " potrzeba relatywnie du\u017cej infrastruktury.", "tokens": [50934, 28577, 4231, 1039, 21398, 14215, 1581, 38493, 6534, 19977, 2598, 13, 51070], "temperature": 0.0, "avg_logprob": -0.13862577165876117, "compression_ratio": 1.356164383561644, "no_speech_prob": 0.009629184380173683}, {"id": 213, "seek": 79028, "start": 804.6, "end": 805.28, "text": " Czyli jakiej?", "tokens": [51080, 37099, 4207, 7764, 30, 51114], "temperature": 0.0, "avg_logprob": -0.13862577165876117, "compression_ratio": 1.356164383561644, "no_speech_prob": 0.009629184380173683}, {"id": 214, "seek": 79028, "start": 805.48, "end": 807.28, "text": " Raport m\u00f3wi o minimum czterech", "tokens": [51124, 16184, 477, 24592, 277, 7285, 269, 2682, 323, 339, 51214], "temperature": 0.0, "avg_logprob": -0.13862577165876117, "compression_ratio": 1.356164383561644, "no_speech_prob": 0.009629184380173683}, {"id": 215, "seek": 79028, "start": 807.48, "end": 811.6, "text": " w\u0119z\u0142ach z 32 magii pi\u0142u dla etapu pre-feeling.", "tokens": [51224, 261, 1274, 89, 1221, 608, 710, 8858, 2258, 5597, 3895, 24066, 12285, 47634, 84, 659, 12, 2106, 11031, 13, 51430], "temperature": 0.0, "avg_logprob": -0.13862577165876117, "compression_ratio": 1.356164383561644, "no_speech_prob": 0.009629184380173683}, {"id": 216, "seek": 79028, "start": 811.8, "end": 816.1999999999999, "text": " To nie jest model, kt\u00f3ry mo\u017cna komfortowo uruchomi\u0107 na jednym serwerze w piwnice.", "tokens": [51440, 1407, 2838, 3492, 2316, 11, 9913, 17790, 5207, 2728, 19941, 4038, 625, 9220, 2162, 1667, 5232, 12996, 816, 1554, 1381, 261, 3895, 895, 573, 13, 51660], "temperature": 0.0, "avg_logprob": -0.13862577165876117, "compression_ratio": 1.356164383561644, "no_speech_prob": 0.009629184380173683}, {"id": 217, "seek": 81620, "start": 816.32, "end": 821.0, "text": " Wspomnia\u0142e\u015b o barierze wej\u015bcia minimum 32 gpu.", "tokens": [50370, 343, 4952, 38131, 8908, 68, 1788, 277, 2159, 811, 1381, 321, 73, 1788, 2755, 7285, 8858, 290, 34859, 13, 50604], "temperature": 0.0, "avg_logprob": -0.12392989682479644, "compression_ratio": 1.3804713804713804, "no_speech_prob": 0.003724887268617749}, {"id": 218, "seek": 81620, "start": 821.2, "end": 823.0, "text": " Kogo to w praktyce wyklucza?", "tokens": [50614, 591, 23515, 281, 261, 3206, 74, 874, 384, 4628, 7837, 1311, 2394, 30, 50704], "temperature": 0.0, "avg_logprob": -0.12392989682479644, "compression_ratio": 1.3804713804713804, "no_speech_prob": 0.003724887268617749}, {"id": 219, "seek": 81620, "start": 823.2, "end": 825.6400000000001, "text": " Czy to oznacza, \u017ce era gara\u017cowych innowacji", "tokens": [50714, 19832, 281, 277, 22672, 326, 2394, 11, 3561, 4249, 290, 2419, 1427, 19605, 294, 3785, 13152, 50836], "temperature": 0.0, "avg_logprob": -0.12392989682479644, "compression_ratio": 1.3804713804713804, "no_speech_prob": 0.003724887268617749}, {"id": 220, "seek": 81620, "start": 825.84, "end": 828.48, "text": " w Open Source AI na tym poziomie si\u0119 ko\u0144czy?", "tokens": [50846, 261, 7238, 29629, 7318, 1667, 8107, 38503, 40120, 3244, 26470, 6522, 30, 50978], "temperature": 0.0, "avg_logprob": -0.12392989682479644, "compression_ratio": 1.3804713804713804, "no_speech_prob": 0.003724887268617749}, {"id": 221, "seek": 81620, "start": 828.6800000000001, "end": 830.9200000000001, "text": " To jest bardzo trafne pytanie.", "tokens": [50988, 1407, 3492, 9034, 944, 69, 716, 36610, 13, 51100], "temperature": 0.0, "avg_logprob": -0.12392989682479644, "compression_ratio": 1.3804713804713804, "no_speech_prob": 0.003724887268617749}, {"id": 222, "seek": 81620, "start": 831.12, "end": 835.84, "text": " Na pewno utrudnia to zadanie uniwersytetom i mniejszym startupom.", "tokens": [51110, 6056, 33002, 2839, 47130, 12679, 281, 42788, 7155, 36435, 5364, 4328, 302, 298, 741, 39513, 7706, 76, 18578, 298, 13, 51346], "temperature": 0.0, "avg_logprob": -0.12392989682479644, "compression_ratio": 1.3804713804713804, "no_speech_prob": 0.003724887268617749}, {"id": 223, "seek": 81620, "start": 836.0400000000001, "end": 839.5200000000001, "text": " Ale z drugiej strony, poniewa\u017c model jest Open Source,", "tokens": [51356, 9366, 710, 47373, 32406, 11, 32426, 2316, 3492, 7238, 29629, 11, 51530], "temperature": 0.0, "avg_logprob": -0.12392989682479644, "compression_ratio": 1.3804713804713804, "no_speech_prob": 0.003724887268617749}, {"id": 224, "seek": 81620, "start": 839.72, "end": 844.9200000000001, "text": " du\u017ce centra obliczeniowe mog\u0105 go wdro\u017cy\u0107 i udost\u0119pni\u0107 spo\u0142eczno\u015bci przez API.", "tokens": [51540, 1581, 2875, 1489, 424, 1111, 1050, 42124, 6880, 34123, 352, 261, 45869, 39687, 741, 11727, 555, 18085, 3722, 2162, 36851, 89, 16438, 14064, 9362, 13, 51800], "temperature": 0.0, "avg_logprob": -0.12392989682479644, "compression_ratio": 1.3804713804713804, "no_speech_prob": 0.003724887268617749}, {"id": 225, "seek": 84492, "start": 845.0799999999999, "end": 847.0, "text": " Czyli zmienia si\u0119 model dost\u0119pu?", "tokens": [50372, 37099, 17020, 18811, 3244, 2316, 48209, 84, 30, 50468], "temperature": 0.0, "avg_logprob": -0.1487527879256776, "compression_ratio": 1.523391812865497, "no_speech_prob": 0.016812225803732872}, {"id": 226, "seek": 84492, "start": 847.1999999999999, "end": 851.92, "text": " Tak, to raczej zmienia model dost\u0119pu i wsp\u00f3\u0142pracy, a nie koniecznie zabija innowacje.", "tokens": [50478, 9118, 11, 281, 4129, 16920, 17020, 18811, 2316, 48209, 84, 741, 39069, 1424, 2551, 11, 257, 2838, 5897, 414, 19923, 24838, 20642, 294, 3785, 29293, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1487527879256776, "compression_ratio": 1.523391812865497, "no_speech_prob": 0.016812225803732872}, {"id": 227, "seek": 84492, "start": 852.12, "end": 856.0799999999999, "text": " Ale tak, samodzielne fine-tune'owanie takiego giganta staje si\u0119 wyzwaniem.", "tokens": [50724, 9366, 991, 11, 3247, 378, 42280, 716, 2489, 12, 83, 2613, 6, 22028, 32296, 8741, 5983, 342, 11153, 3244, 4628, 89, 7916, 4907, 13, 50922], "temperature": 0.0, "avg_logprob": -0.1487527879256776, "compression_ratio": 1.523391812865497, "no_speech_prob": 0.016812225803732872}, {"id": 228, "seek": 84492, "start": 856.28, "end": 859.4, "text": " A co z innymi ograniczeniami? M\u00f3wili\u015bmy o szybko\u015bci.", "tokens": [50932, 316, 598, 710, 294, 31813, 34416, 30732, 2904, 15568, 30, 376, 3901, 43912, 277, 36456, 4093, 6199, 13, 51088], "temperature": 0.0, "avg_logprob": -0.1487527879256776, "compression_ratio": 1.523391812865497, "no_speech_prob": 0.016812225803732872}, {"id": 229, "seek": 84492, "start": 859.5999999999999, "end": 861.56, "text": " Tak, to drugie ograniczenie.", "tokens": [51098, 9118, 11, 281, 4110, 414, 34416, 30732, 16778, 13, 51196], "temperature": 0.0, "avg_logprob": -0.1487527879256776, "compression_ratio": 1.523391812865497, "no_speech_prob": 0.016812225803732872}, {"id": 230, "seek": 84492, "start": 861.76, "end": 864.04, "text": " Chocia\u017c pr\u0119dko\u015b\u0107 generowania jest dobra.", "tokens": [51206, 12366, 42673, 582, 6298, 4093, 7753, 1337, 21308, 3492, 360, 6198, 13, 51320], "temperature": 0.0, "avg_logprob": -0.1487527879256776, "compression_ratio": 1.523391812865497, "no_speech_prob": 0.016812225803732872}, {"id": 231, "seek": 84492, "start": 864.24, "end": 867.28, "text": " Wspominali\u015bmy o tym 1.8X przy spieszeniu dzi\u0119ki MTP.", "tokens": [51330, 343, 4952, 298, 2071, 72, 10513, 277, 8107, 502, 13, 23, 55, 6501, 45858, 39651, 45003, 376, 16804, 13, 51482], "temperature": 0.0, "avg_logprob": -0.1487527879256776, "compression_ratio": 1.523391812865497, "no_speech_prob": 0.016812225803732872}, {"id": 232, "seek": 84492, "start": 867.4799999999999, "end": 871.12, "text": " Wci\u0105\u017c jest pole do poprawy w por\u00f3wnaniu z niekt\u00f3rymi mniejszymi modelami.", "tokens": [51492, 343, 537, 27242, 3492, 13208, 360, 1665, 5131, 88, 261, 1515, 812, 895, 25849, 710, 2838, 43073, 627, 3057, 39513, 7706, 3057, 2316, 4526, 13, 51674], "temperature": 0.0, "avg_logprob": -0.1487527879256776, "compression_ratio": 1.523391812865497, "no_speech_prob": 0.016812225803732872}, {"id": 233, "seek": 84492, "start": 871.3199999999999, "end": 874.5999999999999, "text": " Klasyczny kompromis mi\u0119dzy moc\u0105 a surow\u0105 pr\u0119dko\u015bci\u0105.", "tokens": [51684, 16053, 5871, 3689, 1634, 5207, 28722, 271, 33964, 705, 32557, 257, 1022, 30297, 582, 6298, 4093, 50227, 13, 51848], "temperature": 0.0, "avg_logprob": -0.1487527879256776, "compression_ratio": 1.523391812865497, "no_speech_prob": 0.016812225803732872}, {"id": 234, "seek": 87460, "start": 874.8000000000001, "end": 880.6800000000001, "text": " Dok\u0142adnie. Do prostych zada\u0144 mniejszy, szybszy model mo\u017ce by\u0107 wci\u0105\u017c lepszym wyborem.", "tokens": [50374, 29768, 10358, 2766, 13, 1144, 10293, 16384, 710, 1538, 5248, 39513, 7706, 11, 30526, 929, 1229, 2316, 12034, 15069, 261, 537, 27242, 476, 1878, 26681, 45780, 37956, 13, 50668], "temperature": 0.0, "avg_logprob": -0.12639488751375222, "compression_ratio": 1.436923076923077, "no_speech_prob": 0.0007011461420916021}, {"id": 235, "seek": 87460, "start": 880.88, "end": 884.4, "text": " Wi\u0119c co to wszystko oznacza i jaka jest przysz\u0142o\u015b\u0107 wed\u0142ug tw\u00f3rc\u00f3w?", "tokens": [50678, 32508, 598, 281, 22607, 277, 22672, 326, 2394, 741, 4207, 64, 3492, 44018, 44742, 6393, 34077, 683, 15614, 29268, 30, 50854], "temperature": 0.0, "avg_logprob": -0.12639488751375222, "compression_ratio": 1.436923076923077, "no_speech_prob": 0.0007011461420916021}, {"id": 236, "seek": 87460, "start": 884.6, "end": 886.2, "text": " Gdzie widz\u0105 kolejne kroki?", "tokens": [50864, 460, 13096, 5274, 8925, 23749, 716, 45909, 2984, 30, 50944], "temperature": 0.0, "avg_logprob": -0.12639488751375222, "compression_ratio": 1.436923076923077, "no_speech_prob": 0.0007011461420916021}, {"id": 237, "seek": 87460, "start": 886.4, "end": 889.12, "text": " Planuj\u0105 dalsze prace nad sam\u0105 architektur\u0105.", "tokens": [50954, 8112, 13263, 274, 1124, 1381, 582, 617, 12617, 3247, 1611, 3912, 642, 2320, 374, 1611, 13, 51090], "temperature": 0.0, "avg_logprob": -0.12639488751375222, "compression_ratio": 1.436923076923077, "no_speech_prob": 0.0007011461420916021}, {"id": 238, "seek": 87460, "start": 889.32, "end": 893.9200000000001, "text": " Wspominaj\u0105 o d\u0105\u017ceniu do obs\u0142ugi niesko\u0144czonego kontekstu i by\u0107 mo\u017ce o", "tokens": [51100, 343, 4952, 49217, 8555, 277, 274, 1611, 24930, 5951, 360, 3181, 1221, 24780, 48100, 4093, 5248, 3689, 546, 1571, 14373, 916, 372, 84, 741, 15069, 12034, 277, 51330], "temperature": 0.0, "avg_logprob": -0.12639488751375222, "compression_ratio": 1.436923076923077, "no_speech_prob": 0.0007011461420916021}, {"id": 239, "seek": 87460, "start": 894.12, "end": 897.64, "text": " statecznym prze\u0142amaniu ogranicze\u0144 architektury Transformer.", "tokens": [51340, 1785, 3689, 12996, 8325, 20177, 25849, 34416, 30732, 49689, 3912, 642, 2320, 2598, 27938, 260, 13, 51516], "temperature": 0.0, "avg_logprob": -0.12639488751375222, "compression_ratio": 1.436923076923077, "no_speech_prob": 0.0007011461420916021}, {"id": 240, "seek": 87460, "start": 897.84, "end": 903.88, "text": " Chc\u0105 te\u017c dalej skalowa\u0107 dane i bada\u0107 zdolno\u015bci modeli do jak to uj\u0119li g\u0142\u0119bokiego", "tokens": [51526, 761, 32557, 9516, 34257, 16890, 11445, 49206, 741, 272, 1538, 2162, 16221, 401, 16438, 2316, 72, 360, 4207, 281, 344, 11115, 2081, 18117, 1274, 21666, 12200, 51828], "temperature": 0.0, "avg_logprob": -0.12639488751375222, "compression_ratio": 1.436923076923077, "no_speech_prob": 0.0007011461420916021}, {"id": 241, "seek": 90388, "start": 903.88, "end": 905.12, "text": " my\u015blenia.", "tokens": [50364, 48633, 6698, 654, 13, 50426], "temperature": 0.0, "avg_logprob": -0.16445326638388466, "compression_ratio": 1.4876325088339222, "no_speech_prob": 0.03914954885840416}, {"id": 242, "seek": 90388, "start": 905.32, "end": 907.16, "text": " Cokolwiek to dok\u0142adnie znaczy.", "tokens": [50436, 383, 49207, 44674, 281, 45864, 2766, 36584, 13, 50528], "temperature": 0.0, "avg_logprob": -0.16445326638388466, "compression_ratio": 1.4876325088339222, "no_speech_prob": 0.03914954885840416}, {"id": 243, "seek": 90388, "start": 907.36, "end": 908.4399999999999, "text": " Cokolwiek to znaczy.", "tokens": [50538, 383, 49207, 44674, 281, 36584, 13, 50592], "temperature": 0.0, "avg_logprob": -0.16445326638388466, "compression_ratio": 1.4876325088339222, "no_speech_prob": 0.03914954885840416}, {"id": 244, "seek": 90388, "start": 908.64, "end": 909.96, "text": " Ale jest co\u015b jeszcze.", "tokens": [50602, 9366, 3492, 19241, 14168, 13, 50668], "temperature": 0.0, "avg_logprob": -0.16445326638388466, "compression_ratio": 1.4876325088339222, "no_speech_prob": 0.03914954885840416}, {"id": 245, "seek": 90388, "start": 910.16, "end": 912.12, "text": " Co\u015b co wykracza poza sam model.", "tokens": [50678, 3066, 1788, 598, 39287, 12080, 2394, 714, 2394, 3247, 2316, 13, 50776], "temperature": 0.0, "avg_logprob": -0.16445326638388466, "compression_ratio": 1.4876325088339222, "no_speech_prob": 0.03914954885840416}, {"id": 246, "seek": 90388, "start": 912.32, "end": 917.28, "text": " W raporcie zawarli ca\u0142y rozdzia\u0142 sugestii dla producent\u00f3w sprz\u0119tu AI.", "tokens": [50786, 343, 5099, 284, 4260, 28165, 289, 2081, 35226, 9544, 28168, 8908, 459, 2629, 5597, 12285, 1082, 2207, 3901, 6103, 11052, 9179, 7318, 13, 51034], "temperature": 0.0, "avg_logprob": -0.16445326638388466, "compression_ratio": 1.4876325088339222, "no_speech_prob": 0.03914954885840416}, {"id": 247, "seek": 90388, "start": 917.48, "end": 918.84, "text": " O, to ciekawe.", "tokens": [51044, 422, 11, 281, 30596, 2330, 826, 13, 51112], "temperature": 0.0, "avg_logprob": -0.16445326638388466, "compression_ratio": 1.4876325088339222, "no_speech_prob": 0.03914954885840416}, {"id": 248, "seek": 90388, "start": 919.04, "end": 922.2, "text": " Proponuj\u0105 konkretne zmiany w budowie przysz\u0142ych chip\u00f3w.", "tokens": [51122, 21944, 266, 13263, 36500, 716, 43591, 88, 261, 3265, 13998, 44018, 47655, 11409, 3901, 13, 51280], "temperature": 0.0, "avg_logprob": -0.16445326638388466, "compression_ratio": 1.4876325088339222, "no_speech_prob": 0.03914954885840416}, {"id": 249, "seek": 90388, "start": 922.4, "end": 927.36, "text": " Na przyk\u0142ad wbudowany na poziomie krzemu wsparcie dla ich techniki fine grained", "tokens": [51290, 6056, 23144, 261, 18281, 23341, 1667, 38503, 40120, 15913, 24313, 84, 37647, 2181, 4260, 12285, 1893, 1537, 9850, 2489, 1295, 2001, 51538], "temperature": 0.0, "avg_logprob": -0.16445326638388466, "compression_ratio": 1.4876325088339222, "no_speech_prob": 0.03914954885840416}, {"id": 250, "seek": 90388, "start": 927.56, "end": 932.12, "text": " quantization albo lepsze mechanizmy sprz\u0119towe do ukrywania komunikacji.", "tokens": [51548, 4426, 2144, 22622, 476, 1878, 1381, 4236, 590, 2226, 6103, 11052, 83, 6880, 360, 26769, 47705, 5609, 45359, 1035, 13152, 13, 51776], "temperature": 0.0, "avg_logprob": -0.16445326638388466, "compression_ratio": 1.4876325088339222, "no_speech_prob": 0.03914954885840416}, {"id": 251, "seek": 93212, "start": 932.24, "end": 936.44, "text": " Czyli to ju\u017c nie jest jednokierunkowa ulica, gdzie tw\u00f3rcy oprogramowania", "tokens": [50370, 37099, 281, 10678, 2838, 3492, 5232, 77, 453, 811, 3197, 5528, 344, 44266, 11, 18922, 683, 15614, 1344, 999, 340, 1342, 21308, 50580], "temperature": 0.0, "avg_logprob": -0.1396300641796257, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.0029672973323613405}, {"id": 252, "seek": 93212, "start": 936.64, "end": 938.44, "text": " dostosowuj\u0105 si\u0119 do sprz\u0119tu.", "tokens": [50590, 20568, 329, 305, 13263, 3244, 360, 6103, 11052, 9179, 13, 50680], "temperature": 0.0, "avg_logprob": -0.1396300641796257, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.0029672973323613405}, {"id": 253, "seek": 93212, "start": 938.64, "end": 939.96, "text": " Zdecydowanie nie.", "tokens": [50690, 1176, 1479, 1344, 67, 22028, 2838, 13, 50756], "temperature": 0.0, "avg_logprob": -0.1396300641796257, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.0029672973323613405}, {"id": 254, "seek": 93212, "start": 940.16, "end": 942.2, "text": " Oni doszli do \u015bciany i m\u00f3wi\u0105.", "tokens": [50766, 1282, 72, 4491, 89, 2081, 360, 220, 6199, 1325, 741, 46591, 13, 50868], "temperature": 0.0, "avg_logprob": -0.1396300641796257, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.0029672973323613405}, {"id": 255, "seek": 93212, "start": 942.4, "end": 944.2, "text": " Zrobili\u015bmy co si\u0119 da\u0142o w kodzie.", "tokens": [50878, 1176, 16614, 43912, 598, 3244, 1120, 5249, 261, 350, 378, 3283, 13, 50968], "temperature": 0.0, "avg_logprob": -0.1396300641796257, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.0029672973323613405}, {"id": 256, "seek": 93212, "start": 944.4, "end": 945.96, "text": " Teraz wy producenci chip\u00f3w.", "tokens": [50978, 41810, 4628, 1082, 13037, 537, 11409, 3901, 13, 51056], "temperature": 0.0, "avg_logprob": -0.1396300641796257, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.0029672973323613405}, {"id": 257, "seek": 93212, "start": 946.16, "end": 947.88, "text": " Musicie nam pom\u00f3c p\u00f3j\u015b\u0107 dalej.", "tokens": [51066, 3569, 28434, 8835, 12991, 40993, 280, 18999, 7753, 34257, 13, 51152], "temperature": 0.0, "avg_logprob": -0.1396300641796257, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.0029672973323613405}, {"id": 258, "seek": 93212, "start": 948.08, "end": 948.92, "text": " Dok\u0142adnie.", "tokens": [51162, 29768, 10358, 2766, 13, 51204], "temperature": 0.0, "avg_logprob": -0.1396300641796257, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.0029672973323613405}, {"id": 259, "seek": 93212, "start": 949.12, "end": 954.68, "text": " To pokazuje, \u017ce dalsza optymalizacja wymaga fundamentalnych zmien nie tylko w algorytmach,", "tokens": [51214, 1407, 13010, 43317, 11, 3561, 274, 1124, 2394, 2427, 4199, 304, 590, 23395, 29764, 9286, 8088, 9399, 17020, 1053, 2838, 13219, 261, 3501, 827, 83, 76, 608, 11, 51492], "temperature": 0.0, "avg_logprob": -0.1396300641796257, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.0029672973323613405}, {"id": 260, "seek": 93212, "start": 954.88, "end": 956.24, "text": " ale i w samym krzemie.", "tokens": [51502, 6775, 741, 261, 3247, 4199, 15913, 24313, 414, 13, 51570], "temperature": 0.0, "avg_logprob": -0.1396300641796257, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.0029672973323613405}, {"id": 261, "seek": 93212, "start": 956.44, "end": 958.72, "text": " To jest sygna\u0142 dla ca\u0142ej bran\u017cy.", "tokens": [51580, 1407, 3492, 943, 70, 629, 1221, 12285, 47631, 73, 12029, 7735, 13, 51694], "temperature": 0.0, "avg_logprob": -0.1396300641796257, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.0029672973323613405}, {"id": 262, "seek": 95872, "start": 958.88, "end": 964.88, "text": " Podsumowuj\u0105c, DeepSig V3 to nie jest po prostu kolejny, pot\u0119\u017cny model na szczycie tabeli.", "tokens": [50372, 12646, 82, 449, 305, 44733, 11, 14895, 50, 328, 691, 18, 281, 2838, 3492, 714, 19518, 23749, 1634, 11, 1847, 1274, 1427, 1634, 2316, 1667, 7870, 6522, 4260, 4421, 10148, 13, 50672], "temperature": 0.0, "avg_logprob": -0.1438816272659807, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0028818107675760984}, {"id": 263, "seek": 95872, "start": 965.08, "end": 966.64, "text": " To jest manifest.", "tokens": [50682, 1407, 3492, 10067, 13, 50760], "temperature": 0.0, "avg_logprob": -0.1438816272659807, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0028818107675760984}, {"id": 264, "seek": 95872, "start": 966.84, "end": 968.12, "text": " Tak, dobre s\u0142owo.", "tokens": [50770, 9118, 11, 41959, 15116, 19941, 13, 50834], "temperature": 0.0, "avg_logprob": -0.1438816272659807, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0028818107675760984}, {"id": 265, "seek": 95872, "start": 968.32, "end": 970.24, "text": " Dow\u00f3d na to, \u017ce kluczem do post\u0119pu", "tokens": [50844, 20947, 17081, 1667, 281, 11, 3561, 9671, 1311, 24313, 360, 2183, 18085, 84, 50940], "temperature": 0.0, "avg_logprob": -0.1438816272659807, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0028818107675760984}, {"id": 266, "seek": 95872, "start": 970.44, "end": 974.08, "text": " VI jest teraz inteligentna, holistyczna optymalizacja.", "tokens": [50950, 27619, 3492, 16854, 24777, 25002, 629, 11, 4091, 468, 17466, 629, 2427, 4199, 304, 590, 23395, 13, 51132], "temperature": 0.0, "avg_logprob": -0.1438816272659807, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0028818107675760984}, {"id": 267, "seek": 95872, "start": 974.28, "end": 978.52, "text": " Na ka\u017cdym poziomie od algorytmu przez framework, a\u017c po projekt sprz\u0119tu.", "tokens": [51142, 6056, 31615, 76, 38503, 40120, 3611, 3501, 827, 83, 20140, 14064, 8388, 11, 48134, 714, 26261, 6103, 11052, 9179, 13, 51354], "temperature": 0.0, "avg_logprob": -0.1438816272659807, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0028818107675760984}, {"id": 268, "seek": 95872, "start": 978.72, "end": 983.64, "text": " To model, kt\u00f3ry jest jednocze\u015bnie pot\u0119\u017cniejsze od konkurencji i zaskakuj\u0105co ekonomiczny.", "tokens": [51364, 1407, 2316, 11, 9913, 3492, 5232, 26694, 1381, 12221, 1847, 1274, 1427, 44258, 3611, 21428, 9873, 19649, 741, 710, 3863, 514, 13263, 1291, 13359, 12481, 17946, 1634, 13, 51610], "temperature": 0.0, "avg_logprob": -0.1438816272659807, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0028818107675760984}, {"id": 269, "seek": 95872, "start": 983.84, "end": 986.9200000000001, "text": " I to prowadzi do bardzo wa\u017cnego pytania na przysz\u0142o\u015b\u0107.", "tokens": [51620, 286, 281, 36590, 3992, 360, 9034, 27777, 11858, 25878, 5609, 1667, 44018, 44742, 13, 51774], "temperature": 0.0, "avg_logprob": -0.1438816272659807, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0028818107675760984}, {"id": 270, "seek": 98692, "start": 987.0799999999999, "end": 992.24, "text": " Te sugestie dla producent\u00f3w sprz\u0119tu, o kt\u00f3rych m\u00f3wi\u0142y\u015bmy, to nie jest tylko ciekawostka.", "tokens": [50372, 1989, 459, 2629, 414, 12285, 1082, 2207, 3901, 6103, 11052, 9179, 11, 277, 30382, 24592, 6825, 10513, 11, 281, 2838, 3492, 13219, 46419, 1607, 555, 2330, 13, 50630], "temperature": 0.0, "avg_logprob": -0.13003179222155528, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.004858929198235273}, {"id": 271, "seek": 98692, "start": 992.4399999999999, "end": 994.9599999999999, "text": " To mo\u017ce by\u0107 zwiastun nowego trendu.", "tokens": [50640, 1407, 12034, 15069, 710, 6253, 525, 409, 586, 6308, 6028, 84, 13, 50766], "temperature": 0.0, "avg_logprob": -0.13003179222155528, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.004858929198235273}, {"id": 272, "seek": 98692, "start": 995.16, "end": 995.7199999999999, "text": " Jakiego?", "tokens": [50776, 15029, 12200, 30, 50804], "temperature": 0.0, "avg_logprob": -0.13003179222155528, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.004858929198235273}, {"id": 273, "seek": 98692, "start": 995.92, "end": 1003.5999999999999, "text": " Trendu, w kt\u00f3rym oprogramowanie i sprz\u0119t do AI b\u0119d\u0105 projektowane w \u015bcis\u0142ej, nierozerwalnej synergi.", "tokens": [50814, 37417, 84, 11, 261, 30120, 999, 340, 1342, 22028, 741, 6103, 11052, 83, 360, 7318, 26239, 26261, 23066, 261, 8299, 26720, 19827, 73, 11, 297, 12030, 4527, 29530, 11794, 943, 1193, 7834, 13, 51198], "temperature": 0.0, "avg_logprob": -0.13003179222155528, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.004858929198235273}, {"id": 274, "seek": 98692, "start": 1003.8, "end": 1006.16, "text": " I to jest my\u015bl, z kt\u00f3r\u0105 warto zosta\u0107.", "tokens": [51208, 286, 281, 3492, 452, 19212, 11, 710, 37415, 31830, 23154, 2162, 13, 51326], "temperature": 0.0, "avg_logprob": -0.13003179222155528, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.004858929198235273}, {"id": 275, "seek": 98692, "start": 1006.36, "end": 1011.0799999999999, "text": " Czy w miar\u0119, jak modele staj\u0105 si\u0119 coraz inteligentniejsze, a ich wymagania bardziej specyficzne,", "tokens": [51336, 19832, 261, 2752, 289, 1274, 11, 4207, 4391, 306, 342, 11133, 3244, 25899, 24777, 25002, 44258, 11, 257, 1893, 29764, 559, 5609, 27209, 768, 1344, 1786, 43077, 11, 51572], "temperature": 0.0, "avg_logprob": -0.13003179222155528, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.004858929198235273}, {"id": 276, "seek": 98692, "start": 1011.28, "end": 1016.1999999999999, "text": " to prawdziwe w\u0105skie gard\u0142o przenosi si\u0119 z projektowania algorytm\u00f3w na \u015bcis\u0142\u0105", "tokens": [51582, 281, 41175, 3992, 826, 261, 1611, 5161, 414, 5628, 5249, 582, 2904, 21521, 3244, 710, 26261, 21308, 3501, 827, 83, 76, 3901, 1667, 8299, 26720, 15926, 51828], "temperature": 0.0, "avg_logprob": -0.13003179222155528, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.004858929198235273}, {"id": 277, "seek": 101620, "start": 1016.2800000000001, "end": 1018.5200000000001, "text": " niemal symbiotyczn\u0105 wsp\u00f3\u0142prac\u0119.", "tokens": [50368, 2838, 5579, 943, 2504, 72, 6737, 3689, 13113, 39069, 1424, 326, 1274, 13, 50480], "temperature": 0.0, "avg_logprob": -0.1761541159256645, "compression_ratio": 1.338235294117647, "no_speech_prob": 0.0021167579106986523}, {"id": 278, "seek": 101620, "start": 1018.72, "end": 1022.88, "text": " Wsp\u00f3\u0142prac\u0119 przy projektowaniu oprogramowania i dedykowanego mu krzemu.", "tokens": [50490, 343, 4952, 16181, 1424, 326, 1274, 6501, 26261, 305, 25849, 999, 340, 1342, 21308, 741, 4172, 46127, 37345, 6308, 2992, 15913, 24313, 84, 13, 50698], "temperature": 0.0, "avg_logprob": -0.1761541159256645, "compression_ratio": 1.338235294117647, "no_speech_prob": 0.0021167579106986523}, {"id": 279, "seek": 101620, "start": 1023.08, "end": 1025.96, "text": " To jest fundamentalne pytanie o przysz\u0142o\u015b\u0107.", "tokens": [50708, 1407, 3492, 8088, 716, 36610, 277, 44018, 44742, 13, 50852], "temperature": 0.0, "avg_logprob": -0.1761541159256645, "compression_ratio": 1.338235294117647, "no_speech_prob": 0.0021167579106986523}, {"id": 280, "seek": 101620, "start": 1026.16, "end": 1033.44, "text": " Czy przysz\u0142o\u015b\u0107 AI to ju\u017c nie tylko kod, ale nierozerwalny duet kodu i sprz\u0119tu, stworzonych dla siebie nawzajem?", "tokens": [50862, 19832, 44018, 44742, 7318, 281, 10678, 2838, 13219, 350, 378, 11, 6775, 297, 12030, 4527, 29530, 1634, 1581, 302, 350, 34873, 741, 6103, 11052, 9179, 11, 342, 28321, 44479, 339, 12285, 39137, 18969, 89, 1805, 443, 30, 51226], "temperature": 0.0, "avg_logprob": -0.1761541159256645, "compression_ratio": 1.338235294117647, "no_speech_prob": 0.0021167579106986523}], "language": "pl"}