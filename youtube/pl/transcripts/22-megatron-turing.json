{"text": " Wiesz, tak sobie my\u015bla\u0142em, co by by\u0142o, gdyby\u015bmy wzi\u0119li jeden z tych najs\u0142ynniejszych modeli j\u0119zykowych, powiedzmy GPT-3? No dobrze, i co z nim? I zbudowali co\u015b, ale nie \u017ce o troch\u0119 wi\u0119kszego, tylko tak, trzy razy wi\u0119kszego. Ha! Kiedy pierwszy raz zobaczy\u0142em t\u0119 liczb\u0119 530 miliard\u00f3w parametr\u00f3w, to wiesz, \u015bcie\u017c\u0119 m\u00f3wi\u0105c m\u00f3j m\u00f3zg na chwil\u0119 si\u0119 po prostu zawiesi\u0142. No bo to jest liczba tak abstrakcyjna, \u017ce... Dok\u0142adnie, trudno j\u0105 sobie w og\u00f3le wyobrazi\u0107, jakie wyzwania in\u017cynierne trzeba by pokona\u0107, \u017ceby w og\u00f3le zacz\u0105\u0107 o czym\u015b takim my\u015ble\u0107. I co wa\u017cniejsze, do czego takie narz\u0119dzie by\u0142oby zdolne? Tylko wiesz, to nie jest pytanie hipotetyczne, to jest dok\u0142adnie misja, kt\u00f3rej podj\u0119li si\u0119 badacze z Microsoftu i NVIDI, a praca, kt\u00f3r\u0105 mamy dzisiaj przed sob\u0105, jest jak taki szczeg\u00f3\u0142owy dziennik pok\u0142adowy z tej wyprawy wnieznane. I w\u0142a\u015bnie tym si\u0119 dzisiaj zajmiemy. Naszym g\u0142\u00f3wnym \u017ar\u00f3d\u0142em jest praca naukowa opisuj\u0105ca Megatron Touring, NLG, 530B. Zg\u0142\u0119bimy, jak zbudowano tego ono cyfrowego tytana, co on potrafi, a tak\u017ce i to jest mo\u017ce nawet wa\u017cniejsze, jakie s\u0105 jego istotne ograniczenia. O kt\u00f3rych co wa\u017cne sami autorzy m\u00f3wi\u0105 bardzo otwarcie? Tak. To b\u0119dzie podr\u00f3\u017c przez in\u017cynieri\u0119 oprogramowania na jak\u0105\u015b kosmiczn\u0105 skal\u0119, przez analiz\u0119 danych i no fundamentalne pytania o natur\u0119 sztucznej inteligencji. To mo\u017ce zacznijmy od problemu skali, bo to ona definiuje tu wszystko inne. Jasne. Je\u015bli spojrzymy na wykres z tej pracy, kt\u00f3ry pokazuje rozw\u00f3j modeli, to wida\u0107 tam niemal prawo mura na sterydach. Zaczynamy w 2018 od ELMO, kt\u00f3ry mia\u0142 94 miliony parametr\u00f3w. Potem mamy BERT, GPT-2, wreszcie GPT-3 ze 175 miliardami. AMT-NLG to jest kolejny, po prostu gigantyczny skok na tej wyk\u0142adniczej krzywej. Ale to nie jest sztuka dla sztuki, prawda? Nikt nie buduje wi\u0119kszych modeli, \u017ceby mie\u0107 wi\u0119ksze liczby w tabelce. Absolutnie nie. Ten trend ma bardzo g\u0142\u0119bokie uzasadnienie. Konsekwentnie pokazuj\u0105, \u017ce skalowanie modeli radykalnie poprawia ich zdolno\u015bci, zw\u0142aszcza w dw\u00f3ch obszarach. Zero Shot i Fuse Shot Learning. Czyli m\u00f3wi\u0105c pro\u015bciej, wi\u0119ksze modele potrafi\u0105 robi\u0107 zadania, kt\u00f3rych nigdy nie widzia\u0142y bez przyk\u0142ad\u00f3w, albo z zaledwie kilkoma. Dok\u0142adnie. I to jest fundamentalna zmiana. Bo zamiast trenowa\u0107 osobny model do ka\u017cdego zadania, dostajemy jeden taki uniwersalny silnik poznawczy. \u017beby ten silnik w og\u00f3le odpali\u0142, potrzebna jest w\u0142a\u015bnie ta, no niewyobra\u017calna skala. I ta skala rodzi problemy, kt\u00f3re na pierwszy rzut oka wydaj\u0105 si\u0119 nierozwi\u0105zywalne. Praca wymienia dwa g\u0142\u00f3wne. Pierwszy to pami\u0119\u0107. Tak, sama pami\u0119\u0107. Wytrenowanie modelu z 530 miliardami parametr\u00f3w przy u\u017cyciu standardowych narz\u0119dzi, jak optymalizator Adam, wymaga ponad 10 terabajt\u00f3w pami\u0119ci. 10 terabajt\u00f3w. I to jest tylko na same wagi, gradienty i stany optymalizatora. Do tego dochodzi jeszcze pami\u0119\u0107 na tak zwane activations, czyli po\u015brednie wyniki oblicze\u0144. To jest jak pr\u00f3ba zmieszczenia ca\u0142ej biblioteki kongresu na jednym, nawet najpot\u0119\u017cniejszym smartfonie. To niemo\u017cliwe. \u017baden pojedynczy procesor graficzny na \u015bwiecie nie jest w stanie tego ud\u017awign\u0105\u0107. Dok\u0142adnie. A drugi problem to moc obliczeniowa. No bo mamy super komputery z tysi\u0105cami GPU, ale samo ich posiadanie to jedno. A zaprz\u0119gni\u0119cie ich do efektywnej pracy to drugie. I to jest klasyczny dylemat. Je\u015bli podzielimy prac\u0119 na zbyt ma\u0142e kawa\u0142ki, czyli damy ka\u017cdemu GPU ma\u0142y batch size, to procesory b\u0119d\u0105 si\u0119 po prostu nudzi\u0107, czekaj\u0105c na dane. Wydajno\u015b\u0107 leci na \u0142eb na szyj\u0119. A z drugiej strony? A z drugiej strony, je\u015bli spr\u00f3bujemy po\u0142\u0105czy\u0107 te ma\u0142e kawa\u0142ki w jeden gigantyczny globalny batch size, to mo\u017cemy zaszkodzi\u0107 procesowi uczenia si\u0119 modelu, pogarszaj\u0105c jego jako\u015b\u0107. To jak pr\u00f3ba nakarmienia tysi\u0105ca g\u0142odnych maszyn jednocze\u015bnie. Tak\u017ceba \u017cadna nie czeka\u0142a, ale te\u017c \u017ceby \u017cadna si\u0119 nie przejad\u0142a. Ok, czyli standardowe metody po prostu zawodz\u0105. Jak wi\u0119c autorzy sobie z tym poradzili? I tu, jak rozumiem, dochodzimy do serca tej in\u017cenieryjnej innowacji. Tak, tu dochodzimy do sedna. Bo co ciekawe, oni nie wymy\u015blili tych technik od Zela. Prawdziwy geniusz polega\u0142 na tym, jakie ze sob\u0105 po\u0142\u0105czyli, jakie zorkiestrowali. Stworzyli co\u015b, co nazwali 3D parallelizm. 3D parallelizm. Brzmi skomplikowanie. Ale mo\u017cna to sobie wyobrazi\u0107, jak taki tr\u00f3jwarstwowy tort. Ka\u017cda warstwa rozwi\u0105zuje inny problem, ale dopiero razem tworz\u0105 sp\u00f3jn\u0105 dzia\u0142aj\u0105c\u0105 ca\u0142o\u015b\u0107. Dobrze, to roz\u0142\u00f3\u017cmy ten tort na warstwy. Co jest na samym dole? Jako podstawa. Podstaw\u0105 jest data parallelizm. To najbardziej intuicyjna metoda, taki troch\u0119 brute force. Ka\u017cdy procesor graficzny dostaje pe\u0142n\u0105 kopi\u0119 ca\u0142ego modelu i przetwarza po prostu inny fragment danych. Proste. Proste, ale dla modelu z 530 miliardami parametr\u00f3w kompletnie bezu\u017cyteczne, bo \u017caden GPU nie zmie\u015bci nawet jednej kopii. To jest warstwa, kt\u00f3ra dzia\u0142a, ale tylko gdy model jest ma\u0142y. Rozumiem, czyli to rozwi\u0105zanie odpada na starcie. Co jest w takim razie drug\u0105 warstw\u0105? Druga warstwa to tensor parallelizm. I tu robi si\u0119 znacznie ciekawiej. Zamiast repikowa\u0107 ca\u0142y model kroimy na kawa\u0142ki poszczeg\u00f3lne jego operacje matematyczne, czyli tensory. I rozdzielamy te kawa\u0142ki mi\u0119dzy r\u00f3\u017cne GPU. To jakby jedn\u0105, bardzo skomplikowan\u0105 formu\u0142\u0119 matematyczn\u0105 podzieli\u0107 na mniejsze cz\u0119\u015bci i da\u0107 je r\u00f3\u017cnym osobom do obliczenia w tym samym czasie. To brzmi genialnie pod k\u0105tem oszcz\u0119dzania pami\u0119ci. Tak, ale ma swoj\u0105 cen\u0119. Wymaga absolutnie ultra szybkiej komunikacji mi\u0119dzy procesorami, bo one non-stop musz\u0105 wymienia\u0107 si\u0119 wynikami. Ka\u017cde nawet najmniejsze op\u0142ynienie zabija wydajno\u015b\u0107. Okej. A trzecia ostatnia warstwa tego tortu? To pipeline parallelizm. Tutaj dzielimy model nie w poziomie jak przetensor parallelizm, ale w pionie na kolejny bloki warstw. Tworzymy co\u015b na kszta\u0142t cyfrowej linii monta\u017cowej. Czyli jedna grupa GPU robi pierwszy etap, przekazuje wynik dalej, nast\u0119pna grupa robi drugi etap i tak dalej. Dok\u0142adnie. To \u015bwietnie radzi sobie z modelami, kt\u00f3re s\u0105 za du\u017ce, by zmie\u015bci\u0107 si\u0119 nawet na kilku GPU, ale generuje inny problem. Tak zwany bubble overhead. Bubble overhead. Tak, to jest czas, kiedy na pocz\u0105tku i na ko\u0144cu procesu niekt\u00f3re etapy linii monta\u017cowej po prostu czekaj\u0105, albo nadane, albo ju\u017c sko\u0144czy\u0142y prace. A to jest zmarnowany potencja\u0142. Ok, czyli mamy trzy techniki. Ka\u017cda ma mocne strony, ale i powa\u017cne wady. I m\u00f3wi\u0142a\u015b, \u017ce geniusz polega\u0142 na ich po\u0142\u0105czeniu. Dok\u0142adnie tak. Prawdziwy prze\u0142om to co\u015b, co nazwali topology-aware 3D mapping. Zrozumieli, \u017ce nie wystarczy w\u0142\u0105czy\u0107 tych trzech tryb\u00f3w naraz. Trzeba je inteligentnie zmapowa\u0107 na fizyczn\u0105 architektur\u0119 Superkomputera Selen\u0119. To nie jest tylko in\u017cynieria. To jest choreografia na niespotykan\u0105 skal\u0119. Choreografia? Co to znaczy w praktyce? To znaczy, \u017ce spojrzeli na budow\u0119 komputera i dopasowali do niej oprogramowanie. Wiedzieli, \u017ce tensor paralelizm wymaga najszybszej komunikacji, wi\u0119c zaimplementowali go w obr\u0119bie jednego serwera, gdzie 8 GPU jest po\u0142\u0105czonych ultra szybkim z\u0142\u0105czem and wheeling. Bo tam jest najmniejsze op\u00f3\u017anienie. W\u0142a\u015bnie. Z kolei pipeline paralelizm, kt\u00f3ry jest mniej wra\u017cliwy na op\u00f3\u017anienia, dzia\u0142a\u0142 pomi\u0119dzy r\u00f3\u017cnymi serwerami po\u0142\u0105czonymi nieco wolniejsz\u0105 sieci\u0105 InfiniBand, a na to wszystko na\u0142o\u017cyli jeszcze data paralelizm replikuj\u0105c ca\u0142\u0105 t\u0119 skomplikowan\u0105 wieloserverow\u0105 linie monta\u017cow\u0105 tyle razy, ile by\u0142o potrzeba. Czyli to nie jest tylko kwestia posiadania trzech narz\u0119dzi, ale wiedzy, kt\u00f3rego u\u017cy\u0107 gdzie i kiedy, w zale\u017cno\u015bci od tego, jak zbudowany jest sam superkomputer. To, to jest niesamowite. Jaki by\u0142 efekt ko\u0144cowy tej choreografii? Wyobra\u017a sobie, \u017ce jeden ca\u0142y model zajmowa\u0142 niemal 300 najpot\u0119\u017cniejszych procesor\u00f3w graficznych. Ka\u017cdy by\u0142 cz\u0119\u015bci\u0105 tej precyzyjnie zaplanowanej linii monta\u017cowej, a potem ca\u0142\u0105 t\u0119 konstrukcj\u0119 powielano, \u017ceby trenowa\u0107 model jeszcze szybciej. A wydajno\u015b\u0107? Osi\u0105gn\u0119li wydajno\u015b\u0107 rz\u0119du 113 do 126 teraflops na jedno GPU. To jest astronomicznie wysoki wska\u017anik bior\u0105c pod uwag\u0119 z\u0142o\u017cono\u015b\u0107 komunikacji. To jest absolutne mistrzostwo in\u017cynierii systemowej. Ok, czyli zbudowali ten niesamowity silnik. Ale silnik jest bezu\u017cyteczny, bez paliwa. Czym karmiono tego potwora? Bo sama moc obliczeniowa to, jak wiemy, nie wszystko. Zdecydowanie. I tu jest kolejny kluczowy element, bo to nie by\u0142 po prostu, wiesz, losowy zrzut internetu. Autorzy bardzo starannie skomponowali diet\u0119 dla swojego modelu. Wybrali najlepsze i jako\u015bciowo zbiory danych z kolekcji Depile. Czyli ksi\u0105\u017cki, artyku\u0142y naukowe, kod z GitHub'a. Tak. Generalnie teksty o wysokiej warto\u015bci. Ale du\u017c\u0105 cz\u0119\u015b\u0107 stanowi\u0142 te\u017c os\u0142awiony Common Crawl, czyli w zasadzie surowy, nieprzefiltrowany internet. Ze wszystkim, co w nim dobre i z\u0142e. Jak sobie z tym poradzili? I pierw przepu\u015bcili te dane przez swego rodzaju bramkarza. Inteligentny, klasyfikator, fast text, wytrenowany na tekstach wzorowej jako\u015bci, jak Wikipedia. I ten bramkarz odrzuca\u0142 wszystko, co wygl\u0105da\u0142o na internetowy be\u0142kot albo spam. Dok\u0142adnie. Zosta\u0142o tylko \u015bmietanka. A drugi etap? Drugi etap to fazy deduplication przy u\u017cyciu techniki Locality Sensitive Hashing. W skr\u00f3cie LSH. Czekaj, czyli ten system potrafi\u0142 znale\u017a\u0107 dwa artyku\u0142y na ten sam temat. Nawet je\u015bli by\u0142y napisane zupe\u0142nie innymi s\u0142owami. I uznacie za zbyt podobne? To jest o wiele bardziej zaawansowane ni\u017c zwyk\u0142e szukanie plagiat\u00f3w. Tak, dok\u0142adnie o to chodzi\u0142o. O r\u00f3\u017cnorodno\u015b\u0107 semantyczn\u0105, a nie tylko o unikanie kopii i wklei. Internet jest pe\u0142en powt\u00f3rze\u0144, a karmienie model\u00f3w k\u00f3\u0142ko tymi samymi informacjami, tylko ubranymi winne s\u0142owa, jest nieefektywne. I co bardzo wa\u017cne z naukowego punktu widzenia, u\u017cyli filtr\u00f3w opartych na n gramach. Po co? \u017beby usun\u0105\u0107 z danych treningowych fragmenty, kt\u00f3re mog\u0142yby pokrywa\u0107 si\u0119 z zadaniami testowymi. Czyli, m\u00f3wi\u0105c pro\u015bciej, zadbali o to, \u017ceby model nie m\u00f3g\u0142 \u015bci\u0105ga\u0107 na egzaminie, bo widzia\u0142 wcze\u015bniej pytania. W\u0142a\u015bnie tak. To gwarantuje, \u017ce wyniki, kt\u00f3re uzyskali, s\u0105 miarodajne i faktycznie odzwierciedlaj\u0105 zdolno\u015bci modelu. Dobrze, a wi\u0119c mamy pot\u0119\u017cn\u0105 in\u017cynieri\u0119 i starannie przygotowane dane. Czy ta gigantyczna inwestycja si\u0119 op\u0142aci\u0142a? Jakie by\u0142y rezultaty? W wielu zadaniach model ustanowi\u0142 nowe rekordy, czyli tak zwane SOTA \u2013 State of the Art. Najbardziej spektakularny przyk\u0142ad to chyba zadanie LAMBADA. LAMBADA, czyli ten test przewidywania ostatniego s\u0142owa w zdaniu. Tak, kt\u00f3ry wymaga g\u0142\u0119bokiego zrozumienia kontekstu. I tutaj MTNLG w trybie ZEROSHOT, czyli bez \u017cadnych przyk\u0142ad\u00f3w, osi\u0105gn\u0105\u0142 dok\u0142adno\u015b\u0107 prawie 80%. 80% bez przyk\u0142ad\u00f3w? Tak, i to jest wynik lepszy ni\u017c ten, kt\u00f3ry GPT-3 osi\u0105gn\u0105\u0142 w trybie FUSHOT, czyli po zobaczeniu kilku przyk\u0142ad\u00f3w. Ok, pobi\u0142 rekord w zadaniu LAMBADA. Ale wiesz, czy to nie jest troch\u0119 tak, \u017ce my po prostu budujemy coraz wi\u0119ksze m\u0142otki do wbijania tych samych akademickich gwo\u017adzi? Czy taki wynik naprawd\u0119 przek\u0142ada si\u0119 na lepsze rozumienie \u015bwiata w praktyce? To jest \u015bwietnie pytanie i autorzy te\u017c je sobie zadali. Dlatego poza suchymi liczbami pokazali te\u017c jako\u015bciowe przyk\u0142ady, kt\u00f3re maj\u0105 pokaza\u0107 kreatywno\u015b\u0107 i elastyczno\u015b\u0107 modelu. I one musz\u0119 przyzna\u0107, robi\u0105 wra\u017cenie. Model potrafi\u0142 rozwi\u0105zywa\u0107 zagadki, kt\u00f3re jeden z badaczy sam wymier\u015bli\u0142, \u017ceby mie\u0107 pewno\u015b\u0107, \u017ce nie ma ich w danych treningowych. Odpowiada\u0142 na pytania z teleturnieju je DROPERDEE, co wymaga nie tylko wiedzy, ale te\u017c zrozumienia tej specyficznej odwr\u00f3conej sk\u0142adni. Czyta\u0142em te\u017c o generowaniu kodu. To ju\u017c w og\u00f3le brzmi jak magia. Potrafi\u0142 wygenerowa\u0107 dzia\u0142aj\u0105cy program na podstawie samego komentarza. Tak i to jest chyba najbardziej uderzaj\u0105cy przyk\u0142ad. Poproszono go o napisanie funkcji w Pajtonie, kt\u00f3ra oblicza odleg\u0142o\u015b\u0107 Levensteina. Czyli miar\u0119 podobie\u0144stwa mi\u0119dzy dwoma tekstami. To nie jest trywialny algorytm. Zdecydowanie nie. A on wygenerowa\u0142 kod, kt\u00f3ry nie tylko dzia\u0142a\u0142, ale by\u0142 te\u017c czysty i zgodny z dobrymi praktykami. To pokazuje, \u017ce on nie tylko rozumie j\u0119zyk naturalny, ale potrafi to prze\u0142o\u017cy\u0107 na j\u0119zyk formalny, jakim jest kod programistyczny. Ok, to wszystko brzmi jak historia ogromnego sukcesu. Prawie. Prawie zbyt idealnie. Czytaj\u0105c te prace czeka\u0142em na moment, w kt\u00f3rym autorzy powiedz\u0105 ale i ten moment nadsze\u0142, prawda, po\u015bwi\u0119cili ca\u0142y rozdzia\u0142 na problemy i ograniczenia. I to jest moim zdaniem jeden z najmocniejszych punkt\u00f3w tej publikacji. Naukowa uczciwo\u015b\u0107. Nie pr\u00f3bowali zamiata\u0107 problem\u00f3w pod dywan. Wr\u0119cz przeciwnie bardzo szczeg\u00f3\u0142owo je przeanalizowali. Ale zanim do nich przyjdziemy, jest jeszcze jedno ciekawe pozytywne odkrycie. Dotyczy ono tego, czy model naprawd\u0119 rozumie j\u0119zyk, czy tylko odtwarza wzorce. Tak, to fundamentalne pytanie. U\u017cyli do tego specjalnego zbioru danych o nazwie Hans. On jest tak skonstruowany, \u017ceby testowa\u0107, czy model polega na powierzchniowych heurystykach. Na przyk\u0142ad, czy uwa\u017ca, \u017ce dwa zdania s\u0105 powi\u0105zane, bo maj\u0105 du\u017co wsp\u00f3lnych s\u0142\u00f3w? Dok\u0142adnie, to cz\u0119sto pu\u0142apka. Testowali, czy model faktycznie rozumie sk\u0142adnie i gramatyk\u0119. I tutaj skala przynios\u0142a jako\u015bciow\u0105 zmian\u0119. MTNLG radzi sobie znacznie lepiej ni\u017c mniejsze modele, jak GPT-2. Mniej daje si\u0119 nabra\u0107 na te proste sztuczki. To mocna przes\u0142anka, \u017ce wi\u0119ksze modele faktycznie rozwijaj\u0105 g\u0142\u0119bsze zrozumienie systematyczno\u015bci j\u0119zyka. Czyli wi\u0119kszy nie znaczy tylko wi\u0119cej tego samego, ale co\u015b jako\u015bciowo innego. Wydaje si\u0119, \u017ce tak. Ale jest te\u017c druga strona medalu. Okazuje si\u0119, \u017ce to s\u0142ynne in-context learning wcale nie jest magicznym rozwi\u0105zaniem. Absolutnie nie. I to jest kolejne takie troch\u0119 odsze\u017awiaj\u0105ce odkrycie. Eksperymenty pokaza\u0142y, \u017ce ten mechanizm dzia\u0142a bardzo podobnie do tradycyjnego fine tuning. To znaczy? To znaczy, \u017ce jako\u015b\u0107 i rozk\u0142ad przyk\u0142ad\u00f3w, kt\u00f3re podamy modelowi w pr\u0105bcie, ma kluczowe znaczenie dla wyniku. Je\u015bli damy mu przyk\u0142ady, kt\u00f3re s\u0105 niereprezentatywne albo wr\u0119cz b\u0142\u0119dne, model mo\u017ce przestroi\u0107 si\u0119 w locie i zacz\u0105\u0107 odpowiada\u0107 niepoprawnie. Mo\u017cna go niejako przeuczy\u0107 na podstawie zaledwie kilku przyk\u0142ad\u00f3w. Dok\u0142adnie. To pokazuje, jak bardzo jest wra\u017cliwy na kontekst, kt\u00f3ry mu dostarczamy. Prze\u015bledzili\u015bmy wi\u0119c ca\u0142\u0105 drog\u0119 od gigantycznych wyzwa\u0144 in\u017cyniernych przez imponuj\u0105ce wyniki, a\u017c pot\u0119powa\u017cne i odsze\u017awiaj\u0105ce wady. Co to wszystko oznacza, je\u015bli spojrzymy na to z szerszej perspektywy? My\u015bl\u0119, \u017ce ta praca jest kamieniem milowym. Nie tylko ze wzgl\u0119du na sam\u0105 skal\u0119, kt\u00f3ra przesuwa granice tego, co mo\u017cliwe, ale w\u0142a\u015bnie ze wzgl\u0119du na t\u0119 naukow\u0105 uczciwo\u015b\u0107 w przedstawianiu problem\u00f3w. Ale z pewno\u015bci\u0105 nie jest warunkiem wystarczaj\u0105cym. Czyli przysz\u0142o\u015b\u0107 to nie tylko wi\u0119ksze, ale przede wszystkim m\u0105drzejsze i bardziej niezawodne modele. Dok\u0142adnie. Praca nad architektur\u0105, nad jako\u015bci\u0105 danych, a przede wszystkim nad zwi\u0119kszaniem niezawodno\u015bci i rozumieniem kiedy i dlaczego model pope\u0142nia b\u0142\u0119dy, jest r\u00f3wnie, je\u015bli nie bardziej istotna ni\u017c samoskalowanie. Ta publikacja to zar\u00f3wno celebracja niesamowitych osi\u0105gni\u0119\u0107 in\u017cyniernych, jak i bardzo powa\u017cne wezwanie do dzia\u0142ania dla ca\u0142ej spo\u0142eczno\u015bci badawczej. Na koniec mo\u017ce zostawmy naszym s\u0142uchaczom my\u015bl do refleksji, kt\u00f3ra wy\u0142ania si\u0119 z tej pracy. Hmm, my\u015bl\u0119, \u017ce jest jedna bardzo ciekawa. Ta praca pokazuje, \u017ce nawet pot\u0119\u017cny, wytrenowany na bilionach s\u0142\u00f3w model AI, mo\u017ce zosta\u0107 przeuczony i zacz\u0105\u0107 pope\u0142nia\u0107 systematyczne b\u0142\u0119dy na podstawie zaledwie kilku tendencyjnie dobranych przyk\u0142ad\u00f3w, kt\u00f3re podamy mu w pr\u0105cie. Racja. Wi\u0119c skoro tak \u0142atwo jest p\u0142yn\u0105\u0107 na maszyn\u0119, kt\u00f3ra przetworzy\u0142a wi\u0119cej tekstu ni\u017c jakikolwiek cz\u0142owiek w historii, to co to m\u00f3wi o naszej ludzkiej sk\u0142onno\u015bci do wyci\u0105gania daleko id\u0105cych wniosk\u00f3w na podstawie bardzo ograniczonej i cz\u0119sto stronniczej pr\u00f3bki informacji, z kt\u00f3r\u0105 spotykamy si\u0119 na co dzie\u0144.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.0, "text": " Wiesz, tak sobie my\u015bla\u0142em, co by by\u0142o, gdyby\u015bmy wzi\u0119li jeden z tych najs\u0142ynniejszych modeli j\u0119zykowych, powiedzmy GPT-3?", "tokens": [50364, 343, 15347, 11, 991, 13652, 48633, 875, 11126, 11, 598, 538, 14811, 11, 28405, 2322, 10513, 261, 16706, 2081, 12906, 710, 15180, 11212, 82, 1221, 2534, 10402, 45021, 2316, 72, 49055, 74, 19605, 11, 27617, 2226, 26039, 51, 12, 18, 30, 50864], "temperature": 0.0, "avg_logprob": -0.1657613587859493, "compression_ratio": 1.4104234527687296, "no_speech_prob": 0.008850537240505219}, {"id": 1, "seek": 0, "start": 10.0, "end": 11.5, "text": " No dobrze, i co z nim?", "tokens": [50864, 883, 28335, 11, 741, 598, 710, 24887, 30, 50939], "temperature": 0.0, "avg_logprob": -0.1657613587859493, "compression_ratio": 1.4104234527687296, "no_speech_prob": 0.008850537240505219}, {"id": 2, "seek": 0, "start": 11.5, "end": 18.0, "text": " I zbudowali co\u015b, ale nie \u017ce o troch\u0119 wi\u0119kszego, tylko tak, trzy razy wi\u0119kszego.", "tokens": [50939, 286, 710, 18281, 305, 5103, 19241, 11, 6775, 2838, 3561, 277, 24926, 29968, 27725, 11, 13219, 991, 11, 34573, 9639, 88, 29968, 27725, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1657613587859493, "compression_ratio": 1.4104234527687296, "no_speech_prob": 0.008850537240505219}, {"id": 3, "seek": 0, "start": 18.0, "end": 18.5, "text": " Ha!", "tokens": [51264, 4064, 0, 51289], "temperature": 0.0, "avg_logprob": -0.1657613587859493, "compression_ratio": 1.4104234527687296, "no_speech_prob": 0.008850537240505219}, {"id": 4, "seek": 0, "start": 18.5, "end": 26.5, "text": " Kiedy pierwszy raz zobaczy\u0142em t\u0119 liczb\u0119 530 miliard\u00f3w parametr\u00f3w, to wiesz, \u015bcie\u017c\u0119 m\u00f3wi\u0105c m\u00f3j m\u00f3zg na chwil\u0119 si\u0119 po prostu zawiesi\u0142.", "tokens": [51289, 591, 16446, 34016, 9639, 37273, 11126, 32489, 6169, 89, 65, 1274, 1025, 3446, 1962, 72, 515, 3901, 6220, 27965, 3901, 11, 281, 261, 15347, 11, 8299, 40082, 1274, 46591, 66, 275, 18999, 32515, 89, 70, 1667, 41941, 1274, 3244, 714, 19518, 28165, 530, 40622, 13, 51689], "temperature": 0.0, "avg_logprob": -0.1657613587859493, "compression_ratio": 1.4104234527687296, "no_speech_prob": 0.008850537240505219}, {"id": 5, "seek": 0, "start": 26.5, "end": 28.5, "text": " No bo to jest liczba tak abstrakcyjna, \u017ce...", "tokens": [51689, 883, 748, 281, 3492, 6169, 89, 4231, 991, 10823, 11272, 42949, 629, 11, 3561, 485, 51789], "temperature": 0.0, "avg_logprob": -0.1657613587859493, "compression_ratio": 1.4104234527687296, "no_speech_prob": 0.008850537240505219}, {"id": 6, "seek": 2850, "start": 28.5, "end": 35.5, "text": " Dok\u0142adnie, trudno j\u0105 sobie w og\u00f3le wyobrazi\u0107, jakie wyzwania in\u017cynierne trzeba by pokona\u0107, \u017ceby w og\u00f3le zacz\u0105\u0107 o czym\u015b takim my\u015ble\u0107.", "tokens": [50364, 29768, 10358, 2766, 11, 32007, 1771, 35692, 13652, 261, 29229, 4628, 24393, 28496, 11, 22124, 4628, 14406, 5609, 294, 1427, 2534, 811, 716, 25860, 538, 13010, 4037, 2162, 11, 11316, 261, 29229, 34430, 8925, 2162, 277, 31466, 1788, 31732, 48633, 306, 2162, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07196558959095191, "compression_ratio": 1.4622950819672131, "no_speech_prob": 0.06230496987700462}, {"id": 7, "seek": 2850, "start": 35.5, "end": 38.5, "text": " I co wa\u017cniejsze, do czego takie narz\u0119dzie by\u0142oby zdolne?", "tokens": [50714, 286, 598, 27777, 44258, 11, 360, 36559, 15963, 6714, 89, 42643, 16673, 13944, 16221, 401, 716, 30, 50864], "temperature": 0.0, "avg_logprob": -0.07196558959095191, "compression_ratio": 1.4622950819672131, "no_speech_prob": 0.06230496987700462}, {"id": 8, "seek": 2850, "start": 38.5, "end": 45.5, "text": " Tylko wiesz, to nie jest pytanie hipotetyczne, to jest dok\u0142adnie misja, kt\u00f3rej podj\u0119li si\u0119 badacze z Microsoftu i NVIDI,", "tokens": [50864, 49286, 4093, 261, 15347, 11, 281, 2838, 3492, 36610, 8103, 310, 2210, 38491, 11, 281, 3492, 45864, 2766, 3346, 2938, 11, 36023, 2497, 11115, 2081, 3244, 1578, 326, 1381, 710, 8116, 84, 741, 426, 3958, 40, 11, 51214], "temperature": 0.0, "avg_logprob": -0.07196558959095191, "compression_ratio": 1.4622950819672131, "no_speech_prob": 0.06230496987700462}, {"id": 9, "seek": 2850, "start": 45.5, "end": 51.5, "text": " a praca, kt\u00f3r\u0105 mamy dzisiaj przed sob\u0105, jest jak taki szczeg\u00f3\u0142owy dziennik pok\u0142adowy z tej wyprawy wnieznane.", "tokens": [51214, 257, 582, 6628, 11, 37415, 17335, 25772, 18334, 18253, 1611, 11, 3492, 4207, 20065, 22090, 1146, 16181, 10089, 9758, 1053, 13123, 13010, 10358, 10089, 710, 12573, 46392, 5131, 88, 261, 2766, 22672, 1929, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07196558959095191, "compression_ratio": 1.4622950819672131, "no_speech_prob": 0.06230496987700462}, {"id": 10, "seek": 5150, "start": 51.5, "end": 61.5, "text": " I w\u0142a\u015bnie tym si\u0119 dzisiaj zajmiemy. Naszym g\u0142\u00f3wnym \u017ar\u00f3d\u0142em jest praca naukowa opisuj\u0105ca Megatron Touring, NLG, 530B.", "tokens": [50364, 286, 14234, 8107, 3244, 25772, 33729, 25210, 2226, 13, 16151, 26681, 18117, 812, 895, 4199, 50212, 43678, 11126, 3492, 582, 6628, 35616, 74, 5528, 45477, 13263, 496, 9986, 267, 2044, 13077, 278, 11, 426, 43, 38, 11, 1025, 3446, 33, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08242904118129185, "compression_ratio": 1.3656957928802589, "no_speech_prob": 0.33891066908836365}, {"id": 11, "seek": 5150, "start": 61.5, "end": 71.5, "text": " Zg\u0142\u0119bimy, jak zbudowano tego ono cyfrowego tytana, co on potrafi, a tak\u017ce i to jest mo\u017ce nawet wa\u017cniejsze, jakie s\u0105 jego istotne ograniczenia.", "tokens": [50864, 1176, 70, 46564, 65, 13189, 11, 4207, 710, 18281, 305, 3730, 8627, 322, 78, 3185, 69, 1892, 6308, 1104, 83, 2095, 11, 598, 322, 1847, 10437, 72, 11, 257, 23306, 741, 281, 3492, 12034, 22696, 27777, 44258, 11, 22124, 9015, 26542, 1418, 310, 716, 34416, 30732, 14320, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08242904118129185, "compression_ratio": 1.3656957928802589, "no_speech_prob": 0.33891066908836365}, {"id": 12, "seek": 5150, "start": 71.5, "end": 74.5, "text": " O kt\u00f3rych co wa\u017cne sami autorzy m\u00f3wi\u0105 bardzo otwarcie?", "tokens": [51364, 422, 30382, 598, 46110, 3247, 72, 19510, 1229, 46591, 9034, 4337, 6925, 4260, 30, 51514], "temperature": 0.0, "avg_logprob": -0.08242904118129185, "compression_ratio": 1.3656957928802589, "no_speech_prob": 0.33891066908836365}, {"id": 13, "seek": 5150, "start": 74.5, "end": 79.5, "text": " Tak. To b\u0119dzie podr\u00f3\u017c przez in\u017cynieri\u0119 oprogramowania na jak\u0105\u015b kosmiczn\u0105 skal\u0119,", "tokens": [51514, 9118, 13, 1407, 10562, 2497, 11721, 1427, 14064, 294, 1427, 2534, 811, 5034, 999, 340, 1342, 21308, 1667, 46719, 1788, 19532, 13195, 89, 13113, 16890, 1274, 11, 51764], "temperature": 0.0, "avg_logprob": -0.08242904118129185, "compression_ratio": 1.3656957928802589, "no_speech_prob": 0.33891066908836365}, {"id": 14, "seek": 7950, "start": 79.5, "end": 84.5, "text": " przez analiz\u0119 danych i no fundamentalne pytania o natur\u0119 sztucznej inteligencji.", "tokens": [50364, 14064, 2624, 590, 1274, 274, 34644, 741, 572, 8088, 716, 25878, 5609, 277, 26389, 1274, 262, 2682, 1311, 89, 11794, 24777, 3213, 19649, 13, 50614], "temperature": 0.0, "avg_logprob": -0.06678588119978757, "compression_ratio": 1.3349056603773586, "no_speech_prob": 0.027037041261792183}, {"id": 15, "seek": 7950, "start": 84.5, "end": 89.5, "text": " To mo\u017ce zacznijmy od problemu skali, bo to ona definiuje tu wszystko inne.", "tokens": [50614, 1407, 12034, 710, 14875, 77, 1718, 2226, 3611, 1154, 84, 1110, 5103, 11, 748, 281, 20325, 1561, 5951, 2884, 2604, 22607, 24170, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06678588119978757, "compression_ratio": 1.3349056603773586, "no_speech_prob": 0.027037041261792183}, {"id": 16, "seek": 7950, "start": 89.5, "end": 90.5, "text": " Jasne.", "tokens": [50864, 34023, 716, 13, 50914], "temperature": 0.0, "avg_logprob": -0.06678588119978757, "compression_ratio": 1.3349056603773586, "no_speech_prob": 0.027037041261792183}, {"id": 17, "seek": 7950, "start": 90.5, "end": 97.5, "text": " Je\u015bli spojrzymy na wykres z tej pracy, kt\u00f3ry pokazuje rozw\u00f3j modeli, to wida\u0107 tam niemal prawo mura na sterydach.", "tokens": [50914, 37086, 8243, 73, 13047, 2226, 1667, 39287, 495, 710, 12573, 35591, 11, 9913, 13010, 43317, 9544, 86, 18999, 2316, 72, 11, 281, 261, 46898, 7677, 2838, 5579, 3206, 6120, 275, 2991, 1667, 342, 2109, 67, 608, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06678588119978757, "compression_ratio": 1.3349056603773586, "no_speech_prob": 0.027037041261792183}, {"id": 18, "seek": 9750, "start": 98.5, "end": 103.5, "text": " Zaczynamy w 2018 od ELMO, kt\u00f3ry mia\u0142 94 miliony parametr\u00f3w.", "tokens": [50414, 1176, 14691, 5378, 88, 261, 6096, 3611, 14426, 18976, 11, 9913, 27989, 30849, 1962, 46184, 6220, 27965, 3901, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07492353491587182, "compression_ratio": 1.3103448275862069, "no_speech_prob": 0.4891551434993744}, {"id": 19, "seek": 9750, "start": 103.5, "end": 109.5, "text": " Potem mamy BERT, GPT-2, wreszcie GPT-3 ze 175 miliardami.", "tokens": [50664, 9145, 443, 17335, 363, 31479, 11, 26039, 51, 12, 17, 11, 261, 495, 89, 4260, 26039, 51, 12, 18, 5277, 41165, 1962, 72, 515, 4526, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07492353491587182, "compression_ratio": 1.3103448275862069, "no_speech_prob": 0.4891551434993744}, {"id": 20, "seek": 9750, "start": 109.5, "end": 115.5, "text": " AMT-NLG to jest kolejny, po prostu gigantyczny skok na tej wyk\u0142adniczej krzywej.", "tokens": [50964, 6475, 51, 12, 45, 43, 38, 281, 3492, 23749, 1634, 11, 714, 19518, 8741, 394, 17466, 1634, 1110, 453, 1667, 12573, 4628, 15317, 7692, 16920, 350, 13047, 826, 73, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07492353491587182, "compression_ratio": 1.3103448275862069, "no_speech_prob": 0.4891551434993744}, {"id": 21, "seek": 9750, "start": 115.5, "end": 117.5, "text": " Ale to nie jest sztuka dla sztuki, prawda?", "tokens": [51264, 9366, 281, 2838, 3492, 262, 2682, 13599, 12285, 262, 2682, 11788, 11, 43607, 30, 51364], "temperature": 0.0, "avg_logprob": -0.07492353491587182, "compression_ratio": 1.3103448275862069, "no_speech_prob": 0.4891551434993744}, {"id": 22, "seek": 9750, "start": 117.5, "end": 121.5, "text": " Nikt nie buduje wi\u0119kszych modeli, \u017ceby mie\u0107 wi\u0119ksze liczby w tabelce.", "tokens": [51364, 426, 9874, 2838, 3265, 13008, 29968, 28051, 2316, 72, 11, 11316, 35612, 29968, 1381, 6169, 89, 2322, 261, 4421, 338, 384, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07492353491587182, "compression_ratio": 1.3103448275862069, "no_speech_prob": 0.4891551434993744}, {"id": 23, "seek": 9750, "start": 121.5, "end": 122.5, "text": " Absolutnie nie.", "tokens": [51564, 5813, 2308, 2766, 2838, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07492353491587182, "compression_ratio": 1.3103448275862069, "no_speech_prob": 0.4891551434993744}, {"id": 24, "seek": 9750, "start": 122.5, "end": 125.5, "text": " Ten trend ma bardzo g\u0142\u0119bokie uzasadnienie.", "tokens": [51614, 9380, 6028, 463, 9034, 18117, 1274, 21666, 414, 16851, 296, 345, 77, 27385, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07492353491587182, "compression_ratio": 1.3103448275862069, "no_speech_prob": 0.4891551434993744}, {"id": 25, "seek": 12550, "start": 125.5, "end": 131.5, "text": " Konsekwentnie pokazuj\u0105, \u017ce skalowanie modeli radykalnie poprawia ich zdolno\u015bci, zw\u0142aszcza w dw\u00f3ch obszarach.", "tokens": [50364, 591, 3739, 74, 34798, 2766, 13010, 921, 13263, 11, 3561, 16890, 22028, 2316, 72, 367, 880, 19990, 2766, 1665, 5131, 654, 1893, 16221, 401, 16438, 11, 11873, 1221, 19601, 41524, 261, 27379, 812, 339, 3181, 26236, 608, 13, 50664], "temperature": 0.0, "avg_logprob": -0.0979249443806393, "compression_ratio": 1.430921052631579, "no_speech_prob": 0.022537961602211}, {"id": 26, "seek": 12550, "start": 131.5, "end": 133.5, "text": " Zero Shot i Fuse Shot Learning.", "tokens": [50664, 17182, 28845, 741, 479, 438, 28845, 15205, 13, 50764], "temperature": 0.0, "avg_logprob": -0.0979249443806393, "compression_ratio": 1.430921052631579, "no_speech_prob": 0.022537961602211}, {"id": 27, "seek": 12550, "start": 133.5, "end": 141.5, "text": " Czyli m\u00f3wi\u0105c pro\u015bciej, wi\u0119ksze modele potrafi\u0105 robi\u0107 zadania, kt\u00f3rych nigdy nie widzia\u0142y bez przyk\u0142ad\u00f3w, albo z zaledwie kilkoma.", "tokens": [50764, 37099, 46591, 66, 447, 9815, 73, 11, 29968, 1381, 4391, 306, 1847, 10437, 11404, 46900, 42788, 5609, 11, 30382, 26996, 3173, 2838, 27486, 654, 6825, 10782, 23144, 3901, 11, 22622, 710, 710, 5573, 8699, 5128, 74, 6440, 13, 51164], "temperature": 0.0, "avg_logprob": -0.0979249443806393, "compression_ratio": 1.430921052631579, "no_speech_prob": 0.022537961602211}, {"id": 28, "seek": 12550, "start": 141.5, "end": 142.5, "text": " Dok\u0142adnie.", "tokens": [51164, 29768, 10358, 2766, 13, 51214], "temperature": 0.0, "avg_logprob": -0.0979249443806393, "compression_ratio": 1.430921052631579, "no_speech_prob": 0.022537961602211}, {"id": 29, "seek": 12550, "start": 142.5, "end": 144.5, "text": " I to jest fundamentalna zmiana.", "tokens": [51214, 286, 281, 3492, 8088, 629, 17020, 8497, 13, 51314], "temperature": 0.0, "avg_logprob": -0.0979249443806393, "compression_ratio": 1.430921052631579, "no_speech_prob": 0.022537961602211}, {"id": 30, "seek": 12550, "start": 144.5, "end": 152.5, "text": " Bo zamiast trenowa\u0107 osobny model do ka\u017cdego zadania, dostajemy jeden taki uniwersalny silnik poznawczy.", "tokens": [51314, 3286, 710, 4526, 525, 23136, 11445, 41518, 1634, 2316, 360, 21912, 67, 6308, 42788, 5609, 11, 20568, 1805, 3633, 12906, 20065, 36435, 5364, 304, 1634, 3425, 13123, 21281, 629, 86, 6522, 13, 51714], "temperature": 0.0, "avg_logprob": -0.0979249443806393, "compression_ratio": 1.430921052631579, "no_speech_prob": 0.022537961602211}, {"id": 31, "seek": 15250, "start": 152.5, "end": 157.5, "text": " \u017beby ten silnik w og\u00f3le odpali\u0142, potrzebna jest w\u0142a\u015bnie ta, no niewyobra\u017calna skala.", "tokens": [50364, 46864, 2322, 2064, 3425, 13123, 261, 29229, 3611, 79, 5103, 1221, 11, 37595, 629, 3492, 14234, 1846, 11, 572, 43622, 88, 24393, 1427, 304, 629, 1110, 5159, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07024892819981997, "compression_ratio": 1.4290657439446366, "no_speech_prob": 0.05531228333711624}, {"id": 32, "seek": 15250, "start": 157.5, "end": 162.5, "text": " I ta skala rodzi problemy, kt\u00f3re na pierwszy rzut oka wydaj\u0105 si\u0119 nierozwi\u0105zywalne.", "tokens": [50614, 286, 1846, 1110, 5159, 8685, 3992, 1154, 88, 11, 8864, 1667, 34016, 367, 89, 325, 277, 2330, 25984, 11133, 3244, 297, 12030, 89, 18234, 1229, 29530, 716, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07024892819981997, "compression_ratio": 1.4290657439446366, "no_speech_prob": 0.05531228333711624}, {"id": 33, "seek": 15250, "start": 162.5, "end": 164.5, "text": " Praca wymienia dwa g\u0142\u00f3wne.", "tokens": [50864, 2114, 6628, 29764, 18811, 35045, 18117, 3901, 716, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07024892819981997, "compression_ratio": 1.4290657439446366, "no_speech_prob": 0.05531228333711624}, {"id": 34, "seek": 15250, "start": 164.5, "end": 165.5, "text": " Pierwszy to pami\u0119\u0107.", "tokens": [50964, 16676, 30012, 281, 31088, 2162, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07024892819981997, "compression_ratio": 1.4290657439446366, "no_speech_prob": 0.05531228333711624}, {"id": 35, "seek": 15250, "start": 165.5, "end": 166.5, "text": " Tak, sama pami\u0119\u0107.", "tokens": [51014, 9118, 11, 17768, 31088, 2162, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07024892819981997, "compression_ratio": 1.4290657439446366, "no_speech_prob": 0.05531228333711624}, {"id": 36, "seek": 15250, "start": 166.5, "end": 177.5, "text": " Wytrenowanie modelu z 530 miliardami parametr\u00f3w przy u\u017cyciu standardowych narz\u0119dzi, jak optymalizator Adam, wymaga ponad 10 terabajt\u00f3w pami\u0119ci.", "tokens": [51064, 343, 4328, 1095, 22028, 2316, 84, 710, 1025, 3446, 1962, 72, 515, 4526, 6220, 27965, 3901, 6501, 34097, 30795, 3832, 19605, 6714, 89, 6298, 3992, 11, 4207, 2427, 4199, 304, 590, 1639, 7938, 11, 29764, 9286, 9224, 345, 1266, 1796, 455, 1805, 83, 3901, 31088, 537, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07024892819981997, "compression_ratio": 1.4290657439446366, "no_speech_prob": 0.05531228333711624}, {"id": 37, "seek": 15250, "start": 177.5, "end": 178.5, "text": " 10 terabajt\u00f3w.", "tokens": [51614, 1266, 1796, 455, 1805, 83, 3901, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07024892819981997, "compression_ratio": 1.4290657439446366, "no_speech_prob": 0.05531228333711624}, {"id": 38, "seek": 17850, "start": 178.5, "end": 182.5, "text": " I to jest tylko na same wagi, gradienty i stany optymalizatora.", "tokens": [50364, 286, 281, 3492, 13219, 1667, 912, 261, 20291, 11, 16235, 88, 741, 342, 1325, 2427, 4199, 304, 590, 1639, 64, 13, 50564], "temperature": 0.0, "avg_logprob": -0.052946687578321334, "compression_ratio": 1.4246575342465753, "no_speech_prob": 0.0745578333735466}, {"id": 39, "seek": 17850, "start": 182.5, "end": 187.5, "text": " Do tego dochodzi jeszcze pami\u0119\u0107 na tak zwane activations, czyli po\u015brednie wyniki oblicze\u0144.", "tokens": [50564, 1144, 8627, 9243, 14543, 14168, 31088, 2162, 1667, 991, 11873, 1929, 2430, 763, 11, 16591, 714, 1788, 986, 2766, 31936, 9850, 1111, 1050, 49689, 13, 50814], "temperature": 0.0, "avg_logprob": -0.052946687578321334, "compression_ratio": 1.4246575342465753, "no_speech_prob": 0.0745578333735466}, {"id": 40, "seek": 17850, "start": 187.5, "end": 194.5, "text": " To jest jak pr\u00f3ba zmieszczenia ca\u0142ej biblioteki kongresu na jednym, nawet najpot\u0119\u017cniejszym smartfonie.", "tokens": [50814, 1407, 3492, 4207, 8565, 4231, 17020, 15347, 38517, 47631, 73, 34344, 310, 14753, 350, 556, 495, 84, 1667, 5232, 12996, 11, 22696, 11212, 17698, 1274, 1427, 10402, 7706, 76, 4069, 14338, 414, 13, 51164], "temperature": 0.0, "avg_logprob": -0.052946687578321334, "compression_ratio": 1.4246575342465753, "no_speech_prob": 0.0745578333735466}, {"id": 41, "seek": 17850, "start": 194.5, "end": 195.5, "text": " To niemo\u017cliwe.", "tokens": [51164, 1407, 2838, 3280, 1427, 2081, 826, 13, 51214], "temperature": 0.0, "avg_logprob": -0.052946687578321334, "compression_ratio": 1.4246575342465753, "no_speech_prob": 0.0745578333735466}, {"id": 42, "seek": 17850, "start": 195.5, "end": 200.5, "text": " \u017baden pojedynczy procesor graficzny na \u015bwiecie nie jest w stanie tego ud\u017awign\u0105\u0107.", "tokens": [51214, 29804, 14771, 714, 40543, 2534, 6522, 17565, 284, 1295, 1786, 89, 1634, 1667, 40078, 4260, 2838, 3492, 261, 40013, 8627, 11727, 10659, 86, 788, 36374, 13, 51464], "temperature": 0.0, "avg_logprob": -0.052946687578321334, "compression_ratio": 1.4246575342465753, "no_speech_prob": 0.0745578333735466}, {"id": 43, "seek": 17850, "start": 200.5, "end": 201.5, "text": " Dok\u0142adnie.", "tokens": [51464, 29768, 10358, 2766, 13, 51514], "temperature": 0.0, "avg_logprob": -0.052946687578321334, "compression_ratio": 1.4246575342465753, "no_speech_prob": 0.0745578333735466}, {"id": 44, "seek": 17850, "start": 201.5, "end": 204.5, "text": " A drugi problem to moc obliczeniowa.", "tokens": [51514, 316, 4110, 72, 1154, 281, 34962, 1111, 1050, 42124, 5528, 13, 51664], "temperature": 0.0, "avg_logprob": -0.052946687578321334, "compression_ratio": 1.4246575342465753, "no_speech_prob": 0.0745578333735466}, {"id": 45, "seek": 20450, "start": 204.5, "end": 210.5, "text": " No bo mamy super komputery z tysi\u0105cami GPU, ale samo ich posiadanie to jedno.", "tokens": [50364, 883, 748, 17335, 1687, 5207, 2582, 2109, 710, 38156, 11404, 66, 4526, 18407, 11, 6775, 36422, 1893, 1366, 38069, 7155, 281, 5232, 1771, 13, 50664], "temperature": 0.0, "avg_logprob": -0.05817307847918886, "compression_ratio": 1.3857677902621723, "no_speech_prob": 0.03575722873210907}, {"id": 46, "seek": 20450, "start": 210.5, "end": 213.5, "text": " A zaprz\u0119gni\u0119cie ich do efektywnej pracy to drugie.", "tokens": [50664, 316, 14223, 81, 11052, 70, 35938, 4260, 1893, 360, 31482, 916, 874, 86, 11794, 35591, 281, 4110, 414, 13, 50814], "temperature": 0.0, "avg_logprob": -0.05817307847918886, "compression_ratio": 1.3857677902621723, "no_speech_prob": 0.03575722873210907}, {"id": 47, "seek": 20450, "start": 213.5, "end": 215.5, "text": " I to jest klasyczny dylemat.", "tokens": [50814, 286, 281, 3492, 9671, 5871, 3689, 1634, 274, 2072, 15677, 13, 50914], "temperature": 0.0, "avg_logprob": -0.05817307847918886, "compression_ratio": 1.3857677902621723, "no_speech_prob": 0.03575722873210907}, {"id": 48, "seek": 20450, "start": 215.5, "end": 224.5, "text": " Je\u015bli podzielimy prac\u0119 na zbyt ma\u0142e kawa\u0142ki, czyli damy ka\u017cdemu GPU ma\u0142y batch size, to procesory b\u0119d\u0105 si\u0119 po prostu nudzi\u0107, czekaj\u0105c na dane.", "tokens": [50914, 37086, 2497, 42280, 13189, 22404, 1274, 1667, 710, 2322, 83, 463, 19827, 350, 10449, 1221, 2984, 11, 16591, 2422, 88, 21912, 10730, 84, 18407, 463, 6825, 15245, 2744, 11, 281, 17565, 827, 26239, 3244, 714, 19518, 40045, 28496, 11, 6472, 916, 38757, 1667, 49206, 13, 51364], "temperature": 0.0, "avg_logprob": -0.05817307847918886, "compression_ratio": 1.3857677902621723, "no_speech_prob": 0.03575722873210907}, {"id": 49, "seek": 20450, "start": 224.5, "end": 226.5, "text": " Wydajno\u015b\u0107 leci na \u0142eb na szyj\u0119.", "tokens": [51364, 343, 6655, 1805, 23293, 476, 537, 1667, 220, 19827, 65, 1667, 30526, 11115, 13, 51464], "temperature": 0.0, "avg_logprob": -0.05817307847918886, "compression_ratio": 1.3857677902621723, "no_speech_prob": 0.03575722873210907}, {"id": 50, "seek": 20450, "start": 226.5, "end": 227.5, "text": " A z drugiej strony?", "tokens": [51464, 316, 710, 47373, 32406, 30, 51514], "temperature": 0.0, "avg_logprob": -0.05817307847918886, "compression_ratio": 1.3857677902621723, "no_speech_prob": 0.03575722873210907}, {"id": 51, "seek": 22750, "start": 227.5, "end": 240.5, "text": " A z drugiej strony, je\u015bli spr\u00f3bujemy po\u0142\u0105czy\u0107 te ma\u0142e kawa\u0142ki w jeden gigantyczny globalny batch size, to mo\u017cemy zaszkodzi\u0107 procesowi uczenia si\u0119 modelu, pogarszaj\u0105c jego jako\u015b\u0107.", "tokens": [50364, 316, 710, 47373, 32406, 11, 25630, 6103, 14216, 21767, 714, 15926, 33967, 535, 463, 19827, 350, 10449, 1221, 2984, 261, 12906, 8741, 394, 17466, 1634, 4338, 1634, 15245, 2744, 11, 281, 26500, 710, 19601, 74, 14543, 2162, 17565, 24503, 344, 38517, 3244, 2316, 84, 11, 32037, 685, 89, 38757, 26542, 17123, 7753, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07984565771543063, "compression_ratio": 1.3907563025210083, "no_speech_prob": 0.3096715211868286}, {"id": 52, "seek": 22750, "start": 240.5, "end": 244.5, "text": " To jak pr\u00f3ba nakarmienia tysi\u0105ca g\u0142odnych maszyn jednocze\u015bnie.", "tokens": [51014, 1407, 4207, 8565, 4231, 20332, 4452, 18811, 38156, 11404, 496, 18117, 378, 9399, 2300, 1229, 77, 5232, 26694, 1381, 12221, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07984565771543063, "compression_ratio": 1.3907563025210083, "no_speech_prob": 0.3096715211868286}, {"id": 53, "seek": 22750, "start": 244.5, "end": 248.5, "text": " Tak\u017ceba \u017cadna nie czeka\u0142a, ale te\u017c \u017ceby \u017cadna si\u0119 nie przejad\u0142a.", "tokens": [51214, 9118, 2875, 4231, 39628, 629, 2838, 6472, 36361, 5024, 11, 6775, 9516, 11316, 39628, 629, 3244, 2838, 8325, 73, 345, 5024, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07984565771543063, "compression_ratio": 1.3907563025210083, "no_speech_prob": 0.3096715211868286}, {"id": 54, "seek": 24850, "start": 248.5, "end": 254.5, "text": " Ok, czyli standardowe metody po prostu zawodz\u0105. Jak wi\u0119c autorzy sobie z tym poradzili?", "tokens": [50364, 3477, 11, 16591, 3832, 6880, 1131, 843, 714, 19518, 28165, 378, 8925, 13, 15029, 16677, 19510, 1229, 13652, 710, 8107, 1515, 345, 89, 2312, 30, 50664], "temperature": 0.0, "avg_logprob": -0.09601447489354517, "compression_ratio": 1.4539007092198581, "no_speech_prob": 0.6497965455055237}, {"id": 55, "seek": 24850, "start": 254.5, "end": 258.5, "text": " I tu, jak rozumiem, dochodzimy do serca tej in\u017cenieryjnej innowacji.", "tokens": [50664, 286, 2604, 11, 4207, 48797, 4907, 11, 9243, 378, 89, 13189, 360, 816, 496, 12573, 294, 1427, 15711, 2109, 73, 11794, 294, 3785, 13152, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09601447489354517, "compression_ratio": 1.4539007092198581, "no_speech_prob": 0.6497965455055237}, {"id": 56, "seek": 24850, "start": 258.5, "end": 260.5, "text": " Tak, tu dochodzimy do sedna.", "tokens": [50864, 9118, 11, 2604, 9243, 378, 89, 13189, 360, 9643, 629, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09601447489354517, "compression_ratio": 1.4539007092198581, "no_speech_prob": 0.6497965455055237}, {"id": 57, "seek": 24850, "start": 260.5, "end": 263.5, "text": " Bo co ciekawe, oni nie wymy\u015blili tych technik od Zela.", "tokens": [50964, 3286, 598, 30596, 2330, 826, 11, 36317, 2838, 4628, 2226, 19212, 2312, 15180, 1537, 1035, 3611, 1176, 4053, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09601447489354517, "compression_ratio": 1.4539007092198581, "no_speech_prob": 0.6497965455055237}, {"id": 58, "seek": 24850, "start": 263.5, "end": 268.5, "text": " Prawdziwy geniusz polega\u0142 na tym, jakie ze sob\u0105 po\u0142\u0105czyli, jakie zorkiestrowali.", "tokens": [51114, 430, 15889, 3992, 9726, 14017, 89, 13208, 3680, 1221, 1667, 8107, 11, 22124, 5277, 18253, 1611, 714, 15926, 6522, 2081, 11, 22124, 710, 1284, 6495, 1892, 5103, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09601447489354517, "compression_ratio": 1.4539007092198581, "no_speech_prob": 0.6497965455055237}, {"id": 59, "seek": 24850, "start": 268.5, "end": 272.5, "text": " Stworzyli co\u015b, co nazwali 3D parallelizm.", "tokens": [51364, 745, 28321, 1229, 2081, 19241, 11, 598, 20151, 40054, 805, 35, 8952, 590, 76, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09601447489354517, "compression_ratio": 1.4539007092198581, "no_speech_prob": 0.6497965455055237}, {"id": 60, "seek": 24850, "start": 272.5, "end": 275.5, "text": " 3D parallelizm. Brzmi skomplikowanie.", "tokens": [51564, 805, 35, 8952, 590, 76, 13, 1603, 89, 3057, 1110, 298, 564, 1035, 22028, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09601447489354517, "compression_ratio": 1.4539007092198581, "no_speech_prob": 0.6497965455055237}, {"id": 61, "seek": 27550, "start": 275.5, "end": 278.5, "text": " Ale mo\u017cna to sobie wyobrazi\u0107, jak taki tr\u00f3jwarstwowy tort.", "tokens": [50364, 9366, 17790, 281, 13652, 4628, 24393, 28496, 11, 4207, 20065, 504, 18999, 6925, 372, 86, 10089, 10806, 13, 50514], "temperature": 0.0, "avg_logprob": -0.060189040083634224, "compression_ratio": 1.4458598726114649, "no_speech_prob": 0.006303409114480019}, {"id": 62, "seek": 27550, "start": 278.5, "end": 284.5, "text": " Ka\u017cda warstwa rozwi\u0105zuje inny problem, ale dopiero razem tworz\u0105 sp\u00f3jn\u0105 dzia\u0142aj\u0105c\u0105 ca\u0142o\u015b\u0107.", "tokens": [50514, 10988, 1427, 2675, 1516, 372, 4151, 9544, 18234, 11728, 2884, 294, 1634, 1154, 11, 6775, 21900, 12030, 40225, 46288, 8925, 637, 18999, 13113, 27121, 11133, 32557, 1335, 44742, 13, 50814], "temperature": 0.0, "avg_logprob": -0.060189040083634224, "compression_ratio": 1.4458598726114649, "no_speech_prob": 0.006303409114480019}, {"id": 63, "seek": 27550, "start": 284.5, "end": 289.5, "text": " Dobrze, to roz\u0142\u00f3\u017cmy ten tort na warstwy. Co jest na samym dole? Jako podstawa.", "tokens": [50814, 29679, 13503, 11, 281, 9544, 1221, 812, 1427, 2226, 2064, 10806, 1667, 1516, 372, 9726, 13, 3066, 3492, 1667, 3247, 4199, 360, 306, 30, 15029, 78, 2497, 372, 10449, 13, 51064], "temperature": 0.0, "avg_logprob": -0.060189040083634224, "compression_ratio": 1.4458598726114649, "no_speech_prob": 0.006303409114480019}, {"id": 64, "seek": 27550, "start": 289.5, "end": 295.5, "text": " Podstaw\u0105 jest data parallelizm. To najbardziej intuicyjna metoda, taki troch\u0119 brute force.", "tokens": [51064, 12646, 372, 1607, 1611, 3492, 1412, 8952, 590, 76, 13, 1407, 41857, 560, 84, 2632, 73, 629, 1131, 13449, 11, 20065, 24926, 47909, 3464, 13, 51364], "temperature": 0.0, "avg_logprob": -0.060189040083634224, "compression_ratio": 1.4458598726114649, "no_speech_prob": 0.006303409114480019}, {"id": 65, "seek": 27550, "start": 295.5, "end": 302.5, "text": " Ka\u017cdy procesor graficzny dostaje pe\u0142n\u0105 kopi\u0119 ca\u0142ego modelu i przetwarza po prostu inny fragment danych.", "tokens": [51364, 10988, 1427, 3173, 17565, 284, 1295, 1786, 89, 1634, 20568, 11153, 43205, 13113, 28920, 5034, 35224, 6308, 2316, 84, 741, 6541, 302, 6925, 2394, 714, 19518, 294, 1634, 26424, 274, 34644, 13, 51714], "temperature": 0.0, "avg_logprob": -0.060189040083634224, "compression_ratio": 1.4458598726114649, "no_speech_prob": 0.006303409114480019}, {"id": 66, "seek": 27550, "start": 302.5, "end": 303.5, "text": " Proste.", "tokens": [51714, 2114, 555, 68, 13, 51764], "temperature": 0.0, "avg_logprob": -0.060189040083634224, "compression_ratio": 1.4458598726114649, "no_speech_prob": 0.006303409114480019}, {"id": 67, "seek": 30350, "start": 303.5, "end": 309.5, "text": " Proste, ale dla modelu z 530 miliardami parametr\u00f3w kompletnie bezu\u017cyteczne,", "tokens": [50364, 2114, 555, 68, 11, 6775, 12285, 2316, 84, 710, 1025, 3446, 1962, 72, 515, 4526, 6220, 27965, 3901, 5207, 14657, 2766, 10782, 84, 7735, 975, 38491, 11, 50664], "temperature": 0.0, "avg_logprob": -0.07109792445733296, "compression_ratio": 1.3764705882352941, "no_speech_prob": 0.002830506069585681}, {"id": 68, "seek": 30350, "start": 309.5, "end": 312.5, "text": " bo \u017caden GPU nie zmie\u015bci nawet jednej kopii.", "tokens": [50664, 748, 19625, 14771, 18407, 2838, 17020, 414, 6199, 22696, 5232, 11794, 28920, 5597, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07109792445733296, "compression_ratio": 1.3764705882352941, "no_speech_prob": 0.002830506069585681}, {"id": 69, "seek": 30350, "start": 312.5, "end": 316.5, "text": " To jest warstwa, kt\u00f3ra dzia\u0142a, ale tylko gdy model jest ma\u0142y.", "tokens": [50814, 1407, 3492, 1516, 372, 4151, 11, 19456, 37903, 11, 6775, 13219, 28405, 2316, 3492, 463, 6825, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07109792445733296, "compression_ratio": 1.3764705882352941, "no_speech_prob": 0.002830506069585681}, {"id": 70, "seek": 30350, "start": 316.5, "end": 319.5, "text": " Rozumiem, czyli to rozwi\u0105zanie odpada na starcie.", "tokens": [51014, 43313, 449, 4907, 11, 16591, 281, 9544, 22620, 7155, 3611, 79, 1538, 1667, 3543, 4260, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07109792445733296, "compression_ratio": 1.3764705882352941, "no_speech_prob": 0.002830506069585681}, {"id": 71, "seek": 30350, "start": 319.5, "end": 322.5, "text": " Co jest w takim razie drug\u0105 warstw\u0105?", "tokens": [51164, 3066, 3492, 261, 31732, 9639, 414, 4110, 1611, 1516, 372, 86, 1611, 30, 51314], "temperature": 0.0, "avg_logprob": -0.07109792445733296, "compression_ratio": 1.3764705882352941, "no_speech_prob": 0.002830506069585681}, {"id": 72, "seek": 30350, "start": 322.5, "end": 327.5, "text": " Druga warstwa to tensor parallelizm. I tu robi si\u0119 znacznie ciekawiej.", "tokens": [51314, 2491, 19364, 1516, 372, 4151, 281, 40863, 8952, 590, 76, 13, 286, 2604, 47380, 3244, 15397, 14875, 2766, 46419, 1607, 7764, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07109792445733296, "compression_ratio": 1.3764705882352941, "no_speech_prob": 0.002830506069585681}, {"id": 73, "seek": 32750, "start": 327.5, "end": 334.5, "text": " Zamiast repikowa\u0107 ca\u0142y model kroimy na kawa\u0142ki poszczeg\u00f3lne jego operacje matematyczne, czyli tensory.", "tokens": [50364, 1176, 4526, 525, 1085, 1035, 11445, 35226, 2316, 45909, 13189, 1667, 350, 10449, 1221, 2984, 1366, 43771, 38079, 716, 26542, 2208, 29293, 3803, 8615, 17466, 716, 11, 16591, 10688, 827, 13, 50714], "temperature": 0.0, "avg_logprob": -0.06412829552497064, "compression_ratio": 1.478827361563518, "no_speech_prob": 0.015899771824479103}, {"id": 74, "seek": 32750, "start": 334.5, "end": 337.5, "text": " I rozdzielamy te kawa\u0142ki mi\u0119dzy r\u00f3\u017cne GPU.", "tokens": [50714, 286, 9544, 28168, 1187, 7804, 535, 350, 10449, 1221, 2984, 33964, 47760, 18407, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06412829552497064, "compression_ratio": 1.478827361563518, "no_speech_prob": 0.015899771824479103}, {"id": 75, "seek": 32750, "start": 337.5, "end": 345.5, "text": " To jakby jedn\u0105, bardzo skomplikowan\u0105 formu\u0142\u0119 matematyczn\u0105 podzieli\u0107 na mniejsze cz\u0119\u015bci i da\u0107 je r\u00f3\u017cnym osobom do obliczenia w tym samym czasie.", "tokens": [50864, 1407, 28976, 5232, 13113, 11, 9034, 1110, 298, 564, 1035, 37345, 1611, 1254, 84, 46564, 3803, 8615, 17466, 13113, 2497, 42280, 12757, 1667, 275, 44258, 41314, 741, 1120, 2162, 1506, 19637, 12996, 41518, 298, 360, 1111, 1050, 14320, 261, 8107, 3247, 4199, 42667, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06412829552497064, "compression_ratio": 1.478827361563518, "no_speech_prob": 0.015899771824479103}, {"id": 76, "seek": 32750, "start": 345.5, "end": 348.5, "text": " To brzmi genialnie pod k\u0105tem oszcz\u0119dzania pami\u0119ci.", "tokens": [51264, 1407, 738, 89, 3057, 48228, 2766, 2497, 350, 1611, 18275, 3003, 43771, 6298, 89, 5609, 31088, 537, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06412829552497064, "compression_ratio": 1.478827361563518, "no_speech_prob": 0.015899771824479103}, {"id": 77, "seek": 32750, "start": 348.5, "end": 355.5, "text": " Tak, ale ma swoj\u0105 cen\u0119. Wymaga absolutnie ultra szybkiej komunikacji mi\u0119dzy procesorami,", "tokens": [51414, 9118, 11, 6775, 463, 49194, 27900, 1274, 13, 343, 4199, 9286, 18757, 2766, 14808, 36456, 45145, 45359, 1035, 13152, 33964, 17565, 284, 4526, 11, 51764], "temperature": 0.0, "avg_logprob": -0.06412829552497064, "compression_ratio": 1.478827361563518, "no_speech_prob": 0.015899771824479103}, {"id": 78, "seek": 35550, "start": 355.5, "end": 361.5, "text": " bo one non-stop musz\u0105 wymienia\u0107 si\u0119 wynikami. Ka\u017cde nawet najmniejsze op\u0142ynienie zabija wydajno\u015b\u0107.", "tokens": [50364, 748, 472, 2107, 12, 13559, 1038, 8925, 29764, 18811, 2162, 3244, 31936, 1035, 4526, 13, 10988, 1427, 1479, 22696, 11212, 47658, 82, 1381, 999, 1221, 2534, 27385, 24838, 20642, 25984, 1805, 23293, 13, 50664], "temperature": 0.0, "avg_logprob": -0.0913059221555109, "compression_ratio": 1.483221476510067, "no_speech_prob": 0.030595114454627037}, {"id": 79, "seek": 35550, "start": 361.5, "end": 365.5, "text": " Okej. A trzecia ostatnia warstwa tego tortu?", "tokens": [50664, 29094, 73, 13, 316, 22266, 2755, 32686, 12679, 1516, 372, 4151, 8627, 3930, 9179, 30, 50864], "temperature": 0.0, "avg_logprob": -0.0913059221555109, "compression_ratio": 1.483221476510067, "no_speech_prob": 0.030595114454627037}, {"id": 80, "seek": 35550, "start": 365.5, "end": 374.5, "text": " To pipeline parallelizm. Tutaj dzielimy model nie w poziomie jak przetensor parallelizm, ale w pionie na kolejny bloki warstw.", "tokens": [50864, 1407, 15517, 8952, 590, 76, 13, 41819, 9758, 1187, 13189, 2316, 2838, 261, 38503, 40120, 4207, 6541, 302, 23153, 8952, 590, 76, 11, 6775, 261, 280, 313, 414, 1667, 23749, 1634, 888, 17056, 1516, 372, 86, 13, 51314], "temperature": 0.0, "avg_logprob": -0.0913059221555109, "compression_ratio": 1.483221476510067, "no_speech_prob": 0.030595114454627037}, {"id": 81, "seek": 35550, "start": 374.5, "end": 377.5, "text": " Tworzymy co\u015b na kszta\u0142t cyfrowej linii monta\u017cowej.", "tokens": [51314, 2574, 284, 1229, 2226, 19241, 1667, 350, 15453, 46426, 83, 3185, 69, 1892, 40779, 287, 3812, 72, 8143, 18264, 21091, 13, 51464], "temperature": 0.0, "avg_logprob": -0.0913059221555109, "compression_ratio": 1.483221476510067, "no_speech_prob": 0.030595114454627037}, {"id": 82, "seek": 35550, "start": 377.5, "end": 384.5, "text": " Czyli jedna grupa GPU robi pierwszy etap, przekazuje wynik dalej, nast\u0119pna grupa robi drugi etap i tak dalej.", "tokens": [51464, 37099, 5232, 629, 12740, 64, 18407, 47380, 34016, 47634, 11, 29785, 43317, 31936, 1035, 34257, 11, 39662, 629, 12740, 64, 47380, 4110, 72, 47634, 741, 991, 34257, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0913059221555109, "compression_ratio": 1.483221476510067, "no_speech_prob": 0.030595114454627037}, {"id": 83, "seek": 38450, "start": 384.5, "end": 392.5, "text": " Dok\u0142adnie. To \u015bwietnie radzi sobie z modelami, kt\u00f3re s\u0105 za du\u017ce, by zmie\u015bci\u0107 si\u0119 nawet na kilku GPU, ale generuje inny problem.", "tokens": [50364, 29768, 10358, 2766, 13, 1407, 8299, 39083, 2766, 2843, 3992, 13652, 710, 2316, 4526, 11, 8864, 9015, 7949, 1581, 2875, 11, 538, 17020, 414, 6199, 2162, 3244, 22696, 1667, 5128, 5279, 18407, 11, 6775, 1337, 13008, 294, 1634, 1154, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09525362111754336, "compression_ratio": 1.4131274131274132, "no_speech_prob": 0.013468079268932343}, {"id": 84, "seek": 38450, "start": 392.5, "end": 395.5, "text": " Tak zwany bubble overhead.", "tokens": [50764, 9118, 11873, 1325, 12212, 19922, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09525362111754336, "compression_ratio": 1.4131274131274132, "no_speech_prob": 0.013468079268932343}, {"id": 85, "seek": 38450, "start": 395.5, "end": 396.5, "text": " Bubble overhead.", "tokens": [50914, 43072, 19922, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09525362111754336, "compression_ratio": 1.4131274131274132, "no_speech_prob": 0.013468079268932343}, {"id": 86, "seek": 38450, "start": 396.5, "end": 404.5, "text": " Tak, to jest czas, kiedy na pocz\u0105tku i na ko\u0144cu procesu niekt\u00f3re etapy linii monta\u017cowej po prostu czekaj\u0105, albo nadane, albo ju\u017c sko\u0144czy\u0142y prace.", "tokens": [50964, 9118, 11, 281, 3492, 13190, 11, 18777, 1667, 43959, 741, 1667, 26470, 12032, 17565, 84, 2838, 43073, 265, 47634, 88, 287, 3812, 72, 8143, 18264, 21091, 714, 19518, 6472, 916, 11133, 11, 22622, 12617, 1929, 11, 22622, 10678, 1110, 78, 5248, 6522, 6825, 582, 617, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09525362111754336, "compression_ratio": 1.4131274131274132, "no_speech_prob": 0.013468079268932343}, {"id": 87, "seek": 38450, "start": 404.5, "end": 406.5, "text": " A to jest zmarnowany potencja\u0142.", "tokens": [51364, 316, 281, 3492, 17020, 1083, 23341, 1847, 22660, 2938, 1221, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09525362111754336, "compression_ratio": 1.4131274131274132, "no_speech_prob": 0.013468079268932343}, {"id": 88, "seek": 40650, "start": 406.5, "end": 416.5, "text": " Ok, czyli mamy trzy techniki. Ka\u017cda ma mocne strony, ale i powa\u017cne wady. I m\u00f3wi\u0142a\u015b, \u017ce geniusz polega\u0142 na ich po\u0142\u0105czeniu.", "tokens": [50364, 3477, 11, 16591, 17335, 34573, 1537, 9850, 13, 10988, 1427, 2675, 463, 34962, 716, 32406, 11, 6775, 741, 3388, 18264, 716, 261, 880, 13, 286, 24592, 5024, 1788, 11, 3561, 14017, 89, 13208, 3680, 1221, 1667, 1893, 714, 15926, 66, 39651, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09163845242477778, "compression_ratio": 1.3468634686346863, "no_speech_prob": 0.0556848868727684}, {"id": 89, "seek": 40650, "start": 416.5, "end": 422.5, "text": " Dok\u0142adnie tak. Prawdziwy prze\u0142om to co\u015b, co nazwali topology-aware 3D mapping.", "tokens": [50864, 29768, 10358, 2766, 991, 13, 430, 15889, 3992, 9726, 8325, 1221, 298, 281, 19241, 11, 598, 20151, 40054, 1192, 1793, 12, 17074, 805, 35, 18350, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09163845242477778, "compression_ratio": 1.3468634686346863, "no_speech_prob": 0.0556848868727684}, {"id": 90, "seek": 40650, "start": 422.5, "end": 426.5, "text": " Zrozumieli, \u017ce nie wystarczy w\u0142\u0105czy\u0107 tych trzech tryb\u00f3w naraz.", "tokens": [51164, 1176, 27857, 449, 23099, 11, 3561, 2838, 4628, 9710, 6522, 261, 15926, 33967, 15180, 504, 19439, 853, 65, 3901, 6714, 921, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09163845242477778, "compression_ratio": 1.3468634686346863, "no_speech_prob": 0.0556848868727684}, {"id": 91, "seek": 40650, "start": 426.5, "end": 432.5, "text": " Trzeba je inteligentnie zmapowa\u0107 na fizyczn\u0105 architektur\u0119 Superkomputera Selen\u0119.", "tokens": [51364, 1765, 1381, 4231, 1506, 24777, 25002, 2766, 17020, 569, 11445, 1667, 21000, 17466, 13113, 3912, 642, 2320, 374, 1274, 4548, 20557, 2582, 1663, 10736, 268, 1274, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09163845242477778, "compression_ratio": 1.3468634686346863, "no_speech_prob": 0.0556848868727684}, {"id": 92, "seek": 43250, "start": 432.5, "end": 437.5, "text": " To nie jest tylko in\u017cynieria. To jest choreografia na niespotykan\u0105 skal\u0119.", "tokens": [50364, 1407, 2838, 3492, 13219, 294, 1427, 2534, 811, 654, 13, 1407, 3492, 14625, 19815, 654, 1667, 48100, 79, 6737, 5225, 1611, 16890, 1274, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09111346679479898, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.04601913318037987}, {"id": 93, "seek": 43250, "start": 437.5, "end": 440.5, "text": " Choreografia? Co to znaczy w praktyce?", "tokens": [50614, 761, 418, 19815, 654, 30, 3066, 281, 36584, 261, 3206, 74, 874, 384, 30, 50764], "temperature": 0.0, "avg_logprob": -0.09111346679479898, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.04601913318037987}, {"id": 94, "seek": 43250, "start": 440.5, "end": 445.5, "text": " To znaczy, \u017ce spojrzeli na budow\u0119 komputera i dopasowali do niej oprogramowanie.", "tokens": [50764, 1407, 36584, 11, 3561, 8243, 73, 19390, 10148, 1667, 3265, 305, 1274, 5207, 2582, 1663, 741, 360, 20990, 305, 5103, 360, 2838, 73, 999, 340, 1342, 22028, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09111346679479898, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.04601913318037987}, {"id": 95, "seek": 43250, "start": 445.5, "end": 457.5, "text": " Wiedzieli, \u017ce tensor paralelizm wymaga najszybszej komunikacji, wi\u0119c zaimplementowali go w obr\u0119bie jednego serwera, gdzie 8 GPU jest po\u0142\u0105czonych ultra szybkim z\u0142\u0105czem and wheeling.", "tokens": [51014, 343, 15338, 23099, 11, 3561, 40863, 26009, 338, 590, 76, 29764, 9286, 11212, 7706, 929, 16920, 45359, 1035, 13152, 11, 16677, 7949, 332, 43704, 305, 5103, 352, 261, 1111, 81, 1274, 7392, 5232, 11858, 816, 1554, 64, 11, 18922, 1649, 18407, 3492, 714, 43558, 2526, 339, 14808, 36456, 25112, 710, 15926, 66, 24313, 293, 5589, 278, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09111346679479898, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.04601913318037987}, {"id": 96, "seek": 43250, "start": 457.5, "end": 459.5, "text": " Bo tam jest najmniejsze op\u00f3\u017anienie.", "tokens": [51614, 3286, 7677, 3492, 11212, 76, 44258, 999, 812, 10659, 77, 27385, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09111346679479898, "compression_ratio": 1.4722222222222223, "no_speech_prob": 0.04601913318037987}, {"id": 97, "seek": 45950, "start": 459.5, "end": 470.5, "text": " W\u0142a\u015bnie. Z kolei pipeline paralelizm, kt\u00f3ry jest mniej wra\u017cliwy na op\u00f3\u017anienia, dzia\u0142a\u0142 pomi\u0119dzy r\u00f3\u017cnymi serwerami po\u0142\u0105czonymi nieco wolniejsz\u0105 sieci\u0105 InfiniBand,", "tokens": [50364, 343, 5024, 12221, 13, 1176, 18303, 72, 15517, 26009, 338, 590, 76, 11, 9913, 3492, 39513, 7843, 1427, 2081, 9726, 1667, 999, 812, 10659, 77, 18811, 11, 37903, 1221, 12991, 49485, 19637, 31813, 816, 1554, 4526, 714, 43558, 2526, 3057, 2838, 1291, 20960, 30295, 8925, 2804, 34381, 11537, 3812, 33, 474, 11, 50914], "temperature": 0.0, "avg_logprob": -0.10546937643312941, "compression_ratio": 1.3849372384937237, "no_speech_prob": 0.06056791916489601}, {"id": 98, "seek": 45950, "start": 470.5, "end": 481.5, "text": " a na to wszystko na\u0142o\u017cyli jeszcze data paralelizm replikuj\u0105c ca\u0142\u0105 t\u0119 skomplikowan\u0105 wieloserverow\u0105 linie monta\u017cow\u0105 tyle razy, ile by\u0142o potrzeba.", "tokens": [50914, 257, 1667, 281, 22607, 1667, 5249, 7735, 2081, 14168, 1412, 26009, 338, 590, 76, 3248, 1035, 44733, 1335, 15926, 32489, 1110, 298, 564, 1035, 37345, 1611, 20570, 329, 38241, 30297, 22896, 414, 8143, 18264, 30297, 39293, 9639, 88, 11, 15465, 14811, 28577, 4231, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10546937643312941, "compression_ratio": 1.3849372384937237, "no_speech_prob": 0.06056791916489601}, {"id": 99, "seek": 48150, "start": 481.5, "end": 491.5, "text": " Czyli to nie jest tylko kwestia posiadania trzech narz\u0119dzi, ale wiedzy, kt\u00f3rego u\u017cy\u0107 gdzie i kiedy, w zale\u017cno\u015bci od tego, jak zbudowany jest sam superkomputer.", "tokens": [50364, 37099, 281, 2838, 3492, 13219, 42035, 654, 1366, 38069, 5609, 504, 19439, 6714, 89, 6298, 3992, 11, 6775, 46894, 1229, 11, 46951, 34097, 2162, 18922, 741, 18777, 11, 261, 710, 45494, 16438, 3611, 8627, 11, 4207, 710, 18281, 23341, 3492, 3247, 1687, 20557, 13849, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07397225728401771, "compression_ratio": 1.3215686274509804, "no_speech_prob": 0.036241669207811356}, {"id": 100, "seek": 48150, "start": 491.5, "end": 496.5, "text": " To, to jest niesamowite. Jaki by\u0142 efekt ko\u0144cowy tej choreografii?", "tokens": [50864, 1407, 11, 281, 3492, 48100, 335, 305, 642, 13, 508, 7421, 16673, 31482, 8192, 26470, 66, 10089, 12573, 14625, 19815, 5597, 30, 51114], "temperature": 0.0, "avg_logprob": -0.07397225728401771, "compression_ratio": 1.3215686274509804, "no_speech_prob": 0.036241669207811356}, {"id": 101, "seek": 48150, "start": 496.5, "end": 502.5, "text": " Wyobra\u017a sobie, \u017ce jeden ca\u0142y model zajmowa\u0142 niemal 300 najpot\u0119\u017cniejszych procesor\u00f3w graficznych.", "tokens": [51114, 14458, 24393, 10659, 13652, 11, 3561, 12906, 35226, 2316, 33729, 76, 30105, 2838, 5579, 6641, 11212, 17698, 1274, 1427, 10402, 45021, 17565, 284, 3901, 1295, 1786, 89, 9399, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07397225728401771, "compression_ratio": 1.3215686274509804, "no_speech_prob": 0.036241669207811356}, {"id": 102, "seek": 50250, "start": 502.5, "end": 511.5, "text": " Ka\u017cdy by\u0142 cz\u0119\u015bci\u0105 tej precyzyjnie zaplanowanej linii monta\u017cowej, a potem ca\u0142\u0105 t\u0119 konstrukcj\u0119 powielano, \u017ceby trenowa\u0107 model jeszcze szybciej.", "tokens": [50364, 10988, 1427, 3173, 16673, 41314, 1611, 12573, 659, 1344, 1229, 73, 2766, 7949, 16554, 23066, 73, 287, 3812, 72, 8143, 18264, 21091, 11, 257, 36513, 1335, 15926, 32489, 34208, 25126, 41960, 3388, 1187, 3730, 11, 11316, 23136, 11445, 2316, 14168, 36456, 4260, 73, 13, 50814], "temperature": 0.0, "avg_logprob": -0.0785721664428711, "compression_ratio": 1.364963503649635, "no_speech_prob": 0.7755205631256104}, {"id": 103, "seek": 50250, "start": 511.5, "end": 512.5, "text": " A wydajno\u015b\u0107?", "tokens": [50814, 316, 25984, 1805, 23293, 30, 50864], "temperature": 0.0, "avg_logprob": -0.0785721664428711, "compression_ratio": 1.364963503649635, "no_speech_prob": 0.7755205631256104}, {"id": 104, "seek": 50250, "start": 512.5, "end": 528.5, "text": " Osi\u0105gn\u0119li wydajno\u015b\u0107 rz\u0119du 113 do 126 teraflops na jedno GPU. To jest astronomicznie wysoki wska\u017anik bior\u0105c pod uwag\u0119 z\u0142o\u017cono\u015b\u0107 komunikacji. To jest absolutne mistrzostwo in\u017cynierii systemowej.", "tokens": [50864, 422, 7691, 1611, 4568, 1274, 2081, 25984, 1805, 23293, 367, 11052, 769, 2975, 18, 360, 2272, 21, 256, 1663, 3423, 3370, 1667, 5232, 1771, 18407, 13, 1407, 3492, 26302, 17946, 2766, 27062, 17056, 261, 20771, 10659, 13123, 272, 1973, 1611, 66, 2497, 43696, 710, 5249, 1427, 8957, 7753, 45359, 1035, 13152, 13, 1407, 3492, 18757, 716, 3544, 19390, 555, 6120, 294, 1427, 2534, 811, 5597, 1185, 21091, 13, 51664], "temperature": 0.0, "avg_logprob": -0.0785721664428711, "compression_ratio": 1.364963503649635, "no_speech_prob": 0.7755205631256104}, {"id": 105, "seek": 52850, "start": 528.5, "end": 539.5, "text": " Ok, czyli zbudowali ten niesamowity silnik. Ale silnik jest bezu\u017cyteczny, bez paliwa. Czym karmiono tego potwora? Bo sama moc obliczeniowa to, jak wiemy, nie wszystko.", "tokens": [50364, 3477, 11, 16591, 710, 18281, 305, 5103, 2064, 48100, 335, 305, 507, 3425, 13123, 13, 9366, 3425, 13123, 3492, 10782, 84, 7735, 975, 3689, 1634, 11, 10782, 3984, 72, 4151, 13, 19832, 76, 350, 4452, 49020, 8627, 1847, 86, 3252, 30, 3286, 17768, 34962, 1111, 1050, 42124, 5528, 281, 11, 4207, 3355, 2226, 11, 2838, 22607, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09362624875075526, "compression_ratio": 1.4412811387900355, "no_speech_prob": 0.06399425864219666}, {"id": 106, "seek": 52850, "start": 539.5, "end": 556.5, "text": " Zdecydowanie. I tu jest kolejny kluczowy element, bo to nie by\u0142 po prostu, wiesz, losowy zrzut internetu. Autorzy bardzo starannie skomponowali diet\u0119 dla swojego modelu. Wybrali najlepsze i jako\u015bciowo zbiory danych z kolekcji Depile.", "tokens": [50914, 1176, 1479, 1344, 67, 22028, 13, 286, 2604, 3492, 23749, 1634, 9671, 1311, 89, 10089, 4478, 11, 748, 281, 2838, 16673, 714, 19518, 11, 261, 15347, 11, 1750, 10089, 710, 19390, 325, 4705, 84, 13, 6049, 284, 1229, 9034, 3543, 43433, 1110, 8586, 266, 305, 5103, 6339, 1274, 12285, 13291, 39738, 2316, 84, 13, 14458, 1443, 5103, 41903, 1878, 1381, 741, 17123, 6199, 19941, 710, 5614, 827, 274, 34644, 710, 18303, 74, 19649, 4056, 794, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09362624875075526, "compression_ratio": 1.4412811387900355, "no_speech_prob": 0.06399425864219666}, {"id": 107, "seek": 55650, "start": 557.5, "end": 560.5, "text": " Czyli ksi\u0105\u017cki, artyku\u0142y naukowe, kod z GitHub'a.", "tokens": [50414, 37099, 39311, 2984, 11, 594, 874, 5279, 6825, 35616, 74, 6880, 11, 350, 378, 710, 23331, 6, 64, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10273979021155316, "compression_ratio": 1.2638888888888888, "no_speech_prob": 0.11067862063646317}, {"id": 108, "seek": 55650, "start": 560.5, "end": 564.5, "text": " Tak. Generalnie teksty o wysokiej warto\u015bci.", "tokens": [50564, 9118, 13, 6996, 2766, 16624, 25134, 277, 27062, 453, 7764, 31830, 6199, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10273979021155316, "compression_ratio": 1.2638888888888888, "no_speech_prob": 0.11067862063646317}, {"id": 109, "seek": 55650, "start": 564.5, "end": 575.5, "text": " Ale du\u017c\u0105 cz\u0119\u015b\u0107 stanowi\u0142 te\u017c os\u0142awiony Common Crawl, czyli w zasadzie surowy, nieprzefiltrowany internet. Ze wszystkim, co w nim dobre i z\u0142e. Jak sobie z tym poradzili?", "tokens": [50764, 9366, 21783, 1611, 47149, 27984, 24503, 1221, 9516, 3003, 1221, 1607, 46184, 18235, 37877, 75, 11, 16591, 261, 44585, 3283, 1022, 10089, 11, 2838, 1424, 1381, 69, 2352, 1892, 1325, 4705, 13, 4853, 30481, 11, 598, 261, 24887, 41959, 741, 710, 19827, 13, 15029, 13652, 710, 8107, 1515, 345, 89, 2312, 30, 51314], "temperature": 0.0, "avg_logprob": -0.10273979021155316, "compression_ratio": 1.2638888888888888, "no_speech_prob": 0.11067862063646317}, {"id": 110, "seek": 57550, "start": 575.5, "end": 585.5, "text": " I pierw przepu\u015bcili te dane przez swego rodzaju bramkarza. Inteligentny, klasyfikator, fast text, wytrenowany na tekstach wzorowej jako\u015bci, jak Wikipedia.", "tokens": [50364, 286, 9766, 86, 30829, 84, 1788, 66, 2312, 535, 49206, 14064, 2484, 1571, 28607, 33166, 738, 335, 12303, 2394, 13, 19762, 25002, 1634, 11, 9671, 5871, 31230, 1639, 11, 2370, 2487, 11, 261, 4328, 1095, 23341, 1667, 16624, 372, 608, 24809, 284, 21091, 17123, 6199, 11, 4207, 28999, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13384060393598743, "compression_ratio": 1.380281690140845, "no_speech_prob": 0.7231258153915405}, {"id": 111, "seek": 57550, "start": 585.5, "end": 591.5, "text": " I ten bramkarz odrzuca\u0142 wszystko, co wygl\u0105da\u0142o na internetowy be\u0142kot albo spam.", "tokens": [50864, 286, 2064, 738, 335, 12303, 89, 3611, 81, 11728, 496, 1221, 22607, 11, 598, 32015, 5249, 1667, 4705, 10089, 312, 1221, 74, 310, 22622, 24028, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13384060393598743, "compression_ratio": 1.380281690140845, "no_speech_prob": 0.7231258153915405}, {"id": 112, "seek": 57550, "start": 591.5, "end": 593.5, "text": " Dok\u0142adnie. Zosta\u0142o tylko \u015bmietanka.", "tokens": [51164, 29768, 10358, 2766, 13, 1176, 8638, 5249, 13219, 46991, 1684, 21729, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13384060393598743, "compression_ratio": 1.380281690140845, "no_speech_prob": 0.7231258153915405}, {"id": 113, "seek": 57550, "start": 593.5, "end": 594.5, "text": " A drugi etap?", "tokens": [51264, 316, 4110, 72, 47634, 30, 51314], "temperature": 0.0, "avg_logprob": -0.13384060393598743, "compression_ratio": 1.380281690140845, "no_speech_prob": 0.7231258153915405}, {"id": 114, "seek": 57550, "start": 594.5, "end": 601.5, "text": " Drugi etap to fazy deduplication przy u\u017cyciu techniki Locality Sensitive Hashing. W skr\u00f3cie LSH.", "tokens": [51314, 2491, 24780, 47634, 281, 4375, 88, 4172, 84, 4770, 399, 6501, 34097, 30795, 1537, 9850, 12859, 1860, 318, 34465, 389, 11077, 13, 343, 1110, 11721, 4260, 36657, 39, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13384060393598743, "compression_ratio": 1.380281690140845, "no_speech_prob": 0.7231258153915405}, {"id": 115, "seek": 60150, "start": 602.5, "end": 609.5, "text": " Czekaj, czyli ten system potrafi\u0142 znale\u017a\u0107 dwa artyku\u0142y na ten sam temat. Nawet je\u015bli by\u0142y napisane zupe\u0142nie innymi s\u0142owami.", "tokens": [50414, 383, 19878, 1805, 11, 16591, 2064, 1185, 1847, 10437, 40622, 15397, 1220, 10659, 2162, 35045, 594, 874, 5279, 6825, 1667, 2064, 3247, 32954, 13, 40315, 302, 25630, 26366, 9296, 271, 1929, 49922, 294, 31813, 15116, 305, 4526, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09915016347711736, "compression_ratio": 1.3481781376518218, "no_speech_prob": 0.042738184332847595}, {"id": 116, "seek": 60150, "start": 609.5, "end": 615.5, "text": " I uznacie za zbyt podobne? To jest o wiele bardziej zaawansowane ni\u017c zwyk\u0142e szukanie plagiat\u00f3w.", "tokens": [50764, 286, 16851, 77, 30805, 7949, 710, 2322, 83, 43024, 716, 30, 1407, 3492, 277, 33137, 27209, 7949, 1607, 599, 23066, 28502, 43436, 74, 19827, 7870, 2034, 7155, 33756, 7676, 3901, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09915016347711736, "compression_ratio": 1.3481781376518218, "no_speech_prob": 0.042738184332847595}, {"id": 117, "seek": 60150, "start": 615.5, "end": 621.5, "text": " Tak, dok\u0142adnie o to chodzi\u0142o. O r\u00f3\u017cnorodno\u015b\u0107 semantyczn\u0105, a nie tylko o unikanie kopii i wklei.", "tokens": [51064, 9118, 11, 45864, 2766, 277, 281, 23998, 5249, 13, 422, 19637, 19048, 378, 23293, 4361, 394, 17466, 13113, 11, 257, 2838, 13219, 277, 517, 1035, 7155, 28920, 5597, 741, 261, 14677, 72, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09915016347711736, "compression_ratio": 1.3481781376518218, "no_speech_prob": 0.042738184332847595}, {"id": 118, "seek": 62150, "start": 622.5, "end": 631.5, "text": " Internet jest pe\u0142en powt\u00f3rze\u0144, a karmienie model\u00f3w k\u00f3\u0142ko tymi samymi informacjami, tylko ubranymi winne s\u0142owa, jest nieefektywne.", "tokens": [50414, 7703, 3492, 43205, 268, 3388, 4547, 13503, 5248, 11, 257, 350, 4452, 27385, 2316, 3901, 350, 16181, 4093, 1104, 3057, 3247, 88, 3057, 1356, 326, 73, 4526, 11, 13219, 344, 1443, 1325, 3057, 1942, 716, 15116, 5528, 11, 3492, 2838, 5666, 916, 874, 86, 716, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07387287585766285, "compression_ratio": 1.4245283018867925, "no_speech_prob": 0.44841352105140686}, {"id": 119, "seek": 62150, "start": 631.5, "end": 637.5, "text": " I co bardzo wa\u017cne z naukowego punktu widzenia, u\u017cyli filtr\u00f3w opartych na n gramach.", "tokens": [50864, 286, 598, 9034, 46110, 710, 35616, 74, 26576, 39561, 84, 5274, 14320, 11, 34097, 2081, 1387, 6903, 3901, 999, 446, 16384, 1667, 297, 21353, 608, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07387287585766285, "compression_ratio": 1.4245283018867925, "no_speech_prob": 0.44841352105140686}, {"id": 120, "seek": 62150, "start": 637.5, "end": 638.5, "text": " Po co?", "tokens": [51164, 6165, 598, 30, 51214], "temperature": 0.0, "avg_logprob": -0.07387287585766285, "compression_ratio": 1.4245283018867925, "no_speech_prob": 0.44841352105140686}, {"id": 121, "seek": 62150, "start": 638.5, "end": 644.5, "text": " \u017beby usun\u0105\u0107 z danych treningowych fragmenty, kt\u00f3re mog\u0142yby pokrywa\u0107 si\u0119 z zadaniami testowymi.", "tokens": [51214, 46864, 2322, 505, 409, 36374, 710, 274, 34644, 2192, 773, 19605, 26424, 88, 11, 8864, 13172, 6825, 2322, 13010, 627, 25234, 3244, 710, 710, 11338, 15568, 1500, 10089, 3057, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07387287585766285, "compression_ratio": 1.4245283018867925, "no_speech_prob": 0.44841352105140686}, {"id": 122, "seek": 62150, "start": 644.5, "end": 650.5, "text": " Czyli, m\u00f3wi\u0105c pro\u015bciej, zadbali o to, \u017ceby model nie m\u00f3g\u0142 \u015bci\u0105ga\u0107 na egzaminie, bo widzia\u0142 wcze\u015bniej pytania.", "tokens": [51514, 37099, 11, 46591, 66, 447, 9815, 73, 11, 42788, 2645, 72, 277, 281, 11, 11316, 2316, 2838, 275, 14047, 1221, 220, 50227, 3680, 2162, 1667, 24263, 89, 7428, 414, 11, 748, 27486, 8908, 40785, 25878, 5609, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07387287585766285, "compression_ratio": 1.4245283018867925, "no_speech_prob": 0.44841352105140686}, {"id": 123, "seek": 65050, "start": 650.5, "end": 658.5, "text": " W\u0142a\u015bnie tak. To gwarantuje, \u017ce wyniki, kt\u00f3re uzyskali, s\u0105 miarodajne i faktycznie odzwierciedlaj\u0105 zdolno\u015bci modelu.", "tokens": [50364, 343, 5024, 12221, 991, 13, 1407, 290, 6925, 394, 13008, 11, 3561, 31936, 9850, 11, 8864, 16851, 749, 74, 5103, 11, 9015, 2752, 289, 378, 1805, 716, 741, 33647, 45586, 3611, 14406, 811, 537, 292, 875, 8555, 16221, 401, 16438, 2316, 84, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07209114233652751, "compression_ratio": 1.3525641025641026, "no_speech_prob": 0.015668731182813644}, {"id": 124, "seek": 65050, "start": 658.5, "end": 667.5, "text": " Dobrze, a wi\u0119c mamy pot\u0119\u017cn\u0105 in\u017cynieri\u0119 i starannie przygotowane dane. Czy ta gigantyczna inwestycja si\u0119 op\u0142aci\u0142a? Jakie by\u0142y rezultaty?", "tokens": [50764, 29679, 13503, 11, 257, 16677, 17335, 1847, 1274, 1427, 13113, 294, 1427, 2534, 811, 5034, 741, 3543, 43433, 35914, 23066, 49206, 13, 19832, 1846, 8741, 394, 17466, 629, 294, 86, 7819, 34056, 3244, 999, 1221, 22086, 5024, 30, 15029, 414, 26366, 48060, 723, 21398, 30, 51214], "temperature": 0.0, "avg_logprob": -0.07209114233652751, "compression_ratio": 1.3525641025641026, "no_speech_prob": 0.015668731182813644}, {"id": 125, "seek": 65050, "start": 667.5, "end": 673.5, "text": " W wielu zadaniach model ustanowi\u0142 nowe rekordy, czyli tak zwane SOTA \u2013 State of the Art.", "tokens": [51214, 343, 40437, 42788, 3782, 608, 2316, 26189, 282, 24503, 1221, 586, 68, 33881, 765, 88, 11, 16591, 991, 11873, 1929, 318, 5068, 32, 1662, 4533, 295, 264, 5735, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07209114233652751, "compression_ratio": 1.3525641025641026, "no_speech_prob": 0.015668731182813644}, {"id": 126, "seek": 65050, "start": 673.5, "end": 677.5, "text": " Najbardziej spektakularny przyk\u0142ad to chyba zadanie LAMBADA.", "tokens": [51514, 31576, 40392, 768, 2320, 514, 1040, 1634, 23144, 281, 31532, 42788, 7155, 441, 2865, 33, 45852, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07209114233652751, "compression_ratio": 1.3525641025641026, "no_speech_prob": 0.015668731182813644}, {"id": 127, "seek": 67750, "start": 677.5, "end": 681.5, "text": " LAMBADA, czyli ten test przewidywania ostatniego s\u0142owa w zdaniu.", "tokens": [50364, 441, 2865, 33, 45852, 11, 16591, 2064, 1500, 39758, 327, 27112, 5609, 32686, 2766, 1571, 15116, 5528, 261, 16221, 25849, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09334421527478122, "compression_ratio": 1.4022988505747127, "no_speech_prob": 0.1461988389492035}, {"id": 128, "seek": 67750, "start": 681.5, "end": 693.5, "text": " Tak, kt\u00f3ry wymaga g\u0142\u0119bokiego zrozumienia kontekstu. I tutaj MTNLG w trybie ZEROSHOT, czyli bez \u017cadnych przyk\u0142ad\u00f3w, osi\u0105gn\u0105\u0142 dok\u0142adno\u015b\u0107 prawie 80%.", "tokens": [50564, 9118, 11, 9913, 29764, 9286, 18117, 1274, 21666, 12200, 710, 27857, 449, 18811, 14373, 916, 372, 84, 13, 286, 12749, 37333, 45, 43, 38, 261, 853, 7392, 1176, 1598, 4367, 39, 5068, 11, 16591, 10782, 39628, 9399, 23144, 3901, 11, 3003, 11404, 4568, 1611, 1221, 45864, 23293, 3206, 8699, 4688, 6856, 51164], "temperature": 0.0, "avg_logprob": -0.09334421527478122, "compression_ratio": 1.4022988505747127, "no_speech_prob": 0.1461988389492035}, {"id": 129, "seek": 67750, "start": 693.5, "end": 695.5, "text": " 80% bez przyk\u0142ad\u00f3w?", "tokens": [51164, 4688, 4, 10782, 23144, 3901, 30, 51264], "temperature": 0.0, "avg_logprob": -0.09334421527478122, "compression_ratio": 1.4022988505747127, "no_speech_prob": 0.1461988389492035}, {"id": 130, "seek": 67750, "start": 695.5, "end": 701.5, "text": " Tak, i to jest wynik lepszy ni\u017c ten, kt\u00f3ry GPT-3 osi\u0105gn\u0105\u0142 w trybie FUSHOT, czyli po zobaczeniu kilku przyk\u0142ad\u00f3w.", "tokens": [51264, 9118, 11, 741, 281, 3492, 31936, 1035, 476, 1878, 1229, 28502, 2064, 11, 9913, 26039, 51, 12, 18, 3003, 11404, 4568, 1611, 1221, 261, 853, 7392, 479, 3447, 39, 5068, 11, 16591, 714, 25100, 326, 39651, 5128, 5279, 23144, 3901, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09334421527478122, "compression_ratio": 1.4022988505747127, "no_speech_prob": 0.1461988389492035}, {"id": 131, "seek": 70150, "start": 702.5, "end": 713.5, "text": " Ok, pobi\u0142 rekord w zadaniu LAMBADA. Ale wiesz, czy to nie jest troch\u0119 tak, \u017ce my po prostu budujemy coraz wi\u0119ksze m\u0142otki do wbijania tych samych akademickich gwo\u017adzi?", "tokens": [50414, 3477, 11, 714, 5614, 1221, 33881, 765, 261, 42788, 25849, 441, 2865, 33, 45852, 13, 9366, 261, 15347, 11, 6430, 281, 2838, 3492, 24926, 991, 11, 3561, 452, 714, 19518, 3265, 21767, 25899, 29968, 1381, 40770, 310, 2984, 360, 261, 30418, 5609, 15180, 3247, 16384, 9308, 49290, 618, 480, 290, 6120, 10659, 67, 3992, 30, 50964], "temperature": 0.0, "avg_logprob": -0.06276240810271233, "compression_ratio": 1.4548192771084338, "no_speech_prob": 0.48807790875434875}, {"id": 132, "seek": 70150, "start": 713.5, "end": 717.5, "text": " Czy taki wynik naprawd\u0119 przek\u0142ada si\u0119 na lepsze rozumienie \u015bwiata w praktyce?", "tokens": [50964, 19832, 20065, 31936, 1035, 20970, 29785, 46217, 3244, 1667, 476, 1878, 1381, 48797, 27385, 21485, 3274, 261, 3206, 74, 874, 384, 30, 51164], "temperature": 0.0, "avg_logprob": -0.06276240810271233, "compression_ratio": 1.4548192771084338, "no_speech_prob": 0.48807790875434875}, {"id": 133, "seek": 70150, "start": 717.5, "end": 720.5, "text": " To jest \u015bwietnie pytanie i autorzy te\u017c je sobie zadali.", "tokens": [51164, 1407, 3492, 8299, 39083, 2766, 36610, 741, 19510, 1229, 9516, 1506, 13652, 42788, 5103, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06276240810271233, "compression_ratio": 1.4548192771084338, "no_speech_prob": 0.48807790875434875}, {"id": 134, "seek": 70150, "start": 720.5, "end": 730.5, "text": " Dlatego poza suchymi liczbami pokazali te\u017c jako\u015bciowe przyk\u0142ady, kt\u00f3re maj\u0105 pokaza\u0107 kreatywno\u015b\u0107 i elastyczno\u015b\u0107 modelu. I one musz\u0119 przyzna\u0107, robi\u0105 wra\u017cenie.", "tokens": [51314, 47184, 714, 2394, 1270, 88, 3057, 6169, 89, 65, 4526, 13010, 921, 5103, 9516, 17123, 6199, 6880, 6501, 74, 1221, 880, 11, 8864, 26064, 13010, 12257, 2162, 350, 620, 88, 20944, 7753, 741, 806, 9820, 3689, 23293, 2316, 84, 13, 286, 472, 1038, 11052, 6501, 35458, 2162, 11, 3870, 11404, 7843, 41118, 13, 51814], "temperature": 0.0, "avg_logprob": -0.06276240810271233, "compression_ratio": 1.4548192771084338, "no_speech_prob": 0.48807790875434875}, {"id": 135, "seek": 73050, "start": 730.5, "end": 738.5, "text": " Model potrafi\u0142 rozwi\u0105zywa\u0107 zagadki, kt\u00f3re jeden z badaczy sam wymier\u015bli\u0142, \u017ceby mie\u0107 pewno\u015b\u0107, \u017ce nie ma ich w danych treningowych.", "tokens": [50364, 17105, 1847, 10437, 40622, 9544, 18234, 1229, 25234, 27001, 345, 2984, 11, 8864, 12906, 710, 1578, 14691, 3247, 29764, 811, 15350, 1221, 11, 11316, 35612, 33002, 7753, 11, 3561, 2838, 463, 1893, 261, 274, 34644, 2192, 773, 19605, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10312398046040706, "compression_ratio": 1.4072847682119205, "no_speech_prob": 0.06038695573806763}, {"id": 136, "seek": 73050, "start": 738.5, "end": 747.5, "text": " Odpowiada\u0142 na pytania z teleturnieju je DROPERDEE, co wymaga nie tylko wiedzy, ale te\u017c zrozumienia tej specyficznej odwr\u00f3conej sk\u0142adni.", "tokens": [50764, 12210, 14701, 39018, 1221, 1667, 25878, 5609, 710, 15284, 302, 925, 7764, 84, 1506, 413, 7142, 47, 1598, 35, 7258, 11, 598, 29764, 9286, 2838, 13219, 46894, 1229, 11, 6775, 9516, 710, 27857, 449, 18811, 12573, 768, 1344, 1786, 89, 11794, 3611, 7449, 812, 66, 546, 73, 1110, 10358, 3722, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10312398046040706, "compression_ratio": 1.4072847682119205, "no_speech_prob": 0.06038695573806763}, {"id": 137, "seek": 73050, "start": 747.5, "end": 756.5, "text": " Czyta\u0142em te\u017c o generowaniu kodu. To ju\u017c w og\u00f3le brzmi jak magia. Potrafi\u0142 wygenerowa\u0107 dzia\u0142aj\u0105cy program na podstawie samego komentarza.", "tokens": [51214, 19832, 1328, 11126, 9516, 277, 1337, 305, 25849, 350, 34873, 13, 1407, 10678, 261, 29229, 738, 89, 3057, 4207, 2258, 654, 13, 9145, 10437, 40622, 4628, 21848, 11445, 27121, 11133, 1344, 1461, 1667, 43443, 414, 912, 1571, 5207, 32067, 2394, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10312398046040706, "compression_ratio": 1.4072847682119205, "no_speech_prob": 0.06038695573806763}, {"id": 138, "seek": 75650, "start": 756.5, "end": 765.5, "text": " Tak i to jest chyba najbardziej uderzaj\u0105cy przyk\u0142ad. Poproszono go o napisanie funkcji w Pajtonie, kt\u00f3ra oblicza odleg\u0142o\u015b\u0107 Levensteina.", "tokens": [50364, 9118, 741, 281, 3492, 31532, 41857, 344, 1068, 89, 11133, 1344, 23144, 13, 10215, 2635, 89, 8957, 352, 277, 9296, 271, 7155, 26476, 19649, 261, 430, 1805, 1756, 414, 11, 19456, 1111, 1050, 2394, 277, 2285, 70, 44742, 1456, 553, 2941, 1426, 13, 50814], "temperature": 0.0, "avg_logprob": -0.06437851832463191, "compression_ratio": 1.4664634146341464, "no_speech_prob": 0.23385213315486908}, {"id": 139, "seek": 75650, "start": 765.5, "end": 770.5, "text": " Czyli miar\u0119 podobie\u0144stwa mi\u0119dzy dwoma tekstami. To nie jest trywialny algorytm.", "tokens": [50814, 37099, 2752, 289, 1274, 43024, 414, 12229, 4151, 33964, 27379, 6440, 16624, 372, 4526, 13, 1407, 2838, 3492, 853, 86, 831, 1634, 3501, 827, 83, 76, 13, 51064], "temperature": 0.0, "avg_logprob": -0.06437851832463191, "compression_ratio": 1.4664634146341464, "no_speech_prob": 0.23385213315486908}, {"id": 140, "seek": 75650, "start": 770.5, "end": 777.5, "text": " Zdecydowanie nie. A on wygenerowa\u0142 kod, kt\u00f3ry nie tylko dzia\u0142a\u0142, ale by\u0142 te\u017c czysty i zgodny z dobrymi praktykami.", "tokens": [51064, 1176, 1479, 1344, 67, 22028, 2838, 13, 316, 322, 4628, 21848, 30105, 350, 378, 11, 9913, 2838, 13219, 37903, 1221, 11, 6775, 16673, 9516, 6430, 25134, 741, 710, 21787, 1634, 710, 35884, 3057, 3206, 74, 874, 48737, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06437851832463191, "compression_ratio": 1.4664634146341464, "no_speech_prob": 0.23385213315486908}, {"id": 141, "seek": 75650, "start": 777.5, "end": 785.5, "text": " To pokazuje, \u017ce on nie tylko rozumie j\u0119zyk naturalny, ale potrafi to prze\u0142o\u017cy\u0107 na j\u0119zyk formalny, jakim jest kod programistyczny.", "tokens": [51414, 1407, 13010, 43317, 11, 3561, 322, 2838, 13219, 48797, 414, 49055, 74, 3303, 1634, 11, 6775, 1847, 10437, 72, 281, 8325, 5249, 39687, 1667, 49055, 74, 9860, 1634, 11, 49410, 3492, 350, 378, 1461, 468, 17466, 1634, 13, 51814], "temperature": 0.0, "avg_logprob": -0.06437851832463191, "compression_ratio": 1.4664634146341464, "no_speech_prob": 0.23385213315486908}, {"id": 142, "seek": 78550, "start": 785.5, "end": 791.5, "text": " Ok, to wszystko brzmi jak historia ogromnego sukcesu. Prawie. Prawie zbyt idealnie.", "tokens": [50364, 3477, 11, 281, 22607, 738, 89, 3057, 4207, 18385, 34416, 298, 11858, 46432, 887, 84, 13, 430, 5131, 414, 13, 430, 5131, 414, 710, 2322, 83, 7157, 2766, 13, 50664], "temperature": 0.0, "avg_logprob": -0.094175572298011, "compression_ratio": 1.4635761589403973, "no_speech_prob": 0.13951554894447327}, {"id": 143, "seek": 78550, "start": 791.5, "end": 801.5, "text": " Czytaj\u0105c te prace czeka\u0142em na moment, w kt\u00f3rym autorzy powiedz\u0105 ale i ten moment nadsze\u0142, prawda, po\u015bwi\u0119cili ca\u0142y rozdzia\u0142 na problemy i ograniczenia.", "tokens": [50664, 19832, 1328, 8555, 66, 535, 582, 617, 6472, 36361, 11126, 1667, 1623, 11, 261, 30120, 19510, 1229, 3388, 1091, 8925, 6775, 741, 2064, 1623, 297, 5834, 1381, 1221, 11, 43607, 11, 714, 1788, 22423, 66, 2312, 35226, 9544, 28168, 8908, 1667, 1154, 88, 741, 34416, 30732, 14320, 13, 51164], "temperature": 0.0, "avg_logprob": -0.094175572298011, "compression_ratio": 1.4635761589403973, "no_speech_prob": 0.13951554894447327}, {"id": 144, "seek": 78550, "start": 801.5, "end": 810.5, "text": " I to jest moim zdaniem jeden z najmocniejszych punkt\u00f3w tej publikacji. Naukowa uczciwo\u015b\u0107. Nie pr\u00f3bowali zamiata\u0107 problem\u00f3w pod dywan.", "tokens": [51164, 286, 281, 3492, 48569, 710, 10312, 4907, 12906, 710, 11212, 76, 905, 10402, 45021, 39561, 3901, 12573, 11227, 1035, 13152, 13, 426, 1459, 74, 5528, 35403, 537, 48847, 13, 12016, 8565, 8202, 5103, 710, 4526, 3274, 2162, 1154, 3901, 2497, 14584, 7916, 13, 51614], "temperature": 0.0, "avg_logprob": -0.094175572298011, "compression_ratio": 1.4635761589403973, "no_speech_prob": 0.13951554894447327}, {"id": 145, "seek": 78550, "start": 810.5, "end": 814.5, "text": " Wr\u0119cz przeciwnie bardzo szczeg\u00f3\u0142owo je przeanalizowali.", "tokens": [51614, 10159, 1274, 3689, 39622, 14215, 9034, 22090, 1146, 16181, 19941, 1506, 8325, 29702, 590, 305, 5103, 13, 51814], "temperature": 0.0, "avg_logprob": -0.094175572298011, "compression_ratio": 1.4635761589403973, "no_speech_prob": 0.13951554894447327}, {"id": 146, "seek": 81450, "start": 814.5, "end": 824.5, "text": " Ale zanim do nich przyjdziemy, jest jeszcze jedno ciekawe pozytywne odkrycie. Dotyczy ono tego, czy model naprawd\u0119 rozumie j\u0119zyk, czy tylko odtwarza wzorce.", "tokens": [50364, 9366, 710, 17869, 360, 25570, 6501, 73, 13096, 2226, 11, 3492, 14168, 5232, 1771, 30596, 2330, 826, 49358, 874, 86, 716, 3611, 43298, 4260, 13, 413, 6737, 6522, 322, 78, 8627, 11, 6430, 2316, 20970, 48797, 414, 49055, 74, 11, 6430, 13219, 3611, 83, 6925, 2394, 24809, 284, 384, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06233192158636646, "compression_ratio": 1.4225941422594142, "no_speech_prob": 0.11725599318742752}, {"id": 147, "seek": 81450, "start": 824.5, "end": 836.5, "text": " Tak, to fundamentalne pytanie. U\u017cyli do tego specjalnego zbioru danych o nazwie Hans. On jest tak skonstruowany, \u017ceby testowa\u0107, czy model polega na powierzchniowych heurystykach.", "tokens": [50864, 9118, 11, 281, 8088, 716, 36610, 13, 624, 7735, 2081, 360, 8627, 46433, 11858, 710, 33362, 84, 274, 34644, 277, 20151, 8699, 17926, 13, 1282, 3492, 991, 1110, 4068, 894, 23341, 11, 11316, 1500, 11445, 11, 6430, 2316, 13208, 3680, 1667, 3388, 34602, 1377, 72, 19605, 415, 2598, 25134, 41326, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06233192158636646, "compression_ratio": 1.4225941422594142, "no_speech_prob": 0.11725599318742752}, {"id": 148, "seek": 83650, "start": 836.5, "end": 840.5, "text": " Na przyk\u0142ad, czy uwa\u017ca, \u017ce dwa zdania s\u0105 powi\u0105zane, bo maj\u0105 du\u017co wsp\u00f3lnych s\u0142\u00f3w?", "tokens": [50364, 6056, 23144, 11, 6430, 48089, 64, 11, 3561, 35045, 16221, 5609, 9015, 3388, 11404, 89, 1929, 11, 748, 26064, 26673, 47148, 9399, 15116, 3901, 30, 50564], "temperature": 0.0, "avg_logprob": -0.09308345794677735, "compression_ratio": 1.3017241379310345, "no_speech_prob": 0.08729234337806702}, {"id": 149, "seek": 83650, "start": 840.5, "end": 848.5, "text": " Dok\u0142adnie, to cz\u0119sto pu\u0142apka. Testowali, czy model faktycznie rozumie sk\u0142adnie i gramatyk\u0119.", "tokens": [50564, 29768, 10358, 2766, 11, 281, 34369, 2362, 1221, 569, 2330, 13, 9279, 305, 5103, 11, 6430, 2316, 33647, 45586, 48797, 414, 1110, 10358, 2766, 741, 21353, 21398, 15724, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09308345794677735, "compression_ratio": 1.3017241379310345, "no_speech_prob": 0.08729234337806702}, {"id": 150, "seek": 83650, "start": 848.5, "end": 858.5, "text": " I tutaj skala przynios\u0142a jako\u015bciow\u0105 zmian\u0119. MTNLG radzi sobie znacznie lepiej ni\u017c mniejsze modele, jak GPT-2.", "tokens": [50964, 286, 12749, 1110, 5159, 6501, 77, 2717, 5024, 17123, 6199, 30297, 43591, 1274, 13, 37333, 45, 43, 38, 2843, 3992, 13652, 15397, 14875, 2766, 476, 39699, 28502, 275, 44258, 4391, 306, 11, 4207, 26039, 51, 12, 17, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09308345794677735, "compression_ratio": 1.3017241379310345, "no_speech_prob": 0.08729234337806702}, {"id": 151, "seek": 85850, "start": 858.5, "end": 869.5, "text": " Mniej daje si\u0119 nabra\u0107 na te proste sztuczki. To mocna przes\u0142anka, \u017ce wi\u0119ksze modele faktycznie rozwijaj\u0105 g\u0142\u0119bsze zrozumienie systematyczno\u015bci j\u0119zyka.", "tokens": [50364, 376, 10402, 1120, 2884, 3244, 297, 455, 424, 2162, 1667, 535, 10293, 68, 262, 2682, 1311, 89, 2984, 13, 1407, 34962, 629, 6541, 279, 1221, 21729, 11, 3561, 29968, 1381, 4391, 306, 33647, 45586, 9544, 36652, 11133, 18117, 1274, 929, 1381, 710, 27857, 449, 27385, 1185, 267, 17466, 16438, 42309, 40940, 13, 50914], "temperature": 0.0, "avg_logprob": -0.046483795166015626, "compression_ratio": 1.4659090909090908, "no_speech_prob": 0.4690294563770294}, {"id": 152, "seek": 85850, "start": 869.5, "end": 874.5, "text": " Czyli wi\u0119kszy nie znaczy tylko wi\u0119cej tego samego, ale co\u015b jako\u015bciowo innego.", "tokens": [50914, 37099, 29968, 1229, 2838, 36584, 13219, 26004, 8627, 912, 1571, 11, 6775, 19241, 17123, 6199, 19941, 294, 11858, 13, 51164], "temperature": 0.0, "avg_logprob": -0.046483795166015626, "compression_ratio": 1.4659090909090908, "no_speech_prob": 0.4690294563770294}, {"id": 153, "seek": 85850, "start": 874.5, "end": 876.5, "text": " Wydaje si\u0119, \u017ce tak.", "tokens": [51164, 343, 6655, 11153, 3244, 11, 3561, 991, 13, 51264], "temperature": 0.0, "avg_logprob": -0.046483795166015626, "compression_ratio": 1.4659090909090908, "no_speech_prob": 0.4690294563770294}, {"id": 154, "seek": 85850, "start": 876.5, "end": 882.5, "text": " Ale jest te\u017c druga strona medalu. Okazuje si\u0119, \u017ce to s\u0142ynne in-context learning wcale nie jest magicznym rozwi\u0105zaniem.", "tokens": [51264, 9366, 3492, 9516, 4110, 64, 1056, 4037, 1205, 4929, 13, 3477, 43317, 3244, 11, 3561, 281, 15116, 2534, 716, 294, 12, 9000, 3828, 2539, 261, 37088, 2838, 3492, 5585, 89, 12996, 9544, 18234, 21238, 4907, 13, 51564], "temperature": 0.0, "avg_logprob": -0.046483795166015626, "compression_ratio": 1.4659090909090908, "no_speech_prob": 0.4690294563770294}, {"id": 155, "seek": 88250, "start": 882.5, "end": 887.5, "text": " Absolutnie nie. I to jest kolejne takie troch\u0119 odsze\u017awiaj\u0105ce odkrycie.", "tokens": [50364, 5813, 2308, 2766, 2838, 13, 286, 281, 3492, 23749, 716, 15963, 24926, 3611, 82, 1381, 10659, 86, 48125, 384, 3611, 43298, 4260, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07579865326752534, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.10647837817668915}, {"id": 156, "seek": 88250, "start": 887.5, "end": 893.5, "text": " Eksperymenty pokaza\u0142y, \u017ce ten mechanizm dzia\u0142a bardzo podobnie do tradycyjnego fine tuning.", "tokens": [50614, 462, 1694, 610, 88, 518, 88, 13010, 12257, 6825, 11, 3561, 2064, 4236, 590, 76, 37903, 9034, 43024, 2766, 360, 504, 880, 42949, 11858, 2489, 15164, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07579865326752534, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.10647837817668915}, {"id": 157, "seek": 88250, "start": 893.5, "end": 894.5, "text": " To znaczy?", "tokens": [50914, 1407, 36584, 30, 50964], "temperature": 0.0, "avg_logprob": -0.07579865326752534, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.10647837817668915}, {"id": 158, "seek": 88250, "start": 894.5, "end": 901.5, "text": " To znaczy, \u017ce jako\u015b\u0107 i rozk\u0142ad przyk\u0142ad\u00f3w, kt\u00f3re podamy modelowi w pr\u0105bcie, ma kluczowe znaczenie dla wyniku.", "tokens": [50964, 1407, 36584, 11, 3561, 17123, 7753, 741, 9544, 15317, 23144, 3901, 11, 8864, 2497, 7804, 2316, 24503, 261, 582, 1611, 65, 4260, 11, 463, 9671, 1311, 89, 6880, 15397, 326, 16778, 12285, 31936, 24320, 13, 51314], "temperature": 0.0, "avg_logprob": -0.07579865326752534, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.10647837817668915}, {"id": 159, "seek": 88250, "start": 901.5, "end": 909.5, "text": " Je\u015bli damy mu przyk\u0142ady, kt\u00f3re s\u0105 niereprezentatywne albo wr\u0119cz b\u0142\u0119dne, model mo\u017ce przestroi\u0107 si\u0119 w locie i zacz\u0105\u0107 odpowiada\u0107 niepoprawnie.", "tokens": [51314, 37086, 2422, 88, 2992, 6501, 74, 1221, 880, 11, 8864, 9015, 2838, 19919, 265, 14185, 21398, 86, 716, 22622, 928, 1274, 3689, 272, 1221, 6298, 716, 11, 2316, 12034, 44264, 340, 12757, 3244, 261, 1628, 414, 741, 34430, 8925, 2162, 24314, 39018, 2162, 2838, 13872, 424, 14215, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07579865326752534, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.10647837817668915}, {"id": 160, "seek": 90950, "start": 909.5, "end": 913.5, "text": " Mo\u017cna go niejako przeuczy\u0107 na podstawie zaledwie kilku przyk\u0142ad\u00f3w.", "tokens": [50364, 44736, 629, 352, 2838, 73, 18501, 8325, 1311, 27150, 1667, 43443, 414, 710, 5573, 8699, 5128, 5279, 23144, 3901, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08693470826020112, "compression_ratio": 1.3874172185430464, "no_speech_prob": 0.17249414324760437}, {"id": 161, "seek": 90950, "start": 913.5, "end": 918.5, "text": " Dok\u0142adnie. To pokazuje, jak bardzo jest wra\u017cliwy na kontekst, kt\u00f3ry mu dostarczamy.", "tokens": [50564, 29768, 10358, 2766, 13, 1407, 13010, 43317, 11, 4207, 9034, 3492, 7843, 1427, 2081, 9726, 1667, 14373, 916, 372, 11, 9913, 2992, 20568, 289, 3689, 7804, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08693470826020112, "compression_ratio": 1.3874172185430464, "no_speech_prob": 0.17249414324760437}, {"id": 162, "seek": 90950, "start": 918.5, "end": 928.5, "text": " Prze\u015bledzili\u015bmy wi\u0119c ca\u0142\u0105 drog\u0119 od gigantycznych wyzwa\u0144 in\u017cyniernych przez imponuj\u0105ce wyniki, a\u017c pot\u0119powa\u017cne i odsze\u017awiaj\u0105ce wady.", "tokens": [50814, 2114, 1381, 1788, 1493, 89, 43912, 16677, 1335, 15926, 3789, 70, 1274, 3611, 8741, 394, 17466, 9399, 4628, 89, 4151, 5248, 294, 1427, 2534, 811, 9399, 14064, 704, 266, 13263, 384, 31936, 9850, 11, 48134, 1847, 1274, 14701, 18264, 716, 741, 3611, 82, 1381, 10659, 86, 48125, 384, 261, 880, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08693470826020112, "compression_ratio": 1.3874172185430464, "no_speech_prob": 0.17249414324760437}, {"id": 163, "seek": 90950, "start": 928.5, "end": 933.5, "text": " Co to wszystko oznacza, je\u015bli spojrzymy na to z szerszej perspektywy?", "tokens": [51314, 3066, 281, 22607, 277, 22672, 326, 2394, 11, 25630, 8243, 73, 13047, 2226, 1667, 281, 710, 7870, 433, 16920, 868, 32659, 874, 9726, 30, 51564], "temperature": 0.0, "avg_logprob": -0.08693470826020112, "compression_ratio": 1.3874172185430464, "no_speech_prob": 0.17249414324760437}, {"id": 164, "seek": 90950, "start": 933.5, "end": 935.5, "text": " My\u015bl\u0119, \u017ce ta praca jest kamieniem milowym.", "tokens": [51564, 1222, 28749, 11, 3561, 1846, 582, 6628, 3492, 9727, 1053, 4907, 1962, 31691, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08693470826020112, "compression_ratio": 1.3874172185430464, "no_speech_prob": 0.17249414324760437}, {"id": 165, "seek": 93550, "start": 935.5, "end": 945.5, "text": " Nie tylko ze wzgl\u0119du na sam\u0105 skal\u0119, kt\u00f3ra przesuwa granice tego, co mo\u017cliwe, ale w\u0142a\u015bnie ze wzgl\u0119du na t\u0119 naukow\u0105 uczciwo\u015b\u0107 w przedstawianiu problem\u00f3w.", "tokens": [50364, 12016, 13219, 5277, 48538, 1274, 769, 1667, 3247, 1611, 16890, 1274, 11, 19456, 6541, 279, 84, 4151, 9370, 573, 8627, 11, 598, 30854, 826, 11, 6775, 14234, 5277, 48538, 1274, 769, 1667, 32489, 35616, 74, 30297, 35403, 537, 48847, 261, 45616, 952, 5951, 1154, 3901, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06718929730928862, "compression_ratio": 1.5460992907801419, "no_speech_prob": 0.4840375781059265}, {"id": 166, "seek": 93550, "start": 945.5, "end": 948.5, "text": " Ale z pewno\u015bci\u0105 nie jest warunkiem wystarczaj\u0105cym.", "tokens": [50864, 9366, 710, 33002, 50227, 2838, 3492, 1516, 3197, 4907, 4628, 9710, 3689, 11133, 1344, 76, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06718929730928862, "compression_ratio": 1.5460992907801419, "no_speech_prob": 0.4840375781059265}, {"id": 167, "seek": 93550, "start": 948.5, "end": 955.5, "text": " Czyli przysz\u0142o\u015b\u0107 to nie tylko wi\u0119ksze, ale przede wszystkim m\u0105drzejsze i bardziej niezawodne modele.", "tokens": [51014, 37099, 44018, 44742, 281, 2838, 13219, 29968, 1381, 11, 6775, 44786, 30481, 275, 18962, 13503, 25530, 1381, 741, 27209, 33511, 1607, 378, 716, 4391, 306, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06718929730928862, "compression_ratio": 1.5460992907801419, "no_speech_prob": 0.4840375781059265}, {"id": 168, "seek": 93550, "start": 955.5, "end": 961.5, "text": " Dok\u0142adnie. Praca nad architektur\u0105, nad jako\u015bci\u0105 danych, a przede wszystkim nad zwi\u0119kszaniem niezawodno\u015bci", "tokens": [51364, 29768, 10358, 2766, 13, 2114, 6628, 12617, 3912, 642, 2320, 374, 1611, 11, 12617, 17123, 50227, 274, 34644, 11, 257, 44786, 30481, 12617, 11873, 5034, 1694, 21238, 4907, 33511, 1607, 378, 16438, 51664], "temperature": 0.0, "avg_logprob": -0.06718929730928862, "compression_ratio": 1.5460992907801419, "no_speech_prob": 0.4840375781059265}, {"id": 169, "seek": 96150, "start": 961.5, "end": 969.5, "text": " i rozumieniem kiedy i dlaczego model pope\u0142nia b\u0142\u0119dy, jest r\u00f3wnie, je\u015bli nie bardziej istotna ni\u017c samoskalowanie.", "tokens": [50364, 741, 48797, 1053, 4907, 18777, 741, 37873, 39329, 2316, 42248, 1221, 12679, 272, 46564, 3173, 11, 3492, 11416, 14215, 11, 25630, 2838, 27209, 1418, 310, 629, 28502, 3247, 329, 19990, 22028, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06205696662267049, "compression_ratio": 1.4172932330827068, "no_speech_prob": 0.10540040582418442}, {"id": 170, "seek": 96150, "start": 969.5, "end": 974.5, "text": " Ta publikacja to zar\u00f3wno celebracja niesamowitych osi\u0105gni\u0119\u0107 in\u017cyniernych,", "tokens": [50764, 6551, 11227, 1035, 23395, 281, 22675, 812, 20944, 3886, 23395, 48100, 335, 305, 507, 339, 3003, 11404, 70, 35938, 2162, 294, 1427, 2534, 811, 9399, 11, 51014], "temperature": 0.0, "avg_logprob": -0.06205696662267049, "compression_ratio": 1.4172932330827068, "no_speech_prob": 0.10540040582418442}, {"id": 171, "seek": 96150, "start": 974.5, "end": 978.5, "text": " jak i bardzo powa\u017cne wezwanie do dzia\u0142ania dla ca\u0142ej spo\u0142eczno\u015bci badawczej.", "tokens": [51014, 4207, 741, 9034, 3388, 18264, 716, 321, 14406, 7155, 360, 27121, 5609, 12285, 47631, 73, 36851, 89, 16438, 272, 1538, 86, 9680, 73, 13, 51214], "temperature": 0.0, "avg_logprob": -0.06205696662267049, "compression_ratio": 1.4172932330827068, "no_speech_prob": 0.10540040582418442}, {"id": 172, "seek": 96150, "start": 978.5, "end": 984.5, "text": " Na koniec mo\u017ce zostawmy naszym s\u0142uchaczom my\u015bl do refleksji, kt\u00f3ra wy\u0142ania si\u0119 z tej pracy.", "tokens": [51214, 6056, 5897, 35733, 12034, 31873, 1607, 2226, 48094, 15116, 625, 14875, 298, 452, 19212, 360, 36549, 1694, 4013, 11, 19456, 4628, 1221, 5609, 3244, 710, 12573, 35591, 13, 51514], "temperature": 0.0, "avg_logprob": -0.06205696662267049, "compression_ratio": 1.4172932330827068, "no_speech_prob": 0.10540040582418442}, {"id": 173, "seek": 98450, "start": 984.5, "end": 987.5, "text": " Hmm, my\u015bl\u0119, \u017ce jest jedna bardzo ciekawa.", "tokens": [50364, 8239, 11, 37730, 11, 3561, 3492, 5232, 629, 9034, 46419, 10449, 13, 50514], "temperature": 0.0, "avg_logprob": -0.06987326093714871, "compression_ratio": 1.4305084745762713, "no_speech_prob": 0.008223560638725758}, {"id": 174, "seek": 98450, "start": 987.5, "end": 994.5, "text": " Ta praca pokazuje, \u017ce nawet pot\u0119\u017cny, wytrenowany na bilionach s\u0142\u00f3w model AI,", "tokens": [50514, 6551, 582, 6628, 13010, 43317, 11, 3561, 22696, 1847, 1274, 1427, 1634, 11, 261, 4328, 1095, 23341, 1667, 8588, 313, 608, 15116, 3901, 2316, 7318, 11, 50864], "temperature": 0.0, "avg_logprob": -0.06987326093714871, "compression_ratio": 1.4305084745762713, "no_speech_prob": 0.008223560638725758}, {"id": 175, "seek": 98450, "start": 994.5, "end": 998.5, "text": " mo\u017ce zosta\u0107 przeuczony i zacz\u0105\u0107 pope\u0142nia\u0107 systematyczne b\u0142\u0119dy", "tokens": [50864, 12034, 23154, 2162, 8325, 1311, 44479, 741, 34430, 8925, 2162, 42248, 1221, 12679, 2162, 1185, 267, 17466, 716, 272, 46564, 3173, 51064], "temperature": 0.0, "avg_logprob": -0.06987326093714871, "compression_ratio": 1.4305084745762713, "no_speech_prob": 0.008223560638725758}, {"id": 176, "seek": 98450, "start": 998.5, "end": 1003.5, "text": " na podstawie zaledwie kilku tendencyjnie dobranych przyk\u0142ad\u00f3w, kt\u00f3re podamy mu w pr\u0105cie.", "tokens": [51064, 1667, 43443, 414, 710, 5573, 8699, 5128, 5279, 3928, 3020, 73, 2766, 23067, 34644, 23144, 3901, 11, 8864, 2497, 7804, 2992, 261, 582, 1611, 4260, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06987326093714871, "compression_ratio": 1.4305084745762713, "no_speech_prob": 0.008223560638725758}, {"id": 177, "seek": 98450, "start": 1003.5, "end": 1004.5, "text": " Racja.", "tokens": [51314, 497, 23395, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06987326093714871, "compression_ratio": 1.4305084745762713, "no_speech_prob": 0.008223560638725758}, {"id": 178, "seek": 98450, "start": 1004.5, "end": 1011.5, "text": " Wi\u0119c skoro tak \u0142atwo jest p\u0142yn\u0105\u0107 na maszyn\u0119, kt\u00f3ra przetworzy\u0142a wi\u0119cej tekstu ni\u017c jakikolwiek cz\u0142owiek w historii,", "tokens": [51364, 32508, 1110, 10780, 991, 47759, 6120, 3492, 28695, 2534, 36374, 1667, 2300, 1229, 77, 1274, 11, 19456, 6541, 302, 28321, 1229, 5024, 26004, 16624, 372, 84, 28502, 4207, 1035, 401, 44674, 36282, 74, 261, 4058, 5597, 11, 51714], "temperature": 0.0, "avg_logprob": -0.06987326093714871, "compression_ratio": 1.4305084745762713, "no_speech_prob": 0.008223560638725758}, {"id": 179, "seek": 101150, "start": 1011.5, "end": 1016.5, "text": " to co to m\u00f3wi o naszej ludzkiej sk\u0142onno\u015bci do wyci\u0105gania daleko id\u0105cych wniosk\u00f3w", "tokens": [50364, 281, 598, 281, 24592, 277, 42946, 15946, 30154, 7764, 1110, 1221, 266, 16438, 360, 4628, 34381, 1275, 654, 11702, 34241, 4496, 1611, 31306, 45368, 2717, 23849, 50614], "temperature": 0.0, "avg_logprob": -0.07568772633870442, "compression_ratio": 1.25, "no_speech_prob": 0.024924151599407196}, {"id": 180, "seek": 101150, "start": 1016.5, "end": 1023.5, "text": " na podstawie bardzo ograniczonej i cz\u0119sto stronniczej pr\u00f3bki informacji, z kt\u00f3r\u0105 spotykamy si\u0119 na co dzie\u0144.", "tokens": [50614, 1667, 43443, 414, 9034, 34416, 30732, 16896, 73, 741, 34369, 45766, 7692, 16920, 8565, 65, 2984, 1356, 13152, 11, 710, 37415, 4008, 46127, 7804, 3244, 1667, 598, 47568, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07568772633870442, "compression_ratio": 1.25, "no_speech_prob": 0.024924151599407196}], "language": "pl"}