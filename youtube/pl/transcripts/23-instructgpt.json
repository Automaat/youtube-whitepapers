{"text": " Witajcie. Dzi\u015b zanurzymy si\u0119 w problem, kt\u00f3ry jest, no, absolutnym sercem nowoczesnej sztucznej inteligencji. Mhm. Z jednej strony mamy te wielkie modele j\u0119zykowe. Pot\u0119\u017cniejsze ni\u017c cokolwiek, co stworzyli\u015bmy wcze\u015bniej. To prawda. A z drugiej jest z nimi, no wiesz, fundamentalny k\u0142opot. One cz\u0119sto w og\u00f3le nie rozumiej\u0105 o co nam tak naprawd\u0119 chodzi. S\u0105 jak taki dreamslampy, prawda, spe\u0142nia\u0142 \u017cyczenie, ale dos\u0142ownie, ignoruj\u0105c zupe\u0142nie intencje. Dok\u0142adnie. Co mo\u017ce prowadzi\u0107 do, no, katastrofy? Mog\u0105 generowa\u0107 odpowiedzi nieprawdziwe, toksyczne albo po prostu bezu\u017cyteczne. W badaniach ten problem ma swoj\u0105 nazw\u0119. Missalignment. Tak, niedopasowanie. I dzisiaj przeanalizujemy prac\u0119, kt\u00f3ra c\u00f3\u017c nie tyle poprawi\u0142a t\u0119 sytuacj\u0119, co ca\u0142kowicie zmieni\u0142a zasady gry. Pokaza\u0142a, jak nauczy\u0107 te cyfrowe umys\u0142y, by naprawd\u0119 nas s\u0142ucha\u0142y. Dok\u0142adnie. M\u00f3wimy o czasach tu\u017c po premierze d\u017abit i trzy. Wtedy wszyscy byli pod ogromnym wra\u017ceniem jego mo\u017cliwo\u015bci, ale jednocze\u015bnie... Dostrzegali te irytuj\u0105ce niedoci\u0105gni\u0119cia. W\u0142a\u015bnie. Czasem wr\u0119cz niebezpieczne. I celem tej prze\u0142omowej pracy by\u0142o zmierzenie si\u0119 z tym problemem czo\u0142owo. Ambicja by\u0142a jasno okre\u015blona. Stworzy\u0107 modele, kt\u00f3re s\u0105 bardziej pomocne, czyli helpful, uczciwe, honest i nieszkodliwe, harmless. To s\u0142ynne 3H bezpiecze\u0144stwa AI. I co ciekawe, nie poszli drog\u0105, zbudujmy jeszcze wi\u0119kszy model. Nie. Zamiast tego zastosowali metod\u0119, kt\u00f3ra zdefiniowa\u0142a na nowo ca\u0142\u0105 dziedzin\u0119. Reinforcement learning from human feedback. W skr\u00f3cie RLHF. RLHF to termin, kt\u00f3ry dzi\u015b s\u0142yszymy absolutnie wsz\u0119dzie. Ale to w\u0142a\u015bnie ta praca go tak spopularyzowa\u0142a. Tak jest. Zanim jednak roz\u0142o\u017cymy go na czynniki pierwsze, rzu\u0107my na st\u00f3\u0142 fakt, kt\u00f3ry pokazuje skal\u0119 tego prze\u0142omu. Wiesz, co jest niesamowite? Co takiego? W testach por\u00f3wnawczych model stworzony t\u0105 metod\u0105, nazwany Instruct GPT, mia\u0142 zaledwie jeden przecinek 3 miliarda parametr\u00f3w. A by\u0142 znacznie cz\u0119\u015bciej wybierany przez ludzi ni\u017c oryginalny, pot\u0119\u017cny GPT-3. Kt\u00f3ry mia\u0142 175 miliard\u00f3w parametr\u00f3w. W\u0142a\u015bnie. To jest ponad stukrotna r\u00f3\u017cnica w wielko\u015bci. Jak to w og\u00f3le jest mo\u017cliwie? To kompletnie zaprzeka ca\u0142ej filozofii bigger is better, kt\u00f3ra wtedy dominowa\u0142a. I to jest w\u0142a\u015bnie, wiesz, pi\u0119kno tej pracy. Udowodni\u0142a, \u017ce jako\u015b\u0107 treningu i to dopasowanie do intencji u\u017cytkownika mog\u0105 by\u0107 wa\u017cniejsze ni\u017c surowa moc. ...ni\u017c sama liczba parametr\u00f3w. Czyli nie si\u0142a, a spos\u00f3b? Dok\u0142adnie. Bo przed LHF jakie by\u0142y g\u0142\u00f3wne metody, \u017ceby okie\u0142zna\u0107 te modele? Pr\u00f3bowano, no, generalnie dw\u00f3ch rzeczy. Wi\u0119cej danych? Tak. Po pierwsze jeszcze wi\u0119cej danych z nadziej\u0105, \u017ce model w ko\u0144cu za\u0142apie, a po drugie prompt engineering. A, czyli \u017c\u00f3\u0142udne formu\u0142owanie polece\u0144. Troch\u0119 jak zaklinanie deszczu. Trzeba by\u0142o znale\u017a\u0107 t\u0119 magiczn\u0105 formu\u0142k\u0119. W\u0142a\u015bnie. To by\u0142o niestabilne, nieprzewidywalne, a metoda RLAH wprowadzi\u0142a systematyczno\u015b\u0107 i ona opiera si\u0119 na genialnym swojej prostocie pomy\u015ble. Jakim? Je\u015bli chcesz, \u017ceby model zachowywa\u0142 si\u0119 tak jak lubi\u0105 ludzie, to po prostu zapytaj ludzi o zdanie i u\u017cyj ich opinii jako sygna\u0142u do nauki. Proste. A ca\u0142y proces, oparty na informacjach zwrotnych od specjalnie przeszkolonego zespo\u0142u 40 os\u00f3b, tak zwanych lablers, sk\u0142ada si\u0119 z trzech kluczowych krok\u00f3w. Okej, zatrzymajmy si\u0119 tu na chwil\u0119. 40 lablers. To jest kluczowe pytanie. Kim byli ci ludzie, ekspertami od AI, lingwistami? Doskona\u0142e pytanie, kt\u00f3re od razu dotyka sedna jednego z ogranicze\u0144, o kt\u00f3rym chowiemy p\u00f3\u017aniej, bo to od ich gustu zale\u017ca\u0142 kszta\u0142t finalnego modelu. Tak. Byli to starannie wyselekcjonowani kontraktorzy, niepracownicy OpenAI. Przeszli specjalne szkolenie, dostali bardzo szczeg\u00f3\u0142owe instrukcje, jak ocenia\u0107 odpowiedzi. Jakie na przyk\u0142ad? Mieli zwraca\u0107 uwag\u0119, czy odpowied\u017a jest pomocna, prawdziwa i czy nie jest szkodliwa. Nie byli to wi\u0119c przypadkowi ludzie. Byli szkoleni, by reprezentowa\u0107 po\u017c\u0105danego u\u017cytkownika z perspektywy badaczy. Rozumiem. I ich praca by\u0142a fundamentem pierwszego kroku, czyli Supervised Fine Tuning, w skr\u00f3cie SFT. SFT. Dostrajanie pod nadzorem. Jak to wygl\u0105da\u0142o w praktyce? Na tym etapie lablers po prostu tworzyli idealne odpowiedzi. Pisali je od zera. Tak. Brali r\u00f3\u017cne polecenia, kt\u00f3re realni u\u017cytkownicy zadawali GPT-3 przez API i pisali na niewzorcowe odpowiedzi. Stworzono w ten spos\u00f3b wysokiej jako\u015bci zbi\u00f3r danych. Para polecenie odpowied\u017a. I na tym dostrajano GPT-3. Dok\u0142adnie. To jakby pokaza\u0107 uczniowi wzorowo rozwi\u0105zane zadania z klucza odpowiedzi. \u017beby nauczy\u0142 si\u0119, jak wygl\u0105da oczekiwany rezultat. Czekaj, czyli na tym pierwszym etapie model uczy si\u0119 po prostu na\u015bladowa\u0107. Ale przecie\u017c wiesz, kreatywno\u015b\u0107 polega na tym, \u017ceby nie zawsze trzyma\u0107 si\u0119 wzoru. Czy to nie tworzy\u0142o ryzyka, \u017ce model stanie si\u0119 nudny? Szablonowy? Absolutnie. I badacze byli tego \u015bwiadomi. Samo na\u015bladowanie to za ma\u0142o. No w\u0142a\u015bnie. Dlatego SFT to tylko rozgrzewka. Taki fundament. To uczy model podstawowego formatu, stylu, ale nie uczy go oceny. Co jest lepsze, a co gorsze w bardziej z\u0142o\u017conych sytuacjach. I tu dochodzimy do kroku drugiego, kt\u00f3ry jest znacznie ciekawsze. Czyli? Trening tak zwanego Reward Model. Modelu nagrody. Model nagrody. Brzmi jak co\u015b, co przyznaje punkty. To jest ten cyfrowy s\u0119dzia, o kt\u00f3rym si\u0119 tyle m\u00f3wi? Dok\u0142adnie tak. To jest s\u0119dzia, krytyk, kurator smaku, jakkolwiek go nazwiemy. W tym kroku bierzemy model po tym pierwszym etapie, po SFT i prosimy go, \u017ceby na jedno polecenie wygenerowa\u0142 kilka r\u00f3\u017cnych odpowiedzi. Na przyk\u0142ad cztery, pi\u0119\u0107. Powiedzmy od czterech do dziewi\u0119ciu. I te odpowiedzi trafiaj\u0105 do naszych labelers. I co oni z nimi robi\u0105? Ich zadaniem nie jest ju\u017c pisanie idealnej odpowiedzi, ale uszeregowanie tych wygenerowanych. Od najlepszej do najgorszej. Aha. Czyli zamiast m\u00f3wi\u0107 tak powinno by\u0107, m\u00f3wi\u0105 to jest lepsze od tego, a to jest najgorsze. To jest znacznie bardziej subtelna informacja. Niezwykle subtelna i niezwykle cenna. I na podstawie milion\u00f3w takich ranking\u00f3w trenowany jest zupe\u0142nie osobny model, w\u0142a\u015bnie reward model. Jego jedyny cel to nauczy\u0107 si\u0119 przewidywa\u0107, kt\u00f3r\u0105 odpowied\u017a cz\u0142owiek uzna za lepsz\u0105. Dostaje dwie odpowiedzi i ma wyplu\u0107 liczb\u0119, kt\u00f3ra m\u00f3wi, te ludzie preferowali bardziej. Staje si\u0119 takim zautomatyzowanym odwzierciedleniem ludzkich preferencji. Ok, to jest fascynuj\u0105ce. Mamy wi\u0119c ucznia z pierwszego kroku, kt\u00f3ry umie pisa\u0107 w dobrym stylu. I mamy krytyka z drugiego, kt\u00f3ry wykszta\u0142ci\u0142 sobie gust. Jak teraz sprawi\u0107 by ten ucz\u0119\u015b zacz\u0105\u0142 tworzy\u0107 arcydzie\u0142a pod okiem krytyka? To idealna analogia. I tu dochodzimy do kroku trzeciego, kt\u00f3ry jest sercem ca\u0142ej metody. Reinforcement learning? Tak. U\u017cywaj\u0105c algorytmu znanego jako PPO, czyli Proximal Policy Optimization, model ucze\u0144 z pierwszego kroku jest dalej optymalizowany. Proces wygl\u0105da tak. Model dostaje polecenie, generuje odpowied\u017a. I ta odpowied\u017a idzie do krytyka. Dok\u0142adnie. Model krytyk przyznaje jej punkty, czyli nagrod\u0119. Algorytm PPO dostosowuje parametry ucznia tak, \u017ceby nast\u0119pnym razem wygenerowa\u0142 odpowied\u017a, kt\u00f3ra dostanie jeszcze wi\u0119cej punkt\u00f3w. Czyli to taki cykl pr\u00f3b i b\u0142\u0119d\u00f3w na ogromn\u0105 skal\u0119. Tak, gdzie model aktywnie eksploruje, co sprawia, \u017ce jego odpowiedzi s\u0105 lepsze w oczach tego cyfrowego s\u0119dziego. Rozumiem. Czyli ju\u017c nie na\u015bladuje statycznych przyk\u0142ad\u00f3w, ale dynamicznie uczy si\u0119, co podoba si\u0119 s\u0119dziemu. A s\u0119dzia nauczy\u0142 si\u0119, co podoba si\u0119 ludziom. Trzy kroki. Poka\u017c wz\u00f3r, naucz ocenia\u0107, a potem nagradzaj. Dok\u0142adnie tak. To brzmi logicznie. Wr\u00f3\u0107my wi\u0119c do efekt\u00f3w. M\u00f3wi\u0142y\u015bmy, \u017ce ludzie mia\u017cd\u0105co preferowali Instrakt GPT. Model 175B Instrakt GPT wygrywa\u0142 z 175B GPT-3 w 85% przypadk\u00f3w. 85%. To nie jest drobna poprawa. To knockout. Co konkretnie sprawi\u0142o, \u017ce te odpowiedzi by\u0142y tak du\u017co lepsze? Zacznijmy od praw dom\u00f3wno\u015bci, bo to by\u0142a pi\u0119ta achillesowa wielkich modeli. One cz\u0119sto haluczynuj\u0105. Czyli zmy\u015blaj\u0105 fakty z ogromn\u0105 pewno\u015bci\u0105 siebie. Tak. A Instrakt GPT wykaza\u0142 tu ogromn\u0105 popraw\u0119. W zadaniach o zamkni\u0119tej dziedzinie, jak streszczenia, wska\u017anik halucynacji spad\u0142 o po\u0142ow\u0119. Z 41% dla GPT-3 do 21% dla Instrakt GPT. To jest gigantyczna r\u00f3\u017cnica, je\u015bli chodzi o wiarygodno\u015b\u0107, o u\u017cyteczno\u015b\u0107, mniej zmy\u015blania, to mniej ryzyka. A co z kwestiami bezpiecze\u0144stwa, jak toksyczno\u015b\u0107? Tutaj wyniki s\u0105 bardziej z\u0142o\u017cone i pokazuj\u0105, jak trudny to jest problem. Czyli nie by\u0142o tak r\u00f3\u017cowo. Instrakt GPT faktycznie generowa\u0142 o jakie\u015b 25% mniej toksycznych odpowiedzi, ale i to jest kluczowy haczyk. Dzia\u0142o si\u0119 tak tylko wtedy, gdy w poleceniu jawnie poproszono go o byciu grzecznym lub pe\u0142nym szacunku. To jest kluczowe. Czyli to nie by\u0142a fundamentalna zmiana w jego naturze. Raczej nauczy\u0142 si\u0119 reagowa\u0107 na konkretne s\u0142owo klucz. To troch\u0119 jak samoch\u00f3d, kt\u00f3ry ma sprawne hamulce tylko gdy krzykniez, prosz\u0119 zatrzymaj si\u0119. To bardziej obej\u015bcie problemu ni\u017c jego rozwi\u0105zanie. Trafn\u0105\u0142 waga. To pokazuje, \u017ce RLHF jest pot\u0119\u017cnym narz\u0119dziem do optymalizacji pod k\u0105tem celu, kt\u00f3ry mu wska\u017cysz. A tutaj celem nadrz\u0119dnym by\u0142a pomocno\u015b\u0107, a niekoniecznie nieszkodliwo\u015b\u0107. A co z uprzedzeniami z biasem? Co jeszcze wa\u017cniejsze, badanie nie wykaza\u0142o \u017cadnej poprawy w kwestii bias na standardowych testach. To dowodzi, \u017ce alignment nie jest monolitem. Dopasowanie w jednym wymiarze nie gwarantuje poprawy w innym. To prowadzi do kolejnego pytania. Czy ten intensywny trening, skupiony na byciu mi\u0142ym i pomocnym, nie odbywa\u0142 si\u0119 kosztem og\u00f3lnych akademickich zdolno\u015bci modelu? Czy nie by\u0142o czego\u015b w rodzaju podatku od dopasowania? \u015awietne pytanie. I badacze nazwali ten problem w\u0142a\u015bnie alignment tax. I tak, na pocz\u0105tku zauwa\u017cyli, \u017ce proces RLHF faktycznie powodowa\u0142 pogorszenie wynik\u00f3w. Na czym? W niekt\u00f3rych standardowych publicznych benchmarkach NLP. Jak sk\u0142ad czy drop model stawa\u0142 si\u0119 lepszy w konwersacji, ale traci\u0142 na wydajno\u015bci w bardziej sformalizowanych zadaniach. Czyli klasyczny kompromis. Co\u015b za co\u015b. I jak sobie z tym poradzili? Bo to brzmi jak powa\u017cna wada. Znaleziono na to zaskakuj\u0105co proste i eleganckie rozwi\u0105zanie. W trakcie trzeciego etapu, czyli treningu reinforcement learning, postanowili domiesza\u0107 do procesu odrobin\u0119 oryginalnych danych treningowych GPT-3. Tych sprzed ca\u0142ego procesu fine tuning? Tak, chodzi\u0142o o to, \u017ceby podczas maksymalizowania nagrody od reward model, algorytm PPO by\u0142 jednocze\u015bnie karany za zbytnie oddalanie si\u0119 od oryginalnego, surowego modelu GPT-3. I w ten spos\u00f3b powsta\u0142 model PPO-PTX. Czekaj, to brzmi prawie zbyt prosto. Czy dodanie odrobiny starych danych naprawd\u0119 wystarczy\u0142o? Oczaza\u0142o si\u0119, \u017ce ten prosty zabieg zniwelowa\u0142 wi\u0119kszo\u015b\u0107 strat w wydajno\u015bci. Mo\u017cna to rozumie\u0107 jako tak\u0105 kotwice, kt\u00f3ra nie pozwala\u0142a modelowi zapomnie\u0107 o swoich fundamentalnych zdolno\u015bciach w pogoni zabyciem jak najbardziej pomocnym. Czyli mo\u017cna mie\u0107 ciastko i zje\u015b\u0107 ciastko? Pod warunkiem, \u017ce proces treningu jest odpowiednio zbalansowany. To by\u0142o ogromne odkrycie. To prowadzi nas do kilku innych zaskakuj\u0105cych wniosk\u00f3w. Jednym z nich by\u0142o to, jak bardzo akademickie benchmarki rozmija\u0142y si\u0119 z rzeczywisto\u015bci\u0105. Zdecydowanie. Okaza\u0142o si\u0119, \u017ce publiczne zbiory danych jak Flan czy T0, kt\u00f3re mia\u0142y uczy\u0107 modele pod\u0105\u017cania za instrukcjami wcale nie odzwierciedla\u0142y tego, jak ludzie naprawd\u0119 u\u017cywaj\u0105 tych narz\u0119dzi. A instrukt GPT trenowany na prawdziwych zapytaniach z API? By\u0142o dniech o niebo lepsze. Zanaliza tych zapyta\u0144 pokaza\u0142a, \u017ce ludzie najcz\u0119\u015bciej u\u017cywaj\u0105 modeli do kreatywnego generowania, do brainstormingu, do otwartej konwersacji, a nie do prostych zada\u0144 klasyfikacyjnych, kt\u00f3re dominuj\u0105 w tych benchmarkach. Dok\u0142adnie. Ca\u0142a spo\u0142eczno\u015b\u0107 badawcza optymalizowa\u0142a modele pod k\u0105tem testu, kt\u00f3rego nikt w prawdziwym \u015bwiecie nie zamiera\u0142 zdawa\u0107. A co z generalizacj\u0105? Czy model nauczy\u0142 si\u0119 by\u0107 pomocny tylko w tym, co widzia\u0142 podczas treningu? I tu jest chyba najbardziej magiczna cz\u0119\u015b\u0107. Nie. Nie? Instrukt GPT nauczy\u0142 si\u0119 stosowa\u0107 do instrukcji w zadaniach, do kt\u00f3rych nie by\u0142 bezpo\u015brednio trenowany. Ca\u0142kiem nie\u017ale radzi\u0142 sobie z poleceniami dotycz\u0105cymi kodu programistycznego, nawet w innych j\u0119zykach. Mimo \u017ce takie przyk\u0142ady stanowi\u0142y z nikom\u0105 cz\u0119\u015b\u0107 danych? Wr\u0119cz \u015bladow\u0105, to jest naprawd\u0119 niesamowite. To sugeruje, \u017ce on nie nauczy\u0142 si\u0119 na pami\u0119\u0107, jak odpowiada\u0107 na 20 typ\u00f3w zapyta\u0144. On w jaki\u015b spos\u00f3b poj\u0105\u0142 abstrakcyjn\u0105 koncepcj\u0119 pod\u0105\u017cania za intencj\u0105. Jak maszyna w og\u00f3le uczy si\u0119 czego\u015b tak ludzkiego? To jest pytanie za miliard dolar\u00f3w. Pokazuje, \u017ce metoda RLHF to nie jest tylko powierzchowne polerowanie. Ona faktycznie popycha model w kierunku bardziej og\u00f3lnego rozumienia, czym jest bycia u\u017cytecznym partnerem w rozmowie. Mimo to nie jest to technologia idealna. Oczywi\u015bcie, \u017ce nie i ta praca bardzo uczciwie podchodzi do swoich ogranicze\u0144. No w\u0142a\u015bnie, zejd\u017amy na ziemi\u0119. Mimo tych wszystkich ulepsze\u0144 ten model wci\u0105\u017c pope\u0142nia\u0142 b\u0142\u0119dy, prawda? Oczywi\u015bcie i to czasem bardzo proste. Bywa\u0142 zagubiony, gdy polecenie opiera\u0142o si\u0119 na fa\u0142szywym za\u0142o\u017ceniu. Jaki\u015b przyk\u0142ad. Na przyk\u0142ad pytanie. Dlaczego wa\u017cne jest jedzenie skarpetek po medytacji? Zamiast zidentyfikowa\u0107 absurd, on pr\u00f3bowa\u0142 wymy\u015bli\u0107 jakie\u015b pseudonaukowe uzasadnienie, bo by\u0142 wytrenowany, \u017ceby by\u0107 pomocnym za wszelk\u0105 cen\u0119. Dok\u0142adnie. Czyli wci\u0105\u017c mamy do czynienia z genialnym studentem, kt\u00f3rego mo\u017cna nabra\u0107 pytaniem o jedzenie skarpetek. To pokazuje, jak daleko jeszcze do prawdziwego rozumienia. A to nie wszystko. Bywa\u0142 te\u017c zbyt asekuracyjny, rozwlek\u0142y. Ale najwa\u017cniejsze ograniczenie i fundamentalne pytanie, kt\u00f3re te badania postawi\u0142y brzmi. Do kogo tak naprawd\u0119 dopasowujemy ten model? Wracamy do tych 40 labellers, bo m\u00f3wimy ludzkie preferencje. Ale to by\u0142y preferencje tych konkretnych 40 os\u00f3b. Dzia\u0142aj\u0105cych wed\u0142ug instrukcji odbadaczy z OpenAI. To nie s\u0105 uniwersalne ludzkie warto\u015bci. To jest sedno problemu. No i najwi\u0119ksze otwarte pytanie w ca\u0142ej dziedzinie alignment. Zachowanie instrukt GPT jest bezpo\u015brednim odzwierciedleniem gustu i warto\u015bci bardzo ma\u0142ej, prawdopodobnie ma\u0142o zr\u00f3\u017cnicowanej demograficznie grupy ludzi z globalnej p\u00f3\u0142nocy. Kt\u00f3rzy zostali poinstruowani przez inn\u0105, jeszcze mniejsz\u0105 grup\u0119 badaczy. Wi\u0119c to, co model uznaje za dobre, jest wynikiem decyzji projektowych niewielkiej, elitarnej grupy. I to pokazuje fundamentalne wyzwanie. Jak skalowa\u0107 ten proces, by uwzgl\u0105dnia\u0142 r\u00f3\u017cnorodno\u015b\u0107 ludzkich warto\u015bci na ca\u0142ym \u015bwiecie. Okej, zebrali\u015bmy mn\u00f3stwo informacji. Spr\u00f3bujmy to wszystko podsumowa\u0107. Co to wszystko oznacza? No c\u00f3\u017c. Wygl\u0105da na to, \u017ce g\u0142\u00f3wne przes\u0142anie tej pracy jest jasne. Technika RLHF okaza\u0142a si\u0119 rewolucyjnie skutecznym sposobem na dopasowanie modeli do ludzkich intencji. Zdecydowanie. To badanie udowodni\u0142o, \u017ce inteligentne, ukierunkowane trenowanie mo\u017ce by\u0107 znacznie wa\u017cniejsze ni\u017c sama surowa skala. Lepsze dopasowanie wygra\u0142o z wi\u0119ksz\u0105 liczb\u0105 parametr\u00f3w. Tak, ta praca po\u0142o\u017cy\u0142a podwaliny pod wszystkie modele konwersacyjne, kt\u00f3re znamy i kt\u00f3rych u\u017cywamy dzisiaj. Od czat GPT po wszystkie inne. Skierowa\u0142a uwag\u0119 ca\u0142ej bran\u017cy z obsesji na punkcie samego powi\u0119kszania modeli. Na my\u015blenie o tym, jak uczyni\u0107 je lepszymi, bardziej u\u017cytecznymi i bezpieczniejszymi narz\u0119dziami. To by\u0142 fundamentalny krok w badaniach nad alignment. Czyli nad zapewnieniem, \u017ce przysz\u0142e, jeszcze pot\u0119\u017cniejsze systemy AI b\u0119d\u0105 dzia\u0142a\u0107 zgodnie z naszymi intencjami. I na koniec zostawmy s\u0142uchaczy z jedn\u0105 my\u015bl\u0105, kt\u00f3ra naturalnie wyp\u0142ywa z tej rozmowy. Sami autorze pracy sugeruj\u0105, \u017ce w przysz\u0142o\u015bci mo\u017cna by p\u00f3j\u015b\u0107 o krok dalej. Skoro model mo\u017cna dopasowa\u0107 do preferencji 40 labelers to mo\u017ce, mo\u017cna by trenowa\u0107 modele dopasowane do warto\u015bci r\u00f3\u017cnych grup spo\u0142ecznych. Kultur. A nawet pojedynczych u\u017cytkownik\u00f3w. To jest logiczna konsekwencja. Technologia na to pozwala. Ale konsekwencje spo\u0142eczne s\u0105 ogromne i, no, niejednoznaczne. Z jednej strony mog\u0142oby to prowadzi\u0107 do bardziej pluralistycznej przysz\u0142o\u015bci. Gdzie technologia s\u0142u\u017cy r\u00f3\u017cnorodnym spo\u0142eczno\u015bciom, zamiast narzuca\u0107 jedn\u0105 dominuj\u0105c\u0105 perspektyw\u0119. Tak. AI, kt\u00f3re rozumieniu anse kulturowe, mog\u0142oby by\u0107 niezwykle warto\u015bciowe. A z drugiej strony. Czy nie grozi\u0142oby to stworzeniem ostatecznych baniek informacyjnych? No w\u0142a\u015bnie. Wyobra\u017amy sobie \u015bwiat, w kt\u00f3rym ka\u017cdy ma swoje w\u0142asne AI, kt\u00f3re nie tylko potwierdza jego \u015bwiatopogl\u0105d, ale robi to w spos\u00f3b niezwykle inteligentny i przekonuj\u0105cy. Czy to nie zabi\u0142oby debaty publicznej? Nie zamkn\u0119\u0142o nas w spersonalizowanych, cyfrowych echo chambers, z kt\u00f3rych nie by\u0142oby ucieczki? To jest pytanie, kt\u00f3re te badania otworzy\u0142y i z kt\u00f3rym jako spo\u0142ecze\u0144stwo dopiero zaczynamy si\u0119 mierzy\u0107.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.5, "text": " Witajcie. Dzi\u015b zanurzymy si\u0119 w problem, kt\u00f3ry jest, no, absolutnym sercem nowoczesnej sztucznej inteligencji.", "tokens": [50364, 42299, 47276, 13, 413, 3992, 1788, 710, 282, 374, 1229, 2226, 3244, 261, 1154, 11, 9913, 3492, 11, 572, 11, 18757, 12996, 816, 26422, 586, 905, 12214, 11794, 262, 2682, 1311, 89, 11794, 24777, 3213, 19649, 13, 50739], "temperature": 0.0, "avg_logprob": -0.15867406078892896, "compression_ratio": 1.3494423791821561, "no_speech_prob": 0.018180742859840393}, {"id": 1, "seek": 0, "start": 7.5, "end": 8.5, "text": " Mhm.", "tokens": [50739, 26272, 13, 50789], "temperature": 0.0, "avg_logprob": -0.15867406078892896, "compression_ratio": 1.3494423791821561, "no_speech_prob": 0.018180742859840393}, {"id": 2, "seek": 0, "start": 8.5, "end": 15.3, "text": " Z jednej strony mamy te wielkie modele j\u0119zykowe. Pot\u0119\u017cniejsze ni\u017c cokolwiek, co stworzyli\u015bmy wcze\u015bniej.", "tokens": [50789, 1176, 5232, 11794, 32406, 17335, 535, 20570, 22872, 4391, 306, 49055, 74, 6880, 13, 9145, 1274, 1427, 44258, 28502, 269, 49207, 44674, 11, 598, 342, 28321, 1229, 38452, 40785, 13, 51129], "temperature": 0.0, "avg_logprob": -0.15867406078892896, "compression_ratio": 1.3494423791821561, "no_speech_prob": 0.018180742859840393}, {"id": 3, "seek": 0, "start": 15.3, "end": 16.3, "text": " To prawda.", "tokens": [51129, 1407, 43607, 13, 51179], "temperature": 0.0, "avg_logprob": -0.15867406078892896, "compression_ratio": 1.3494423791821561, "no_speech_prob": 0.018180742859840393}, {"id": 4, "seek": 0, "start": 16.3, "end": 23.5, "text": " A z drugiej jest z nimi, no wiesz, fundamentalny k\u0142opot. One cz\u0119sto w og\u00f3le nie rozumiej\u0105 o co nam tak naprawd\u0119 chodzi.", "tokens": [51179, 316, 710, 47373, 3492, 710, 297, 10121, 11, 572, 261, 15347, 11, 8088, 1634, 350, 1221, 45225, 13, 1485, 34369, 261, 29229, 2838, 48797, 7764, 1611, 277, 598, 8835, 991, 20970, 23998, 13, 51539], "temperature": 0.0, "avg_logprob": -0.15867406078892896, "compression_ratio": 1.3494423791821561, "no_speech_prob": 0.018180742859840393}, {"id": 5, "seek": 2350, "start": 23.7, "end": 30.2, "text": " S\u0105 jak taki dreamslampy, prawda, spe\u0142nia\u0142 \u017cyczenie, ale dos\u0142ownie, ignoruj\u0105c zupe\u0142nie intencje.", "tokens": [50374, 318, 1611, 4207, 20065, 7505, 75, 1215, 88, 11, 43607, 11, 768, 1221, 77, 8908, 16136, 39043, 11, 6775, 4491, 1221, 648, 414, 11, 14698, 44733, 49922, 560, 22660, 2884, 13, 50699], "temperature": 0.0, "avg_logprob": -0.16897242409842356, "compression_ratio": 1.4104234527687296, "no_speech_prob": 0.3297927677631378}, {"id": 6, "seek": 2350, "start": 30.2, "end": 38.3, "text": " Dok\u0142adnie. Co mo\u017ce prowadzi\u0107 do, no, katastrofy? Mog\u0105 generowa\u0107 odpowiedzi nieprawdziwe, toksyczne albo po prostu bezu\u017cyteczne.", "tokens": [50699, 29768, 10358, 2766, 13, 3066, 12034, 36590, 28496, 360, 11, 572, 11, 16536, 525, 340, 22522, 30, 34327, 1611, 1337, 11445, 36574, 3992, 2838, 79, 15889, 3992, 826, 11, 281, 1694, 17466, 716, 22622, 714, 19518, 10782, 84, 7735, 975, 38491, 13, 51104], "temperature": 0.0, "avg_logprob": -0.16897242409842356, "compression_ratio": 1.4104234527687296, "no_speech_prob": 0.3297927677631378}, {"id": 7, "seek": 2350, "start": 38.3, "end": 42.0, "text": " W badaniach ten problem ma swoj\u0105 nazw\u0119. Missalignment.", "tokens": [51104, 343, 1578, 3782, 608, 2064, 1154, 463, 49194, 20151, 86, 1274, 13, 5275, 304, 41134, 13, 51289], "temperature": 0.0, "avg_logprob": -0.16897242409842356, "compression_ratio": 1.4104234527687296, "no_speech_prob": 0.3297927677631378}, {"id": 8, "seek": 2350, "start": 42.0, "end": 50.7, "text": " Tak, niedopasowanie. I dzisiaj przeanalizujemy prac\u0119, kt\u00f3ra c\u00f3\u017c nie tyle poprawi\u0142a t\u0119 sytuacj\u0119, co ca\u0142kowicie zmieni\u0142a zasady gry.", "tokens": [51289, 9118, 11, 32488, 404, 296, 22028, 13, 286, 25772, 8325, 29702, 590, 21767, 22404, 1274, 11, 19456, 6333, 1427, 2838, 39293, 1665, 5131, 72, 5024, 32489, 28275, 29924, 11, 598, 35224, 74, 305, 28434, 17020, 35462, 5024, 26530, 880, 41974, 13, 51724], "temperature": 0.0, "avg_logprob": -0.16897242409842356, "compression_ratio": 1.4104234527687296, "no_speech_prob": 0.3297927677631378}, {"id": 9, "seek": 5070, "start": 50.7, "end": 55.400000000000006, "text": " Pokaza\u0142a, jak nauczy\u0107 te cyfrowe umys\u0142y, by naprawd\u0119 nas s\u0142ucha\u0142y.", "tokens": [50364, 14958, 12257, 5024, 11, 4207, 49103, 27150, 535, 3185, 69, 1892, 68, 1105, 749, 6825, 11, 538, 20970, 5382, 15116, 26042, 6825, 13, 50599], "temperature": 0.0, "avg_logprob": -0.11096500454092384, "compression_ratio": 1.4044117647058822, "no_speech_prob": 0.0066312020644545555}, {"id": 10, "seek": 5070, "start": 55.400000000000006, "end": 65.4, "text": " Dok\u0142adnie. M\u00f3wimy o czasach tu\u017c po premierze d\u017abit i trzy. Wtedy wszyscy byli pod ogromnym wra\u017ceniem jego mo\u017cliwo\u015bci, ale jednocze\u015bnie...", "tokens": [50599, 29768, 10358, 2766, 13, 376, 3901, 13189, 277, 13190, 608, 2604, 1427, 714, 12689, 1381, 274, 10659, 5260, 741, 34573, 13, 343, 83, 6038, 44232, 538, 2081, 2497, 34416, 298, 12996, 7843, 24930, 4907, 26542, 30854, 36476, 11, 6775, 5232, 26694, 1381, 12221, 485, 51099], "temperature": 0.0, "avg_logprob": -0.11096500454092384, "compression_ratio": 1.4044117647058822, "no_speech_prob": 0.0066312020644545555}, {"id": 11, "seek": 5070, "start": 65.4, "end": 67.7, "text": " Dostrzegali te irytuj\u0105ce niedoci\u0105gni\u0119cia.", "tokens": [51099, 413, 555, 19390, 1146, 5103, 535, 3418, 4328, 13263, 384, 32488, 78, 34381, 70, 35938, 2755, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11096500454092384, "compression_ratio": 1.4044117647058822, "no_speech_prob": 0.0066312020644545555}, {"id": 12, "seek": 5070, "start": 67.7, "end": 77.10000000000001, "text": " W\u0142a\u015bnie. Czasem wr\u0119cz niebezpieczne. I celem tej prze\u0142omowej pracy by\u0142o zmierzenie si\u0119 z tym problemem czo\u0142owo.", "tokens": [51214, 343, 5024, 12221, 13, 383, 24561, 443, 928, 1274, 3689, 2838, 650, 89, 9144, 38491, 13, 286, 1769, 10386, 12573, 8325, 1221, 298, 21091, 35591, 14811, 17020, 811, 16778, 3244, 710, 8107, 1154, 443, 269, 4765, 1221, 19941, 13, 51684], "temperature": 0.0, "avg_logprob": -0.11096500454092384, "compression_ratio": 1.4044117647058822, "no_speech_prob": 0.0066312020644545555}, {"id": 13, "seek": 7710, "start": 77.1, "end": 87.39999999999999, "text": " Ambicja by\u0142a jasno okre\u015blona. Stworzy\u0107 modele, kt\u00f3re s\u0105 bardziej pomocne, czyli helpful, uczciwe, honest i nieszkodliwe, harmless.", "tokens": [50364, 17196, 299, 2938, 23936, 361, 296, 1771, 3133, 265, 19212, 4037, 13, 745, 28321, 27150, 4391, 306, 11, 8864, 9015, 27209, 48962, 716, 11, 16591, 4961, 11, 35403, 537, 826, 11, 2157, 377, 741, 297, 15347, 74, 378, 2081, 826, 11, 40160, 13, 50879], "temperature": 0.0, "avg_logprob": -0.13154062102822697, "compression_ratio": 1.3344947735191637, "no_speech_prob": 0.02287849225103855}, {"id": 14, "seek": 7710, "start": 87.39999999999999, "end": 90.6, "text": " To s\u0142ynne 3H bezpiecze\u0144stwa AI.", "tokens": [50879, 1407, 15116, 2534, 716, 805, 39, 47153, 9680, 12229, 4151, 7318, 13, 51039], "temperature": 0.0, "avg_logprob": -0.13154062102822697, "compression_ratio": 1.3344947735191637, "no_speech_prob": 0.02287849225103855}, {"id": 15, "seek": 7710, "start": 90.6, "end": 94.89999999999999, "text": " I co ciekawe, nie poszli drog\u0105, zbudujmy jeszcze wi\u0119kszy model. Nie.", "tokens": [51039, 286, 598, 30596, 2330, 826, 11, 2838, 1366, 89, 2081, 3789, 70, 1611, 11, 710, 18281, 4579, 2226, 14168, 29968, 1229, 2316, 13, 12016, 13, 51254], "temperature": 0.0, "avg_logprob": -0.13154062102822697, "compression_ratio": 1.3344947735191637, "no_speech_prob": 0.02287849225103855}, {"id": 16, "seek": 7710, "start": 94.89999999999999, "end": 100.0, "text": " Zamiast tego zastosowali metod\u0119, kt\u00f3ra zdefiniowa\u0142a na nowo ca\u0142\u0105 dziedzin\u0119.", "tokens": [51254, 1176, 4526, 525, 8627, 36746, 329, 305, 5103, 1131, 378, 1274, 11, 19456, 710, 20595, 3812, 5528, 5024, 1667, 586, 78, 1335, 15926, 9758, 15338, 259, 1274, 13, 51509], "temperature": 0.0, "avg_logprob": -0.13154062102822697, "compression_ratio": 1.3344947735191637, "no_speech_prob": 0.02287849225103855}, {"id": 17, "seek": 7710, "start": 100.0, "end": 105.69999999999999, "text": " Reinforcement learning from human feedback. W skr\u00f3cie RLHF.", "tokens": [51509, 42116, 9382, 2539, 490, 1952, 5824, 13, 343, 1110, 11721, 4260, 497, 43, 39, 37, 13, 51794], "temperature": 0.0, "avg_logprob": -0.13154062102822697, "compression_ratio": 1.3344947735191637, "no_speech_prob": 0.02287849225103855}, {"id": 18, "seek": 10570, "start": 105.7, "end": 113.0, "text": " RLHF to termin, kt\u00f3ry dzi\u015b s\u0142yszymy absolutnie wsz\u0119dzie. Ale to w\u0142a\u015bnie ta praca go tak spopularyzowa\u0142a.", "tokens": [50364, 497, 43, 39, 37, 281, 10761, 11, 9913, 31981, 1788, 15116, 749, 1229, 2226, 18757, 2766, 38322, 42643, 13, 9366, 281, 14234, 1846, 582, 6628, 352, 991, 637, 404, 425, 822, 89, 5528, 5024, 13, 50729], "temperature": 0.0, "avg_logprob": -0.12334757743121909, "compression_ratio": 1.35, "no_speech_prob": 0.028995437547564507}, {"id": 19, "seek": 10570, "start": 113.0, "end": 113.7, "text": " Tak jest.", "tokens": [50729, 9118, 3492, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12334757743121909, "compression_ratio": 1.35, "no_speech_prob": 0.028995437547564507}, {"id": 20, "seek": 10570, "start": 113.7, "end": 121.10000000000001, "text": " Zanim jednak roz\u0142o\u017cymy go na czynniki pierwsze, rzu\u0107my na st\u00f3\u0142 fakt, kt\u00f3ry pokazuje skal\u0119 tego prze\u0142omu. Wiesz, co jest niesamowite?", "tokens": [50764, 1176, 17869, 25897, 9544, 5249, 7735, 2226, 352, 1667, 6430, 26384, 9850, 45994, 11, 367, 11728, 2162, 2226, 1667, 342, 16181, 21310, 11, 9913, 13010, 43317, 16890, 1274, 8627, 8325, 1221, 298, 84, 13, 343, 15347, 11, 598, 3492, 48100, 335, 305, 642, 30, 51134], "temperature": 0.0, "avg_logprob": -0.12334757743121909, "compression_ratio": 1.35, "no_speech_prob": 0.028995437547564507}, {"id": 21, "seek": 10570, "start": 121.10000000000001, "end": 122.0, "text": " Co takiego?", "tokens": [51134, 3066, 32296, 30, 51179], "temperature": 0.0, "avg_logprob": -0.12334757743121909, "compression_ratio": 1.35, "no_speech_prob": 0.028995437547564507}, {"id": 22, "seek": 10570, "start": 122.0, "end": 131.7, "text": " W testach por\u00f3wnawczych model stworzony t\u0105 metod\u0105, nazwany Instruct GPT, mia\u0142 zaledwie jeden przecinek 3 miliarda parametr\u00f3w.", "tokens": [51179, 343, 1500, 608, 1515, 3901, 629, 86, 6522, 339, 2316, 342, 28321, 44479, 32294, 1131, 378, 1611, 11, 20151, 86, 1325, 2730, 1757, 26039, 51, 11, 27989, 710, 5573, 8699, 12906, 8325, 66, 48421, 805, 1962, 72, 19218, 6220, 27965, 3901, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12334757743121909, "compression_ratio": 1.35, "no_speech_prob": 0.028995437547564507}, {"id": 23, "seek": 13170, "start": 131.7, "end": 136.89999999999998, "text": " A by\u0142 znacznie cz\u0119\u015bciej wybierany przez ludzi ni\u017c oryginalny, pot\u0119\u017cny GPT-3.", "tokens": [50364, 316, 16673, 15397, 14875, 2766, 18544, 9815, 73, 45780, 811, 1325, 14064, 29586, 28502, 420, 88, 1494, 304, 1634, 11, 1847, 1274, 1427, 1634, 26039, 51, 12, 18, 13, 50624], "temperature": 0.0, "avg_logprob": -0.10744220364478327, "compression_ratio": 1.3957703927492446, "no_speech_prob": 0.013625076971948147}, {"id": 24, "seek": 13170, "start": 136.89999999999998, "end": 139.5, "text": " Kt\u00f3ry mia\u0142 175 miliard\u00f3w parametr\u00f3w.", "tokens": [50624, 591, 4547, 627, 27989, 41165, 1962, 72, 515, 3901, 6220, 27965, 3901, 13, 50754], "temperature": 0.0, "avg_logprob": -0.10744220364478327, "compression_ratio": 1.3957703927492446, "no_speech_prob": 0.013625076971948147}, {"id": 25, "seek": 13170, "start": 139.5, "end": 145.0, "text": " W\u0142a\u015bnie. To jest ponad stukrotna r\u00f3\u017cnica w wielko\u015bci. Jak to w og\u00f3le jest mo\u017cliwie?", "tokens": [50754, 343, 5024, 12221, 13, 1407, 3492, 9224, 345, 342, 2034, 10536, 629, 19637, 32687, 261, 20570, 4093, 6199, 13, 15029, 281, 261, 29229, 3492, 30854, 8699, 30, 51029], "temperature": 0.0, "avg_logprob": -0.10744220364478327, "compression_ratio": 1.3957703927492446, "no_speech_prob": 0.013625076971948147}, {"id": 26, "seek": 13170, "start": 145.0, "end": 149.5, "text": " To kompletnie zaprzeka ca\u0142ej filozofii bigger is better, kt\u00f3ra wtedy dominowa\u0142a.", "tokens": [51029, 1407, 5207, 14657, 2766, 14223, 19390, 36361, 47631, 73, 1387, 15151, 2670, 5597, 3801, 307, 1101, 11, 19456, 26959, 8859, 5528, 5024, 13, 51254], "temperature": 0.0, "avg_logprob": -0.10744220364478327, "compression_ratio": 1.3957703927492446, "no_speech_prob": 0.013625076971948147}, {"id": 27, "seek": 13170, "start": 149.5, "end": 152.1, "text": " I to jest w\u0142a\u015bnie, wiesz, pi\u0119kno tej pracy.", "tokens": [51254, 286, 281, 3492, 14234, 11, 261, 15347, 11, 48085, 1771, 12573, 35591, 13, 51384], "temperature": 0.0, "avg_logprob": -0.10744220364478327, "compression_ratio": 1.3957703927492446, "no_speech_prob": 0.013625076971948147}, {"id": 28, "seek": 13170, "start": 152.1, "end": 160.79999999999998, "text": " Udowodni\u0142a, \u017ce jako\u015b\u0107 treningu i to dopasowanie do intencji u\u017cytkownika mog\u0105 by\u0107 wa\u017cniejsze ni\u017c surowa moc.", "tokens": [51384, 624, 67, 305, 378, 3722, 5024, 11, 3561, 17123, 7753, 2192, 773, 84, 741, 281, 360, 20990, 22028, 360, 43094, 19649, 344, 1427, 4328, 74, 648, 5439, 34123, 15069, 27777, 44258, 28502, 1022, 5528, 34962, 13, 51819], "temperature": 0.0, "avg_logprob": -0.10744220364478327, "compression_ratio": 1.3957703927492446, "no_speech_prob": 0.013625076971948147}, {"id": 29, "seek": 16080, "start": 160.8, "end": 162.4, "text": " ...ni\u017c sama liczba parametr\u00f3w.", "tokens": [50364, 1097, 3722, 1427, 17768, 6169, 89, 4231, 6220, 27965, 3901, 13, 50444], "temperature": 0.0, "avg_logprob": -0.1550925827026367, "compression_ratio": 1.3968253968253967, "no_speech_prob": 0.028193634003400803}, {"id": 30, "seek": 16080, "start": 162.4, "end": 163.8, "text": " Czyli nie si\u0142a, a spos\u00f3b?", "tokens": [50444, 37099, 2838, 1511, 5024, 11, 257, 22904, 30, 50514], "temperature": 0.0, "avg_logprob": -0.1550925827026367, "compression_ratio": 1.3968253968253967, "no_speech_prob": 0.028193634003400803}, {"id": 31, "seek": 16080, "start": 163.8, "end": 173.10000000000002, "text": " Dok\u0142adnie. Bo przed LHF jakie by\u0142y g\u0142\u00f3wne metody, \u017ceby okie\u0142zna\u0107 te modele? Pr\u00f3bowano, no, generalnie dw\u00f3ch rzeczy.", "tokens": [50514, 29768, 10358, 2766, 13, 3286, 18334, 441, 39, 37, 22124, 26366, 18117, 3901, 716, 1131, 843, 11, 11316, 3133, 414, 1221, 35458, 2162, 535, 4391, 306, 30, 2114, 812, 8202, 3730, 11, 572, 11, 2674, 2766, 27379, 812, 339, 26297, 13, 50979], "temperature": 0.0, "avg_logprob": -0.1550925827026367, "compression_ratio": 1.3968253968253967, "no_speech_prob": 0.028193634003400803}, {"id": 32, "seek": 16080, "start": 173.10000000000002, "end": 174.10000000000002, "text": " Wi\u0119cej danych?", "tokens": [50979, 30127, 20811, 274, 34644, 30, 51029], "temperature": 0.0, "avg_logprob": -0.1550925827026367, "compression_ratio": 1.3968253968253967, "no_speech_prob": 0.028193634003400803}, {"id": 33, "seek": 16080, "start": 174.10000000000002, "end": 181.3, "text": " Tak. Po pierwsze jeszcze wi\u0119cej danych z nadziej\u0105, \u017ce model w ko\u0144cu za\u0142apie, a po drugie prompt engineering.", "tokens": [51029, 9118, 13, 6165, 45994, 14168, 26004, 274, 34644, 710, 43693, 8555, 11, 3561, 2316, 261, 26470, 12032, 7949, 1221, 569, 414, 11, 257, 714, 4110, 414, 12391, 7043, 13, 51389], "temperature": 0.0, "avg_logprob": -0.1550925827026367, "compression_ratio": 1.3968253968253967, "no_speech_prob": 0.028193634003400803}, {"id": 34, "seek": 16080, "start": 181.3, "end": 190.10000000000002, "text": " A, czyli \u017c\u00f3\u0142udne formu\u0142owanie polece\u0144. Troch\u0119 jak zaklinanie deszczu. Trzeba by\u0142o znale\u017a\u0107 t\u0119 magiczn\u0105 formu\u0142k\u0119.", "tokens": [51389, 316, 11, 16591, 19625, 16181, 532, 716, 1254, 84, 1221, 22028, 13208, 384, 5248, 13, 19406, 23006, 4207, 23810, 5045, 7155, 730, 43771, 84, 13, 1765, 1381, 4231, 14811, 15397, 1220, 10659, 2162, 32489, 5585, 89, 13113, 1254, 84, 1221, 15724, 13, 51829], "temperature": 0.0, "avg_logprob": -0.1550925827026367, "compression_ratio": 1.3968253968253967, "no_speech_prob": 0.028193634003400803}, {"id": 35, "seek": 19010, "start": 190.1, "end": 201.2, "text": " W\u0142a\u015bnie. To by\u0142o niestabilne, nieprzewidywalne, a metoda RLAH wprowadzi\u0142a systematyczno\u015b\u0107 i ona opiera si\u0119 na genialnym swojej prostocie pomy\u015ble.", "tokens": [50364, 343, 5024, 12221, 13, 1407, 14811, 3867, 377, 5177, 716, 11, 2838, 1424, 43551, 327, 27112, 304, 716, 11, 257, 1131, 13449, 497, 43, 10566, 46733, 3992, 5024, 1185, 267, 17466, 23293, 741, 20325, 999, 10609, 3244, 1667, 48228, 12996, 29489, 73, 10293, 905, 414, 280, 8488, 1788, 306, 13, 50919], "temperature": 0.0, "avg_logprob": -0.10987391905351118, "compression_ratio": 1.3418803418803418, "no_speech_prob": 0.00877347681671381}, {"id": 36, "seek": 19010, "start": 201.2, "end": 201.9, "text": " Jakim?", "tokens": [50919, 15029, 332, 30, 50954], "temperature": 0.0, "avg_logprob": -0.10987391905351118, "compression_ratio": 1.3418803418803418, "no_speech_prob": 0.00877347681671381}, {"id": 37, "seek": 19010, "start": 201.9, "end": 210.5, "text": " Je\u015bli chcesz, \u017ceby model zachowywa\u0142 si\u0119 tak jak lubi\u0105 ludzie, to po prostu zapytaj ludzi o zdanie i u\u017cyj ich opinii jako sygna\u0142u do nauki.", "tokens": [50954, 37086, 417, 887, 89, 11, 11316, 2316, 29303, 10089, 44603, 3244, 991, 4207, 15980, 11404, 37025, 11, 281, 714, 19518, 14223, 88, 1328, 73, 29586, 277, 16221, 7155, 741, 34097, 73, 1893, 46784, 72, 17123, 943, 70, 629, 24066, 360, 35616, 2984, 13, 51384], "temperature": 0.0, "avg_logprob": -0.10987391905351118, "compression_ratio": 1.3418803418803418, "no_speech_prob": 0.00877347681671381}, {"id": 38, "seek": 19010, "start": 210.5, "end": 211.2, "text": " Proste.", "tokens": [51384, 2114, 555, 68, 13, 51419], "temperature": 0.0, "avg_logprob": -0.10987391905351118, "compression_ratio": 1.3418803418803418, "no_speech_prob": 0.00877347681671381}, {"id": 39, "seek": 21120, "start": 211.2, "end": 220.5, "text": " A ca\u0142y proces, oparty na informacjach zwrotnych od specjalnie przeszkolonego zespo\u0142u 40 os\u00f3b, tak zwanych lablers, sk\u0142ada si\u0119 z trzech kluczowych krok\u00f3w.", "tokens": [50364, 316, 35226, 17565, 11, 999, 446, 88, 1667, 1356, 326, 45059, 49111, 310, 9399, 3611, 46433, 2766, 6541, 10430, 36620, 546, 1571, 710, 279, 2259, 24066, 3356, 32089, 11, 991, 11873, 34644, 2715, 11977, 11, 1110, 46217, 3244, 710, 504, 19439, 9671, 1311, 89, 19605, 45909, 23849, 13, 50829], "temperature": 0.0, "avg_logprob": -0.10925081752290662, "compression_ratio": 1.4405144694533762, "no_speech_prob": 0.38093727827072144}, {"id": 40, "seek": 21120, "start": 220.5, "end": 230.5, "text": " Okej, zatrzymajmy si\u0119 tu na chwil\u0119. 40 lablers. To jest kluczowe pytanie. Kim byli ci ludzie, ekspertami od AI, lingwistami?", "tokens": [50829, 29094, 73, 11, 35802, 13047, 1696, 73, 2226, 3244, 2604, 1667, 41941, 1274, 13, 3356, 2715, 11977, 13, 1407, 3492, 9671, 1311, 89, 6880, 36610, 13, 5652, 538, 2081, 6983, 37025, 11, 30724, 15346, 4526, 3611, 7318, 11, 22949, 86, 468, 4526, 30, 51329], "temperature": 0.0, "avg_logprob": -0.10925081752290662, "compression_ratio": 1.4405144694533762, "no_speech_prob": 0.38093727827072144}, {"id": 41, "seek": 21120, "start": 230.5, "end": 239.2, "text": " Doskona\u0142e pytanie, kt\u00f3re od razu dotyka sedna jednego z ogranicze\u0144, o kt\u00f3rym chowiemy p\u00f3\u017aniej, bo to od ich gustu zale\u017ca\u0142 kszta\u0142t finalnego modelu. Tak.", "tokens": [51329, 33474, 74, 4037, 19827, 36610, 11, 8864, 3611, 367, 8813, 5893, 88, 2330, 9643, 629, 5232, 11858, 710, 34416, 30732, 49689, 11, 277, 30120, 417, 13998, 2226, 36968, 11, 748, 281, 3611, 1893, 9679, 84, 710, 1220, 35075, 1221, 350, 15453, 46426, 83, 2572, 11858, 2316, 84, 13, 9118, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10925081752290662, "compression_ratio": 1.4405144694533762, "no_speech_prob": 0.38093727827072144}, {"id": 42, "seek": 23920, "start": 239.2, "end": 251.5, "text": " Byli to starannie wyselekcjonowani kontraktorzy, niepracownicy OpenAI. Przeszli specjalne szkolenie, dostali bardzo szczeg\u00f3\u0142owe instrukcje, jak ocenia\u0107 odpowiedzi.", "tokens": [50364, 3146, 2081, 281, 3543, 43433, 4628, 405, 29205, 45677, 305, 3782, 14373, 32249, 284, 1229, 11, 2838, 1424, 326, 648, 2632, 7238, 48698, 13, 2114, 89, 10430, 2081, 46433, 716, 7870, 74, 11940, 414, 11, 20568, 5103, 9034, 22090, 1146, 16181, 6880, 1058, 25126, 44261, 11, 4207, 10409, 268, 654, 2162, 36574, 3992, 13, 50979], "temperature": 0.0, "avg_logprob": -0.08721312394378879, "compression_ratio": 1.4175438596491228, "no_speech_prob": 0.029372047632932663}, {"id": 43, "seek": 23920, "start": 251.5, "end": 252.79999999999998, "text": " Jakie na przyk\u0142ad?", "tokens": [50979, 15029, 414, 1667, 23144, 30, 51044], "temperature": 0.0, "avg_logprob": -0.08721312394378879, "compression_ratio": 1.4175438596491228, "no_speech_prob": 0.029372047632932663}, {"id": 44, "seek": 23920, "start": 252.79999999999998, "end": 266.59999999999997, "text": " Mieli zwraca\u0107 uwag\u0119, czy odpowied\u017a jest pomocna, prawdziwa i czy nie jest szkodliwa. Nie byli to wi\u0119c przypadkowi ludzie. Byli szkoleni, by reprezentowa\u0107 po\u017c\u0105danego u\u017cytkownika z perspektywy badaczy.", "tokens": [51044, 376, 23099, 49111, 6628, 2162, 43696, 11, 6430, 36574, 10659, 3492, 48962, 629, 11, 41175, 3992, 4151, 741, 6430, 2838, 3492, 7870, 74, 378, 2081, 4151, 13, 12016, 538, 2081, 281, 16677, 33100, 74, 24503, 37025, 13, 3146, 2081, 7870, 74, 11940, 72, 11, 538, 1085, 265, 14185, 11445, 714, 1427, 18962, 282, 6308, 344, 1427, 4328, 74, 648, 5439, 710, 868, 32659, 874, 9726, 1578, 14691, 13, 51734], "temperature": 0.0, "avg_logprob": -0.08721312394378879, "compression_ratio": 1.4175438596491228, "no_speech_prob": 0.029372047632932663}, {"id": 45, "seek": 23920, "start": 266.59999999999997, "end": 267.3, "text": " Rozumiem.", "tokens": [51734, 43313, 449, 4907, 13, 51769], "temperature": 0.0, "avg_logprob": -0.08721312394378879, "compression_ratio": 1.4175438596491228, "no_speech_prob": 0.029372047632932663}, {"id": 46, "seek": 26730, "start": 267.3, "end": 274.6, "text": " I ich praca by\u0142a fundamentem pierwszego kroku, czyli Supervised Fine Tuning, w skr\u00f3cie SFT.", "tokens": [50364, 286, 1893, 582, 6628, 23936, 6073, 443, 27623, 27725, 45909, 5279, 11, 16591, 4548, 24420, 12024, 21363, 278, 11, 261, 1110, 11721, 4260, 31095, 51, 13, 50729], "temperature": 0.0, "avg_logprob": -0.10862669668906977, "compression_ratio": 1.3473282442748091, "no_speech_prob": 0.04366837069392204}, {"id": 47, "seek": 26730, "start": 274.6, "end": 279.3, "text": " SFT. Dostrajanie pod nadzorem. Jak to wygl\u0105da\u0142o w praktyce?", "tokens": [50729, 31095, 51, 13, 413, 555, 48690, 7155, 2497, 12617, 89, 37956, 13, 15029, 281, 32015, 5249, 261, 3206, 74, 874, 384, 30, 50964], "temperature": 0.0, "avg_logprob": -0.10862669668906977, "compression_ratio": 1.3473282442748091, "no_speech_prob": 0.04366837069392204}, {"id": 48, "seek": 26730, "start": 279.3, "end": 283.40000000000003, "text": " Na tym etapie lablers po prostu tworzyli idealne odpowiedzi.", "tokens": [50964, 6056, 8107, 47634, 414, 2715, 11977, 714, 19518, 46288, 1229, 2081, 7157, 716, 36574, 3992, 13, 51169], "temperature": 0.0, "avg_logprob": -0.10862669668906977, "compression_ratio": 1.3473282442748091, "no_speech_prob": 0.04366837069392204}, {"id": 49, "seek": 26730, "start": 283.40000000000003, "end": 292.90000000000003, "text": " Pisali je od zera. Tak. Brali r\u00f3\u017cne polecenia, kt\u00f3re realni u\u017cytkownicy zadawali GPT-3 przez API i pisali na niewzorcowe odpowiedzi.", "tokens": [51169, 43263, 5103, 1506, 3611, 710, 1663, 13, 9118, 13, 1603, 5103, 47760, 13208, 13037, 654, 11, 8864, 957, 3722, 344, 1427, 4328, 74, 648, 2632, 710, 1538, 40054, 26039, 51, 12, 18, 14064, 9362, 741, 26584, 5103, 1667, 2838, 86, 89, 284, 66, 6880, 36574, 3992, 13, 51644], "temperature": 0.0, "avg_logprob": -0.10862669668906977, "compression_ratio": 1.3473282442748091, "no_speech_prob": 0.04366837069392204}, {"id": 50, "seek": 29290, "start": 293.0, "end": 300.29999999999995, "text": " Stworzono w ten spos\u00f3b wysokiej jako\u015bci zbi\u00f3r danych. Para polecenie odpowied\u017a. I na tym dostrajano GPT-3.", "tokens": [50369, 745, 28321, 89, 8957, 261, 2064, 22904, 27062, 453, 7764, 17123, 6199, 710, 5614, 15614, 274, 34644, 13, 11107, 13208, 13037, 414, 36574, 10659, 13, 286, 1667, 8107, 20568, 48690, 3730, 26039, 51, 12, 18, 13, 50734], "temperature": 0.0, "avg_logprob": -0.07566612132274321, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.0014334990410134196}, {"id": 51, "seek": 29290, "start": 300.29999999999995, "end": 310.59999999999997, "text": " Dok\u0142adnie. To jakby pokaza\u0107 uczniowi wzorowo rozwi\u0105zane zadania z klucza odpowiedzi. \u017beby nauczy\u0142 si\u0119, jak wygl\u0105da oczekiwany rezultat.", "tokens": [50734, 29768, 10358, 2766, 13, 1407, 28976, 13010, 12257, 2162, 35403, 3722, 24503, 24809, 284, 19941, 9544, 22620, 1929, 42788, 5609, 710, 9671, 1311, 2394, 36574, 3992, 13, 46864, 2322, 49103, 1229, 1221, 3244, 11, 4207, 32015, 277, 3689, 14753, 86, 1325, 48060, 723, 267, 13, 51249], "temperature": 0.0, "avg_logprob": -0.07566612132274321, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.0014334990410134196}, {"id": 52, "seek": 29290, "start": 310.59999999999997, "end": 319.9, "text": " Czekaj, czyli na tym pierwszym etapie model uczy si\u0119 po prostu na\u015bladowa\u0107. Ale przecie\u017c wiesz, kreatywno\u015b\u0107 polega na tym, \u017ceby nie zawsze trzyma\u0107 si\u0119 wzoru.", "tokens": [51249, 383, 19878, 1805, 11, 16591, 1667, 8107, 34016, 76, 47634, 414, 2316, 344, 6522, 3244, 714, 19518, 1667, 1788, 9290, 11445, 13, 9366, 8325, 40082, 261, 15347, 11, 350, 620, 88, 20944, 7753, 13208, 3680, 1667, 8107, 11, 11316, 2838, 30964, 34573, 1696, 2162, 3244, 24809, 32963, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07566612132274321, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.0014334990410134196}, {"id": 53, "seek": 31990, "start": 319.9, "end": 329.59999999999997, "text": " Czy to nie tworzy\u0142o ryzyka, \u017ce model stanie si\u0119 nudny? Szablonowy? Absolutnie. I badacze byli tego \u015bwiadomi. Samo na\u015bladowanie to za ma\u0142o.", "tokens": [50364, 19832, 281, 2838, 46288, 1229, 5249, 20791, 40940, 11, 3561, 2316, 40013, 3244, 40045, 1634, 30, 24699, 455, 14864, 10089, 30, 5813, 2308, 2766, 13, 286, 1578, 326, 1381, 538, 2081, 8627, 21485, 345, 9220, 13, 4832, 78, 1667, 1788, 9290, 22028, 281, 7949, 463, 5249, 13, 50849], "temperature": 0.0, "avg_logprob": -0.09652226006806787, "compression_ratio": 1.4240282685512367, "no_speech_prob": 0.009377861395478249}, {"id": 54, "seek": 31990, "start": 329.59999999999997, "end": 339.9, "text": " No w\u0142a\u015bnie. Dlatego SFT to tylko rozgrzewka. Taki fundament. To uczy model podstawowego formatu, stylu, ale nie uczy go oceny.", "tokens": [50849, 883, 14234, 13, 47184, 31095, 51, 281, 13219, 9544, 861, 43551, 2330, 13, 314, 7421, 6073, 13, 1407, 344, 6522, 2316, 43443, 26576, 7877, 84, 11, 7952, 2781, 11, 6775, 2838, 344, 6522, 352, 10409, 43100, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09652226006806787, "compression_ratio": 1.4240282685512367, "no_speech_prob": 0.009377861395478249}, {"id": 55, "seek": 31990, "start": 339.9, "end": 347.0, "text": " Co jest lepsze, a co gorsze w bardziej z\u0142o\u017conych sytuacjach. I tu dochodzimy do kroku drugiego, kt\u00f3ry jest znacznie ciekawsze.", "tokens": [51364, 3066, 3492, 476, 1878, 1381, 11, 257, 598, 290, 830, 1381, 261, 27209, 710, 5249, 1427, 2526, 339, 28275, 326, 45059, 13, 286, 2604, 9243, 378, 89, 13189, 360, 45909, 5279, 4110, 12200, 11, 9913, 3492, 15397, 14875, 2766, 46419, 28354, 13, 51719], "temperature": 0.0, "avg_logprob": -0.09652226006806787, "compression_ratio": 1.4240282685512367, "no_speech_prob": 0.009377861395478249}, {"id": 56, "seek": 34700, "start": 347.0, "end": 357.3, "text": " Czyli? Trening tak zwanego Reward Model. Modelu nagrody. Model nagrody. Brzmi jak co\u015b, co przyznaje punkty. To jest ten cyfrowy s\u0119dzia, o kt\u00f3rym si\u0119 tyle m\u00f3wi?", "tokens": [50364, 37099, 30, 8648, 773, 991, 710, 7916, 6308, 1300, 1007, 17105, 13, 17105, 84, 17096, 340, 3173, 13, 17105, 17096, 340, 3173, 13, 1603, 89, 3057, 4207, 19241, 11, 598, 6501, 35458, 2884, 25188, 874, 13, 1407, 3492, 2064, 3185, 69, 1892, 88, 262, 6298, 40395, 11, 277, 30120, 3244, 39293, 24592, 30, 50879], "temperature": 0.0, "avg_logprob": -0.11372218322753906, "compression_ratio": 1.4398496240601504, "no_speech_prob": 0.03434865549206734}, {"id": 57, "seek": 34700, "start": 357.3, "end": 373.0, "text": " Dok\u0142adnie tak. To jest s\u0119dzia, krytyk, kurator smaku, jakkolwiek go nazwiemy. W tym kroku bierzemy model po tym pierwszym etapie, po SFT i prosimy go, \u017ceby na jedno polecenie wygenerowa\u0142 kilka r\u00f3\u017cnych odpowiedzi.", "tokens": [50879, 29768, 10358, 2766, 991, 13, 1407, 3492, 262, 6298, 40395, 11, 34847, 874, 74, 11, 10072, 1639, 899, 15803, 11, 4207, 36620, 44674, 352, 20151, 8699, 2226, 13, 343, 8107, 45909, 5279, 272, 34602, 3633, 2316, 714, 8107, 34016, 76, 47634, 414, 11, 714, 31095, 51, 741, 6267, 13189, 352, 11, 11316, 1667, 5232, 1771, 13208, 13037, 414, 4628, 21848, 30105, 36466, 42602, 36574, 3992, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11372218322753906, "compression_ratio": 1.4398496240601504, "no_speech_prob": 0.03434865549206734}, {"id": 58, "seek": 37300, "start": 373.0, "end": 381.0, "text": " Na przyk\u0142ad cztery, pi\u0119\u0107. Powiedzmy od czterech do dziewi\u0119ciu. I te odpowiedzi trafiaj\u0105 do naszych labelers. I co oni z nimi robi\u0105?", "tokens": [50364, 6056, 23144, 6472, 12733, 11, 32677, 2162, 13, 14762, 15338, 2226, 3611, 269, 2682, 323, 339, 360, 9758, 1093, 5034, 30795, 13, 286, 535, 36574, 3992, 944, 22054, 8555, 360, 45002, 7645, 433, 13, 286, 598, 36317, 710, 297, 10121, 3870, 11404, 30, 50764], "temperature": 0.0, "avg_logprob": -0.10712710092234057, "compression_ratio": 1.5149253731343284, "no_speech_prob": 0.1345188170671463}, {"id": 59, "seek": 37300, "start": 381.0, "end": 389.0, "text": " Ich zadaniem nie jest ju\u017c pisanie idealnej odpowiedzi, ale uszeregowanie tych wygenerowanych. Od najlepszej do najgorszej.", "tokens": [50764, 3141, 710, 11338, 4907, 2838, 3492, 10678, 26584, 7155, 7157, 11794, 36574, 3992, 11, 6775, 505, 89, 323, 70, 22028, 15180, 4628, 21848, 23341, 339, 13, 12210, 41903, 1878, 16920, 360, 11212, 70, 830, 16920, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10712710092234057, "compression_ratio": 1.5149253731343284, "no_speech_prob": 0.1345188170671463}, {"id": 60, "seek": 37300, "start": 389.0, "end": 397.0, "text": " Aha. Czyli zamiast m\u00f3wi\u0107 tak powinno by\u0107, m\u00f3wi\u0105 to jest lepsze od tego, a to jest najgorsze. To jest znacznie bardziej subtelna informacja.", "tokens": [51164, 27448, 13, 37099, 710, 4526, 525, 13489, 12757, 991, 27310, 1771, 15069, 11, 46591, 281, 3492, 476, 1878, 1381, 3611, 8627, 11, 257, 281, 3492, 11212, 70, 830, 1381, 13, 1407, 3492, 15397, 14875, 2766, 27209, 7257, 338, 629, 1356, 23395, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10712710092234057, "compression_ratio": 1.5149253731343284, "no_speech_prob": 0.1345188170671463}, {"id": 61, "seek": 39700, "start": 397.0, "end": 406.0, "text": " Niezwykle subtelna i niezwykle cenna. I na podstawie milion\u00f3w takich ranking\u00f3w trenowany jest zupe\u0142nie osobny model, w\u0142a\u015bnie reward model.", "tokens": [50364, 12016, 89, 9726, 14677, 7257, 338, 629, 741, 33511, 9726, 14677, 27900, 629, 13, 286, 1667, 43443, 414, 1962, 313, 3901, 29607, 17833, 3901, 23136, 23341, 3492, 49922, 41518, 1634, 2316, 11, 14234, 7782, 2316, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09416720271110535, "compression_ratio": 1.3781512605042017, "no_speech_prob": 0.16949273645877838}, {"id": 62, "seek": 39700, "start": 406.0, "end": 418.0, "text": " Jego jedyny cel to nauczy\u0107 si\u0119 przewidywa\u0107, kt\u00f3r\u0105 odpowied\u017a cz\u0142owiek uzna za lepsz\u0105. Dostaje dwie odpowiedzi i ma wyplu\u0107 liczb\u0119, kt\u00f3ra m\u00f3wi, te ludzie preferowali bardziej.", "tokens": [50814, 508, 6308, 5232, 88, 1634, 9277, 281, 49103, 27150, 3244, 39758, 38836, 25234, 11, 37415, 36574, 10659, 36282, 74, 16851, 629, 7949, 476, 1878, 8925, 13, 413, 555, 11153, 274, 8699, 36574, 3992, 741, 463, 4628, 564, 84, 2162, 6169, 89, 65, 1274, 11, 19456, 24592, 11, 535, 37025, 4382, 305, 5103, 27209, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09416720271110535, "compression_ratio": 1.3781512605042017, "no_speech_prob": 0.16949273645877838}, {"id": 63, "seek": 41800, "start": 418.0, "end": 428.0, "text": " Staje si\u0119 takim zautomatyzowanym odwzierciedleniem ludzkich preferencji. Ok, to jest fascynuj\u0105ce. Mamy wi\u0119c ucznia z pierwszego kroku, kt\u00f3ry umie pisa\u0107 w dobrym stylu.", "tokens": [50364, 745, 11153, 3244, 31732, 710, 1375, 298, 21398, 89, 23341, 76, 3611, 86, 89, 811, 537, 292, 6698, 4907, 15946, 30154, 480, 4382, 268, 19649, 13, 3477, 11, 281, 3492, 30632, 1344, 77, 13263, 384, 13, 376, 7804, 16677, 35403, 12679, 710, 27623, 27725, 45909, 5279, 11, 9913, 1105, 414, 280, 3837, 2162, 261, 35884, 76, 7952, 2781, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0905108616269868, "compression_ratio": 1.4381270903010033, "no_speech_prob": 0.6819651126861572}, {"id": 64, "seek": 41800, "start": 428.0, "end": 436.0, "text": " I mamy krytyka z drugiego, kt\u00f3ry wykszta\u0142ci\u0142 sobie gust. Jak teraz sprawi\u0107 by ten ucz\u0119\u015b zacz\u0105\u0142 tworzy\u0107 arcydzie\u0142a pod okiem krytyka?", "tokens": [50864, 286, 17335, 34847, 874, 2330, 710, 4110, 12200, 11, 9913, 4628, 1694, 89, 46426, 537, 1221, 13652, 9679, 13, 15029, 16854, 22734, 12757, 538, 2064, 35403, 1274, 1788, 34430, 8925, 1221, 46288, 27150, 594, 1344, 13096, 5024, 2497, 3133, 4907, 34847, 874, 2330, 30, 51264], "temperature": 0.0, "avg_logprob": -0.0905108616269868, "compression_ratio": 1.4381270903010033, "no_speech_prob": 0.6819651126861572}, {"id": 65, "seek": 41800, "start": 436.0, "end": 443.0, "text": " To idealna analogia. I tu dochodzimy do kroku trzeciego, kt\u00f3ry jest sercem ca\u0142ej metody.", "tokens": [51264, 1407, 7157, 629, 16660, 654, 13, 286, 2604, 9243, 378, 89, 13189, 360, 45909, 5279, 22266, 4260, 1571, 11, 9913, 3492, 816, 26422, 47631, 73, 1131, 843, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0905108616269868, "compression_ratio": 1.4381270903010033, "no_speech_prob": 0.6819651126861572}, {"id": 66, "seek": 41800, "start": 443.0, "end": 444.0, "text": " Reinforcement learning?", "tokens": [51614, 42116, 9382, 2539, 30, 51664], "temperature": 0.0, "avg_logprob": -0.0905108616269868, "compression_ratio": 1.4381270903010033, "no_speech_prob": 0.6819651126861572}, {"id": 67, "seek": 44400, "start": 444.0, "end": 454.0, "text": " Tak. U\u017cywaj\u0105c algorytmu znanego jako PPO, czyli Proximal Policy Optimization, model ucze\u0144 z pierwszego kroku jest dalej optymalizowany.", "tokens": [50364, 9118, 13, 624, 7735, 86, 38757, 3501, 827, 83, 20140, 15397, 282, 6308, 17123, 430, 34885, 11, 16591, 1705, 3081, 304, 21708, 35013, 2144, 11, 2316, 344, 9680, 5248, 710, 27623, 27725, 45909, 5279, 3492, 34257, 2427, 4199, 304, 590, 23341, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07878980344655562, "compression_ratio": 1.3603603603603605, "no_speech_prob": 0.15008951723575592}, {"id": 68, "seek": 44400, "start": 454.0, "end": 461.0, "text": " Proces wygl\u0105da tak. Model dostaje polecenie, generuje odpowied\u017a. I ta odpowied\u017a idzie do krytyka.", "tokens": [50864, 1705, 887, 32015, 991, 13, 17105, 20568, 11153, 13208, 13037, 414, 11, 1337, 13008, 36574, 10659, 13, 286, 1846, 36574, 10659, 4496, 3283, 360, 34847, 874, 2330, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07878980344655562, "compression_ratio": 1.3603603603603605, "no_speech_prob": 0.15008951723575592}, {"id": 69, "seek": 44400, "start": 461.0, "end": 466.0, "text": " Dok\u0142adnie. Model krytyk przyznaje jej punkty, czyli nagrod\u0119.", "tokens": [51214, 29768, 10358, 2766, 13, 17105, 34847, 874, 74, 6501, 35458, 2884, 28924, 25188, 874, 11, 16591, 17096, 11452, 1274, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07878980344655562, "compression_ratio": 1.3603603603603605, "no_speech_prob": 0.15008951723575592}, {"id": 70, "seek": 46600, "start": 466.0, "end": 475.0, "text": " Algorytm PPO dostosowuje parametry ucznia tak, \u017ceby nast\u0119pnym razem wygenerowa\u0142 odpowied\u017a, kt\u00f3ra dostanie jeszcze wi\u0119cej punkt\u00f3w.", "tokens": [50364, 35014, 827, 83, 76, 430, 34885, 20568, 329, 305, 13008, 6220, 9889, 35403, 12679, 991, 11, 11316, 39662, 12996, 40225, 4628, 21848, 30105, 36574, 10659, 11, 19456, 20568, 7155, 14168, 26004, 39561, 3901, 13, 50814], "temperature": 0.0, "avg_logprob": -0.046846171070758565, "compression_ratio": 1.4421768707482994, "no_speech_prob": 0.01324596256017685}, {"id": 71, "seek": 46600, "start": 475.0, "end": 479.0, "text": " Czyli to taki cykl pr\u00f3b i b\u0142\u0119d\u00f3w na ogromn\u0105 skal\u0119.", "tokens": [50814, 37099, 281, 20065, 3185, 7837, 8565, 65, 741, 272, 1221, 6298, 3901, 1667, 34416, 298, 13113, 16890, 1274, 13, 51014], "temperature": 0.0, "avg_logprob": -0.046846171070758565, "compression_ratio": 1.4421768707482994, "no_speech_prob": 0.01324596256017685}, {"id": 72, "seek": 46600, "start": 479.0, "end": 486.0, "text": " Tak, gdzie model aktywnie eksploruje, co sprawia, \u017ce jego odpowiedzi s\u0105 lepsze w oczach tego cyfrowego s\u0119dziego.", "tokens": [51014, 9118, 11, 18922, 2316, 9308, 874, 14215, 30724, 564, 284, 13008, 11, 598, 22734, 654, 11, 3561, 26542, 36574, 3992, 9015, 476, 1878, 1381, 261, 277, 3689, 608, 8627, 3185, 69, 1892, 6308, 262, 42643, 1571, 13, 51364], "temperature": 0.0, "avg_logprob": -0.046846171070758565, "compression_ratio": 1.4421768707482994, "no_speech_prob": 0.01324596256017685}, {"id": 73, "seek": 46600, "start": 486.0, "end": 493.0, "text": " Rozumiem. Czyli ju\u017c nie na\u015bladuje statycznych przyk\u0142ad\u00f3w, ale dynamicznie uczy si\u0119, co podoba si\u0119 s\u0119dziemu.", "tokens": [51364, 43313, 449, 4907, 13, 37099, 10678, 2838, 1667, 1788, 9290, 13008, 2219, 17466, 9399, 23144, 3901, 11, 6775, 8546, 89, 2766, 344, 6522, 3244, 11, 598, 2497, 19481, 3244, 262, 42643, 20140, 13, 51714], "temperature": 0.0, "avg_logprob": -0.046846171070758565, "compression_ratio": 1.4421768707482994, "no_speech_prob": 0.01324596256017685}, {"id": 74, "seek": 49300, "start": 493.0, "end": 501.0, "text": " A s\u0119dzia nauczy\u0142 si\u0119, co podoba si\u0119 ludziom. Trzy kroki. Poka\u017c wz\u00f3r, naucz ocenia\u0107, a potem nagradzaj.", "tokens": [50364, 316, 262, 6298, 40395, 49103, 1229, 1221, 3244, 11, 598, 2497, 19481, 3244, 29586, 298, 13, 1765, 1229, 45909, 2984, 13, 14958, 18264, 24809, 15614, 11, 49103, 89, 10409, 268, 654, 2162, 11, 257, 36513, 17096, 6206, 89, 1805, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10060009463080044, "compression_ratio": 1.2808510638297872, "no_speech_prob": 0.018514562398195267}, {"id": 75, "seek": 49300, "start": 501.0, "end": 509.0, "text": " Dok\u0142adnie tak. To brzmi logicznie. Wr\u00f3\u0107my wi\u0119c do efekt\u00f3w. M\u00f3wi\u0142y\u015bmy, \u017ce ludzie mia\u017cd\u0105co preferowali Instrakt GPT.", "tokens": [50764, 29768, 10358, 2766, 991, 13, 1407, 738, 89, 3057, 9952, 89, 2766, 13, 10159, 812, 2162, 2226, 16677, 360, 31482, 8192, 3901, 13, 376, 3901, 72, 6825, 10513, 11, 3561, 37025, 21290, 1427, 67, 1611, 1291, 4382, 305, 5103, 2730, 32249, 26039, 51, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10060009463080044, "compression_ratio": 1.2808510638297872, "no_speech_prob": 0.018514562398195267}, {"id": 76, "seek": 49300, "start": 509.0, "end": 517.0, "text": " Model 175B Instrakt GPT wygrywa\u0142 z 175B GPT-3 w 85% przypadk\u00f3w.", "tokens": [51164, 17105, 41165, 33, 2730, 32249, 26039, 51, 4628, 70, 627, 44603, 710, 41165, 33, 26039, 51, 12, 18, 261, 14695, 4, 33100, 23849, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10060009463080044, "compression_ratio": 1.2808510638297872, "no_speech_prob": 0.018514562398195267}, {"id": 77, "seek": 51700, "start": 517.0, "end": 525.0, "text": " 85%. To nie jest drobna poprawa. To knockout. Co konkretnie sprawi\u0142o, \u017ce te odpowiedzi by\u0142y tak du\u017co lepsze?", "tokens": [50364, 14695, 6856, 1407, 2838, 3492, 3789, 65, 629, 1665, 424, 4151, 13, 1407, 6728, 346, 13, 3066, 36500, 2766, 22734, 72, 5249, 11, 3561, 535, 36574, 3992, 26366, 991, 26673, 476, 1878, 1381, 30, 50764], "temperature": 0.0, "avg_logprob": -0.10881050957573785, "compression_ratio": 1.2651162790697674, "no_speech_prob": 0.1053331196308136}, {"id": 78, "seek": 51700, "start": 525.0, "end": 533.0, "text": " Zacznijmy od praw dom\u00f3wno\u015bci, bo to by\u0142a pi\u0119ta achillesowa wielkich modeli. One cz\u0119sto haluczynuj\u0105.", "tokens": [50764, 1176, 14875, 77, 1718, 2226, 3611, 22508, 3285, 3901, 16438, 11, 748, 281, 23936, 32677, 1328, 2800, 14835, 5528, 20570, 48349, 2316, 72, 13, 1485, 34369, 7523, 1311, 1229, 77, 13263, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10881050957573785, "compression_ratio": 1.2651162790697674, "no_speech_prob": 0.1053331196308136}, {"id": 79, "seek": 51700, "start": 533.0, "end": 537.0, "text": " Czyli zmy\u015blaj\u0105 fakty z ogromn\u0105 pewno\u015bci\u0105 siebie.", "tokens": [51164, 37099, 710, 2226, 1788, 875, 8555, 33647, 874, 710, 34416, 298, 13113, 33002, 50227, 39137, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10881050957573785, "compression_ratio": 1.2651162790697674, "no_speech_prob": 0.1053331196308136}, {"id": 80, "seek": 53700, "start": 537.0, "end": 548.0, "text": " Tak. A Instrakt GPT wykaza\u0142 tu ogromn\u0105 popraw\u0119. W zadaniach o zamkni\u0119tej dziedzinie, jak streszczenia, wska\u017anik halucynacji spad\u0142 o po\u0142ow\u0119.", "tokens": [50364, 9118, 13, 316, 2730, 32249, 26039, 51, 39287, 12257, 1221, 2604, 34416, 298, 13113, 1665, 5131, 1274, 13, 343, 42788, 3782, 608, 277, 19876, 74, 35938, 975, 73, 9758, 15338, 259, 414, 11, 4207, 342, 495, 89, 38517, 11, 261, 20771, 10659, 13123, 7523, 1311, 2534, 13152, 637, 345, 1221, 277, 714, 1221, 305, 1274, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0712915959249016, "compression_ratio": 1.3522727272727273, "no_speech_prob": 0.5550998449325562}, {"id": 81, "seek": 53700, "start": 548.0, "end": 554.0, "text": " Z 41% dla GPT-3 do 21% dla Instrakt GPT.", "tokens": [50914, 1176, 18173, 4, 12285, 26039, 51, 12, 18, 360, 5080, 4, 12285, 2730, 32249, 26039, 51, 13, 51214], "temperature": 0.0, "avg_logprob": -0.0712915959249016, "compression_ratio": 1.3522727272727273, "no_speech_prob": 0.5550998449325562}, {"id": 82, "seek": 53700, "start": 554.0, "end": 565.0, "text": " To jest gigantyczna r\u00f3\u017cnica, je\u015bli chodzi o wiarygodno\u015b\u0107, o u\u017cyteczno\u015b\u0107, mniej zmy\u015blania, to mniej ryzyka. A co z kwestiami bezpiecze\u0144stwa, jak toksyczno\u015b\u0107?", "tokens": [51214, 1407, 3492, 8741, 394, 17466, 629, 19637, 32687, 11, 25630, 23998, 277, 26393, 822, 21787, 23293, 11, 277, 34097, 975, 3689, 23293, 11, 39513, 710, 2226, 19212, 5609, 11, 281, 39513, 20791, 40940, 13, 316, 598, 710, 42035, 15568, 47153, 9680, 12229, 4151, 11, 4207, 281, 1694, 17466, 23293, 30, 51764], "temperature": 0.0, "avg_logprob": -0.0712915959249016, "compression_ratio": 1.3522727272727273, "no_speech_prob": 0.5550998449325562}, {"id": 83, "seek": 56500, "start": 566.0, "end": 571.0, "text": " Tutaj wyniki s\u0105 bardziej z\u0142o\u017cone i pokazuj\u0105, jak trudny to jest problem.", "tokens": [50414, 41819, 31936, 9850, 9015, 27209, 710, 5249, 1427, 546, 741, 13010, 921, 13263, 11, 4207, 32007, 1634, 281, 3492, 1154, 13, 50664], "temperature": 0.0, "avg_logprob": -0.059265659405634954, "compression_ratio": 1.3183673469387756, "no_speech_prob": 0.17577680945396423}, {"id": 84, "seek": 56500, "start": 571.0, "end": 573.0, "text": " Czyli nie by\u0142o tak r\u00f3\u017cowo.", "tokens": [50664, 37099, 2838, 14811, 991, 19637, 19941, 13, 50764], "temperature": 0.0, "avg_logprob": -0.059265659405634954, "compression_ratio": 1.3183673469387756, "no_speech_prob": 0.17577680945396423}, {"id": 85, "seek": 56500, "start": 573.0, "end": 581.0, "text": " Instrakt GPT faktycznie generowa\u0142 o jakie\u015b 25% mniej toksycznych odpowiedzi, ale i to jest kluczowy haczyk.", "tokens": [50764, 2730, 32249, 26039, 51, 33647, 45586, 1337, 30105, 277, 31163, 3552, 4, 39513, 281, 1694, 17466, 9399, 36574, 3992, 11, 6775, 741, 281, 3492, 9671, 1311, 89, 10089, 324, 6522, 74, 13, 51164], "temperature": 0.0, "avg_logprob": -0.059265659405634954, "compression_ratio": 1.3183673469387756, "no_speech_prob": 0.17577680945396423}, {"id": 86, "seek": 56500, "start": 581.0, "end": 589.0, "text": " Dzia\u0142o si\u0119 tak tylko wtedy, gdy w poleceniu jawnie poproszono go o byciu grzecznym lub pe\u0142nym szacunku.", "tokens": [51164, 39448, 654, 5249, 3244, 991, 13219, 26959, 11, 28405, 261, 13208, 13037, 5951, 2784, 14215, 1665, 2635, 89, 8957, 352, 277, 538, 30795, 677, 1381, 3689, 12996, 15980, 43205, 12996, 7870, 326, 49910, 13, 51564], "temperature": 0.0, "avg_logprob": -0.059265659405634954, "compression_ratio": 1.3183673469387756, "no_speech_prob": 0.17577680945396423}, {"id": 87, "seek": 58900, "start": 590.0, "end": 596.0, "text": " To jest kluczowe. Czyli to nie by\u0142a fundamentalna zmiana w jego naturze.", "tokens": [50414, 1407, 3492, 9671, 1311, 89, 6880, 13, 37099, 281, 2838, 23936, 8088, 629, 17020, 8497, 261, 26542, 26389, 1381, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12335847029045446, "compression_ratio": 1.4125874125874125, "no_speech_prob": 0.5967420339584351}, {"id": 88, "seek": 58900, "start": 596.0, "end": 600.0, "text": " Raczej nauczy\u0142 si\u0119 reagowa\u0107 na konkretne s\u0142owo klucz.", "tokens": [50714, 42033, 16920, 49103, 1229, 1221, 3244, 26949, 11445, 1667, 36500, 716, 15116, 19941, 9671, 1311, 89, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12335847029045446, "compression_ratio": 1.4125874125874125, "no_speech_prob": 0.5967420339584351}, {"id": 89, "seek": 58900, "start": 600.0, "end": 608.0, "text": " To troch\u0119 jak samoch\u00f3d, kt\u00f3ry ma sprawne hamulce tylko gdy krzykniez, prosz\u0119 zatrzymaj si\u0119. To bardziej obej\u015bcie problemu ni\u017c jego rozwi\u0105zanie.", "tokens": [50914, 1407, 24926, 4207, 3247, 8997, 17081, 11, 9913, 463, 22734, 716, 7852, 425, 384, 13219, 28405, 350, 13047, 74, 2766, 89, 11, 39677, 35802, 13047, 1696, 73, 3244, 13, 1407, 27209, 36346, 73, 9815, 1154, 84, 28502, 26542, 9544, 22620, 7155, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12335847029045446, "compression_ratio": 1.4125874125874125, "no_speech_prob": 0.5967420339584351}, {"id": 90, "seek": 58900, "start": 608.0, "end": 617.0, "text": " Trafn\u0105\u0142 waga. To pokazuje, \u017ce RLHF jest pot\u0119\u017cnym narz\u0119dziem do optymalizacji pod k\u0105tem celu, kt\u00f3ry mu wska\u017cysz.", "tokens": [51314, 5403, 69, 13113, 1221, 261, 9286, 13, 1407, 13010, 43317, 11, 3561, 497, 43, 39, 37, 3492, 1847, 1274, 1427, 12996, 6714, 89, 42643, 76, 360, 2427, 4199, 304, 590, 13152, 2497, 350, 1611, 18275, 9277, 84, 11, 9913, 2992, 261, 20771, 1427, 20589, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12335847029045446, "compression_ratio": 1.4125874125874125, "no_speech_prob": 0.5967420339584351}, {"id": 91, "seek": 61700, "start": 617.0, "end": 621.0, "text": " A tutaj celem nadrz\u0119dnym by\u0142a pomocno\u015b\u0107, a niekoniecznie nieszkodliwo\u015b\u0107.", "tokens": [50364, 316, 12749, 1769, 10386, 12617, 19390, 6298, 12996, 23936, 48962, 23293, 11, 257, 2838, 18295, 414, 19923, 297, 15347, 74, 378, 2081, 48847, 13, 50564], "temperature": 0.0, "avg_logprob": -0.06924104253086474, "compression_ratio": 1.4035087719298245, "no_speech_prob": 0.05338847637176514}, {"id": 92, "seek": 61700, "start": 621.0, "end": 623.0, "text": " A co z uprzedzeniami z biasem?", "tokens": [50564, 316, 598, 710, 493, 81, 11312, 2904, 15568, 710, 12577, 443, 30, 50664], "temperature": 0.0, "avg_logprob": -0.06924104253086474, "compression_ratio": 1.4035087719298245, "no_speech_prob": 0.05338847637176514}, {"id": 93, "seek": 61700, "start": 623.0, "end": 629.0, "text": " Co jeszcze wa\u017cniejsze, badanie nie wykaza\u0142o \u017cadnej poprawy w kwestii bias na standardowych testach.", "tokens": [50664, 3066, 14168, 27777, 44258, 11, 1578, 7155, 2838, 39287, 12257, 5249, 39628, 11794, 1665, 5131, 88, 261, 42035, 5597, 12577, 1667, 3832, 19605, 1500, 608, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06924104253086474, "compression_ratio": 1.4035087719298245, "no_speech_prob": 0.05338847637176514}, {"id": 94, "seek": 61700, "start": 629.0, "end": 633.0, "text": " To dowodzi, \u017ce alignment nie jest monolitem.", "tokens": [50964, 1407, 9459, 14543, 11, 3561, 18515, 2838, 3492, 1108, 401, 270, 443, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06924104253086474, "compression_ratio": 1.4035087719298245, "no_speech_prob": 0.05338847637176514}, {"id": 95, "seek": 61700, "start": 633.0, "end": 637.0, "text": " Dopasowanie w jednym wymiarze nie gwarantuje poprawy w innym.", "tokens": [51164, 42657, 296, 22028, 261, 5232, 12996, 4628, 3057, 289, 1381, 2838, 290, 6925, 394, 13008, 1665, 5131, 88, 261, 294, 12996, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06924104253086474, "compression_ratio": 1.4035087719298245, "no_speech_prob": 0.05338847637176514}, {"id": 96, "seek": 63700, "start": 637.0, "end": 639.0, "text": " To prowadzi do kolejnego pytania.", "tokens": [50364, 1407, 36590, 3992, 360, 23749, 11858, 25878, 5609, 13, 50464], "temperature": 0.0, "avg_logprob": -0.0525060747608994, "compression_ratio": 1.3655172413793104, "no_speech_prob": 0.21689622104167938}, {"id": 97, "seek": 63700, "start": 639.0, "end": 648.0, "text": " Czy ten intensywny trening, skupiony na byciu mi\u0142ym i pomocnym, nie odbywa\u0142 si\u0119 kosztem og\u00f3lnych akademickich zdolno\u015bci modelu?", "tokens": [50464, 19832, 2064, 14056, 88, 43682, 2192, 773, 11, 1110, 1010, 46184, 1667, 538, 30795, 2752, 1221, 4199, 741, 48962, 12996, 11, 2838, 3611, 2322, 44603, 3244, 19532, 2682, 443, 5360, 15741, 9399, 9308, 49290, 618, 480, 16221, 401, 16438, 2316, 84, 30, 50914], "temperature": 0.0, "avg_logprob": -0.0525060747608994, "compression_ratio": 1.3655172413793104, "no_speech_prob": 0.21689622104167938}, {"id": 98, "seek": 63700, "start": 648.0, "end": 651.0, "text": " Czy nie by\u0142o czego\u015b w rodzaju podatku od dopasowania?", "tokens": [50914, 19832, 2838, 14811, 36559, 1788, 261, 28607, 33166, 2497, 267, 5279, 3611, 360, 20990, 21308, 30, 51064], "temperature": 0.0, "avg_logprob": -0.0525060747608994, "compression_ratio": 1.3655172413793104, "no_speech_prob": 0.21689622104167938}, {"id": 99, "seek": 63700, "start": 651.0, "end": 652.0, "text": " \u015awietne pytanie.", "tokens": [51064, 27933, 39083, 716, 36610, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0525060747608994, "compression_ratio": 1.3655172413793104, "no_speech_prob": 0.21689622104167938}, {"id": 100, "seek": 63700, "start": 652.0, "end": 656.0, "text": " I badacze nazwali ten problem w\u0142a\u015bnie alignment tax.", "tokens": [51114, 286, 1578, 326, 1381, 20151, 40054, 2064, 1154, 14234, 18515, 3366, 13, 51314], "temperature": 0.0, "avg_logprob": -0.0525060747608994, "compression_ratio": 1.3655172413793104, "no_speech_prob": 0.21689622104167938}, {"id": 101, "seek": 63700, "start": 656.0, "end": 663.0, "text": " I tak, na pocz\u0105tku zauwa\u017cyli, \u017ce proces RLHF faktycznie powodowa\u0142 pogorszenie wynik\u00f3w.", "tokens": [51314, 286, 991, 11, 1667, 43959, 710, 1459, 4151, 7735, 2081, 11, 3561, 17565, 497, 43, 39, 37, 33647, 45586, 3388, 378, 30105, 32037, 830, 16778, 31936, 1035, 3901, 13, 51664], "temperature": 0.0, "avg_logprob": -0.0525060747608994, "compression_ratio": 1.3655172413793104, "no_speech_prob": 0.21689622104167938}, {"id": 102, "seek": 63700, "start": 663.0, "end": 664.0, "text": " Na czym?", "tokens": [51664, 6056, 31466, 30, 51714], "temperature": 0.0, "avg_logprob": -0.0525060747608994, "compression_ratio": 1.3655172413793104, "no_speech_prob": 0.21689622104167938}, {"id": 103, "seek": 66400, "start": 664.0, "end": 668.0, "text": " W niekt\u00f3rych standardowych publicznych benchmarkach NLP.", "tokens": [50364, 343, 2838, 43073, 627, 339, 3832, 19605, 1908, 89, 9399, 18927, 608, 426, 45196, 13, 50564], "temperature": 0.0, "avg_logprob": -0.0957194425928311, "compression_ratio": 1.36328125, "no_speech_prob": 0.05320132523775101}, {"id": 104, "seek": 66400, "start": 668.0, "end": 676.0, "text": " Jak sk\u0142ad czy drop model stawa\u0142 si\u0119 lepszy w konwersacji, ale traci\u0142 na wydajno\u015bci w bardziej sformalizowanych zadaniach.", "tokens": [50564, 15029, 1110, 10358, 6430, 3270, 2316, 342, 10449, 1221, 3244, 476, 1878, 1229, 261, 5897, 5364, 13152, 11, 6775, 504, 22086, 1221, 1667, 25984, 1805, 16438, 261, 27209, 262, 837, 304, 590, 23341, 339, 42788, 3782, 608, 13, 50964], "temperature": 0.0, "avg_logprob": -0.0957194425928311, "compression_ratio": 1.36328125, "no_speech_prob": 0.05320132523775101}, {"id": 105, "seek": 66400, "start": 676.0, "end": 678.0, "text": " Czyli klasyczny kompromis.", "tokens": [50964, 37099, 9671, 5871, 3689, 1634, 5207, 28722, 271, 13, 51064], "temperature": 0.0, "avg_logprob": -0.0957194425928311, "compression_ratio": 1.36328125, "no_speech_prob": 0.05320132523775101}, {"id": 106, "seek": 66400, "start": 678.0, "end": 679.0, "text": " Co\u015b za co\u015b.", "tokens": [51064, 3066, 1788, 7949, 19241, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0957194425928311, "compression_ratio": 1.36328125, "no_speech_prob": 0.05320132523775101}, {"id": 107, "seek": 66400, "start": 679.0, "end": 681.0, "text": " I jak sobie z tym poradzili?", "tokens": [51114, 286, 4207, 13652, 710, 8107, 1515, 345, 89, 2312, 30, 51214], "temperature": 0.0, "avg_logprob": -0.0957194425928311, "compression_ratio": 1.36328125, "no_speech_prob": 0.05320132523775101}, {"id": 108, "seek": 66400, "start": 681.0, "end": 682.0, "text": " Bo to brzmi jak powa\u017cna wada.", "tokens": [51214, 3286, 281, 738, 89, 3057, 4207, 3388, 18264, 629, 261, 1538, 13, 51264], "temperature": 0.0, "avg_logprob": -0.0957194425928311, "compression_ratio": 1.36328125, "no_speech_prob": 0.05320132523775101}, {"id": 109, "seek": 66400, "start": 682.0, "end": 687.0, "text": " Znaleziono na to zaskakuj\u0105co proste i eleganckie rozwi\u0105zanie.", "tokens": [51264, 1176, 77, 37646, 49020, 1667, 281, 710, 3863, 514, 13263, 1291, 10293, 68, 741, 1118, 1275, 547, 414, 9544, 22620, 7155, 13, 51514], "temperature": 0.0, "avg_logprob": -0.0957194425928311, "compression_ratio": 1.36328125, "no_speech_prob": 0.05320132523775101}, {"id": 110, "seek": 68700, "start": 687.0, "end": 691.0, "text": " W trakcie trzeciego etapu, czyli treningu reinforcement learning,", "tokens": [50364, 343, 944, 74, 4260, 22266, 4260, 1571, 47634, 84, 11, 16591, 2192, 773, 84, 29280, 2539, 11, 50564], "temperature": 0.0, "avg_logprob": -0.07894132561879615, "compression_ratio": 1.4405594405594406, "no_speech_prob": 0.4787103533744812}, {"id": 111, "seek": 68700, "start": 691.0, "end": 697.0, "text": " postanowili domiesza\u0107 do procesu odrobin\u0119 oryginalnych danych treningowych GPT-3.", "tokens": [50564, 2183, 282, 305, 2312, 3285, 530, 35873, 360, 17565, 84, 3611, 340, 13496, 1274, 420, 88, 1494, 304, 9399, 274, 34644, 2192, 773, 19605, 26039, 51, 12, 18, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07894132561879615, "compression_ratio": 1.4405594405594406, "no_speech_prob": 0.4787103533744812}, {"id": 112, "seek": 68700, "start": 697.0, "end": 699.0, "text": " Tych sprzed ca\u0142ego procesu fine tuning?", "tokens": [50864, 5569, 339, 6103, 11312, 35224, 6308, 17565, 84, 2489, 15164, 30, 50964], "temperature": 0.0, "avg_logprob": -0.07894132561879615, "compression_ratio": 1.4405594405594406, "no_speech_prob": 0.4787103533744812}, {"id": 113, "seek": 68700, "start": 699.0, "end": 704.0, "text": " Tak, chodzi\u0142o o to, \u017ceby podczas maksymalizowania nagrody od reward model,", "tokens": [50964, 9118, 11, 23998, 5249, 277, 281, 11, 11316, 2497, 30989, 963, 3187, 5579, 590, 21308, 17096, 340, 3173, 3611, 7782, 2316, 11, 51214], "temperature": 0.0, "avg_logprob": -0.07894132561879615, "compression_ratio": 1.4405594405594406, "no_speech_prob": 0.4787103533744812}, {"id": 114, "seek": 68700, "start": 704.0, "end": 712.0, "text": " algorytm PPO by\u0142 jednocze\u015bnie karany za zbytnie oddalanie si\u0119 od oryginalnego, surowego modelu GPT-3.", "tokens": [51214, 3501, 827, 83, 76, 430, 34885, 16673, 5232, 26694, 1381, 12221, 7917, 1325, 7949, 710, 2322, 83, 2766, 7401, 304, 7155, 3244, 3611, 420, 88, 1494, 304, 11858, 11, 1022, 26576, 2316, 84, 26039, 51, 12, 18, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07894132561879615, "compression_ratio": 1.4405594405594406, "no_speech_prob": 0.4787103533744812}, {"id": 115, "seek": 68700, "start": 712.0, "end": 715.0, "text": " I w ten spos\u00f3b powsta\u0142 model PPO-PTX.", "tokens": [51614, 286, 261, 2064, 22904, 3388, 9140, 1221, 2316, 430, 34885, 12, 47, 51, 55, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07894132561879615, "compression_ratio": 1.4405594405594406, "no_speech_prob": 0.4787103533744812}, {"id": 116, "seek": 71500, "start": 715.0, "end": 718.0, "text": " Czekaj, to brzmi prawie zbyt prosto.", "tokens": [50364, 383, 19878, 1805, 11, 281, 738, 89, 3057, 3206, 8699, 710, 2322, 83, 10293, 78, 13, 50514], "temperature": 0.0, "avg_logprob": -0.06334480238549503, "compression_ratio": 1.5031055900621118, "no_speech_prob": 0.019948337227106094}, {"id": 117, "seek": 71500, "start": 718.0, "end": 722.0, "text": " Czy dodanie odrobiny starych danych naprawd\u0119 wystarczy\u0142o?", "tokens": [50514, 19832, 13886, 7155, 3611, 340, 65, 3519, 342, 822, 339, 274, 34644, 20970, 4628, 9710, 6522, 5249, 30, 50714], "temperature": 0.0, "avg_logprob": -0.06334480238549503, "compression_ratio": 1.5031055900621118, "no_speech_prob": 0.019948337227106094}, {"id": 118, "seek": 71500, "start": 722.0, "end": 727.0, "text": " Oczaza\u0142o si\u0119, \u017ce ten prosty zabieg zniwelowa\u0142 wi\u0119kszo\u015b\u0107 strat w wydajno\u015bci.", "tokens": [50714, 422, 3689, 12257, 5249, 3244, 11, 3561, 2064, 10293, 88, 24838, 20408, 710, 3722, 45512, 30105, 29968, 4765, 7753, 23674, 261, 25984, 1805, 16438, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06334480238549503, "compression_ratio": 1.5031055900621118, "no_speech_prob": 0.019948337227106094}, {"id": 119, "seek": 71500, "start": 727.0, "end": 733.0, "text": " Mo\u017cna to rozumie\u0107 jako tak\u0105 kotwice, kt\u00f3ra nie pozwala\u0142a modelowi zapomnie\u0107 o swoich fundamentalnych zdolno\u015bciach", "tokens": [50964, 44736, 629, 281, 48797, 414, 2162, 17123, 31069, 43029, 86, 573, 11, 19456, 2838, 40557, 5159, 5024, 2316, 24503, 14223, 298, 2766, 2162, 277, 13291, 480, 8088, 9399, 16221, 401, 16438, 608, 51264], "temperature": 0.0, "avg_logprob": -0.06334480238549503, "compression_ratio": 1.5031055900621118, "no_speech_prob": 0.019948337227106094}, {"id": 120, "seek": 71500, "start": 733.0, "end": 736.0, "text": " w pogoni zabyciem jak najbardziej pomocnym.", "tokens": [51264, 261, 32037, 17049, 710, 2509, 4260, 76, 4207, 41857, 48962, 12996, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06334480238549503, "compression_ratio": 1.5031055900621118, "no_speech_prob": 0.019948337227106094}, {"id": 121, "seek": 71500, "start": 736.0, "end": 739.0, "text": " Czyli mo\u017cna mie\u0107 ciastko i zje\u015b\u0107 ciastko?", "tokens": [51414, 37099, 17790, 35612, 6983, 525, 4093, 741, 710, 2884, 7753, 6983, 525, 4093, 30, 51564], "temperature": 0.0, "avg_logprob": -0.06334480238549503, "compression_ratio": 1.5031055900621118, "no_speech_prob": 0.019948337227106094}, {"id": 122, "seek": 71500, "start": 739.0, "end": 742.0, "text": " Pod warunkiem, \u017ce proces treningu jest odpowiednio zbalansowany.", "tokens": [51564, 12646, 1516, 3197, 4907, 11, 3561, 17565, 2192, 773, 84, 3492, 36574, 41084, 710, 2645, 599, 23341, 13, 51714], "temperature": 0.0, "avg_logprob": -0.06334480238549503, "compression_ratio": 1.5031055900621118, "no_speech_prob": 0.019948337227106094}, {"id": 123, "seek": 71500, "start": 742.0, "end": 744.0, "text": " To by\u0142o ogromne odkrycie.", "tokens": [51714, 1407, 14811, 34416, 298, 716, 3611, 43298, 4260, 13, 51814], "temperature": 0.0, "avg_logprob": -0.06334480238549503, "compression_ratio": 1.5031055900621118, "no_speech_prob": 0.019948337227106094}, {"id": 124, "seek": 74400, "start": 744.0, "end": 747.0, "text": " To prowadzi nas do kilku innych zaskakuj\u0105cych wniosk\u00f3w.", "tokens": [50364, 1407, 36590, 3992, 5382, 360, 5128, 5279, 36286, 710, 3863, 514, 13263, 31306, 45368, 2717, 23849, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09664331949674167, "compression_ratio": 1.4150943396226414, "no_speech_prob": 0.04977778345346451}, {"id": 125, "seek": 74400, "start": 747.0, "end": 753.0, "text": " Jednym z nich by\u0142o to, jak bardzo akademickie benchmarki rozmija\u0142y si\u0119 z rzeczywisto\u015bci\u0105.", "tokens": [50514, 27076, 12996, 710, 25570, 14811, 281, 11, 4207, 9034, 9308, 49290, 618, 414, 18927, 72, 9544, 76, 20642, 6825, 3244, 710, 26297, 86, 9334, 50227, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09664331949674167, "compression_ratio": 1.4150943396226414, "no_speech_prob": 0.04977778345346451}, {"id": 126, "seek": 74400, "start": 753.0, "end": 758.0, "text": " Zdecydowanie. Okaza\u0142o si\u0119, \u017ce publiczne zbiory danych jak Flan czy T0,", "tokens": [50814, 1176, 1479, 1344, 67, 22028, 13, 3477, 12257, 5249, 3244, 11, 3561, 1908, 43077, 710, 5614, 827, 274, 34644, 4207, 3235, 282, 6430, 314, 15, 11, 51064], "temperature": 0.0, "avg_logprob": -0.09664331949674167, "compression_ratio": 1.4150943396226414, "no_speech_prob": 0.04977778345346451}, {"id": 127, "seek": 74400, "start": 758.0, "end": 762.0, "text": " kt\u00f3re mia\u0142y uczy\u0107 modele pod\u0105\u017cania za instrukcjami wcale nie odzwierciedla\u0142y tego,", "tokens": [51064, 8864, 21290, 6825, 344, 33967, 4391, 306, 2497, 27242, 5609, 7949, 1058, 25126, 66, 73, 4526, 261, 37088, 2838, 3611, 14406, 811, 537, 292, 875, 6825, 8627, 11, 51264], "temperature": 0.0, "avg_logprob": -0.09664331949674167, "compression_ratio": 1.4150943396226414, "no_speech_prob": 0.04977778345346451}, {"id": 128, "seek": 74400, "start": 762.0, "end": 765.0, "text": " jak ludzie naprawd\u0119 u\u017cywaj\u0105 tych narz\u0119dzi.", "tokens": [51264, 4207, 37025, 20970, 34097, 86, 11133, 15180, 6714, 89, 6298, 3992, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09664331949674167, "compression_ratio": 1.4150943396226414, "no_speech_prob": 0.04977778345346451}, {"id": 129, "seek": 74400, "start": 765.0, "end": 769.0, "text": " A instrukt GPT trenowany na prawdziwych zapytaniach z API?", "tokens": [51414, 316, 1058, 19977, 26039, 51, 23136, 23341, 1667, 41175, 3992, 9726, 339, 14223, 4328, 3782, 608, 710, 9362, 30, 51614], "temperature": 0.0, "avg_logprob": -0.09664331949674167, "compression_ratio": 1.4150943396226414, "no_speech_prob": 0.04977778345346451}, {"id": 130, "seek": 74400, "start": 769.0, "end": 771.0, "text": " By\u0142o dniech o niebo lepsze.", "tokens": [51614, 3146, 5249, 274, 2766, 339, 277, 2838, 1763, 476, 1878, 1381, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09664331949674167, "compression_ratio": 1.4150943396226414, "no_speech_prob": 0.04977778345346451}, {"id": 131, "seek": 77100, "start": 771.0, "end": 777.0, "text": " Zanaliza tych zapyta\u0144 pokaza\u0142a, \u017ce ludzie najcz\u0119\u015bciej u\u017cywaj\u0105 modeli do kreatywnego generowania,", "tokens": [50364, 1176, 29702, 13427, 15180, 14223, 88, 1328, 5248, 13010, 12257, 5024, 11, 3561, 37025, 11212, 41151, 9815, 73, 34097, 86, 11133, 2316, 72, 360, 350, 620, 27112, 11858, 1337, 21308, 11, 50664], "temperature": 0.0, "avg_logprob": -0.07185964706616524, "compression_ratio": 1.4735202492211839, "no_speech_prob": 0.11254329979419708}, {"id": 132, "seek": 77100, "start": 777.0, "end": 781.0, "text": " do brainstormingu, do otwartej konwersacji,", "tokens": [50664, 360, 35245, 7050, 11, 360, 4337, 86, 11026, 73, 5897, 5364, 13152, 11, 50864], "temperature": 0.0, "avg_logprob": -0.07185964706616524, "compression_ratio": 1.4735202492211839, "no_speech_prob": 0.11254329979419708}, {"id": 133, "seek": 77100, "start": 781.0, "end": 785.0, "text": " a nie do prostych zada\u0144 klasyfikacyjnych, kt\u00f3re dominuj\u0105 w tych benchmarkach.", "tokens": [50864, 257, 2838, 360, 10293, 16384, 710, 1538, 5248, 9671, 5871, 31230, 31285, 9399, 11, 8864, 8859, 13263, 261, 15180, 18927, 608, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07185964706616524, "compression_ratio": 1.4735202492211839, "no_speech_prob": 0.11254329979419708}, {"id": 134, "seek": 77100, "start": 785.0, "end": 789.0, "text": " Dok\u0142adnie. Ca\u0142a spo\u0142eczno\u015b\u0107 badawcza optymalizowa\u0142a modele pod k\u0105tem testu,", "tokens": [51064, 29768, 10358, 2766, 13, 7544, 5024, 36851, 89, 23293, 272, 1538, 86, 41524, 2427, 4199, 304, 590, 5528, 5024, 4391, 306, 2497, 350, 1611, 18275, 1500, 84, 11, 51264], "temperature": 0.0, "avg_logprob": -0.07185964706616524, "compression_ratio": 1.4735202492211839, "no_speech_prob": 0.11254329979419708}, {"id": 135, "seek": 77100, "start": 789.0, "end": 792.0, "text": " kt\u00f3rego nikt w prawdziwym \u015bwiecie nie zamiera\u0142 zdawa\u0107.", "tokens": [51264, 46951, 297, 9874, 261, 41175, 3992, 86, 4199, 40078, 4260, 2838, 19876, 10609, 1221, 16221, 10449, 2162, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07185964706616524, "compression_ratio": 1.4735202492211839, "no_speech_prob": 0.11254329979419708}, {"id": 136, "seek": 77100, "start": 792.0, "end": 797.0, "text": " A co z generalizacj\u0105? Czy model nauczy\u0142 si\u0119 by\u0107 pomocny tylko w tym, co widzia\u0142 podczas treningu?", "tokens": [51414, 316, 598, 710, 2674, 590, 326, 8555, 30, 19832, 2316, 49103, 1229, 1221, 3244, 15069, 48962, 1634, 13219, 261, 8107, 11, 598, 27486, 8908, 2497, 30989, 2192, 773, 84, 30, 51664], "temperature": 0.0, "avg_logprob": -0.07185964706616524, "compression_ratio": 1.4735202492211839, "no_speech_prob": 0.11254329979419708}, {"id": 137, "seek": 79700, "start": 797.0, "end": 801.0, "text": " I tu jest chyba najbardziej magiczna cz\u0119\u015b\u0107. Nie.", "tokens": [50364, 286, 2604, 3492, 31532, 41857, 5585, 35458, 47149, 13, 12016, 13, 50564], "temperature": 0.0, "avg_logprob": -0.070055115607477, "compression_ratio": 1.3531468531468531, "no_speech_prob": 0.04460905119776726}, {"id": 138, "seek": 79700, "start": 801.0, "end": 802.0, "text": " Nie?", "tokens": [50564, 12016, 30, 50614], "temperature": 0.0, "avg_logprob": -0.070055115607477, "compression_ratio": 1.3531468531468531, "no_speech_prob": 0.04460905119776726}, {"id": 139, "seek": 79700, "start": 802.0, "end": 809.0, "text": " Instrukt GPT nauczy\u0142 si\u0119 stosowa\u0107 do instrukcji w zadaniach, do kt\u00f3rych nie by\u0142 bezpo\u015brednio trenowany.", "tokens": [50614, 2730, 19977, 26039, 51, 49103, 1229, 1221, 3244, 43581, 11445, 360, 1058, 25126, 19649, 261, 42788, 3782, 608, 11, 360, 30382, 2838, 16673, 10782, 2259, 1788, 986, 41084, 23136, 23341, 13, 50964], "temperature": 0.0, "avg_logprob": -0.070055115607477, "compression_ratio": 1.3531468531468531, "no_speech_prob": 0.04460905119776726}, {"id": 140, "seek": 79700, "start": 809.0, "end": 815.0, "text": " Ca\u0142kiem nie\u017ale radzi\u0142 sobie z poleceniami dotycz\u0105cymi kodu programistycznego, nawet w innych j\u0119zykach.", "tokens": [50964, 7544, 1221, 26116, 2838, 10659, 306, 2843, 3992, 1221, 13652, 710, 13208, 13037, 15568, 5893, 17466, 1611, 1344, 3057, 350, 34873, 1461, 468, 17466, 11858, 11, 22696, 261, 36286, 49055, 41326, 13, 51264], "temperature": 0.0, "avg_logprob": -0.070055115607477, "compression_ratio": 1.3531468531468531, "no_speech_prob": 0.04460905119776726}, {"id": 141, "seek": 79700, "start": 815.0, "end": 818.0, "text": " Mimo \u017ce takie przyk\u0142ady stanowi\u0142y z nikom\u0105 cz\u0119\u015b\u0107 danych?", "tokens": [51264, 376, 6934, 3561, 15963, 6501, 74, 1221, 880, 27984, 24503, 6825, 710, 44336, 298, 1611, 47149, 274, 34644, 30, 51414], "temperature": 0.0, "avg_logprob": -0.070055115607477, "compression_ratio": 1.3531468531468531, "no_speech_prob": 0.04460905119776726}, {"id": 142, "seek": 79700, "start": 818.0, "end": 822.0, "text": " Wr\u0119cz \u015bladow\u0105, to jest naprawd\u0119 niesamowite.", "tokens": [51414, 10159, 1274, 3689, 8299, 9290, 30297, 11, 281, 3492, 20970, 48100, 335, 305, 642, 13, 51614], "temperature": 0.0, "avg_logprob": -0.070055115607477, "compression_ratio": 1.3531468531468531, "no_speech_prob": 0.04460905119776726}, {"id": 143, "seek": 82200, "start": 822.0, "end": 827.0, "text": " To sugeruje, \u017ce on nie nauczy\u0142 si\u0119 na pami\u0119\u0107, jak odpowiada\u0107 na 20 typ\u00f3w zapyta\u0144.", "tokens": [50364, 1407, 459, 1321, 13008, 11, 3561, 322, 2838, 49103, 1229, 1221, 3244, 1667, 31088, 2162, 11, 4207, 24314, 39018, 2162, 1667, 945, 2125, 3901, 14223, 88, 1328, 5248, 13, 50614], "temperature": 0.0, "avg_logprob": -0.0551178147192715, "compression_ratio": 1.4182389937106918, "no_speech_prob": 0.6420889496803284}, {"id": 144, "seek": 82200, "start": 827.0, "end": 832.0, "text": " On w jaki\u015b spos\u00f3b poj\u0105\u0142 abstrakcyjn\u0105 koncepcj\u0119 pod\u0105\u017cania za intencj\u0105.", "tokens": [50614, 1282, 261, 34721, 22904, 714, 8555, 1221, 10823, 11272, 42949, 13113, 5897, 27493, 41960, 2497, 27242, 5609, 7949, 560, 22660, 8555, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0551178147192715, "compression_ratio": 1.4182389937106918, "no_speech_prob": 0.6420889496803284}, {"id": 145, "seek": 82200, "start": 832.0, "end": 835.0, "text": " Jak maszyna w og\u00f3le uczy si\u0119 czego\u015b tak ludzkiego?", "tokens": [50864, 15029, 2300, 1229, 629, 261, 29229, 344, 6522, 3244, 36559, 1788, 991, 15946, 30154, 12200, 30, 51014], "temperature": 0.0, "avg_logprob": -0.0551178147192715, "compression_ratio": 1.4182389937106918, "no_speech_prob": 0.6420889496803284}, {"id": 146, "seek": 82200, "start": 835.0, "end": 837.0, "text": " To jest pytanie za miliard dolar\u00f3w.", "tokens": [51014, 1407, 3492, 36610, 7949, 1962, 72, 515, 360, 2200, 3901, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0551178147192715, "compression_ratio": 1.4182389937106918, "no_speech_prob": 0.6420889496803284}, {"id": 147, "seek": 82200, "start": 837.0, "end": 843.0, "text": " Pokazuje, \u017ce metoda RLHF to nie jest tylko powierzchowne polerowanie.", "tokens": [51114, 14958, 43317, 11, 3561, 1131, 13449, 497, 43, 39, 37, 281, 2838, 3492, 13219, 3388, 34602, 339, 648, 68, 1180, 260, 22028, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0551178147192715, "compression_ratio": 1.4182389937106918, "no_speech_prob": 0.6420889496803284}, {"id": 148, "seek": 82200, "start": 843.0, "end": 850.0, "text": " Ona faktycznie popycha model w kierunku bardziej og\u00f3lnego rozumienia, czym jest bycia u\u017cytecznym partnerem w rozmowie.", "tokens": [51414, 49793, 33647, 45586, 1665, 88, 4413, 2316, 261, 38767, 49910, 27209, 5360, 15741, 11858, 48797, 18811, 11, 31466, 3492, 538, 2755, 34097, 975, 3689, 12996, 644, 77, 7333, 261, 35234, 13998, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0551178147192715, "compression_ratio": 1.4182389937106918, "no_speech_prob": 0.6420889496803284}, {"id": 149, "seek": 85000, "start": 851.0, "end": 854.0, "text": " Mimo to nie jest to technologia idealna.", "tokens": [50414, 376, 6934, 281, 2838, 3492, 281, 1537, 24103, 7157, 629, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07850402022061283, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.09431351721286774}, {"id": 150, "seek": 85000, "start": 854.0, "end": 858.0, "text": " Oczywi\u015bcie, \u017ce nie i ta praca bardzo uczciwie podchodzi do swoich ogranicze\u0144.", "tokens": [50564, 42980, 11, 3561, 2838, 741, 1846, 582, 6628, 9034, 35403, 537, 8699, 2497, 34616, 360, 13291, 480, 34416, 30732, 49689, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07850402022061283, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.09431351721286774}, {"id": 151, "seek": 85000, "start": 858.0, "end": 860.0, "text": " No w\u0142a\u015bnie, zejd\u017amy na ziemi\u0119.", "tokens": [50764, 883, 14234, 11, 5277, 37109, 10659, 2226, 1667, 16503, 3057, 1274, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07850402022061283, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.09431351721286774}, {"id": 152, "seek": 85000, "start": 860.0, "end": 864.0, "text": " Mimo tych wszystkich ulepsze\u0144 ten model wci\u0105\u017c pope\u0142nia\u0142 b\u0142\u0119dy, prawda?", "tokens": [50864, 376, 6934, 15180, 34234, 344, 306, 1878, 49689, 2064, 2316, 261, 537, 27242, 42248, 1221, 77, 8908, 272, 46564, 3173, 11, 43607, 30, 51064], "temperature": 0.0, "avg_logprob": -0.07850402022061283, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.09431351721286774}, {"id": 153, "seek": 85000, "start": 864.0, "end": 866.0, "text": " Oczywi\u015bcie i to czasem bardzo proste.", "tokens": [51064, 42980, 741, 281, 13190, 443, 9034, 10293, 68, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07850402022061283, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.09431351721286774}, {"id": 154, "seek": 85000, "start": 866.0, "end": 870.0, "text": " Bywa\u0142 zagubiony, gdy polecenie opiera\u0142o si\u0119 na fa\u0142szywym za\u0142o\u017ceniu.", "tokens": [51164, 3146, 44603, 27001, 836, 46184, 11, 28405, 13208, 13037, 414, 999, 10609, 5249, 3244, 1667, 2050, 1221, 7706, 86, 4199, 7949, 5249, 24930, 5951, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07850402022061283, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.09431351721286774}, {"id": 155, "seek": 85000, "start": 870.0, "end": 871.0, "text": " Jaki\u015b przyk\u0142ad.", "tokens": [51364, 508, 7421, 1788, 23144, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07850402022061283, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.09431351721286774}, {"id": 156, "seek": 85000, "start": 871.0, "end": 873.0, "text": " Na przyk\u0142ad pytanie.", "tokens": [51414, 6056, 23144, 36610, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07850402022061283, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.09431351721286774}, {"id": 157, "seek": 85000, "start": 873.0, "end": 876.0, "text": " Dlaczego wa\u017cne jest jedzenie skarpetek po medytacji?", "tokens": [51514, 413, 75, 39329, 46110, 3492, 5232, 16778, 1110, 6529, 302, 916, 714, 1205, 4328, 13152, 30, 51664], "temperature": 0.0, "avg_logprob": -0.07850402022061283, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.09431351721286774}, {"id": 158, "seek": 87600, "start": 877.0, "end": 883.0, "text": " Zamiast zidentyfikowa\u0107 absurd, on pr\u00f3bowa\u0142 wymy\u015bli\u0107 jakie\u015b pseudonaukowe uzasadnienie,", "tokens": [50414, 1176, 4526, 525, 710, 1078, 88, 31230, 11445, 19774, 11, 322, 8565, 65, 30105, 4628, 2226, 15350, 2162, 31163, 25505, 532, 266, 1459, 74, 6880, 16851, 296, 345, 77, 27385, 11, 50714], "temperature": 0.0, "avg_logprob": -0.052834794876423286, "compression_ratio": 1.400709219858156, "no_speech_prob": 0.002724400954321027}, {"id": 159, "seek": 87600, "start": 883.0, "end": 887.0, "text": " bo by\u0142 wytrenowany, \u017ceby by\u0107 pomocnym za wszelk\u0105 cen\u0119.", "tokens": [50714, 748, 16673, 261, 4328, 1095, 23341, 11, 11316, 15069, 48962, 12996, 7949, 37647, 12971, 26304, 27900, 1274, 13, 50914], "temperature": 0.0, "avg_logprob": -0.052834794876423286, "compression_ratio": 1.400709219858156, "no_speech_prob": 0.002724400954321027}, {"id": 160, "seek": 87600, "start": 887.0, "end": 888.0, "text": " Dok\u0142adnie.", "tokens": [50914, 29768, 10358, 2766, 13, 50964], "temperature": 0.0, "avg_logprob": -0.052834794876423286, "compression_ratio": 1.400709219858156, "no_speech_prob": 0.002724400954321027}, {"id": 161, "seek": 87600, "start": 888.0, "end": 891.0, "text": " Czyli wci\u0105\u017c mamy do czynienia z genialnym studentem,", "tokens": [50964, 37099, 261, 537, 27242, 17335, 360, 6430, 77, 18811, 710, 48228, 12996, 3107, 443, 11, 51114], "temperature": 0.0, "avg_logprob": -0.052834794876423286, "compression_ratio": 1.400709219858156, "no_speech_prob": 0.002724400954321027}, {"id": 162, "seek": 87600, "start": 891.0, "end": 894.0, "text": " kt\u00f3rego mo\u017cna nabra\u0107 pytaniem o jedzenie skarpetek.", "tokens": [51114, 46951, 17790, 297, 455, 424, 2162, 25878, 282, 4907, 277, 5232, 16778, 1110, 6529, 302, 916, 13, 51264], "temperature": 0.0, "avg_logprob": -0.052834794876423286, "compression_ratio": 1.400709219858156, "no_speech_prob": 0.002724400954321027}, {"id": 163, "seek": 87600, "start": 894.0, "end": 898.0, "text": " To pokazuje, jak daleko jeszcze do prawdziwego rozumienia.", "tokens": [51264, 1407, 13010, 43317, 11, 4207, 11702, 34241, 14168, 360, 41175, 3992, 826, 1571, 48797, 18811, 13, 51464], "temperature": 0.0, "avg_logprob": -0.052834794876423286, "compression_ratio": 1.400709219858156, "no_speech_prob": 0.002724400954321027}, {"id": 164, "seek": 87600, "start": 898.0, "end": 899.0, "text": " A to nie wszystko.", "tokens": [51464, 316, 281, 2838, 22607, 13, 51514], "temperature": 0.0, "avg_logprob": -0.052834794876423286, "compression_ratio": 1.400709219858156, "no_speech_prob": 0.002724400954321027}, {"id": 165, "seek": 87600, "start": 899.0, "end": 902.0, "text": " Bywa\u0142 te\u017c zbyt asekuracyjny, rozwlek\u0142y.", "tokens": [51514, 3146, 44603, 9516, 710, 2322, 83, 382, 916, 374, 31285, 1634, 11, 9544, 86, 29205, 6825, 13, 51664], "temperature": 0.0, "avg_logprob": -0.052834794876423286, "compression_ratio": 1.400709219858156, "no_speech_prob": 0.002724400954321027}, {"id": 166, "seek": 90200, "start": 902.0, "end": 907.0, "text": " Ale najwa\u017cniejsze ograniczenie i fundamentalne pytanie, kt\u00f3re te badania postawi\u0142y brzmi.", "tokens": [50364, 9366, 11212, 27111, 44258, 34416, 30732, 16778, 741, 8088, 716, 36610, 11, 8864, 535, 1578, 5609, 2183, 38402, 6825, 738, 89, 3057, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07667903076830528, "compression_ratio": 1.4248366013071896, "no_speech_prob": 0.037034716457128525}, {"id": 167, "seek": 90200, "start": 907.0, "end": 910.0, "text": " Do kogo tak naprawd\u0119 dopasowujemy ten model?", "tokens": [50614, 1144, 350, 23515, 991, 20970, 360, 20990, 305, 21767, 2064, 2316, 30, 50764], "temperature": 0.0, "avg_logprob": -0.07667903076830528, "compression_ratio": 1.4248366013071896, "no_speech_prob": 0.037034716457128525}, {"id": 168, "seek": 90200, "start": 910.0, "end": 915.0, "text": " Wracamy do tych 40 labellers, bo m\u00f3wimy ludzkie preferencje.", "tokens": [50764, 10159, 326, 7804, 360, 15180, 3356, 2715, 898, 433, 11, 748, 13489, 13189, 15946, 89, 22872, 4382, 22660, 2884, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07667903076830528, "compression_ratio": 1.4248366013071896, "no_speech_prob": 0.037034716457128525}, {"id": 169, "seek": 90200, "start": 915.0, "end": 919.0, "text": " Ale to by\u0142y preferencje tych konkretnych 40 os\u00f3b.", "tokens": [51014, 9366, 281, 26366, 4382, 22660, 2884, 15180, 36500, 9399, 3356, 32089, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07667903076830528, "compression_ratio": 1.4248366013071896, "no_speech_prob": 0.037034716457128525}, {"id": 170, "seek": 90200, "start": 919.0, "end": 922.0, "text": " Dzia\u0142aj\u0105cych wed\u0142ug instrukcji odbadaczy z OpenAI.", "tokens": [51214, 39448, 8908, 11133, 31306, 6393, 34077, 1058, 25126, 19649, 3611, 27580, 14691, 710, 7238, 48698, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07667903076830528, "compression_ratio": 1.4248366013071896, "no_speech_prob": 0.037034716457128525}, {"id": 171, "seek": 90200, "start": 922.0, "end": 925.0, "text": " To nie s\u0105 uniwersalne ludzkie warto\u015bci.", "tokens": [51364, 1407, 2838, 9015, 36435, 5364, 304, 716, 15946, 89, 22872, 31830, 6199, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07667903076830528, "compression_ratio": 1.4248366013071896, "no_speech_prob": 0.037034716457128525}, {"id": 172, "seek": 90200, "start": 925.0, "end": 927.0, "text": " To jest sedno problemu.", "tokens": [51514, 1407, 3492, 9643, 1771, 1154, 84, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07667903076830528, "compression_ratio": 1.4248366013071896, "no_speech_prob": 0.037034716457128525}, {"id": 173, "seek": 90200, "start": 927.0, "end": 931.0, "text": " No i najwi\u0119ksze otwarte pytanie w ca\u0142ej dziedzinie alignment.", "tokens": [51614, 883, 741, 48636, 1694, 1381, 4337, 86, 11026, 36610, 261, 47631, 73, 9758, 15338, 259, 414, 18515, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07667903076830528, "compression_ratio": 1.4248366013071896, "no_speech_prob": 0.037034716457128525}, {"id": 174, "seek": 93100, "start": 931.0, "end": 937.0, "text": " Zachowanie instrukt GPT jest bezpo\u015brednim odzwierciedleniem gustu i warto\u015bci bardzo ma\u0142ej,", "tokens": [50364, 21028, 22028, 1058, 19977, 26039, 51, 3492, 10782, 2259, 1788, 986, 39223, 3611, 14406, 811, 537, 292, 6698, 4907, 9679, 84, 741, 31830, 6199, 9034, 463, 19827, 73, 11, 50664], "temperature": 0.0, "avg_logprob": -0.07195109496881932, "compression_ratio": 1.4558823529411764, "no_speech_prob": 0.0006567080854438245}, {"id": 175, "seek": 93100, "start": 937.0, "end": 942.0, "text": " prawdopodobnie ma\u0142o zr\u00f3\u017cnicowanej demograficznie grupy ludzi z globalnej p\u00f3\u0142nocy.", "tokens": [50664, 41175, 46684, 996, 2766, 463, 5249, 710, 11721, 1427, 7692, 23066, 73, 1371, 664, 424, 1786, 89, 2766, 12740, 88, 29586, 710, 4338, 11794, 47907, 26694, 88, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07195109496881932, "compression_ratio": 1.4558823529411764, "no_speech_prob": 0.0006567080854438245}, {"id": 176, "seek": 93100, "start": 942.0, "end": 946.0, "text": " Kt\u00f3rzy zostali poinstruowani przez inn\u0105, jeszcze mniejsz\u0105 grup\u0119 badaczy.", "tokens": [50914, 591, 4547, 13047, 31873, 5103, 714, 13911, 894, 305, 3782, 14064, 7714, 1611, 11, 14168, 275, 30295, 8925, 12740, 1274, 1578, 14691, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07195109496881932, "compression_ratio": 1.4558823529411764, "no_speech_prob": 0.0006567080854438245}, {"id": 177, "seek": 93100, "start": 946.0, "end": 953.0, "text": " Wi\u0119c to, co model uznaje za dobre, jest wynikiem decyzji projektowych niewielkiej, elitarnej grupy.", "tokens": [51114, 32508, 281, 11, 598, 2316, 16851, 629, 2884, 7949, 41959, 11, 3492, 31936, 1035, 4907, 979, 37433, 4013, 26261, 19605, 43622, 1187, 45145, 11, 806, 3981, 11794, 12740, 88, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07195109496881932, "compression_ratio": 1.4558823529411764, "no_speech_prob": 0.0006567080854438245}, {"id": 178, "seek": 93100, "start": 953.0, "end": 955.0, "text": " I to pokazuje fundamentalne wyzwanie.", "tokens": [51464, 286, 281, 13010, 43317, 8088, 716, 4628, 14406, 7155, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07195109496881932, "compression_ratio": 1.4558823529411764, "no_speech_prob": 0.0006567080854438245}, {"id": 179, "seek": 93100, "start": 955.0, "end": 960.0, "text": " Jak skalowa\u0107 ten proces, by uwzgl\u0105dnia\u0142 r\u00f3\u017cnorodno\u015b\u0107 ludzkich warto\u015bci na ca\u0142ym \u015bwiecie.", "tokens": [51564, 15029, 16890, 11445, 2064, 17565, 11, 538, 23147, 89, 7191, 18962, 12679, 1221, 19637, 19048, 378, 23293, 15946, 30154, 480, 31830, 6199, 1667, 35224, 4199, 40078, 4260, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07195109496881932, "compression_ratio": 1.4558823529411764, "no_speech_prob": 0.0006567080854438245}, {"id": 180, "seek": 96000, "start": 961.0, "end": 963.0, "text": " Okej, zebrali\u015bmy mn\u00f3stwo informacji.", "tokens": [50414, 29094, 73, 11, 5277, 1443, 33955, 275, 77, 45052, 6120, 1356, 13152, 13, 50514], "temperature": 0.0, "avg_logprob": -0.06620933532714844, "compression_ratio": 1.3952702702702702, "no_speech_prob": 0.029173927381634712}, {"id": 181, "seek": 96000, "start": 963.0, "end": 965.0, "text": " Spr\u00f3bujmy to wszystko podsumowa\u0107.", "tokens": [50514, 7702, 14216, 4579, 2226, 281, 22607, 31925, 449, 11445, 13, 50614], "temperature": 0.0, "avg_logprob": -0.06620933532714844, "compression_ratio": 1.3952702702702702, "no_speech_prob": 0.029173927381634712}, {"id": 182, "seek": 96000, "start": 965.0, "end": 966.0, "text": " Co to wszystko oznacza?", "tokens": [50614, 3066, 281, 22607, 277, 22672, 326, 2394, 30, 50664], "temperature": 0.0, "avg_logprob": -0.06620933532714844, "compression_ratio": 1.3952702702702702, "no_speech_prob": 0.029173927381634712}, {"id": 183, "seek": 96000, "start": 966.0, "end": 967.0, "text": " No c\u00f3\u017c.", "tokens": [50664, 883, 6333, 1427, 13, 50714], "temperature": 0.0, "avg_logprob": -0.06620933532714844, "compression_ratio": 1.3952702702702702, "no_speech_prob": 0.029173927381634712}, {"id": 184, "seek": 96000, "start": 968.0, "end": 971.0, "text": " Wygl\u0105da na to, \u017ce g\u0142\u00f3wne przes\u0142anie tej pracy jest jasne.", "tokens": [50764, 14458, 7191, 26398, 1667, 281, 11, 3561, 18117, 3901, 716, 6541, 279, 1221, 7155, 12573, 35591, 3492, 361, 296, 716, 13, 50914], "temperature": 0.0, "avg_logprob": -0.06620933532714844, "compression_ratio": 1.3952702702702702, "no_speech_prob": 0.029173927381634712}, {"id": 185, "seek": 96000, "start": 971.0, "end": 977.0, "text": " Technika RLHF okaza\u0142a si\u0119 rewolucyjnie skutecznym sposobem na dopasowanie modeli do ludzkich intencji.", "tokens": [50914, 8337, 5439, 497, 43, 39, 37, 3133, 12257, 5024, 3244, 319, 48481, 1311, 88, 73, 2766, 1110, 1169, 3689, 12996, 20443, 996, 443, 1667, 360, 20990, 22028, 2316, 72, 360, 15946, 30154, 480, 43094, 19649, 13, 51214], "temperature": 0.0, "avg_logprob": -0.06620933532714844, "compression_ratio": 1.3952702702702702, "no_speech_prob": 0.029173927381634712}, {"id": 186, "seek": 96000, "start": 977.0, "end": 978.0, "text": " Zdecydowanie.", "tokens": [51214, 1176, 1479, 1344, 67, 22028, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06620933532714844, "compression_ratio": 1.3952702702702702, "no_speech_prob": 0.029173927381634712}, {"id": 187, "seek": 96000, "start": 978.0, "end": 987.0, "text": " To badanie udowodni\u0142o, \u017ce inteligentne, ukierunkowane trenowanie mo\u017ce by\u0107 znacznie wa\u017cniejsze ni\u017c sama surowa skala.", "tokens": [51264, 1407, 1578, 7155, 11727, 305, 378, 3722, 5249, 11, 3561, 24777, 25002, 716, 11, 26769, 811, 3197, 23066, 23136, 22028, 12034, 15069, 15397, 14875, 2766, 27777, 44258, 28502, 17768, 1022, 5528, 1110, 5159, 13, 51714], "temperature": 0.0, "avg_logprob": -0.06620933532714844, "compression_ratio": 1.3952702702702702, "no_speech_prob": 0.029173927381634712}, {"id": 188, "seek": 98700, "start": 987.0, "end": 991.0, "text": " Lepsze dopasowanie wygra\u0142o z wi\u0119ksz\u0105 liczb\u0105 parametr\u00f3w.", "tokens": [50364, 441, 10653, 1381, 360, 20990, 22028, 4628, 20735, 5249, 710, 29968, 8925, 6169, 89, 65, 1611, 6220, 27965, 3901, 13, 50564], "temperature": 0.0, "avg_logprob": -0.06597075366333827, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.00905949342995882}, {"id": 189, "seek": 98700, "start": 991.0, "end": 997.0, "text": " Tak, ta praca po\u0142o\u017cy\u0142a podwaliny pod wszystkie modele konwersacyjne, kt\u00f3re znamy i kt\u00f3rych u\u017cywamy dzisiaj.", "tokens": [50564, 9118, 11, 1846, 582, 6628, 714, 5249, 7735, 5024, 2497, 29530, 3519, 2497, 31723, 4391, 306, 5897, 5364, 31285, 716, 11, 8864, 710, 5378, 88, 741, 30382, 34097, 86, 7804, 25772, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06597075366333827, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.00905949342995882}, {"id": 190, "seek": 98700, "start": 997.0, "end": 1000.0, "text": " Od czat GPT po wszystkie inne.", "tokens": [50864, 12210, 6472, 267, 26039, 51, 714, 31723, 24170, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06597075366333827, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.00905949342995882}, {"id": 191, "seek": 98700, "start": 1000.0, "end": 1005.0, "text": " Skierowa\u0142a uwag\u0119 ca\u0142ej bran\u017cy z obsesji na punkcie samego powi\u0119kszania modeli.", "tokens": [51014, 7324, 811, 5528, 5024, 43696, 47631, 73, 12029, 7735, 710, 3181, 279, 4013, 1667, 25188, 4260, 912, 1571, 3388, 5034, 1694, 89, 5609, 2316, 72, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06597075366333827, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.00905949342995882}, {"id": 192, "seek": 98700, "start": 1005.0, "end": 1012.0, "text": " Na my\u015blenie o tym, jak uczyni\u0107 je lepszymi, bardziej u\u017cytecznymi i bezpieczniejszymi narz\u0119dziami.", "tokens": [51264, 6056, 48633, 6698, 414, 277, 8107, 11, 4207, 344, 6522, 3722, 2162, 1506, 476, 1878, 1229, 3057, 11, 27209, 34097, 975, 3689, 31813, 741, 47153, 3689, 10402, 7706, 3057, 6714, 89, 6298, 89, 15568, 13, 51614], "temperature": 0.0, "avg_logprob": -0.06597075366333827, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.00905949342995882}, {"id": 193, "seek": 98700, "start": 1012.0, "end": 1015.0, "text": " To by\u0142 fundamentalny krok w badaniach nad alignment.", "tokens": [51614, 1407, 16673, 8088, 1634, 350, 31621, 261, 1578, 3782, 608, 12617, 18515, 13, 51764], "temperature": 0.0, "avg_logprob": -0.06597075366333827, "compression_ratio": 1.459016393442623, "no_speech_prob": 0.00905949342995882}, {"id": 194, "seek": 101500, "start": 1015.0, "end": 1022.0, "text": " Czyli nad zapewnieniem, \u017ce przysz\u0142e, jeszcze pot\u0119\u017cniejsze systemy AI b\u0119d\u0105 dzia\u0142a\u0107 zgodnie z naszymi intencjami.", "tokens": [50364, 37099, 12617, 7949, 494, 895, 1053, 4907, 11, 3561, 44018, 19827, 11, 14168, 1847, 1274, 1427, 44258, 1185, 88, 7318, 26239, 37903, 2162, 710, 21787, 2766, 710, 5382, 1229, 3057, 560, 22660, 73, 4526, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07228769678058046, "compression_ratio": 1.47, "no_speech_prob": 0.006979734171181917}, {"id": 195, "seek": 101500, "start": 1022.0, "end": 1027.0, "text": " I na koniec zostawmy s\u0142uchaczy z jedn\u0105 my\u015bl\u0105, kt\u00f3ra naturalnie wyp\u0142ywa z tej rozmowy.", "tokens": [50714, 286, 1667, 5897, 35733, 31873, 1607, 2226, 15116, 625, 14691, 710, 5232, 13113, 452, 19212, 1611, 11, 19456, 3303, 2766, 46392, 6825, 4151, 710, 12573, 35234, 10089, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07228769678058046, "compression_ratio": 1.47, "no_speech_prob": 0.006979734171181917}, {"id": 196, "seek": 101500, "start": 1027.0, "end": 1032.0, "text": " Sami autorze pracy sugeruj\u0105, \u017ce w przysz\u0142o\u015bci mo\u017cna by p\u00f3j\u015b\u0107 o krok dalej.", "tokens": [50964, 44029, 19510, 1381, 35591, 459, 1321, 13263, 11, 3561, 261, 44018, 35059, 17790, 538, 280, 18999, 7753, 277, 350, 31621, 34257, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07228769678058046, "compression_ratio": 1.47, "no_speech_prob": 0.006979734171181917}, {"id": 197, "seek": 101500, "start": 1032.0, "end": 1037.0, "text": " Skoro model mo\u017cna dopasowa\u0107 do preferencji 40 labelers to mo\u017ce,", "tokens": [51214, 7324, 10780, 2316, 17790, 360, 20990, 11445, 360, 4382, 268, 19649, 3356, 7645, 433, 281, 12034, 11, 51464], "temperature": 0.0, "avg_logprob": -0.07228769678058046, "compression_ratio": 1.47, "no_speech_prob": 0.006979734171181917}, {"id": 198, "seek": 101500, "start": 1037.0, "end": 1041.0, "text": " mo\u017cna by trenowa\u0107 modele dopasowane do warto\u015bci r\u00f3\u017cnych grup spo\u0142ecznych.", "tokens": [51464, 17790, 538, 23136, 11445, 4391, 306, 360, 20990, 23066, 360, 31830, 6199, 42602, 12740, 36851, 89, 9399, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07228769678058046, "compression_ratio": 1.47, "no_speech_prob": 0.006979734171181917}, {"id": 199, "seek": 104100, "start": 1041.0, "end": 1045.0, "text": " Kultur. A nawet pojedynczych u\u017cytkownik\u00f3w.", "tokens": [50364, 591, 723, 374, 13, 316, 22696, 714, 40543, 2534, 6522, 339, 344, 1427, 4328, 74, 44895, 3901, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09445568059114802, "compression_ratio": 1.4831081081081081, "no_speech_prob": 0.23128405213356018}, {"id": 200, "seek": 104100, "start": 1045.0, "end": 1049.0, "text": " To jest logiczna konsekwencja. Technologia na to pozwala.", "tokens": [50564, 1407, 3492, 9952, 35458, 47020, 74, 15615, 34056, 13, 8337, 24103, 1667, 281, 40557, 5159, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09445568059114802, "compression_ratio": 1.4831081081081081, "no_speech_prob": 0.23128405213356018}, {"id": 201, "seek": 104100, "start": 1049.0, "end": 1054.0, "text": " Ale konsekwencje spo\u0142eczne s\u0105 ogromne i, no, niejednoznaczne.", "tokens": [50764, 9366, 47020, 74, 15615, 44261, 36851, 43077, 9015, 34416, 298, 716, 741, 11, 572, 11, 2838, 40543, 1771, 22672, 14875, 716, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09445568059114802, "compression_ratio": 1.4831081081081081, "no_speech_prob": 0.23128405213356018}, {"id": 202, "seek": 104100, "start": 1054.0, "end": 1058.0, "text": " Z jednej strony mog\u0142oby to prowadzi\u0107 do bardziej pluralistycznej przysz\u0142o\u015bci.", "tokens": [51014, 1176, 5232, 11794, 32406, 13172, 1221, 13944, 281, 36590, 28496, 360, 27209, 25377, 468, 17466, 11794, 44018, 35059, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09445568059114802, "compression_ratio": 1.4831081081081081, "no_speech_prob": 0.23128405213356018}, {"id": 203, "seek": 104100, "start": 1058.0, "end": 1061.0, "text": " Gdzie technologia s\u0142u\u017cy r\u00f3\u017cnorodnym spo\u0142eczno\u015bciom,", "tokens": [51214, 460, 13096, 1537, 24103, 48459, 7735, 19637, 19048, 378, 12996, 36851, 89, 16438, 298, 11, 51364], "temperature": 0.0, "avg_logprob": -0.09445568059114802, "compression_ratio": 1.4831081081081081, "no_speech_prob": 0.23128405213356018}, {"id": 204, "seek": 104100, "start": 1061.0, "end": 1065.0, "text": " zamiast narzuca\u0107 jedn\u0105 dominuj\u0105c\u0105 perspektyw\u0119.", "tokens": [51364, 710, 4526, 525, 6714, 11728, 496, 2162, 5232, 13113, 8859, 13263, 32557, 868, 32659, 874, 86, 1274, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09445568059114802, "compression_ratio": 1.4831081081081081, "no_speech_prob": 0.23128405213356018}, {"id": 205, "seek": 104100, "start": 1065.0, "end": 1070.0, "text": " Tak. AI, kt\u00f3re rozumieniu anse kulturowe, mog\u0142oby by\u0107 niezwykle warto\u015bciowe.", "tokens": [51564, 9118, 13, 7318, 11, 8864, 48797, 1053, 5951, 364, 405, 350, 26099, 6880, 11, 13172, 1221, 13944, 15069, 33511, 9726, 14677, 31830, 6199, 6880, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09445568059114802, "compression_ratio": 1.4831081081081081, "no_speech_prob": 0.23128405213356018}, {"id": 206, "seek": 107000, "start": 1070.0, "end": 1072.0, "text": " A z drugiej strony.", "tokens": [50364, 316, 710, 47373, 32406, 13, 50464], "temperature": 0.0, "avg_logprob": -0.07077533640760056, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.1374320685863495}, {"id": 207, "seek": 107000, "start": 1072.0, "end": 1076.0, "text": " Czy nie grozi\u0142oby to stworzeniem ostatecznych baniek informacyjnych?", "tokens": [50464, 19832, 2838, 4634, 3992, 1221, 13944, 281, 342, 28321, 2904, 4907, 277, 15406, 3689, 9399, 272, 3782, 916, 1356, 31285, 9399, 30, 50664], "temperature": 0.0, "avg_logprob": -0.07077533640760056, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.1374320685863495}, {"id": 208, "seek": 107000, "start": 1076.0, "end": 1077.0, "text": " No w\u0142a\u015bnie.", "tokens": [50664, 883, 14234, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07077533640760056, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.1374320685863495}, {"id": 209, "seek": 107000, "start": 1077.0, "end": 1080.0, "text": " Wyobra\u017amy sobie \u015bwiat, w kt\u00f3rym ka\u017cdy ma swoje w\u0142asne AI,", "tokens": [50714, 14458, 24393, 10659, 2226, 13652, 36425, 11, 261, 30120, 31615, 463, 29489, 43572, 716, 7318, 11, 50864], "temperature": 0.0, "avg_logprob": -0.07077533640760056, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.1374320685863495}, {"id": 210, "seek": 107000, "start": 1080.0, "end": 1083.0, "text": " kt\u00f3re nie tylko potwierdza jego \u015bwiatopogl\u0105d,", "tokens": [50864, 8864, 2838, 13219, 1847, 40717, 67, 2394, 26542, 36425, 404, 664, 75, 18962, 11, 51014], "temperature": 0.0, "avg_logprob": -0.07077533640760056, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.1374320685863495}, {"id": 211, "seek": 107000, "start": 1083.0, "end": 1087.0, "text": " ale robi to w spos\u00f3b niezwykle inteligentny i przekonuj\u0105cy.", "tokens": [51014, 6775, 47380, 281, 261, 22904, 33511, 9726, 14677, 24777, 25002, 1634, 741, 29785, 266, 13263, 1344, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07077533640760056, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.1374320685863495}, {"id": 212, "seek": 107000, "start": 1087.0, "end": 1090.0, "text": " Czy to nie zabi\u0142oby debaty publicznej?", "tokens": [51214, 19832, 281, 2838, 710, 18884, 1221, 13944, 3001, 21398, 1908, 89, 11794, 30, 51364], "temperature": 0.0, "avg_logprob": -0.07077533640760056, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.1374320685863495}, {"id": 213, "seek": 107000, "start": 1090.0, "end": 1093.0, "text": " Nie zamkn\u0119\u0142o nas w spersonalizowanych, cyfrowych echo chambers,", "tokens": [51364, 12016, 19876, 5457, 1274, 5249, 5382, 261, 637, 3953, 304, 590, 23341, 339, 11, 3185, 69, 1892, 16384, 14300, 34513, 11, 51514], "temperature": 0.0, "avg_logprob": -0.07077533640760056, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.1374320685863495}, {"id": 214, "seek": 107000, "start": 1093.0, "end": 1096.0, "text": " z kt\u00f3rych nie by\u0142oby ucieczki?", "tokens": [51514, 710, 30382, 2838, 16673, 13944, 344, 4260, 3689, 2984, 30, 51664], "temperature": 0.0, "avg_logprob": -0.07077533640760056, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.1374320685863495}, {"id": 215, "seek": 109600, "start": 1096.0, "end": 1099.0, "text": " To jest pytanie, kt\u00f3re te badania otworzy\u0142y", "tokens": [50364, 1407, 3492, 36610, 11, 8864, 535, 1578, 5609, 4337, 28321, 1229, 6825, 50514], "temperature": 0.0, "avg_logprob": -0.05051229981815114, "compression_ratio": 1.0571428571428572, "no_speech_prob": 0.016423124819993973}, {"id": 216, "seek": 109600, "start": 1099.0, "end": 1103.0, "text": " i z kt\u00f3rym jako spo\u0142ecze\u0144stwo dopiero zaczynamy si\u0119 mierzy\u0107.", "tokens": [50514, 741, 710, 30120, 17123, 36851, 1381, 12229, 6120, 21900, 12030, 43811, 5378, 88, 3244, 47448, 27150, 13, 50714], "temperature": 0.0, "avg_logprob": -0.05051229981815114, "compression_ratio": 1.0571428571428572, "no_speech_prob": 0.016423124819993973}], "language": "pl"}