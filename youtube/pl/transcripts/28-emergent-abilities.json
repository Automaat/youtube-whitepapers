{"text": " Wiesz, w \u015bwiecie AI przez lata panowa\u0142a taka, no, bardzo prosta mantra. Im wi\u0119kszy, tym lepszy. Dok\u0142adnie, im wi\u0119kszy model, tym lepszy. To by\u0142o traktowane niemal jak, nie wiem, prawo fizyki. In\u017cynierowie wierzyli, \u017ce id\u0105 tak\u0105 prost\u0105, przewidwaln\u0105 drog\u0105, prosto na szczyt. Dok\u0142adasz parametr\u00f3w, dok\u0142adasz danych i wszystko na wykresie ro\u015bnie. Pi\u0119knie, liniowo. Tak. Ale co, je\u015bli to wszystko jest z\u0142udzeniem? Co, je\u015bli w pewnym momencie ta prosta \u015bcie\u017cka si\u0119 ko\u0144czy, a model nagle skr\u0119ca w bok i zaczyna robi\u0107 rzeczy, kt\u00f3rych nikt, kompletnie nikt si\u0119 nie spodziewa\u0142. I to jest, wiesz, dok\u0142adnie ten moment, w kt\u00f3rym przestajemy m\u00f3wi\u0107 o in\u017cynierii, a zaczynamy m\u00f3wi\u0107 o odkryciach. Wkraczamy na terytorium opisane w artykule, kt\u00f3remu dzisiaj si\u0119 przygl\u0105damy. Emergent abilities of large language models. To praca, kt\u00f3ra rzuci\u0142a wyzwanie ca\u0142ej tej prostej mantrze. Autorze pokazuj\u0105, \u017ce skalowanie nie jest p\u0142ynnym procesem. Jest raczej, jak podgrzewanie wody przez d\u0142ugi czas nic si\u0119 nie dzieje, a potem w jednym konkretnym momencie zaczyna brze\u0107. I te momenty wrzenia to s\u0105 w\u0142a\u015bnie. To s\u0105 w\u0142a\u015bnie zdolno\u015bci emergentne. Dobrze, rozpakujmy to. Celem na dzi\u015b nie jest wi\u0119c tylko streszczenie artyku\u0142u, chcemy zbada\u0107 t\u0105 tajemnic\u0119. Zajdziemy, powiedzmy, z utartego szlaku, \u017ceby zobaczy\u0107, czym dok\u0142adnie s\u0105 te emergentnie zdolno\u015bci. Poszukamy dowod\u00f3w, \u017ce modele AI potrafi\u0105 zaskakiwa\u0107 i spr\u00f3bujemy zrozumie\u0107, co to oznacza dla ka\u017cdego, kto z tej technologii korzysta. Sprawdzimy, co si\u0119 dzieje, gdy maszyna zaczyna zachowywa\u0107 si\u0119 w spos\u00f3b, kt\u00f3rego nie przewidzieli\u015bmy. Zacznijmy od podstaw. Jak w og\u00f3le zdefiniowa\u0107 co\u015b, co rzekomo pojawia si\u0119 znik\u0105d? Czym wed\u0142ug autor\u00f3w jest ta emergencja? Czy to tylko takie \u0142adne s\u0142owo na to, \u017ce model sta\u0142 si\u0119 m\u0105drzejszy? Nie, i to jest kluczone. Definicja jest znacznie bardziej precyzyjna. Zdolno\u015b\u0107 jest emergentna, je\u015bli spe\u0142nia dwa warunki. Okej. Po pierwsze, praktycznie nie wyst\u0119puje w mniejszych modelach. A po drugie, pojawia si\u0119 i staje si\u0119 mierzalna dopiero w tych wi\u0119kszych. I co najwa\u017cniejsze, przej\u015bcie nie jest p\u0142ynne. Czyli na wykresie wida\u0107. Na wykresie wydajno\u015bci widzimy d\u0142ug\u0105 p\u0142ask\u0105 lini\u0119, gdzie model, mimo \u017ce ro\u015bnie jego skala, radzi sobie na poziomie, powiedzmy, losowego zgadywania. A\u017c nagle, po przekroczeniu pewnego progu, wykres gwa\u0142townie strzela w g\u00f3r\u0119. Czyli to nie jest jak \u015bciemniacz \u015bwiat\u0142a, kt\u00f3ry stopniowo je rozja\u015bnia, to bardziej klasyczny w\u0142\u0105cznik. Jest wy\u0142\u0105czony, wy\u0142\u0105czony, wy\u0142\u0105czony, a potem nagle w\u0142\u0105czony. Czy to dobra analogia? Doskona\u0142a. Autorzy sami u\u017cywaj\u0105 bardzo podobnego por\u00f3wnania z fizyki. M\u00f3wi\u0105 o przej\u015bciu fazowym. Phase transition. Aha, phase transition. Tak, to nawi\u0105zanie do s\u0142ynnego eseju fizyka Filipa Andersona, more is different. Idea jest taka, \u017ce zmiana ilo\u015bciowa, wi\u0119cej cz\u0105steczek wody, wi\u0119cej parametr\u00f3w modelu, po przekroczeniu pewnego punktu krytycznego, prowadzi do fundamentalnej zmiany jako\u015bciowej. Czyli woda nie staje si\u0119 troch\u0119 bardziej lodowata. Dok\u0142adnie. W temperaturze zera stopni nagle i gwa\u0142townie zmienia sw\u00f3j stan skupienia w lud. Tak samo model j\u0119zykowy nie staje si\u0119 troch\u0119 lepszy w arytmetyce. Nagle po prostu zaczyna umie\u0107 liczy\u0107. A co jest t\u0105 temperatur\u0105 dla modeli AI? Co mierzymy na osi X tego wykresu, \u017ceby zobaczy\u0107 to przej\u015bcie fazowe? G\u0142\u00f3wnie patrzymy na dwa wska\u017aniki, kt\u00f3re s\u0105 ze sob\u0105 mocno powi\u0105zane. Pierwszy to training flops, czyli \u0142\u0105czna moc obliczeniowa zu\u017cyta na wytrenowanie modelu. Ok. A drugi to po prostu liczba jego parametr\u00f3w, te s\u0142ynne miliardy czy biliony, o kt\u00f3rych ci\u0105gle s\u0142yszymy. Im wi\u0119cej parametr\u00f3w, tym zazwyczaj wi\u0119cej oblicze\u0144 potrzeba. I to w\u0142a\u015bnie ta skala jest czynnikiem, kt\u00f3ry prowadzi do emergencji. Ok. Teoria o przej\u015bciu fazowym brzmi przekonuj\u0105co, ale wci\u0105\u017c jest, no wiesz, abstrakcyjna. Potrzebuje dowod\u00f3w. Poka\u017c mi ten moment, kiedy model, kt\u00f3ry do tej pory be\u0142kota\u0142 nagle zaczyna m\u00f3wi\u0107 sensem. Ale zanim to, musimy chyba ustali\u0107, jak w og\u00f3le z nim rozmawiano w tych testach. Tak, to wa\u017cne. Bo wiele z nich opiera si\u0119 na technice zwanej Fuse Shot Prompting. Zgadza si\u0119. Fuse Shot Prompting to bardzo intuicyjna metoda. Zamiast m\u0119czy\u0107 si\u0119 z Fine Tuning, czyli dostrajaniem ca\u0142ego modelu do nowego zadania, po prostu pokazujemy mu w zapytaniu kilka przyk\u0142ad\u00f3w. Czyli na przyk\u0142ad, dajemy mu zdanie. Ten film jest okropny, z etykiet\u0105 Sentiment negatywny. Potem uwielbiam ten film z etykiet\u0105 Sentiment pozytywny. A na ko\u0144cu podajemy nowe zdanie, na przyk\u0142ad Seans by\u0142 strat\u0105 czasu. I prosimy o doko\u0144czenie etykiety. I model musi sam za\u0142apa\u0107, o co chodzi. Dok\u0142adnie, na podstawie tych kilku przyk\u0142ad\u00f3w musi zrozumie\u0107 regu\u0142y gry. Dysys artyku\u0142u s\u0105 naprawd\u0119 uderzaj\u0105ce. We\u017amy prost\u0105, trzycyfrow\u0105 arytmetyk\u0119. Dodawanie. Tak. Okazuje si\u0119, \u017ce modele takie jak GPT-3 czy Lemda przez d\u0142ugi czas maj\u0105 skuteczno\u015b\u0107 blisk\u0105 zero. Po prostu nie potrafi\u0105 dodawa\u0107. Chwila, chcesz powiedzie\u0107, \u017ce model maj\u0105cy powiedzmy 10 miliard\u00f3w parametr\u00f3w, co i tak jest ogromn\u0105 liczb\u0105. Jest w dodawaniu liczby nie lepszy ni\u017c losowy generator cyfr? To brzmi. Jakby by\u0142 zepsuty. Dok\u0142adnie tak to wygl\u0105da. Ale nie jest zepsuty, jest po prostu za ma\u0142y. Dla modelu GPT-3 to przej\u015bcie fazowe nast\u0119puje w okolicach 13 miliard\u00f3w parametr\u00f3w. Dla Lemda ten pr\u00f3g jest jeszcze wy\u017cej oko\u0142o 68 miliard\u00f3w. Niesamowite. Dopiero po przekroczeniu tego punktu ich wydajno\u015b\u0107 nagle wystrzeliwuje do ca\u0142kiem przyzwoitego poziomu. I ten sam schemat powtarza si\u0119 w masie innych zada\u0144. Rozszyfrowywanie s\u0142\u00f3w z pomieszanych liter, odpowiadanie na pytania zwiedzie og\u00f3lnej w te\u015bcie Trutful QA. Zawsze jest ten sam wzorzec. D\u0142ugo, d\u0142ugo nic, a potem nag\u0142y skok. W artykule jest te\u017c \u015bwietna historia z benchmarkiem Word in Context w skr\u00f3cie Week. To chyba doskona\u0142y przyk\u0142ad nieprzewidywalno\u015bci, wr\u0119cz taka, wiesz, detektywistyczna zagadka. To jest idealny przyk\u0142ad. Zadanie jest proste. Powiedz, czy dane s\u0142owo jest u\u017cyte w tym samym znaczeniu w dw\u00f3ch r\u00f3\u017cnych zdaniach. Ok. Ot\u00f3\u017c model GPT-3, nawet w swojej najwi\u0119kszej 175-miliardowej wersji, kompletnie sobie z tym nie radzi\u0142. Jego wyniki by\u0142y na poziomie rzutu monet\u0105. I jego tw\u00f3rcy z tego, co pami\u0119tam, w zasadzie si\u0119 poddali i stwierdzili, \u017ce problem le\u017cy w architekturze Decoder Only. W\u0142a\u015bnie, tak jakby detektyw obwinia\u0142 zazbrodnie rodzaj u\u017cytego no\u017ca. Sugerowali, \u017ce taka architektura po prostu nie nadaje si\u0119 do tego typu zada\u0144 por\u00f3wnawczych. A\u017c tu nagle na scen\u0119 wchodzi palm. Model o bardzo podobnej architekturze Decoder Only, ale... Wi\u0119kszy. Znacznie wi\u0119kszy, przeskalowany do 540 miliard\u00f3w parametr\u00f3w. I co si\u0119 okaza\u0142o? Nagle zacz\u0105\u0142 rozwi\u0105zywa\u0107 to zadanie z bardzo wysok\u0105 spotyczno\u015bci\u0105. Okaza\u0142o si\u0119, \u017ce n\u00f3\u017c by\u0142 w porz\u0105dku. Po prostu nie uderzono nim wystarczaj\u0105co mocno. Czyli problem le\u017ca\u0142 wy\u0142\u0105cznie w niewystarczaj\u0105cej skali. To pokazuje, \u017ce czasem odpowied\u017a na pytanie, dlaczego to nie dzia\u0142a brzmi po prostu, bo jest jeszcze za ma\u0142e. Ale emergencja to nie tylko nowe zdolno\u015bci. Z artyku\u0142u wynika co\u015b jeszcze bardziej zaskakuj\u0105cego. Tak i to jest by\u0107 mo\u017ce jeszcze dziwniejsze. Okazuje si\u0119, \u017ce emergencja dotyczy nie tylko tego, co modele potrafi\u0105, ale te\u017c tego, jak powinni\u015bmy z nimi rozmawia\u0107. Co masz na my\u015bli? Niekt\u00f3re techniki interakcji, kt\u00f3re dzi\u015b s\u0105 standardem w prompt engineering, okazuj\u0105 si\u0119 kompletnie bezu\u017cyteczne, a nawet szkodliwe dla mniejszych modeli. Subierujesz, \u017ce dana technika mo\u017ce dzia\u0142a\u0107 na jednym modelu, a na nieco mniejszej wersji tego samego modelu ju\u017c nie? To brzmi bardzo kontrontuicyjnie. We\u017amy najs\u0142ynniejszy przyk\u0142ad. Chain of Thought Prompting. Aha, my\u015blenie na g\u0142os? Dok\u0142adnie. Technika, w kt\u00f3rej prosimy model, by zanim poda ostateczn\u0105 odpowied\u017a na z\u0142o\u017cone pytanie, opisa\u0142 krok po kroku sw\u00f3j tok rozumowania. Dzi\u015b to absolutna podstawa przy problemach logicznych czy matematycznych, ale badania pokazuj\u0105, \u017ce ta metoda zaczyna przynosi\u0107 jak\u0105kolwiek korzy\u015b\u0107 dopiero w modelach oskali oko\u0142o 100 miliard\u00f3w parametr\u00f3w. A poni\u017cej? Poni\u017cej tego progu nie daje \u017cadnej przewagi. To jest fascynuj\u0105ce. Oznacza to, \u017ce przez lata mogli\u015bmy stosowa\u0107 techniki, kt\u00f3re wydawa\u0142y si\u0119 intuicyjnie dobre, a w rzeczywisto\u015bci sabotowali\u015bmy w\u0142asne wyniki, bo nie rozumieli\u015bmy, \u017ce model nie osi\u0105gn\u0105\u0142 jeszcze odpowiedniej dojrza\u0142o\u015bci. Dok\u0142adnie. A jest jeszcze bardziej jaskrawy przyk\u0142ad, kt\u00f3ry pokazuje, jak bardzo nasze intuicje mog\u0105 nas myli\u0107. Chodzi o Instruction Fine Tuning, czyli proces dostrenania modelu, by lepiej pod\u0105\u017ca\u0142 za poleceniami. No to wydaje si\u0119 oczywiste, \u017ce to zawsze powinno pomaga\u0107, prawda? Wydawa\u0142oby si\u0119. Ale badania pokaza\u0142y co\u015b zupe\u0142nie innego. Chwila, to brzmi kompletnie absurdalnie. M\u00f3wisz mi, \u017ce technika zwana dostrajaniem do instrukcji, w rzeczywisto\u015bci sprawia, \u017ce model gorzej wykonuje instrukcj\u0119? I jak to w og\u00f3le mo\u017cliwe? To tak jakby lekcja czytania sprawia\u0142a, \u017ce dziecko zaczyna zapomina\u0107 litery? Co tam si\u0119 dzieje na fundamentalnym poziomie? Dok\u0142adnie tak. Badania pokaza\u0142y, \u017ce w przypadku modeli mniejszych ni\u017c oko\u0142o 8 miliard\u00f3w parametr\u00f3w, Instruction Fine Tuning pogarsa ich og\u00f3ln\u0105 wydajno\u015b\u0107. Nie wiarygodne. Dopiero przy skali rz\u0119du 100 miliard\u00f3w parametr\u00f3w, ta technika zaczyna przynosi\u0107 oczekiwane pozytywne rezultaty. Co do pytania, dlaczego? Na razie nie mamy jednozmacznej odpowiedzi. Artyku\u0142 przedstawia kilka hipotez. Jakich? Jedna z nich m\u00f3wi o g\u0142\u0119boko\u015bci obliczeniowej. Z\u0142o\u017cone zadania, wymagaj\u0105ce wiele etapowego rozumowania, potrzebuj\u0105 pewnej minimalnej liczby krok\u00f3w. Mo\u017cliwe, \u017ce mniejszy model po prostu nie ma wystarczaj\u0105cej liczby warstw, czyli przestrzeni na przeprowadzenie tak z\u0142o\u017conych operacji. To ma sens, \u017ce potrzebna jest pewna g\u0142\u0119boko\u015b\u0107 do z\u0142o\u017conego my\u015blenia. Ale czy to nie jest troch\u0119 niepokoj\u0105ce? Oznacza\u0142oby to, \u017ce istnieje twarda fizyczna bariera, kt\u00f3rej ma\u0142e modele nigdy nie przeskocz\u0105, niezale\u017cnie od tego, jak sprytnie je wytrenujemy. Czy to skazuje mniejsze, bardziej dost\u0119pne modele na bycie g\u0142upszym i na zawsze? Niekoniecznie i tu dochodzimy do kluczowego niuansu. Odpowied\u017a brzmi, sama skala to nie wszystko. Artyku\u0142 jasno pokazuje, \u017ce lepsze jako\u015bci dane treningowe lub nowatorskie architektury mog\u0105 odblokowa\u0107 pewne zdolno\u015bci w znacznie mniejszych modelach, czyli skala nie jest jedyny pokr\u0119t\u0142em, kt\u00f3rym mo\u017cemy kr\u0119ci\u0107. Nie jest to jak z silnikiem samochodowym. Mo\u017cna zwi\u0119ksza\u0107 jego pojemno\u015b\u0107, ale mo\u017cna te\u017c wla\u0107 do niego lepsze paliwo. Dobre por\u00f3wnanie, dok\u0142adnie. Autorzy podaj\u0105 przyk\u0142ad model Palm o wielko\u015bci 62 miliard\u00f3w parametr\u00f3w. Radzi\u0142 sobie z 14 zada\u0144ami z benchmarku Big Bench, z kt\u00f3rymi nie dawa\u0142y sobie rady znacznie wi\u0119ksze modele jak GPT-3 czy Lambda. A dlaczego? Prawdopodobn\u0105 przyczyn\u0105 by\u0142a w\u0142a\u015bnie jako\u015b\u0107 paliwa. Palm by\u0142 trenowany na lepszych danych z wi\u0119kszym udzia\u0142em kodu i materia\u0142\u00f3w wieloj\u0119zycznych. Co wi\u0119cej, gdy jaka\u015b zdolno\u015b\u0107 zostanie ju\u017c odkryta w du\u017cym modelu, badacze cz\u0119sto znajduj\u0105 sposoby, by nauczy\u0107 jej mniejsze modele, np. przez bardziej celowany fine tuning. OK, czyli mamy te nieprzewidywalne dobre zdolno\u015bci, kt\u00f3re si\u0119 pojawiaj\u0105. Ale te drzy musz\u0105 si\u0119 otwiera\u0107 w obie strony, prawda? Je\u015bli dobre rzeczy mog\u0105 pojawi\u0107 si\u0119 znik\u0105d, to co zeszkodliwem. To jest druga mroczniejsza strona medalu, kt\u00f3r\u0105 autorzy nazywaj\u0105 emergent risks, czyli ryzykami emergentnymi. Wraz ze skal\u0105 mog\u0105 nieprzewidywalnie rosn\u0105\u0107 problemy takie jak generowanie dezinformacji, wzmacnianie uprzedze\u0144, czyli bias, czy toksyczno\u015b\u0107 j\u0119zyka. Czyli niekt\u00f3re wady modelu nie malej\u0105 ze skal\u0105? Wr\u0119cz przeciwnie, nasilaj\u0105 si\u0119. S\u0105 na to jakie\u015b twarde dowody, podobne do tych z arytmetyk\u0105? Tak. W tym samym te\u015bcie Truthful QA, o kt\u00f3rym m\u00f3wili\u015bmy, zauwa\u017cono, \u017ce wi\u0119ksze modele GPT-3, chocia\u017c stawa\u0142y j\u0105 inteligentniejsze, by\u0142y jednocze\u015bnie bardziej sk\u0142onne do na\u015bladowania i powtarzania powszechnych ludzkich fa\u0142sz\u00f3w. Ciekawe. Inny benchmark, BBQ, pokaza\u0142, \u017ce w niejednoznacznych kontekstach, kt\u00f3re mog\u0105 prowokowa\u0107 stereotypowa my\u015blenie uprzedzenia w odpowiedziach modeli, rosn\u0105 wraz z ich skal\u0105. A co z prywatno\u015bci\u0105? Wi\u0119kszy model to wi\u0119ksza pami\u0119\u0107. No i to jest kolejny emergent risk. Wi\u0119ksze modele s\u0105 znacznie bardziej podatne na zapami\u0119tywanie i odtwarzanie fragment\u00f3w swoich danych treningowych. Im wi\u0119ksza pami\u0119\u0107, tym wi\u0119ksza szansa, \u017ce model dos\u0142ownie odtworzy, czy i\u015b prywatny e-mail, czy dane medyczne, kt\u00f3re przypadkiem znalaz\u0142y si\u0119 w zbiorze treningowym. To powa\u017cne zagro\u017cenie. Skoro ju\u017c wiemy, \u017ce mapa rozwoju AI, kt\u00f3r\u0105 mia\u0142y\u015bmy, jest b\u0142\u0119dna, to gdzie powinni\u015bmy i\u015b\u0107 dalej? Jakie kierunki bada\u0144 wytycza ten artyku\u0142? Po pierwsze, oczywi\u015bcie dalsze skalowanie. Wci\u0105\u017c jest wiele zdolno\u015bci np. abstrakcyjne rozumowanie potrzebne do gry w szachy na poziomie strategicznym, kt\u00f3re jeszcze si\u0119 nie pojawi\u0142y. By\u0107 mo\u017ce czekaj\u0105 za kolejnym progiem skali. Ale co wa\u017cniejsze, poszukiwanie bardziej efektywnych sposob\u00f3w na osi\u0105gni\u0119cie tej skali. I tu wchodz\u0105 w gr\u0119 pomys\u0142owe architektury. Pomy\u015blmy np. o Sparse Mixture of Experts. Jak to dzia\u0142a? Zamiast jednego, gigantycznego m\u00f3zgu, kt\u00f3ry musi zna\u0107 si\u0119 na wszystkim, to tak jakby\u015bmy mieli zesp\u00f3\u0142 wyspecjalizowanych ekspert\u00f3w. Kiedy pojawia si\u0119 problem, aktywujemy tylko tego jednego, odpowiedniego specjalist\u0119. To pozwala budowa\u0107 modele z bilionami parametr\u00f3w, kt\u00f3re s\u0105 znacznie ta\u0144sze w u\u017cyciu, bo w danym momencie pracuje tylko ma\u0142ych ich fragment. Czyli inteligencja rozproszona zamiast monolitycznej, a opr\u00f3cz samej architektury. Dwa pozosta\u0142e kluczowe obszary, to dane i interakcja. Potrzebujemy jeszcze wi\u0119kszych i co wa\u017cniejsze czystszych zbior\u00f3w danych. I musimy g\u0142\u0119biej zrozumie\u0107 mechanizmy prompting, zw\u0142aszcza te bardziej z\u0142o\u017cone jak Chain of Thought, \u017ceby m\u00f3c efektywniej wydobywa\u0107 zdolno\u015bci z ju\u017c istniej\u0105cych modeli. My\u015bl\u0119, \u017ce kluczowy wniosek z tej analizy jest taki, \u017ce droga rozwo\u0142u sztucznej inteligencji nie jest g\u0142adk\u0105, przewidywaln\u0105 autostrad\u0105. To bardziej dzika, nieodkryta kraina, pe\u0142na niespodzianek, skok\u00f3w i w\u0142a\u015bnie przej\u015b\u0107 fazowych. Dok\u0142adnie. W kt\u00f3rych nagle pojawiaj\u0105 si\u0119 zupe\u0142nie nowe mo\u017cliwo\u015bci. Okazuje si\u0119, \u017ce skalowanie modeli to nie tylko kwestia in\u017cynierii i dok\u0142adania mocy obliczeniowej. To tak\u017ce, a mo\u017ce przede wszystkim, proces odkrywania nieznanego. I jest jeszcze jedna, prowokacyjna my\u015bl na koniec, kt\u00f3ra wykracza poza sam\u0105 technologi\u0119. Artyku\u0142 skupia si\u0119 na emergencji w zachowaniu modeli, ale w dyskusji autorzy wspominaj\u0105 o innym, fascynuj\u0105cym typie emergencji, emergencji socjologicznej. Sociologicznej? Tak. Zwracaj\u0105 uwag\u0119, \u017ce samo pojawienie si\u0119 tych pot\u0119\u017cnych, uniwersalnych modeli j\u0119zykowych jako\u015bciowo zmieni\u0142o ca\u0142\u0105 dziedzin\u0119 Natural Language Processing. Gwa\u0142townie odeszli\u015bmy od ery ma\u0142ych, wyspecjalizowanych modeli trenowanych do jednego zadania na rzecz jednego, gigantycznego modelu, kt\u00f3ry ma potencja\u0142 wyrobi\u0107 wszystko. To jest fundamentalna, emergentna zmiana w samej nauce. I to prowadzi do ostatniego pytania, kt\u00f3re musimy sobie zada\u0107. Wiemy.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.32, "text": " Wiesz, w \u015bwiecie AI przez lata panowa\u0142a taka, no, bardzo prosta mantra.", "tokens": [50364, 343, 15347, 11, 261, 40078, 4260, 7318, 14064, 46722, 2462, 5528, 5024, 28017, 11, 572, 11, 9034, 582, 8638, 32094, 13, 50680], "temperature": 0.0, "avg_logprob": -0.20294382837083605, "compression_ratio": 1.49609375, "no_speech_prob": 0.004653519485145807}, {"id": 1, "seek": 0, "start": 6.32, "end": 7.6000000000000005, "text": " Im wi\u0119kszy, tym lepszy.", "tokens": [50680, 4331, 29968, 1229, 11, 8107, 476, 1878, 1229, 13, 50744], "temperature": 0.0, "avg_logprob": -0.20294382837083605, "compression_ratio": 1.49609375, "no_speech_prob": 0.004653519485145807}, {"id": 2, "seek": 0, "start": 7.92, "end": 10.4, "text": " Dok\u0142adnie, im wi\u0119kszy model, tym lepszy.", "tokens": [50760, 29768, 10358, 2766, 11, 566, 29968, 1229, 2316, 11, 8107, 476, 1878, 1229, 13, 50884], "temperature": 0.0, "avg_logprob": -0.20294382837083605, "compression_ratio": 1.49609375, "no_speech_prob": 0.004653519485145807}, {"id": 3, "seek": 0, "start": 11.0, "end": 14.08, "text": " To by\u0142o traktowane niemal jak, nie wiem, prawo fizyki.", "tokens": [50914, 1407, 14811, 944, 2320, 23066, 2838, 5579, 4207, 11, 2838, 26522, 11, 3206, 6120, 21000, 88, 2984, 13, 51068], "temperature": 0.0, "avg_logprob": -0.20294382837083605, "compression_ratio": 1.49609375, "no_speech_prob": 0.004653519485145807}, {"id": 4, "seek": 0, "start": 14.36, "end": 19.240000000000002, "text": " In\u017cynierowie wierzyli, \u017ce id\u0105 tak\u0105 prost\u0105, przewidwaln\u0105 drog\u0105, prosto na szczyt.", "tokens": [51082, 682, 1427, 2534, 811, 13998, 261, 811, 1229, 2081, 11, 3561, 4496, 1611, 31069, 10293, 1611, 11, 39758, 327, 29530, 13113, 3789, 70, 1611, 11, 10293, 78, 1667, 7870, 6522, 83, 13, 51326], "temperature": 0.0, "avg_logprob": -0.20294382837083605, "compression_ratio": 1.49609375, "no_speech_prob": 0.004653519485145807}, {"id": 5, "seek": 0, "start": 19.68, "end": 24.12, "text": " Dok\u0142adasz parametr\u00f3w, dok\u0142adasz danych i wszystko na wykresie ro\u015bnie.", "tokens": [51348, 29768, 10358, 19601, 6220, 27965, 3901, 11, 25037, 10358, 19601, 274, 34644, 741, 22607, 1667, 39287, 495, 414, 744, 12221, 13, 51570], "temperature": 0.0, "avg_logprob": -0.20294382837083605, "compression_ratio": 1.49609375, "no_speech_prob": 0.004653519485145807}, {"id": 6, "seek": 0, "start": 24.48, "end": 25.72, "text": " Pi\u0119knie, liniowo.", "tokens": [51588, 430, 5034, 74, 2766, 11, 287, 3812, 19941, 13, 51650], "temperature": 0.0, "avg_logprob": -0.20294382837083605, "compression_ratio": 1.49609375, "no_speech_prob": 0.004653519485145807}, {"id": 7, "seek": 0, "start": 25.96, "end": 26.46, "text": " Tak.", "tokens": [51662, 9118, 13, 51687], "temperature": 0.0, "avg_logprob": -0.20294382837083605, "compression_ratio": 1.49609375, "no_speech_prob": 0.004653519485145807}, {"id": 8, "seek": 2646, "start": 27.46, "end": 30.66, "text": " Ale co, je\u015bli to wszystko jest z\u0142udzeniem?", "tokens": [50414, 9366, 598, 11, 25630, 281, 22607, 3492, 31614, 532, 2904, 4907, 30, 50574], "temperature": 0.0, "avg_logprob": -0.1393770451168362, "compression_ratio": 1.4963768115942029, "no_speech_prob": 0.02202405221760273}, {"id": 9, "seek": 2646, "start": 31.1, "end": 43.86, "text": " Co, je\u015bli w pewnym momencie ta prosta \u015bcie\u017cka si\u0119 ko\u0144czy, a model nagle skr\u0119ca w bok i zaczyna robi\u0107 rzeczy, kt\u00f3rych nikt, kompletnie nikt si\u0119 nie spodziewa\u0142.", "tokens": [50596, 3066, 11, 25630, 261, 47160, 4199, 40883, 1846, 582, 8638, 8299, 40082, 2330, 3244, 26470, 6522, 11, 257, 2316, 297, 15088, 1110, 81, 1274, 496, 261, 41882, 741, 43811, 629, 46900, 26297, 11, 30382, 297, 9874, 11, 5207, 14657, 2766, 297, 9874, 3244, 2838, 637, 378, 89, 1093, 64, 1221, 13, 51234], "temperature": 0.0, "avg_logprob": -0.1393770451168362, "compression_ratio": 1.4963768115942029, "no_speech_prob": 0.02202405221760273}, {"id": 10, "seek": 2646, "start": 44.46, "end": 50.78, "text": " I to jest, wiesz, dok\u0142adnie ten moment, w kt\u00f3rym przestajemy m\u00f3wi\u0107 o in\u017cynierii, a zaczynamy m\u00f3wi\u0107 o odkryciach.", "tokens": [51264, 286, 281, 3492, 11, 261, 15347, 11, 45864, 2766, 2064, 1623, 11, 261, 30120, 44264, 1805, 3633, 13489, 12757, 277, 294, 1427, 2534, 811, 5597, 11, 257, 43811, 5378, 88, 13489, 12757, 277, 3611, 43298, 537, 608, 13, 51580], "temperature": 0.0, "avg_logprob": -0.1393770451168362, "compression_ratio": 1.4963768115942029, "no_speech_prob": 0.02202405221760273}, {"id": 11, "seek": 2646, "start": 51.1, "end": 55.58, "text": " Wkraczamy na terytorium opisane w artykule, kt\u00f3remu dzisiaj si\u0119 przygl\u0105damy.", "tokens": [51596, 343, 74, 12080, 89, 7804, 1667, 1796, 4328, 284, 2197, 45477, 1929, 261, 594, 874, 74, 2271, 11, 4695, 2579, 84, 25772, 3244, 6501, 7191, 18962, 7804, 13, 51820], "temperature": 0.0, "avg_logprob": -0.1393770451168362, "compression_ratio": 1.4963768115942029, "no_speech_prob": 0.02202405221760273}, {"id": 12, "seek": 5558, "start": 56.379999999999995, "end": 59.78, "text": " Emergent abilities of large language models.", "tokens": [50404, 18477, 6930, 11582, 295, 2416, 2856, 5245, 13, 50574], "temperature": 0.0, "avg_logprob": -0.14763436998639787, "compression_ratio": 1.4525547445255473, "no_speech_prob": 0.002256683772429824}, {"id": 13, "seek": 5558, "start": 60.1, "end": 64.46, "text": " To praca, kt\u00f3ra rzuci\u0142a wyzwanie ca\u0142ej tej prostej mantrze.", "tokens": [50590, 1407, 582, 6628, 11, 19456, 367, 11728, 537, 5024, 4628, 14406, 7155, 47631, 73, 12573, 10293, 40779, 10845, 13503, 13, 50808], "temperature": 0.0, "avg_logprob": -0.14763436998639787, "compression_ratio": 1.4525547445255473, "no_speech_prob": 0.002256683772429824}, {"id": 14, "seek": 5558, "start": 65.1, "end": 68.62, "text": " Autorze pokazuj\u0105, \u017ce skalowanie nie jest p\u0142ynnym procesem.", "tokens": [50840, 6049, 284, 1381, 13010, 921, 13263, 11, 3561, 16890, 22028, 2838, 3492, 28695, 2534, 12996, 17565, 443, 13, 51016], "temperature": 0.0, "avg_logprob": -0.14763436998639787, "compression_ratio": 1.4525547445255473, "no_speech_prob": 0.002256683772429824}, {"id": 15, "seek": 5558, "start": 69.06, "end": 77.25999999999999, "text": " Jest raczej, jak podgrzewanie wody przez d\u0142ugi czas nic si\u0119 nie dzieje, a potem w jednym konkretnym momencie zaczyna brze\u0107.", "tokens": [51038, 24918, 4129, 16920, 11, 4207, 2497, 861, 43551, 7155, 261, 843, 14064, 44042, 24780, 13190, 6201, 3244, 2838, 17953, 2884, 11, 257, 36513, 261, 5232, 12996, 36500, 12996, 40883, 43811, 629, 738, 1381, 2162, 13, 51448], "temperature": 0.0, "avg_logprob": -0.14763436998639787, "compression_ratio": 1.4525547445255473, "no_speech_prob": 0.002256683772429824}, {"id": 16, "seek": 5558, "start": 77.5, "end": 80.14, "text": " I te momenty wrzenia to s\u0105 w\u0142a\u015bnie.", "tokens": [51460, 286, 535, 1623, 88, 928, 14320, 281, 9015, 14234, 13, 51592], "temperature": 0.0, "avg_logprob": -0.14763436998639787, "compression_ratio": 1.4525547445255473, "no_speech_prob": 0.002256683772429824}, {"id": 17, "seek": 5558, "start": 80.38, "end": 82.9, "text": " To s\u0105 w\u0142a\u015bnie zdolno\u015bci emergentne.", "tokens": [51604, 1407, 9015, 14234, 16221, 401, 16438, 4345, 6930, 716, 13, 51730], "temperature": 0.0, "avg_logprob": -0.14763436998639787, "compression_ratio": 1.4525547445255473, "no_speech_prob": 0.002256683772429824}, {"id": 18, "seek": 5558, "start": 83.1, "end": 84.25999999999999, "text": " Dobrze, rozpakujmy to.", "tokens": [51740, 29679, 13503, 11, 9544, 45944, 4579, 2226, 281, 13, 51798], "temperature": 0.0, "avg_logprob": -0.14763436998639787, "compression_ratio": 1.4525547445255473, "no_speech_prob": 0.002256683772429824}, {"id": 19, "seek": 8426, "start": 84.66000000000001, "end": 89.82000000000001, "text": " Celem na dzi\u015b nie jest wi\u0119c tylko streszczenie artyku\u0142u, chcemy zbada\u0107 t\u0105 tajemnic\u0119.", "tokens": [50384, 8257, 10386, 1667, 31981, 1788, 2838, 3492, 16677, 13219, 342, 495, 89, 39043, 594, 874, 5279, 24066, 11, 28928, 2226, 710, 65, 1538, 2162, 32294, 256, 1805, 443, 7692, 1274, 13, 50642], "temperature": 0.0, "avg_logprob": -0.09862937425312243, "compression_ratio": 1.4322580645161291, "no_speech_prob": 0.0010171782923862338}, {"id": 20, "seek": 8426, "start": 90.5, "end": 96.74000000000001, "text": " Zajdziemy, powiedzmy, z utartego szlaku, \u017ceby zobaczy\u0107, czym dok\u0142adnie s\u0105 te emergentnie zdolno\u015bci.", "tokens": [50676, 1176, 1805, 13096, 2226, 11, 27617, 2226, 11, 710, 2839, 446, 6308, 7870, 75, 15803, 11, 11316, 37273, 2162, 11, 31466, 45864, 2766, 9015, 535, 4345, 6930, 2766, 16221, 401, 16438, 13, 50988], "temperature": 0.0, "avg_logprob": -0.09862937425312243, "compression_ratio": 1.4322580645161291, "no_speech_prob": 0.0010171782923862338}, {"id": 21, "seek": 8426, "start": 97.5, "end": 105.98, "text": " Poszukamy dowod\u00f3w, \u017ce modele AI potrafi\u0105 zaskakiwa\u0107 i spr\u00f3bujemy zrozumie\u0107, co to oznacza dla ka\u017cdego, kto z tej technologii korzysta.", "tokens": [51026, 25906, 43994, 7804, 9459, 378, 3901, 11, 3561, 4391, 306, 7318, 1847, 10437, 11404, 710, 3863, 7421, 25234, 741, 6103, 14216, 21767, 710, 27857, 449, 414, 2162, 11, 598, 281, 277, 22672, 326, 2394, 12285, 21912, 67, 6308, 11, 23780, 710, 12573, 1537, 1132, 5597, 14784, 49590, 13, 51450], "temperature": 0.0, "avg_logprob": -0.09862937425312243, "compression_ratio": 1.4322580645161291, "no_speech_prob": 0.0010171782923862338}, {"id": 22, "seek": 8426, "start": 106.34, "end": 112.02000000000001, "text": " Sprawdzimy, co si\u0119 dzieje, gdy maszyna zaczyna zachowywa\u0107 si\u0119 w spos\u00f3b, kt\u00f3rego nie przewidzieli\u015bmy.", "tokens": [51468, 1738, 15889, 89, 13189, 11, 598, 3244, 17953, 2884, 11, 28405, 2300, 1229, 629, 43811, 629, 29303, 10089, 25234, 3244, 261, 22904, 11, 46951, 2838, 39758, 327, 89, 23099, 10513, 13, 51752], "temperature": 0.0, "avg_logprob": -0.09862937425312243, "compression_ratio": 1.4322580645161291, "no_speech_prob": 0.0010171782923862338}, {"id": 23, "seek": 11202, "start": 112.46, "end": 114.17999999999999, "text": " Zacznijmy od podstaw.", "tokens": [50386, 1176, 14875, 77, 1718, 2226, 3611, 43443, 13, 50472], "temperature": 0.0, "avg_logprob": -0.1521823372639401, "compression_ratio": 1.4063604240282686, "no_speech_prob": 0.008978107944130898}, {"id": 24, "seek": 11202, "start": 114.46, "end": 119.14, "text": " Jak w og\u00f3le zdefiniowa\u0107 co\u015b, co rzekomo pojawia si\u0119 znik\u0105d?", "tokens": [50486, 15029, 261, 29229, 710, 20595, 3812, 11445, 19241, 11, 598, 367, 19878, 13395, 30655, 654, 3244, 710, 13123, 18962, 30, 50720], "temperature": 0.0, "avg_logprob": -0.1521823372639401, "compression_ratio": 1.4063604240282686, "no_speech_prob": 0.008978107944130898}, {"id": 25, "seek": 11202, "start": 119.58, "end": 122.42, "text": " Czym wed\u0142ug autor\u00f3w jest ta emergencja?", "tokens": [50742, 19832, 76, 6393, 34077, 19510, 3901, 3492, 1846, 4345, 1766, 34056, 30, 50884], "temperature": 0.0, "avg_logprob": -0.1521823372639401, "compression_ratio": 1.4063604240282686, "no_speech_prob": 0.008978107944130898}, {"id": 26, "seek": 11202, "start": 122.66, "end": 126.94, "text": " Czy to tylko takie \u0142adne s\u0142owo na to, \u017ce model sta\u0142 si\u0119 m\u0105drzejszy?", "tokens": [50896, 19832, 281, 13219, 15963, 47910, 716, 15116, 19941, 1667, 281, 11, 3561, 2316, 11135, 1221, 3244, 275, 18962, 13503, 73, 7706, 30, 51110], "temperature": 0.0, "avg_logprob": -0.1521823372639401, "compression_ratio": 1.4063604240282686, "no_speech_prob": 0.008978107944130898}, {"id": 27, "seek": 11202, "start": 127.61999999999999, "end": 128.78, "text": " Nie, i to jest kluczone.", "tokens": [51144, 12016, 11, 741, 281, 3492, 9671, 1311, 16896, 13, 51202], "temperature": 0.0, "avg_logprob": -0.1521823372639401, "compression_ratio": 1.4063604240282686, "no_speech_prob": 0.008978107944130898}, {"id": 28, "seek": 11202, "start": 128.82, "end": 131.34, "text": " Definicja jest znacznie bardziej precyzyjna.", "tokens": [51204, 46245, 299, 2938, 3492, 15397, 14875, 2766, 27209, 659, 1344, 1229, 73, 629, 13, 51330], "temperature": 0.0, "avg_logprob": -0.1521823372639401, "compression_ratio": 1.4063604240282686, "no_speech_prob": 0.008978107944130898}, {"id": 29, "seek": 11202, "start": 131.94, "end": 135.18, "text": " Zdolno\u015b\u0107 jest emergentna, je\u015bli spe\u0142nia dwa warunki.", "tokens": [51360, 1176, 67, 401, 23293, 3492, 4345, 6930, 629, 11, 25630, 768, 1221, 12679, 35045, 1516, 409, 2984, 13, 51522], "temperature": 0.0, "avg_logprob": -0.1521823372639401, "compression_ratio": 1.4063604240282686, "no_speech_prob": 0.008978107944130898}, {"id": 30, "seek": 11202, "start": 135.5, "end": 136.1, "text": " Okej.", "tokens": [51538, 29094, 73, 13, 51568], "temperature": 0.0, "avg_logprob": -0.1521823372639401, "compression_ratio": 1.4063604240282686, "no_speech_prob": 0.008978107944130898}, {"id": 31, "seek": 11202, "start": 136.57999999999998, "end": 140.1, "text": " Po pierwsze, praktycznie nie wyst\u0119puje w mniejszych modelach.", "tokens": [51592, 6165, 45994, 11, 3206, 74, 45586, 2838, 48255, 18085, 13008, 261, 39513, 45021, 2316, 608, 13, 51768], "temperature": 0.0, "avg_logprob": -0.1521823372639401, "compression_ratio": 1.4063604240282686, "no_speech_prob": 0.008978107944130898}, {"id": 32, "seek": 14010, "start": 140.66, "end": 144.66, "text": " A po drugie, pojawia si\u0119 i staje si\u0119 mierzalna dopiero w tych wi\u0119kszych.", "tokens": [50392, 316, 714, 4110, 414, 11, 30655, 654, 3244, 741, 342, 11153, 3244, 47448, 89, 304, 629, 21900, 12030, 261, 15180, 29968, 28051, 13, 50592], "temperature": 0.0, "avg_logprob": -0.12354870696565998, "compression_ratio": 1.460377358490566, "no_speech_prob": 0.01905510015785694}, {"id": 33, "seek": 14010, "start": 145.18, "end": 147.78, "text": " I co najwa\u017cniejsze, przej\u015bcie nie jest p\u0142ynne.", "tokens": [50618, 286, 598, 11212, 27111, 44258, 11, 8325, 73, 9815, 2838, 3492, 28695, 2534, 716, 13, 50748], "temperature": 0.0, "avg_logprob": -0.12354870696565998, "compression_ratio": 1.460377358490566, "no_speech_prob": 0.01905510015785694}, {"id": 34, "seek": 14010, "start": 148.06, "end": 149.5, "text": " Czyli na wykresie wida\u0107.", "tokens": [50762, 37099, 1667, 39287, 495, 414, 261, 46898, 13, 50834], "temperature": 0.0, "avg_logprob": -0.12354870696565998, "compression_ratio": 1.460377358490566, "no_speech_prob": 0.01905510015785694}, {"id": 35, "seek": 14010, "start": 149.54, "end": 159.7, "text": " Na wykresie wydajno\u015bci widzimy d\u0142ug\u0105 p\u0142ask\u0105 lini\u0119, gdzie model, mimo \u017ce ro\u015bnie jego skala, radzi sobie na poziomie, powiedzmy, losowego zgadywania.", "tokens": [50836, 6056, 39287, 495, 414, 25984, 1805, 16438, 27486, 13189, 274, 34077, 1611, 28695, 3863, 1611, 287, 3812, 1274, 11, 18922, 2316, 11, 275, 6934, 3561, 744, 12221, 26542, 1110, 5159, 11, 2843, 3992, 13652, 1667, 38503, 40120, 11, 27617, 2226, 11, 1750, 26576, 40948, 880, 86, 5609, 13, 51344], "temperature": 0.0, "avg_logprob": -0.12354870696565998, "compression_ratio": 1.460377358490566, "no_speech_prob": 0.01905510015785694}, {"id": 36, "seek": 14010, "start": 160.45999999999998, "end": 164.94, "text": " A\u017c nagle, po przekroczeniu pewnego progu, wykres gwa\u0142townie strzela w g\u00f3r\u0119.", "tokens": [51382, 316, 1427, 297, 15088, 11, 714, 29785, 24174, 39651, 25889, 11858, 447, 2794, 11, 39287, 495, 290, 44603, 30401, 414, 1056, 89, 4053, 261, 290, 15614, 1274, 13, 51606], "temperature": 0.0, "avg_logprob": -0.12354870696565998, "compression_ratio": 1.460377358490566, "no_speech_prob": 0.01905510015785694}, {"id": 37, "seek": 16494, "start": 165.54, "end": 171.46, "text": " Czyli to nie jest jak \u015bciemniacz \u015bwiat\u0142a, kt\u00f3ry stopniowo je rozja\u015bnia, to bardziej klasyczny w\u0142\u0105cznik.", "tokens": [50394, 37099, 281, 2838, 3492, 4207, 220, 6199, 443, 3722, 14875, 36425, 5024, 11, 9913, 1590, 3722, 19941, 1506, 9544, 2938, 1788, 12679, 11, 281, 27209, 9671, 5871, 3689, 1634, 261, 43558, 13123, 13, 50690], "temperature": 0.0, "avg_logprob": -0.16471939598953964, "compression_ratio": 1.5229681978798586, "no_speech_prob": 0.25622043013572693}, {"id": 38, "seek": 16494, "start": 171.66, "end": 176.66, "text": " Jest wy\u0142\u0105czony, wy\u0142\u0105czony, wy\u0142\u0105czony, a potem nagle w\u0142\u0105czony.", "tokens": [50700, 24918, 4628, 43558, 2526, 11, 4628, 43558, 2526, 11, 4628, 43558, 2526, 11, 257, 36513, 297, 15088, 261, 43558, 2526, 13, 50950], "temperature": 0.0, "avg_logprob": -0.16471939598953964, "compression_ratio": 1.5229681978798586, "no_speech_prob": 0.25622043013572693}, {"id": 39, "seek": 16494, "start": 177.1, "end": 178.34, "text": " Czy to dobra analogia?", "tokens": [50972, 19832, 281, 360, 6198, 16660, 654, 30, 51034], "temperature": 0.0, "avg_logprob": -0.16471939598953964, "compression_ratio": 1.5229681978798586, "no_speech_prob": 0.25622043013572693}, {"id": 40, "seek": 16494, "start": 178.66, "end": 179.46, "text": " Doskona\u0142a.", "tokens": [51050, 33474, 74, 4037, 5024, 13, 51090], "temperature": 0.0, "avg_logprob": -0.16471939598953964, "compression_ratio": 1.5229681978798586, "no_speech_prob": 0.25622043013572693}, {"id": 41, "seek": 16494, "start": 180.3, "end": 183.42, "text": " Autorzy sami u\u017cywaj\u0105 bardzo podobnego por\u00f3wnania z fizyki.", "tokens": [51132, 6049, 284, 1229, 3247, 72, 34097, 86, 11133, 9034, 43024, 11858, 1515, 812, 895, 5609, 710, 21000, 88, 2984, 13, 51288], "temperature": 0.0, "avg_logprob": -0.16471939598953964, "compression_ratio": 1.5229681978798586, "no_speech_prob": 0.25622043013572693}, {"id": 42, "seek": 16494, "start": 183.82, "end": 185.3, "text": " M\u00f3wi\u0105 o przej\u015bciu fazowym.", "tokens": [51308, 376, 3901, 11404, 277, 8325, 73, 6199, 84, 4375, 31691, 13, 51382], "temperature": 0.0, "avg_logprob": -0.16471939598953964, "compression_ratio": 1.5229681978798586, "no_speech_prob": 0.25622043013572693}, {"id": 43, "seek": 16494, "start": 185.54, "end": 186.66, "text": " Phase transition.", "tokens": [51394, 24432, 6034, 13, 51450], "temperature": 0.0, "avg_logprob": -0.16471939598953964, "compression_ratio": 1.5229681978798586, "no_speech_prob": 0.25622043013572693}, {"id": 44, "seek": 16494, "start": 187.26, "end": 188.82, "text": " Aha, phase transition.", "tokens": [51480, 27448, 11, 5574, 6034, 13, 51558], "temperature": 0.0, "avg_logprob": -0.16471939598953964, "compression_ratio": 1.5229681978798586, "no_speech_prob": 0.25622043013572693}, {"id": 45, "seek": 16494, "start": 188.9, "end": 194.22, "text": " Tak, to nawi\u0105zanie do s\u0142ynnego eseju fizyka Filipa Andersona, more is different.", "tokens": [51562, 9118, 11, 281, 18969, 11404, 89, 7155, 360, 15116, 2534, 11858, 10167, 8954, 21000, 88, 2330, 28241, 64, 33988, 4037, 11, 544, 307, 819, 13, 51828], "temperature": 0.0, "avg_logprob": -0.16471939598953964, "compression_ratio": 1.5229681978798586, "no_speech_prob": 0.25622043013572693}, {"id": 46, "seek": 19494, "start": 194.94, "end": 206.66, "text": " Idea jest taka, \u017ce zmiana ilo\u015bciowa, wi\u0119cej cz\u0105steczek wody, wi\u0119cej parametr\u00f3w modelu, po przekroczeniu pewnego punktu krytycznego, prowadzi do fundamentalnej zmiany jako\u015bciowej.", "tokens": [50364, 47245, 3492, 28017, 11, 3561, 17020, 8497, 1930, 44468, 5528, 11, 26004, 6472, 1611, 2941, 3689, 916, 261, 843, 11, 26004, 6220, 27965, 3901, 2316, 84, 11, 714, 29785, 24174, 39651, 25889, 11858, 39561, 84, 34847, 874, 3689, 11858, 11, 36590, 3992, 360, 8088, 11794, 43591, 88, 17123, 6199, 21091, 13, 50950], "temperature": 0.0, "avg_logprob": -0.12090343448287207, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.0016220523975789547}, {"id": 47, "seek": 19494, "start": 207.34, "end": 210.38, "text": " Czyli woda nie staje si\u0119 troch\u0119 bardziej lodowata.", "tokens": [50984, 37099, 261, 13449, 2838, 342, 11153, 3244, 24926, 27209, 33311, 305, 3274, 13, 51136], "temperature": 0.0, "avg_logprob": -0.12090343448287207, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.0016220523975789547}, {"id": 48, "seek": 19494, "start": 210.62, "end": 211.38, "text": " Dok\u0142adnie.", "tokens": [51148, 29768, 10358, 2766, 13, 51186], "temperature": 0.0, "avg_logprob": -0.12090343448287207, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.0016220523975789547}, {"id": 49, "seek": 19494, "start": 211.94, "end": 216.62, "text": " W temperaturze zera stopni nagle i gwa\u0142townie zmienia sw\u00f3j stan skupienia w lud.", "tokens": [51214, 343, 3393, 19493, 1381, 710, 1663, 1590, 3722, 297, 15088, 741, 290, 44603, 30401, 414, 17020, 18811, 1693, 18999, 27984, 1110, 1010, 18811, 261, 15946, 13, 51448], "temperature": 0.0, "avg_logprob": -0.12090343448287207, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.0016220523975789547}, {"id": 50, "seek": 19494, "start": 217.02, "end": 221.14, "text": " Tak samo model j\u0119zykowy nie staje si\u0119 troch\u0119 lepszy w arytmetyce.", "tokens": [51468, 9118, 36422, 2316, 49055, 74, 10089, 2838, 342, 11153, 3244, 24926, 476, 1878, 1229, 261, 594, 4328, 76, 2210, 384, 13, 51674], "temperature": 0.0, "avg_logprob": -0.12090343448287207, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.0016220523975789547}, {"id": 51, "seek": 19494, "start": 221.5, "end": 223.82, "text": " Nagle po prostu zaczyna umie\u0107 liczy\u0107.", "tokens": [51692, 426, 15088, 714, 19518, 43811, 629, 1105, 414, 2162, 6169, 27150, 13, 51808], "temperature": 0.0, "avg_logprob": -0.12090343448287207, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.0016220523975789547}, {"id": 52, "seek": 22382, "start": 224.06, "end": 227.29999999999998, "text": " A co jest t\u0105 temperatur\u0105 dla modeli AI?", "tokens": [50376, 316, 598, 3492, 32294, 3393, 19493, 1611, 12285, 2316, 72, 7318, 30, 50538], "temperature": 0.0, "avg_logprob": -0.11204284259251186, "compression_ratio": 1.381118881118881, "no_speech_prob": 0.0016945544630289078}, {"id": 53, "seek": 22382, "start": 227.78, "end": 232.54, "text": " Co mierzymy na osi X tego wykresu, \u017ceby zobaczy\u0107 to przej\u015bcie fazowe?", "tokens": [50562, 3066, 47448, 1229, 2226, 1667, 3003, 72, 1783, 8627, 39287, 495, 84, 11, 11316, 37273, 2162, 281, 8325, 73, 9815, 4375, 6880, 30, 50800], "temperature": 0.0, "avg_logprob": -0.11204284259251186, "compression_ratio": 1.381118881118881, "no_speech_prob": 0.0016945544630289078}, {"id": 54, "seek": 22382, "start": 232.85999999999999, "end": 236.5, "text": " G\u0142\u00f3wnie patrzymy na dwa wska\u017aniki, kt\u00f3re s\u0105 ze sob\u0105 mocno powi\u0105zane.", "tokens": [50816, 460, 1221, 812, 14215, 1947, 13047, 2226, 1667, 35045, 261, 20771, 10659, 77, 9850, 11, 8864, 9015, 5277, 18253, 1611, 34962, 1771, 3388, 11404, 89, 1929, 13, 50998], "temperature": 0.0, "avg_logprob": -0.11204284259251186, "compression_ratio": 1.381118881118881, "no_speech_prob": 0.0016945544630289078}, {"id": 55, "seek": 22382, "start": 237.1, "end": 243.22, "text": " Pierwszy to training flops, czyli \u0142\u0105czna moc obliczeniowa zu\u017cyta na wytrenowanie modelu.", "tokens": [51028, 16676, 30012, 281, 3097, 932, 3370, 11, 16591, 220, 43558, 629, 34962, 1111, 1050, 42124, 5528, 2164, 7735, 1328, 1667, 261, 4328, 1095, 22028, 2316, 84, 13, 51334], "temperature": 0.0, "avg_logprob": -0.11204284259251186, "compression_ratio": 1.381118881118881, "no_speech_prob": 0.0016945544630289078}, {"id": 56, "seek": 22382, "start": 243.57999999999998, "end": 244.06, "text": " Ok.", "tokens": [51352, 3477, 13, 51376], "temperature": 0.0, "avg_logprob": -0.11204284259251186, "compression_ratio": 1.381118881118881, "no_speech_prob": 0.0016945544630289078}, {"id": 57, "seek": 22382, "start": 244.45999999999998, "end": 250.01999999999998, "text": " A drugi to po prostu liczba jego parametr\u00f3w, te s\u0142ynne miliardy czy biliony, o kt\u00f3rych ci\u0105gle s\u0142yszymy.", "tokens": [51396, 316, 4110, 72, 281, 714, 19518, 6169, 89, 4231, 26542, 6220, 27965, 3901, 11, 535, 15116, 2534, 716, 1962, 72, 515, 88, 6430, 8588, 46184, 11, 277, 30382, 42398, 22631, 15116, 749, 1229, 2226, 13, 51674], "temperature": 0.0, "avg_logprob": -0.11204284259251186, "compression_ratio": 1.381118881118881, "no_speech_prob": 0.0016945544630289078}, {"id": 58, "seek": 25002, "start": 250.62, "end": 253.98000000000002, "text": " Im wi\u0119cej parametr\u00f3w, tym zazwyczaj wi\u0119cej oblicze\u0144 potrzeba.", "tokens": [50394, 4331, 26004, 6220, 27965, 3901, 11, 8107, 710, 921, 9726, 3689, 1805, 26004, 1111, 1050, 49689, 28577, 4231, 13, 50562], "temperature": 0.0, "avg_logprob": -0.15247226071048092, "compression_ratio": 1.4419354838709677, "no_speech_prob": 0.004166119731962681}, {"id": 59, "seek": 25002, "start": 254.5, "end": 258.18, "text": " I to w\u0142a\u015bnie ta skala jest czynnikiem, kt\u00f3ry prowadzi do emergencji.", "tokens": [50588, 286, 281, 14234, 1846, 1110, 5159, 3492, 6430, 77, 13123, 4907, 11, 9913, 36590, 3992, 360, 33983, 19649, 13, 50772], "temperature": 0.0, "avg_logprob": -0.15247226071048092, "compression_ratio": 1.4419354838709677, "no_speech_prob": 0.004166119731962681}, {"id": 60, "seek": 25002, "start": 258.42, "end": 258.82, "text": " Ok.", "tokens": [50784, 3477, 13, 50804], "temperature": 0.0, "avg_logprob": -0.15247226071048092, "compression_ratio": 1.4419354838709677, "no_speech_prob": 0.004166119731962681}, {"id": 61, "seek": 25002, "start": 258.86, "end": 265.3, "text": " Teoria o przej\u015bciu fazowym brzmi przekonuj\u0105co, ale wci\u0105\u017c jest, no wiesz, abstrakcyjna.", "tokens": [50806, 1989, 8172, 277, 8325, 73, 6199, 84, 4375, 31691, 738, 89, 3057, 29785, 266, 13263, 1291, 11, 6775, 261, 537, 27242, 3492, 11, 572, 261, 15347, 11, 10823, 11272, 42949, 629, 13, 51128], "temperature": 0.0, "avg_logprob": -0.15247226071048092, "compression_ratio": 1.4419354838709677, "no_speech_prob": 0.004166119731962681}, {"id": 62, "seek": 25002, "start": 265.78000000000003, "end": 266.82, "text": " Potrzebuje dowod\u00f3w.", "tokens": [51152, 9145, 13503, 6021, 2884, 9459, 378, 3901, 13, 51204], "temperature": 0.0, "avg_logprob": -0.15247226071048092, "compression_ratio": 1.4419354838709677, "no_speech_prob": 0.004166119731962681}, {"id": 63, "seek": 25002, "start": 267.06, "end": 272.62, "text": " Poka\u017c mi ten moment, kiedy model, kt\u00f3ry do tej pory be\u0142kota\u0142 nagle zaczyna m\u00f3wi\u0107 sensem.", "tokens": [51216, 14958, 18264, 2752, 2064, 1623, 11, 18777, 2316, 11, 9913, 360, 12573, 280, 827, 312, 1221, 74, 5377, 1221, 297, 15088, 43811, 629, 13489, 12757, 2923, 443, 13, 51494], "temperature": 0.0, "avg_logprob": -0.15247226071048092, "compression_ratio": 1.4419354838709677, "no_speech_prob": 0.004166119731962681}, {"id": 64, "seek": 25002, "start": 273.02, "end": 277.42, "text": " Ale zanim to, musimy chyba ustali\u0107, jak w og\u00f3le z nim rozmawiano w tych testach.", "tokens": [51514, 9366, 710, 17869, 281, 11, 43449, 31532, 26189, 5103, 2162, 11, 4207, 261, 29229, 710, 24887, 35234, 1607, 6254, 261, 15180, 1500, 608, 13, 51734], "temperature": 0.0, "avg_logprob": -0.15247226071048092, "compression_ratio": 1.4419354838709677, "no_speech_prob": 0.004166119731962681}, {"id": 65, "seek": 25002, "start": 277.46000000000004, "end": 279.38, "text": " Tak, to wa\u017cne.", "tokens": [51736, 9118, 11, 281, 46110, 13, 51832], "temperature": 0.0, "avg_logprob": -0.15247226071048092, "compression_ratio": 1.4419354838709677, "no_speech_prob": 0.004166119731962681}, {"id": 66, "seek": 28002, "start": 280.34, "end": 284.06, "text": " Bo wiele z nich opiera si\u0119 na technice zwanej Fuse Shot Prompting.", "tokens": [50380, 3286, 33137, 710, 25570, 999, 10609, 3244, 1667, 1537, 573, 11873, 1929, 73, 479, 438, 28845, 15833, 662, 278, 13, 50566], "temperature": 0.0, "avg_logprob": -0.19828395545482635, "compression_ratio": 1.4642857142857142, "no_speech_prob": 0.005413785111159086}, {"id": 67, "seek": 28002, "start": 284.09999999999997, "end": 284.97999999999996, "text": " Zgadza si\u0119.", "tokens": [50568, 1176, 70, 345, 2394, 3244, 13, 50612], "temperature": 0.0, "avg_logprob": -0.19828395545482635, "compression_ratio": 1.4642857142857142, "no_speech_prob": 0.005413785111159086}, {"id": 68, "seek": 28002, "start": 285.41999999999996, "end": 288.26, "text": " Fuse Shot Prompting to bardzo intuicyjna metoda.", "tokens": [50634, 479, 438, 28845, 15833, 662, 278, 281, 9034, 560, 84, 2632, 73, 629, 1131, 13449, 13, 50776], "temperature": 0.0, "avg_logprob": -0.19828395545482635, "compression_ratio": 1.4642857142857142, "no_speech_prob": 0.005413785111159086}, {"id": 69, "seek": 28002, "start": 288.62, "end": 293.46, "text": " Zamiast m\u0119czy\u0107 si\u0119 z Fine Tuning, czyli dostrajaniem ca\u0142ego modelu do nowego zadania,", "tokens": [50794, 1176, 4526, 525, 275, 1274, 33967, 3244, 710, 12024, 21363, 278, 11, 16591, 20568, 48690, 282, 4907, 35224, 6308, 2316, 84, 360, 586, 6308, 42788, 5609, 11, 51036], "temperature": 0.0, "avg_logprob": -0.19828395545482635, "compression_ratio": 1.4642857142857142, "no_speech_prob": 0.005413785111159086}, {"id": 70, "seek": 28002, "start": 293.65999999999997, "end": 296.78, "text": " po prostu pokazujemy mu w zapytaniu kilka przyk\u0142ad\u00f3w.", "tokens": [51046, 714, 19518, 13010, 921, 21767, 2992, 261, 14223, 4328, 25849, 36466, 23144, 3901, 13, 51202], "temperature": 0.0, "avg_logprob": -0.19828395545482635, "compression_ratio": 1.4642857142857142, "no_speech_prob": 0.005413785111159086}, {"id": 71, "seek": 28002, "start": 297.21999999999997, "end": 299.74, "text": " Czyli na przyk\u0142ad, dajemy mu zdanie.", "tokens": [51224, 37099, 1667, 23144, 11, 1120, 73, 3633, 2992, 16221, 7155, 13, 51350], "temperature": 0.0, "avg_logprob": -0.19828395545482635, "compression_ratio": 1.4642857142857142, "no_speech_prob": 0.005413785111159086}, {"id": 72, "seek": 28002, "start": 300.5, "end": 305.34, "text": " Ten film jest okropny, z etykiet\u0105 Sentiment negatywny.", "tokens": [51388, 9380, 2007, 3492, 3133, 1513, 1634, 11, 710, 1030, 46127, 1684, 1611, 23652, 2328, 2485, 21398, 43682, 13, 51630], "temperature": 0.0, "avg_logprob": -0.19828395545482635, "compression_ratio": 1.4642857142857142, "no_speech_prob": 0.005413785111159086}, {"id": 73, "seek": 30534, "start": 305.94, "end": 310.7, "text": " Potem uwielbiam ten film z etykiet\u0105 Sentiment pozytywny.", "tokens": [50394, 9145, 443, 23147, 1187, 65, 2918, 2064, 2007, 710, 1030, 46127, 1684, 1611, 23652, 2328, 49358, 874, 43682, 13, 50632], "temperature": 0.0, "avg_logprob": -0.19689246399761878, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.019599076360464096}, {"id": 74, "seek": 30534, "start": 311.26, "end": 315.17999999999995, "text": " A na ko\u0144cu podajemy nowe zdanie, na przyk\u0142ad Seans by\u0142 strat\u0105 czasu.", "tokens": [50660, 316, 1667, 26470, 12032, 2497, 1805, 3633, 586, 68, 16221, 7155, 11, 1667, 23144, 1100, 599, 16673, 23674, 1611, 40860, 13, 50856], "temperature": 0.0, "avg_logprob": -0.19689246399761878, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.019599076360464096}, {"id": 75, "seek": 30534, "start": 315.65999999999997, "end": 317.65999999999997, "text": " I prosimy o doko\u0144czenie etykiety.", "tokens": [50880, 286, 6267, 13189, 277, 360, 4093, 5248, 39043, 1030, 46127, 4014, 13, 50980], "temperature": 0.0, "avg_logprob": -0.19689246399761878, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.019599076360464096}, {"id": 76, "seek": 30534, "start": 318.14, "end": 320.58, "text": " I model musi sam za\u0142apa\u0107, o co chodzi.", "tokens": [51004, 286, 2316, 37587, 3247, 7949, 1221, 7961, 2162, 11, 277, 598, 23998, 13, 51126], "temperature": 0.0, "avg_logprob": -0.19689246399761878, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.019599076360464096}, {"id": 77, "seek": 30534, "start": 320.65999999999997, "end": 324.58, "text": " Dok\u0142adnie, na podstawie tych kilku przyk\u0142ad\u00f3w musi zrozumie\u0107 regu\u0142y gry.", "tokens": [51130, 29768, 10358, 2766, 11, 1667, 43443, 414, 15180, 5128, 5279, 23144, 3901, 37587, 710, 27857, 449, 414, 2162, 1121, 84, 6825, 41974, 13, 51326], "temperature": 0.0, "avg_logprob": -0.19689246399761878, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.019599076360464096}, {"id": 78, "seek": 30534, "start": 324.73999999999995, "end": 327.26, "text": " Dysys artyku\u0142u s\u0105 naprawd\u0119 uderzaj\u0105ce.", "tokens": [51334, 413, 749, 749, 594, 874, 5279, 24066, 9015, 20970, 344, 1068, 89, 11133, 384, 13, 51460], "temperature": 0.0, "avg_logprob": -0.19689246399761878, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.019599076360464096}, {"id": 79, "seek": 30534, "start": 327.85999999999996, "end": 330.17999999999995, "text": " We\u017amy prost\u0105, trzycyfrow\u0105 arytmetyk\u0119.", "tokens": [51490, 492, 10659, 2226, 10293, 1611, 11, 34573, 1344, 69, 1892, 1611, 594, 4328, 76, 2210, 15724, 13, 51606], "temperature": 0.0, "avg_logprob": -0.19689246399761878, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.019599076360464096}, {"id": 80, "seek": 30534, "start": 330.26, "end": 330.94, "text": " Dodawanie.", "tokens": [51610, 26904, 1607, 7155, 13, 51644], "temperature": 0.0, "avg_logprob": -0.19689246399761878, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.019599076360464096}, {"id": 81, "seek": 30534, "start": 330.97999999999996, "end": 331.26, "text": " Tak.", "tokens": [51646, 9118, 13, 51660], "temperature": 0.0, "avg_logprob": -0.19689246399761878, "compression_ratio": 1.4102564102564104, "no_speech_prob": 0.019599076360464096}, {"id": 82, "seek": 33126, "start": 331.9, "end": 338.3, "text": " Okazuje si\u0119, \u017ce modele takie jak GPT-3 czy Lemda przez d\u0142ugi czas maj\u0105 skuteczno\u015b\u0107 blisk\u0105 zero.", "tokens": [50396, 3477, 43317, 3244, 11, 3561, 4391, 306, 15963, 4207, 26039, 51, 12, 18, 6430, 16905, 2675, 14064, 44042, 24780, 13190, 26064, 1110, 1169, 3689, 23293, 888, 7797, 1611, 4018, 13, 50716], "temperature": 0.0, "avg_logprob": -0.1754181744301156, "compression_ratio": 1.4155405405405406, "no_speech_prob": 0.030110836029052734}, {"id": 83, "seek": 33126, "start": 338.98, "end": 340.7, "text": " Po prostu nie potrafi\u0105 dodawa\u0107.", "tokens": [50750, 6165, 19518, 2838, 1847, 10437, 11404, 13886, 10449, 2162, 13, 50836], "temperature": 0.0, "avg_logprob": -0.1754181744301156, "compression_ratio": 1.4155405405405406, "no_speech_prob": 0.030110836029052734}, {"id": 84, "seek": 33126, "start": 340.94, "end": 347.3, "text": " Chwila, chcesz powiedzie\u0107, \u017ce model maj\u0105cy powiedzmy 10 miliard\u00f3w parametr\u00f3w, co i tak jest ogromn\u0105 liczb\u0105.", "tokens": [50848, 761, 86, 7371, 11, 417, 887, 89, 27886, 11, 3561, 2316, 26064, 1344, 27617, 2226, 1266, 1962, 72, 515, 3901, 6220, 27965, 3901, 11, 598, 741, 991, 3492, 34416, 298, 13113, 6169, 89, 65, 1611, 13, 51166], "temperature": 0.0, "avg_logprob": -0.1754181744301156, "compression_ratio": 1.4155405405405406, "no_speech_prob": 0.030110836029052734}, {"id": 85, "seek": 33126, "start": 347.7, "end": 351.62, "text": " Jest w dodawaniu liczby nie lepszy ni\u017c losowy generator cyfr?", "tokens": [51186, 24918, 261, 13886, 1607, 25849, 6169, 89, 2322, 2838, 476, 1878, 1229, 28502, 1750, 10089, 19265, 3185, 5779, 30, 51382], "temperature": 0.0, "avg_logprob": -0.1754181744301156, "compression_ratio": 1.4155405405405406, "no_speech_prob": 0.030110836029052734}, {"id": 86, "seek": 33126, "start": 351.94, "end": 352.5, "text": " To brzmi.", "tokens": [51398, 1407, 738, 89, 3057, 13, 51426], "temperature": 0.0, "avg_logprob": -0.1754181744301156, "compression_ratio": 1.4155405405405406, "no_speech_prob": 0.030110836029052734}, {"id": 87, "seek": 33126, "start": 353.21999999999997, "end": 354.34, "text": " Jakby by\u0142 zepsuty.", "tokens": [51462, 15029, 2322, 16673, 710, 10653, 6432, 13, 51518], "temperature": 0.0, "avg_logprob": -0.1754181744301156, "compression_ratio": 1.4155405405405406, "no_speech_prob": 0.030110836029052734}, {"id": 88, "seek": 33126, "start": 354.98, "end": 356.42, "text": " Dok\u0142adnie tak to wygl\u0105da.", "tokens": [51550, 29768, 10358, 2766, 991, 281, 32015, 13, 51622], "temperature": 0.0, "avg_logprob": -0.1754181744301156, "compression_ratio": 1.4155405405405406, "no_speech_prob": 0.030110836029052734}, {"id": 89, "seek": 33126, "start": 356.94, "end": 359.18, "text": " Ale nie jest zepsuty, jest po prostu za ma\u0142y.", "tokens": [51648, 9366, 2838, 3492, 710, 10653, 6432, 11, 3492, 714, 19518, 7949, 463, 6825, 13, 51760], "temperature": 0.0, "avg_logprob": -0.1754181744301156, "compression_ratio": 1.4155405405405406, "no_speech_prob": 0.030110836029052734}, {"id": 90, "seek": 35918, "start": 359.58, "end": 364.38, "text": " Dla modelu GPT-3 to przej\u015bcie fazowe nast\u0119puje w okolicach 13 miliard\u00f3w parametr\u00f3w.", "tokens": [50384, 413, 875, 2316, 84, 26039, 51, 12, 18, 281, 8325, 73, 9815, 4375, 6880, 39662, 13008, 261, 3133, 7940, 608, 3705, 1962, 72, 515, 3901, 6220, 27965, 3901, 13, 50624], "temperature": 0.0, "avg_logprob": -0.140241932559323, "compression_ratio": 1.391025641025641, "no_speech_prob": 0.02780795283615589}, {"id": 91, "seek": 35918, "start": 364.78000000000003, "end": 368.42, "text": " Dla Lemda ten pr\u00f3g jest jeszcze wy\u017cej oko\u0142o 68 miliard\u00f3w.", "tokens": [50644, 413, 875, 16905, 2675, 2064, 8565, 70, 3492, 14168, 4628, 38493, 45730, 5249, 23317, 1962, 72, 515, 3901, 13, 50826], "temperature": 0.0, "avg_logprob": -0.140241932559323, "compression_ratio": 1.391025641025641, "no_speech_prob": 0.02780795283615589}, {"id": 92, "seek": 35918, "start": 368.54, "end": 369.34000000000003, "text": " Niesamowite.", "tokens": [50832, 426, 530, 335, 305, 642, 13, 50872], "temperature": 0.0, "avg_logprob": -0.140241932559323, "compression_ratio": 1.391025641025641, "no_speech_prob": 0.02780795283615589}, {"id": 93, "seek": 35918, "start": 369.86, "end": 375.62, "text": " Dopiero po przekroczeniu tego punktu ich wydajno\u015b\u0107 nagle wystrzeliwuje do ca\u0142kiem przyzwoitego poziomu.", "tokens": [50898, 42657, 12030, 714, 29785, 24174, 39651, 8627, 39561, 84, 1893, 25984, 1805, 23293, 297, 15088, 4628, 9733, 89, 10148, 86, 13008, 360, 35224, 26116, 6501, 89, 6120, 642, 1571, 38503, 298, 84, 13, 51186], "temperature": 0.0, "avg_logprob": -0.140241932559323, "compression_ratio": 1.391025641025641, "no_speech_prob": 0.02780795283615589}, {"id": 94, "seek": 35918, "start": 375.86, "end": 378.9, "text": " I ten sam schemat powtarza si\u0119 w masie innych zada\u0144.", "tokens": [51198, 286, 2064, 3247, 956, 8615, 3388, 23480, 2394, 3244, 261, 2300, 414, 36286, 710, 1538, 5248, 13, 51350], "temperature": 0.0, "avg_logprob": -0.140241932559323, "compression_ratio": 1.391025641025641, "no_speech_prob": 0.02780795283615589}, {"id": 95, "seek": 35918, "start": 379.22, "end": 384.98, "text": " Rozszyfrowywanie s\u0142\u00f3w z pomieszanych liter, odpowiadanie na pytania zwiedzie og\u00f3lnej w te\u015bcie Trutful QA.", "tokens": [51366, 43313, 7706, 69, 1892, 88, 86, 7155, 15116, 3901, 710, 12991, 15347, 34644, 2733, 11, 24314, 38069, 7155, 1667, 25878, 5609, 11873, 1091, 3283, 5360, 15741, 11794, 261, 535, 9815, 1765, 325, 906, 1249, 32, 13, 51654], "temperature": 0.0, "avg_logprob": -0.140241932559323, "compression_ratio": 1.391025641025641, "no_speech_prob": 0.02780795283615589}, {"id": 96, "seek": 38498, "start": 385.38, "end": 387.1, "text": " Zawsze jest ten sam wzorzec.", "tokens": [50384, 1176, 28354, 3492, 2064, 3247, 24809, 284, 1381, 66, 13, 50470], "temperature": 0.0, "avg_logprob": -0.16259051040864328, "compression_ratio": 1.4432234432234432, "no_speech_prob": 0.2596113979816437}, {"id": 97, "seek": 38498, "start": 387.26, "end": 389.90000000000003, "text": " D\u0142ugo, d\u0142ugo nic, a potem nag\u0142y skok.", "tokens": [50478, 413, 1221, 20746, 11, 44042, 20746, 6201, 11, 257, 36513, 17096, 6825, 1110, 453, 13, 50610], "temperature": 0.0, "avg_logprob": -0.16259051040864328, "compression_ratio": 1.4432234432234432, "no_speech_prob": 0.2596113979816437}, {"id": 98, "seek": 38498, "start": 390.34000000000003, "end": 394.98, "text": " W artykule jest te\u017c \u015bwietna historia z benchmarkiem Word in Context w skr\u00f3cie Week.", "tokens": [50632, 343, 594, 874, 74, 2271, 3492, 9516, 8299, 39083, 629, 18385, 710, 18927, 4907, 8725, 294, 4839, 3828, 261, 1110, 11721, 4260, 12615, 13, 50864], "temperature": 0.0, "avg_logprob": -0.16259051040864328, "compression_ratio": 1.4432234432234432, "no_speech_prob": 0.2596113979816437}, {"id": 99, "seek": 38498, "start": 395.34000000000003, "end": 400.90000000000003, "text": " To chyba doskona\u0142y przyk\u0142ad nieprzewidywalno\u015bci, wr\u0119cz taka, wiesz, detektywistyczna zagadka.", "tokens": [50882, 1407, 31532, 4491, 74, 4037, 6825, 23144, 2838, 1424, 43551, 327, 27112, 304, 16438, 11, 928, 1274, 3689, 28017, 11, 261, 15347, 11, 1141, 916, 874, 86, 468, 17466, 629, 27001, 345, 2330, 13, 51160], "temperature": 0.0, "avg_logprob": -0.16259051040864328, "compression_ratio": 1.4432234432234432, "no_speech_prob": 0.2596113979816437}, {"id": 100, "seek": 38498, "start": 401.02000000000004, "end": 402.1, "text": " To jest idealny przyk\u0142ad.", "tokens": [51166, 1407, 3492, 7157, 1634, 23144, 13, 51220], "temperature": 0.0, "avg_logprob": -0.16259051040864328, "compression_ratio": 1.4432234432234432, "no_speech_prob": 0.2596113979816437}, {"id": 101, "seek": 38498, "start": 402.62, "end": 403.62, "text": " Zadanie jest proste.", "tokens": [51246, 1176, 345, 7155, 3492, 10293, 68, 13, 51296], "temperature": 0.0, "avg_logprob": -0.16259051040864328, "compression_ratio": 1.4432234432234432, "no_speech_prob": 0.2596113979816437}, {"id": 102, "seek": 38498, "start": 403.94, "end": 407.98, "text": " Powiedz, czy dane s\u0142owo jest u\u017cyte w tym samym znaczeniu w dw\u00f3ch r\u00f3\u017cnych zdaniach.", "tokens": [51312, 14762, 15338, 11, 6430, 49206, 15116, 19941, 3492, 34097, 975, 261, 8107, 3247, 4199, 15397, 326, 39651, 261, 27379, 812, 339, 42602, 16221, 3782, 608, 13, 51514], "temperature": 0.0, "avg_logprob": -0.16259051040864328, "compression_ratio": 1.4432234432234432, "no_speech_prob": 0.2596113979816437}, {"id": 103, "seek": 38498, "start": 408.46000000000004, "end": 408.94, "text": " Ok.", "tokens": [51538, 3477, 13, 51562], "temperature": 0.0, "avg_logprob": -0.16259051040864328, "compression_ratio": 1.4432234432234432, "no_speech_prob": 0.2596113979816437}, {"id": 104, "seek": 40894, "start": 409.21999999999997, "end": 417.46, "text": " Ot\u00f3\u017c model GPT-3, nawet w swojej najwi\u0119kszej 175-miliardowej wersji, kompletnie sobie z tym nie radzi\u0142.", "tokens": [50378, 422, 4547, 1427, 2316, 26039, 51, 12, 18, 11, 22696, 261, 29489, 73, 48636, 1694, 16920, 41165, 12, 76, 2312, 515, 21091, 261, 433, 4013, 11, 5207, 14657, 2766, 13652, 710, 8107, 2838, 2843, 3992, 1221, 13, 50790], "temperature": 0.0, "avg_logprob": -0.12126725912094116, "compression_ratio": 1.4069400630914826, "no_speech_prob": 0.010658646002411842}, {"id": 105, "seek": 40894, "start": 417.86, "end": 420.18, "text": " Jego wyniki by\u0142y na poziomie rzutu monet\u0105.", "tokens": [50810, 508, 6308, 31936, 9850, 26366, 1667, 38503, 40120, 367, 89, 325, 84, 15556, 1611, 13, 50926], "temperature": 0.0, "avg_logprob": -0.12126725912094116, "compression_ratio": 1.4069400630914826, "no_speech_prob": 0.010658646002411842}, {"id": 106, "seek": 40894, "start": 420.34, "end": 426.5, "text": " I jego tw\u00f3rcy z tego, co pami\u0119tam, w zasadzie si\u0119 poddali i stwierdzili, \u017ce problem le\u017cy w architekturze Decoder Only.", "tokens": [50934, 286, 26542, 683, 15614, 1344, 710, 8627, 11, 598, 31088, 37323, 11, 261, 44585, 3283, 3244, 2497, 67, 5103, 741, 342, 40717, 28168, 2312, 11, 3561, 1154, 476, 7735, 261, 3912, 642, 2320, 374, 1381, 12427, 19866, 5686, 13, 51242], "temperature": 0.0, "avg_logprob": -0.12126725912094116, "compression_ratio": 1.4069400630914826, "no_speech_prob": 0.010658646002411842}, {"id": 107, "seek": 40894, "start": 426.78, "end": 432.02, "text": " W\u0142a\u015bnie, tak jakby detektyw obwinia\u0142 zazbrodnie rodzaj u\u017cytego no\u017ca.", "tokens": [51256, 343, 5024, 12221, 11, 991, 28976, 1141, 916, 874, 86, 1111, 9136, 8908, 710, 921, 9120, 67, 2766, 28607, 1805, 34097, 975, 1571, 572, 35075, 13, 51518], "temperature": 0.0, "avg_logprob": -0.12126725912094116, "compression_ratio": 1.4069400630914826, "no_speech_prob": 0.010658646002411842}, {"id": 108, "seek": 40894, "start": 432.18, "end": 437.3, "text": " Sugerowali, \u017ce taka architektura po prostu nie nadaje si\u0119 do tego typu zada\u0144 por\u00f3wnawczych.", "tokens": [51526, 39131, 260, 305, 5103, 11, 3561, 28017, 3912, 642, 2320, 2991, 714, 19518, 2838, 8096, 2884, 3244, 360, 8627, 2125, 84, 710, 1538, 5248, 1515, 3901, 629, 86, 6522, 339, 13, 51782], "temperature": 0.0, "avg_logprob": -0.12126725912094116, "compression_ratio": 1.4069400630914826, "no_speech_prob": 0.010658646002411842}, {"id": 109, "seek": 43730, "start": 437.78000000000003, "end": 439.74, "text": " A\u017c tu nagle na scen\u0119 wchodzi palm.", "tokens": [50388, 316, 1427, 2604, 297, 15088, 1667, 4191, 1274, 261, 34616, 17018, 13, 50486], "temperature": 0.0, "avg_logprob": -0.14467998529901568, "compression_ratio": 1.4135593220338982, "no_speech_prob": 0.005280610173940659}, {"id": 110, "seek": 43730, "start": 439.98, "end": 444.06, "text": " Model o bardzo podobnej architekturze Decoder Only, ale...", "tokens": [50498, 17105, 277, 9034, 43024, 11794, 3912, 642, 2320, 374, 1381, 12427, 19866, 5686, 11, 6775, 485, 50702], "temperature": 0.0, "avg_logprob": -0.14467998529901568, "compression_ratio": 1.4135593220338982, "no_speech_prob": 0.005280610173940659}, {"id": 111, "seek": 43730, "start": 444.22, "end": 444.74, "text": " Wi\u0119kszy.", "tokens": [50710, 30127, 1694, 1229, 13, 50736], "temperature": 0.0, "avg_logprob": -0.14467998529901568, "compression_ratio": 1.4135593220338982, "no_speech_prob": 0.005280610173940659}, {"id": 112, "seek": 43730, "start": 444.78000000000003, "end": 449.42, "text": " Znacznie wi\u0119kszy, przeskalowany do 540 miliard\u00f3w parametr\u00f3w.", "tokens": [50738, 1176, 77, 14875, 2766, 29968, 1229, 11, 6541, 279, 19990, 23341, 360, 1025, 5254, 1962, 72, 515, 3901, 6220, 27965, 3901, 13, 50970], "temperature": 0.0, "avg_logprob": -0.14467998529901568, "compression_ratio": 1.4135593220338982, "no_speech_prob": 0.005280610173940659}, {"id": 113, "seek": 43730, "start": 449.90000000000003, "end": 451.02000000000004, "text": " I co si\u0119 okaza\u0142o?", "tokens": [50994, 286, 598, 3244, 3133, 12257, 5249, 30, 51050], "temperature": 0.0, "avg_logprob": -0.14467998529901568, "compression_ratio": 1.4135593220338982, "no_speech_prob": 0.005280610173940659}, {"id": 114, "seek": 43730, "start": 451.38, "end": 455.18, "text": " Nagle zacz\u0105\u0142 rozwi\u0105zywa\u0107 to zadanie z bardzo wysok\u0105 spotyczno\u015bci\u0105.", "tokens": [51068, 426, 15088, 34430, 8925, 1221, 9544, 18234, 1229, 25234, 281, 42788, 7155, 710, 9034, 27062, 453, 1611, 4008, 17466, 16438, 1611, 13, 51258], "temperature": 0.0, "avg_logprob": -0.14467998529901568, "compression_ratio": 1.4135593220338982, "no_speech_prob": 0.005280610173940659}, {"id": 115, "seek": 43730, "start": 455.42, "end": 457.86, "text": " Okaza\u0142o si\u0119, \u017ce n\u00f3\u017c by\u0142 w porz\u0105dku.", "tokens": [51270, 3477, 12257, 5249, 3244, 11, 3561, 6604, 1427, 16673, 261, 1515, 23876, 5279, 13, 51392], "temperature": 0.0, "avg_logprob": -0.14467998529901568, "compression_ratio": 1.4135593220338982, "no_speech_prob": 0.005280610173940659}, {"id": 116, "seek": 43730, "start": 458.14, "end": 460.86, "text": " Po prostu nie uderzono nim wystarczaj\u0105co mocno.", "tokens": [51406, 6165, 19518, 2838, 344, 1068, 89, 8957, 24887, 4628, 9710, 3689, 11133, 1291, 34962, 1771, 13, 51542], "temperature": 0.0, "avg_logprob": -0.14467998529901568, "compression_ratio": 1.4135593220338982, "no_speech_prob": 0.005280610173940659}, {"id": 117, "seek": 43730, "start": 461.3, "end": 464.1, "text": " Czyli problem le\u017ca\u0142 wy\u0142\u0105cznie w niewystarczaj\u0105cej skali.", "tokens": [51564, 37099, 1154, 476, 35075, 1221, 4628, 15926, 19923, 261, 43622, 88, 9710, 3689, 11133, 20811, 1110, 5103, 13, 51704], "temperature": 0.0, "avg_logprob": -0.14467998529901568, "compression_ratio": 1.4135593220338982, "no_speech_prob": 0.005280610173940659}, {"id": 118, "seek": 46410, "start": 464.38, "end": 471.06, "text": " To pokazuje, \u017ce czasem odpowied\u017a na pytanie, dlaczego to nie dzia\u0142a brzmi po prostu, bo jest jeszcze za ma\u0142e.", "tokens": [50378, 1407, 13010, 43317, 11, 3561, 13190, 443, 36574, 10659, 1667, 36610, 11, 37873, 39329, 281, 2838, 37903, 738, 89, 3057, 714, 19518, 11, 748, 3492, 14168, 7949, 463, 19827, 13, 50712], "temperature": 0.0, "avg_logprob": -0.13477104187011718, "compression_ratio": 1.4969135802469136, "no_speech_prob": 0.026395298540592194}, {"id": 119, "seek": 46410, "start": 471.18, "end": 473.34000000000003, "text": " Ale emergencja to nie tylko nowe zdolno\u015bci.", "tokens": [50718, 9366, 33983, 34056, 281, 2838, 13219, 586, 68, 16221, 401, 16438, 13, 50826], "temperature": 0.0, "avg_logprob": -0.13477104187011718, "compression_ratio": 1.4969135802469136, "no_speech_prob": 0.026395298540592194}, {"id": 120, "seek": 46410, "start": 473.66, "end": 476.86, "text": " Z artyku\u0142u wynika co\u015b jeszcze bardziej zaskakuj\u0105cego.", "tokens": [50842, 1176, 594, 874, 5279, 24066, 31936, 5439, 19241, 14168, 27209, 710, 3863, 514, 13263, 384, 1571, 13, 51002], "temperature": 0.0, "avg_logprob": -0.13477104187011718, "compression_ratio": 1.4969135802469136, "no_speech_prob": 0.026395298540592194}, {"id": 121, "seek": 46410, "start": 477.02000000000004, "end": 479.54, "text": " Tak i to jest by\u0107 mo\u017ce jeszcze dziwniejsze.", "tokens": [51010, 9118, 741, 281, 3492, 15069, 12034, 14168, 31981, 895, 7764, 82, 1381, 13, 51136], "temperature": 0.0, "avg_logprob": -0.13477104187011718, "compression_ratio": 1.4969135802469136, "no_speech_prob": 0.026395298540592194}, {"id": 122, "seek": 46410, "start": 479.86, "end": 487.62, "text": " Okazuje si\u0119, \u017ce emergencja dotyczy nie tylko tego, co modele potrafi\u0105, ale te\u017c tego, jak powinni\u015bmy z nimi rozmawia\u0107.", "tokens": [51152, 3477, 43317, 3244, 11, 3561, 33983, 34056, 5893, 88, 6522, 2838, 13219, 8627, 11, 598, 4391, 306, 1847, 10437, 11404, 11, 6775, 9516, 8627, 11, 4207, 27310, 3722, 10513, 710, 297, 10121, 35234, 34953, 2162, 13, 51540], "temperature": 0.0, "avg_logprob": -0.13477104187011718, "compression_ratio": 1.4969135802469136, "no_speech_prob": 0.026395298540592194}, {"id": 123, "seek": 46410, "start": 487.74, "end": 488.58000000000004, "text": " Co masz na my\u015bli?", "tokens": [51546, 3066, 2300, 89, 1667, 452, 15350, 30, 51588], "temperature": 0.0, "avg_logprob": -0.13477104187011718, "compression_ratio": 1.4969135802469136, "no_speech_prob": 0.026395298540592194}, {"id": 124, "seek": 46410, "start": 488.66, "end": 493.34000000000003, "text": " Niekt\u00f3re techniki interakcji, kt\u00f3re dzi\u015b s\u0105 standardem w prompt engineering,", "tokens": [51592, 12016, 43073, 265, 1537, 9850, 728, 514, 19649, 11, 8864, 31981, 1788, 9015, 3832, 443, 261, 12391, 7043, 11, 51826], "temperature": 0.0, "avg_logprob": -0.13477104187011718, "compression_ratio": 1.4969135802469136, "no_speech_prob": 0.026395298540592194}, {"id": 125, "seek": 49334, "start": 493.61999999999995, "end": 497.73999999999995, "text": " okazuj\u0105 si\u0119 kompletnie bezu\u017cyteczne, a nawet szkodliwe dla mniejszych modeli.", "tokens": [50378, 3133, 921, 13263, 3244, 5207, 14657, 2766, 10782, 84, 7735, 975, 38491, 11, 257, 22696, 7870, 74, 378, 2081, 826, 12285, 39513, 45021, 2316, 72, 13, 50584], "temperature": 0.0, "avg_logprob": -0.17624004158431184, "compression_ratio": 1.4539877300613497, "no_speech_prob": 0.0010878609027713537}, {"id": 126, "seek": 49334, "start": 498.06, "end": 504.73999999999995, "text": " Subierujesz, \u017ce dana technika mo\u017ce dzia\u0142a\u0107 na jednym modelu, a na nieco mniejszej wersji tego samego modelu ju\u017c nie?", "tokens": [50600, 8511, 811, 4579, 10430, 11, 3561, 274, 2095, 1537, 5439, 12034, 37903, 2162, 1667, 5232, 12996, 2316, 84, 11, 257, 1667, 2838, 1291, 275, 30295, 16920, 261, 433, 4013, 8627, 912, 1571, 2316, 84, 10678, 2838, 30, 50934], "temperature": 0.0, "avg_logprob": -0.17624004158431184, "compression_ratio": 1.4539877300613497, "no_speech_prob": 0.0010878609027713537}, {"id": 127, "seek": 49334, "start": 505.21999999999997, "end": 506.85999999999996, "text": " To brzmi bardzo kontrontuicyjnie.", "tokens": [50958, 1407, 738, 89, 3057, 9034, 14373, 340, 580, 84, 2632, 73, 2766, 13, 51040], "temperature": 0.0, "avg_logprob": -0.17624004158431184, "compression_ratio": 1.4539877300613497, "no_speech_prob": 0.0010878609027713537}, {"id": 128, "seek": 49334, "start": 507.26, "end": 508.9, "text": " We\u017amy najs\u0142ynniejszy przyk\u0142ad.", "tokens": [51060, 492, 10659, 2226, 11212, 82, 1221, 2534, 10402, 7706, 23144, 13, 51142], "temperature": 0.0, "avg_logprob": -0.17624004158431184, "compression_ratio": 1.4539877300613497, "no_speech_prob": 0.0010878609027713537}, {"id": 129, "seek": 49334, "start": 509.02, "end": 510.78, "text": " Chain of Thought Prompting.", "tokens": [51148, 33252, 295, 23058, 15833, 662, 278, 13, 51236], "temperature": 0.0, "avg_logprob": -0.17624004158431184, "compression_ratio": 1.4539877300613497, "no_speech_prob": 0.0010878609027713537}, {"id": 130, "seek": 49334, "start": 510.97999999999996, "end": 512.54, "text": " Aha, my\u015blenie na g\u0142os?", "tokens": [51246, 27448, 11, 48633, 6698, 414, 1667, 43767, 30, 51324], "temperature": 0.0, "avg_logprob": -0.17624004158431184, "compression_ratio": 1.4539877300613497, "no_speech_prob": 0.0010878609027713537}, {"id": 131, "seek": 49334, "start": 512.6999999999999, "end": 513.42, "text": " Dok\u0142adnie.", "tokens": [51332, 29768, 10358, 2766, 13, 51368], "temperature": 0.0, "avg_logprob": -0.17624004158431184, "compression_ratio": 1.4539877300613497, "no_speech_prob": 0.0010878609027713537}, {"id": 132, "seek": 49334, "start": 513.9, "end": 519.02, "text": " Technika, w kt\u00f3rej prosimy model, by zanim poda ostateczn\u0105 odpowied\u017a na z\u0142o\u017cone pytanie,", "tokens": [51392, 8337, 5439, 11, 261, 36023, 6267, 13189, 2316, 11, 538, 710, 17869, 2497, 64, 277, 15406, 3689, 13113, 36574, 10659, 1667, 710, 5249, 1427, 546, 36610, 11, 51648], "temperature": 0.0, "avg_logprob": -0.17624004158431184, "compression_ratio": 1.4539877300613497, "no_speech_prob": 0.0010878609027713537}, {"id": 133, "seek": 49334, "start": 519.38, "end": 522.18, "text": " opisa\u0142 krok po kroku sw\u00f3j tok rozumowania.", "tokens": [51666, 999, 3837, 1221, 350, 31621, 714, 45909, 5279, 1693, 18999, 19164, 48797, 21308, 13, 51806], "temperature": 0.0, "avg_logprob": -0.17624004158431184, "compression_ratio": 1.4539877300613497, "no_speech_prob": 0.0010878609027713537}, {"id": 134, "seek": 52218, "start": 522.66, "end": 526.66, "text": " Dzi\u015b to absolutna podstawa przy problemach logicznych czy matematycznych,", "tokens": [50388, 413, 3992, 1788, 281, 18757, 629, 2497, 372, 10449, 6501, 1154, 608, 9952, 89, 9399, 6430, 3803, 8615, 17466, 9399, 11, 50588], "temperature": 0.0, "avg_logprob": -0.12894620393451892, "compression_ratio": 1.3936170212765957, "no_speech_prob": 0.001015699584968388}, {"id": 135, "seek": 52218, "start": 527.02, "end": 531.18, "text": " ale badania pokazuj\u0105, \u017ce ta metoda zaczyna przynosi\u0107 jak\u0105kolwiek korzy\u015b\u0107", "tokens": [50606, 6775, 1578, 5609, 13010, 921, 13263, 11, 3561, 1846, 1131, 13449, 43811, 629, 6501, 16751, 12757, 46719, 36620, 44674, 14784, 1229, 7753, 50814], "temperature": 0.0, "avg_logprob": -0.12894620393451892, "compression_ratio": 1.3936170212765957, "no_speech_prob": 0.001015699584968388}, {"id": 136, "seek": 52218, "start": 531.38, "end": 535.14, "text": " dopiero w modelach oskali oko\u0142o 100 miliard\u00f3w parametr\u00f3w.", "tokens": [50824, 21900, 12030, 261, 2316, 608, 3003, 74, 5103, 45730, 5249, 2319, 1962, 72, 515, 3901, 6220, 27965, 3901, 13, 51012], "temperature": 0.0, "avg_logprob": -0.12894620393451892, "compression_ratio": 1.3936170212765957, "no_speech_prob": 0.001015699584968388}, {"id": 137, "seek": 52218, "start": 535.54, "end": 536.4599999999999, "text": " A poni\u017cej?", "tokens": [51032, 316, 9224, 72, 38493, 30, 51078], "temperature": 0.0, "avg_logprob": -0.12894620393451892, "compression_ratio": 1.3936170212765957, "no_speech_prob": 0.001015699584968388}, {"id": 138, "seek": 52218, "start": 536.6999999999999, "end": 539.38, "text": " Poni\u017cej tego progu nie daje \u017cadnej przewagi.", "tokens": [51090, 31756, 72, 38493, 8627, 447, 2794, 2838, 1120, 2884, 39628, 11794, 39758, 20291, 13, 51224], "temperature": 0.0, "avg_logprob": -0.12894620393451892, "compression_ratio": 1.3936170212765957, "no_speech_prob": 0.001015699584968388}, {"id": 139, "seek": 52218, "start": 539.62, "end": 540.8199999999999, "text": " To jest fascynuj\u0105ce.", "tokens": [51236, 1407, 3492, 30632, 1344, 77, 13263, 384, 13, 51296], "temperature": 0.0, "avg_logprob": -0.12894620393451892, "compression_ratio": 1.3936170212765957, "no_speech_prob": 0.001015699584968388}, {"id": 140, "seek": 52218, "start": 541.3, "end": 546.9, "text": " Oznacza to, \u017ce przez lata mogli\u015bmy stosowa\u0107 techniki, kt\u00f3re wydawa\u0142y si\u0119 intuicyjnie dobre,", "tokens": [51320, 422, 22672, 326, 2394, 281, 11, 3561, 14064, 46722, 13172, 38452, 43581, 11445, 1537, 9850, 11, 8864, 25984, 10449, 6825, 3244, 560, 84, 2632, 73, 2766, 41959, 11, 51600], "temperature": 0.0, "avg_logprob": -0.12894620393451892, "compression_ratio": 1.3936170212765957, "no_speech_prob": 0.001015699584968388}, {"id": 141, "seek": 54690, "start": 547.42, "end": 550.06, "text": " a w rzeczywisto\u015bci sabotowali\u015bmy w\u0142asne wyniki,", "tokens": [50390, 257, 261, 26297, 86, 9334, 6199, 37167, 305, 33955, 43572, 716, 31936, 9850, 11, 50522], "temperature": 0.0, "avg_logprob": -0.1390828151328891, "compression_ratio": 1.4847560975609757, "no_speech_prob": 0.16522595286369324}, {"id": 142, "seek": 54690, "start": 550.26, "end": 554.6999999999999, "text": " bo nie rozumieli\u015bmy, \u017ce model nie osi\u0105gn\u0105\u0142 jeszcze odpowiedniej dojrza\u0142o\u015bci.", "tokens": [50532, 748, 2838, 48797, 23099, 10513, 11, 3561, 2316, 2838, 3003, 11404, 4568, 1611, 1221, 14168, 36574, 10402, 360, 73, 81, 2394, 35059, 13, 50754], "temperature": 0.0, "avg_logprob": -0.1390828151328891, "compression_ratio": 1.4847560975609757, "no_speech_prob": 0.16522595286369324}, {"id": 143, "seek": 54690, "start": 555.02, "end": 555.66, "text": " Dok\u0142adnie.", "tokens": [50770, 29768, 10358, 2766, 13, 50802], "temperature": 0.0, "avg_logprob": -0.1390828151328891, "compression_ratio": 1.4847560975609757, "no_speech_prob": 0.16522595286369324}, {"id": 144, "seek": 54690, "start": 556.3, "end": 561.34, "text": " A jest jeszcze bardziej jaskrawy przyk\u0142ad, kt\u00f3ry pokazuje, jak bardzo nasze intuicje mog\u0105 nas myli\u0107.", "tokens": [50834, 316, 3492, 14168, 27209, 361, 3863, 5131, 88, 23144, 11, 9913, 13010, 43317, 11, 4207, 9034, 43394, 560, 84, 299, 2884, 34123, 5382, 452, 2081, 2162, 13, 51086], "temperature": 0.0, "avg_logprob": -0.1390828151328891, "compression_ratio": 1.4847560975609757, "no_speech_prob": 0.16522595286369324}, {"id": 145, "seek": 54690, "start": 561.5799999999999, "end": 567.3, "text": " Chodzi o Instruction Fine Tuning, czyli proces dostrenania modelu, by lepiej pod\u0105\u017ca\u0142 za poleceniami.", "tokens": [51098, 761, 14543, 277, 2730, 3826, 12024, 21363, 278, 11, 16591, 17565, 20568, 1095, 5609, 2316, 84, 11, 538, 476, 39699, 2497, 27242, 64, 1221, 7949, 13208, 13037, 15568, 13, 51384], "temperature": 0.0, "avg_logprob": -0.1390828151328891, "compression_ratio": 1.4847560975609757, "no_speech_prob": 0.16522595286369324}, {"id": 146, "seek": 54690, "start": 567.54, "end": 571.06, "text": " No to wydaje si\u0119 oczywiste, \u017ce to zawsze powinno pomaga\u0107, prawda?", "tokens": [51396, 883, 281, 49165, 3244, 277, 6522, 86, 8375, 11, 3561, 281, 30964, 27310, 1771, 12991, 9286, 2162, 11, 43607, 30, 51572], "temperature": 0.0, "avg_logprob": -0.1390828151328891, "compression_ratio": 1.4847560975609757, "no_speech_prob": 0.16522595286369324}, {"id": 147, "seek": 54690, "start": 571.3, "end": 572.22, "text": " Wydawa\u0142oby si\u0119.", "tokens": [51584, 343, 6655, 10449, 1221, 13944, 3244, 13, 51630], "temperature": 0.0, "avg_logprob": -0.1390828151328891, "compression_ratio": 1.4847560975609757, "no_speech_prob": 0.16522595286369324}, {"id": 148, "seek": 54690, "start": 572.9, "end": 575.38, "text": " Ale badania pokaza\u0142y co\u015b zupe\u0142nie innego.", "tokens": [51664, 9366, 1578, 5609, 13010, 12257, 6825, 19241, 49922, 294, 11858, 13, 51788], "temperature": 0.0, "avg_logprob": -0.1390828151328891, "compression_ratio": 1.4847560975609757, "no_speech_prob": 0.16522595286369324}, {"id": 149, "seek": 57538, "start": 575.62, "end": 578.02, "text": " Chwila, to brzmi kompletnie absurdalnie.", "tokens": [50376, 761, 86, 7371, 11, 281, 738, 89, 3057, 5207, 14657, 2766, 19774, 304, 2766, 13, 50496], "temperature": 0.0, "avg_logprob": -0.1662895121472947, "compression_ratio": 1.4523809523809523, "no_speech_prob": 0.006473352666944265}, {"id": 150, "seek": 57538, "start": 578.58, "end": 582.58, "text": " M\u00f3wisz mi, \u017ce technika zwana dostrajaniem do instrukcji,", "tokens": [50524, 376, 3901, 23848, 2752, 11, 3561, 1537, 5439, 11873, 2095, 20568, 424, 14763, 4907, 360, 1058, 25126, 19649, 11, 50724], "temperature": 0.0, "avg_logprob": -0.1662895121472947, "compression_ratio": 1.4523809523809523, "no_speech_prob": 0.006473352666944265}, {"id": 151, "seek": 57538, "start": 582.78, "end": 586.3, "text": " w rzeczywisto\u015bci sprawia, \u017ce model gorzej wykonuje instrukcj\u0119?", "tokens": [50734, 261, 26297, 86, 9334, 6199, 22734, 654, 11, 3561, 2316, 24012, 16920, 46702, 13008, 1058, 25126, 41960, 30, 50910], "temperature": 0.0, "avg_logprob": -0.1662895121472947, "compression_ratio": 1.4523809523809523, "no_speech_prob": 0.006473352666944265}, {"id": 152, "seek": 57538, "start": 586.78, "end": 588.18, "text": " I jak to w og\u00f3le mo\u017cliwe?", "tokens": [50934, 286, 4207, 281, 261, 29229, 30854, 826, 30, 51004], "temperature": 0.0, "avg_logprob": -0.1662895121472947, "compression_ratio": 1.4523809523809523, "no_speech_prob": 0.006473352666944265}, {"id": 153, "seek": 57538, "start": 588.7, "end": 592.78, "text": " To tak jakby lekcja czytania sprawia\u0142a, \u017ce dziecko zaczyna zapomina\u0107 litery?", "tokens": [51030, 1407, 991, 28976, 30863, 34056, 6430, 83, 5609, 22734, 25605, 11, 3561, 17953, 41416, 43811, 629, 14223, 49217, 2162, 2733, 88, 30, 51234], "temperature": 0.0, "avg_logprob": -0.1662895121472947, "compression_ratio": 1.4523809523809523, "no_speech_prob": 0.006473352666944265}, {"id": 154, "seek": 57538, "start": 593.14, "end": 595.58, "text": " Co tam si\u0119 dzieje na fundamentalnym poziomie?", "tokens": [51252, 3066, 7677, 3244, 17953, 2884, 1667, 8088, 12996, 38503, 40120, 30, 51374], "temperature": 0.0, "avg_logprob": -0.1662895121472947, "compression_ratio": 1.4523809523809523, "no_speech_prob": 0.006473352666944265}, {"id": 155, "seek": 57538, "start": 595.9, "end": 596.9, "text": " Dok\u0142adnie tak.", "tokens": [51390, 29768, 10358, 2766, 991, 13, 51440], "temperature": 0.0, "avg_logprob": -0.1662895121472947, "compression_ratio": 1.4523809523809523, "no_speech_prob": 0.006473352666944265}, {"id": 156, "seek": 57538, "start": 597.3, "end": 602.3, "text": " Badania pokaza\u0142y, \u017ce w przypadku modeli mniejszych ni\u017c oko\u0142o 8 miliard\u00f3w parametr\u00f3w,", "tokens": [51460, 11523, 5609, 13010, 12257, 6825, 11, 3561, 261, 41955, 2316, 72, 39513, 45021, 28502, 45730, 5249, 1649, 1962, 72, 515, 3901, 6220, 27965, 3901, 11, 51710], "temperature": 0.0, "avg_logprob": -0.1662895121472947, "compression_ratio": 1.4523809523809523, "no_speech_prob": 0.006473352666944265}, {"id": 157, "seek": 60230, "start": 602.6999999999999, "end": 606.2199999999999, "text": " Instruction Fine Tuning pogarsa ich og\u00f3ln\u0105 wydajno\u015b\u0107.", "tokens": [50384, 2730, 3826, 12024, 21363, 278, 32037, 29720, 1893, 5360, 15741, 13113, 25984, 1805, 23293, 13, 50560], "temperature": 0.0, "avg_logprob": -0.1502491652599873, "compression_ratio": 1.3825301204819278, "no_speech_prob": 0.014091462828218937}, {"id": 158, "seek": 60230, "start": 606.4599999999999, "end": 607.5, "text": " Nie wiarygodne.", "tokens": [50572, 12016, 26393, 822, 21787, 716, 13, 50624], "temperature": 0.0, "avg_logprob": -0.1502491652599873, "compression_ratio": 1.3825301204819278, "no_speech_prob": 0.014091462828218937}, {"id": 159, "seek": 60230, "start": 607.8599999999999, "end": 610.5, "text": " Dopiero przy skali rz\u0119du 100 miliard\u00f3w parametr\u00f3w,", "tokens": [50642, 42657, 12030, 6501, 1110, 5103, 367, 11052, 769, 2319, 1962, 72, 515, 3901, 6220, 27965, 3901, 11, 50774], "temperature": 0.0, "avg_logprob": -0.1502491652599873, "compression_ratio": 1.3825301204819278, "no_speech_prob": 0.014091462828218937}, {"id": 160, "seek": 60230, "start": 610.78, "end": 614.3, "text": " ta technika zaczyna przynosi\u0107 oczekiwane pozytywne rezultaty.", "tokens": [50788, 1846, 1537, 5439, 43811, 629, 6501, 16751, 12757, 277, 3689, 14753, 86, 1929, 49358, 874, 86, 716, 48060, 723, 21398, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1502491652599873, "compression_ratio": 1.3825301204819278, "no_speech_prob": 0.014091462828218937}, {"id": 161, "seek": 60230, "start": 614.8199999999999, "end": 616.2199999999999, "text": " Co do pytania, dlaczego?", "tokens": [50990, 3066, 360, 25878, 5609, 11, 37873, 39329, 30, 51060], "temperature": 0.0, "avg_logprob": -0.1502491652599873, "compression_ratio": 1.3825301204819278, "no_speech_prob": 0.014091462828218937}, {"id": 162, "seek": 60230, "start": 616.42, "end": 618.54, "text": " Na razie nie mamy jednozmacznej odpowiedzi.", "tokens": [51070, 6056, 9639, 414, 2838, 17335, 5232, 1771, 89, 76, 14875, 11794, 36574, 3992, 13, 51176], "temperature": 0.0, "avg_logprob": -0.1502491652599873, "compression_ratio": 1.3825301204819278, "no_speech_prob": 0.014091462828218937}, {"id": 163, "seek": 60230, "start": 618.9399999999999, "end": 620.9, "text": " Artyku\u0142 przedstawia kilka hipotez.", "tokens": [51196, 1587, 874, 5279, 1221, 45616, 654, 36466, 8103, 1370, 89, 13, 51294], "temperature": 0.0, "avg_logprob": -0.1502491652599873, "compression_ratio": 1.3825301204819278, "no_speech_prob": 0.014091462828218937}, {"id": 164, "seek": 60230, "start": 621.2199999999999, "end": 621.8199999999999, "text": " Jakich?", "tokens": [51310, 15029, 480, 30, 51340], "temperature": 0.0, "avg_logprob": -0.1502491652599873, "compression_ratio": 1.3825301204819278, "no_speech_prob": 0.014091462828218937}, {"id": 165, "seek": 60230, "start": 621.8199999999999, "end": 625.02, "text": " Jedna z nich m\u00f3wi o g\u0142\u0119boko\u015bci obliczeniowej.", "tokens": [51340, 27076, 629, 710, 25570, 24592, 277, 18117, 1274, 65, 13704, 6199, 1111, 1050, 42124, 21091, 13, 51500], "temperature": 0.0, "avg_logprob": -0.1502491652599873, "compression_ratio": 1.3825301204819278, "no_speech_prob": 0.014091462828218937}, {"id": 166, "seek": 60230, "start": 625.42, "end": 628.9, "text": " Z\u0142o\u017cone zadania, wymagaj\u0105ce wiele etapowego rozumowania,", "tokens": [51520, 1176, 5249, 1427, 546, 42788, 5609, 11, 29764, 559, 11133, 384, 33137, 47634, 26576, 48797, 21308, 11, 51694], "temperature": 0.0, "avg_logprob": -0.1502491652599873, "compression_ratio": 1.3825301204819278, "no_speech_prob": 0.014091462828218937}, {"id": 167, "seek": 60230, "start": 629.18, "end": 631.78, "text": " potrzebuj\u0105 pewnej minimalnej liczby krok\u00f3w.", "tokens": [51708, 37595, 13263, 25889, 11794, 13206, 11794, 6169, 89, 2322, 45909, 23849, 13, 51838], "temperature": 0.0, "avg_logprob": -0.1502491652599873, "compression_ratio": 1.3825301204819278, "no_speech_prob": 0.014091462828218937}, {"id": 168, "seek": 63230, "start": 632.38, "end": 636.38, "text": " Mo\u017cliwe, \u017ce mniejszy model po prostu nie ma wystarczaj\u0105cej liczby warstw,", "tokens": [50368, 44736, 2081, 826, 11, 3561, 39513, 7706, 2316, 714, 19518, 2838, 463, 4628, 9710, 3689, 11133, 20811, 6169, 89, 2322, 1516, 372, 86, 11, 50568], "temperature": 0.0, "avg_logprob": -0.13224933572011452, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.0001595003850525245}, {"id": 169, "seek": 63230, "start": 636.5, "end": 641.9399999999999, "text": " czyli przestrzeni na przeprowadzenie tak z\u0142o\u017conych operacji.", "tokens": [50574, 16591, 44264, 81, 42124, 1667, 30829, 1892, 345, 16778, 991, 710, 5249, 1427, 2526, 339, 2208, 13152, 13, 50846], "temperature": 0.0, "avg_logprob": -0.13224933572011452, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.0001595003850525245}, {"id": 170, "seek": 63230, "start": 642.14, "end": 647.02, "text": " To ma sens, \u017ce potrzebna jest pewna g\u0142\u0119boko\u015b\u0107 do z\u0142o\u017conego my\u015blenia.", "tokens": [50856, 1407, 463, 2923, 11, 3561, 37595, 629, 3492, 25889, 629, 18117, 1274, 65, 13704, 7753, 360, 710, 5249, 1427, 546, 1571, 48633, 6698, 654, 13, 51100], "temperature": 0.0, "avg_logprob": -0.13224933572011452, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.0001595003850525245}, {"id": 171, "seek": 63230, "start": 647.5, "end": 649.6999999999999, "text": " Ale czy to nie jest troch\u0119 niepokoj\u0105ce?", "tokens": [51124, 9366, 6430, 281, 2838, 3492, 24926, 2838, 79, 13704, 8555, 384, 30, 51234], "temperature": 0.0, "avg_logprob": -0.13224933572011452, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.0001595003850525245}, {"id": 172, "seek": 63230, "start": 649.78, "end": 653.78, "text": " Oznacza\u0142oby to, \u017ce istnieje twarda fizyczna bariera,", "tokens": [51238, 422, 22672, 326, 2394, 1221, 13944, 281, 11, 3561, 1418, 2766, 2884, 683, 19218, 21000, 17466, 629, 2159, 10609, 11, 51438], "temperature": 0.0, "avg_logprob": -0.13224933572011452, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.0001595003850525245}, {"id": 173, "seek": 63230, "start": 654.14, "end": 656.78, "text": " kt\u00f3rej ma\u0142e modele nigdy nie przeskocz\u0105,", "tokens": [51456, 36023, 463, 19827, 4391, 306, 26996, 3173, 2838, 6541, 279, 74, 905, 8925, 11, 51588], "temperature": 0.0, "avg_logprob": -0.13224933572011452, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.0001595003850525245}, {"id": 174, "seek": 63230, "start": 656.9, "end": 659.5799999999999, "text": " niezale\u017cnie od tego, jak sprytnie je wytrenujemy.", "tokens": [51594, 33511, 45494, 2766, 3611, 8627, 11, 4207, 637, 627, 83, 2766, 1506, 261, 4328, 1095, 21767, 13, 51728], "temperature": 0.0, "avg_logprob": -0.13224933572011452, "compression_ratio": 1.457142857142857, "no_speech_prob": 0.0001595003850525245}, {"id": 175, "seek": 65958, "start": 660.0600000000001, "end": 665.1, "text": " Czy to skazuje mniejsze, bardziej dost\u0119pne modele na bycie g\u0142upszym i na zawsze?", "tokens": [50388, 19832, 281, 1110, 43317, 275, 44258, 11, 27209, 48209, 716, 4391, 306, 1667, 538, 4260, 18117, 1010, 7706, 76, 741, 1667, 30964, 30, 50640], "temperature": 0.0, "avg_logprob": -0.1562685285295759, "compression_ratio": 1.426056338028169, "no_speech_prob": 0.015169177204370499}, {"id": 176, "seek": 65958, "start": 665.82, "end": 668.5400000000001, "text": " Niekoniecznie i tu dochodzimy do kluczowego niuansu.", "tokens": [50676, 12016, 18295, 414, 19923, 741, 2604, 9243, 378, 89, 13189, 360, 9671, 1311, 89, 26576, 3867, 84, 599, 84, 13, 50812], "temperature": 0.0, "avg_logprob": -0.1562685285295759, "compression_ratio": 1.426056338028169, "no_speech_prob": 0.015169177204370499}, {"id": 177, "seek": 65958, "start": 669.0200000000001, "end": 672.14, "text": " Odpowied\u017a brzmi, sama skala to nie wszystko.", "tokens": [50836, 12210, 14701, 1091, 10659, 738, 89, 3057, 11, 17768, 1110, 5159, 281, 2838, 22607, 13, 50992], "temperature": 0.0, "avg_logprob": -0.1562685285295759, "compression_ratio": 1.426056338028169, "no_speech_prob": 0.015169177204370499}, {"id": 178, "seek": 65958, "start": 672.82, "end": 676.1, "text": " Artyku\u0142 jasno pokazuje, \u017ce lepsze jako\u015bci dane treningowe", "tokens": [51026, 1587, 874, 5279, 1221, 361, 296, 1771, 13010, 43317, 11, 3561, 476, 1878, 1381, 17123, 6199, 49206, 2192, 773, 6880, 51190], "temperature": 0.0, "avg_logprob": -0.1562685285295759, "compression_ratio": 1.426056338028169, "no_speech_prob": 0.015169177204370499}, {"id": 179, "seek": 65958, "start": 676.34, "end": 680.74, "text": " lub nowatorskie architektury mog\u0105 odblokowa\u0107 pewne zdolno\u015bci", "tokens": [51202, 15980, 586, 3391, 22872, 3912, 642, 2320, 2598, 34123, 3611, 5199, 453, 11445, 25889, 716, 16221, 401, 16438, 51422], "temperature": 0.0, "avg_logprob": -0.1562685285295759, "compression_ratio": 1.426056338028169, "no_speech_prob": 0.015169177204370499}, {"id": 180, "seek": 65958, "start": 680.74, "end": 685.26, "text": " w znacznie mniejszych modelach, czyli skala nie jest jedyny pokr\u0119t\u0142em,", "tokens": [51422, 261, 15397, 14875, 2766, 39513, 45021, 2316, 608, 11, 16591, 1110, 5159, 2838, 3492, 5232, 88, 1634, 13010, 81, 46788, 11126, 11, 51648], "temperature": 0.0, "avg_logprob": -0.1562685285295759, "compression_ratio": 1.426056338028169, "no_speech_prob": 0.015169177204370499}, {"id": 181, "seek": 65958, "start": 685.26, "end": 686.4200000000001, "text": " kt\u00f3rym mo\u017cemy kr\u0119ci\u0107.", "tokens": [51648, 30120, 26500, 15913, 1274, 39162, 13, 51706], "temperature": 0.0, "avg_logprob": -0.1562685285295759, "compression_ratio": 1.426056338028169, "no_speech_prob": 0.015169177204370499}, {"id": 182, "seek": 68642, "start": 686.6999999999999, "end": 689.54, "text": " Nie jest to jak z silnikiem samochodowym.", "tokens": [50378, 12016, 3492, 281, 4207, 710, 3425, 13123, 4907, 3247, 8997, 378, 31691, 13, 50520], "temperature": 0.0, "avg_logprob": -0.17970041795210404, "compression_ratio": 1.35, "no_speech_prob": 0.003409076016396284}, {"id": 183, "seek": 68642, "start": 689.66, "end": 693.5, "text": " Mo\u017cna zwi\u0119ksza\u0107 jego pojemno\u015b\u0107, ale mo\u017cna te\u017c wla\u0107 do niego lepsze paliwo.", "tokens": [50526, 44736, 629, 11873, 5034, 1694, 35873, 26542, 714, 30833, 23293, 11, 6775, 17790, 9516, 261, 875, 2162, 360, 49615, 476, 1878, 1381, 3984, 72, 6120, 13, 50718], "temperature": 0.0, "avg_logprob": -0.17970041795210404, "compression_ratio": 1.35, "no_speech_prob": 0.003409076016396284}, {"id": 184, "seek": 68642, "start": 693.9399999999999, "end": 695.6999999999999, "text": " Dobre por\u00f3wnanie, dok\u0142adnie.", "tokens": [50740, 29679, 265, 1515, 812, 895, 7155, 11, 45864, 2766, 13, 50828], "temperature": 0.0, "avg_logprob": -0.17970041795210404, "compression_ratio": 1.35, "no_speech_prob": 0.003409076016396284}, {"id": 185, "seek": 68642, "start": 695.6999999999999, "end": 702.66, "text": " Autorzy podaj\u0105 przyk\u0142ad model Palm o wielko\u015bci 62 miliard\u00f3w parametr\u00f3w.", "tokens": [50828, 6049, 284, 1229, 2497, 11133, 23144, 2316, 32668, 277, 20570, 4093, 6199, 24536, 1962, 72, 515, 3901, 6220, 27965, 3901, 13, 51176], "temperature": 0.0, "avg_logprob": -0.17970041795210404, "compression_ratio": 1.35, "no_speech_prob": 0.003409076016396284}, {"id": 186, "seek": 68642, "start": 703.26, "end": 707.26, "text": " Radzi\u0142 sobie z 14 zada\u0144ami z benchmarku Big Bench,", "tokens": [51206, 9654, 3992, 1221, 13652, 710, 3499, 710, 1538, 5248, 4526, 710, 18927, 84, 5429, 3964, 339, 11, 51406], "temperature": 0.0, "avg_logprob": -0.17970041795210404, "compression_ratio": 1.35, "no_speech_prob": 0.003409076016396284}, {"id": 187, "seek": 68642, "start": 707.54, "end": 713.4599999999999, "text": " z kt\u00f3rymi nie dawa\u0142y sobie rady znacznie wi\u0119ksze modele jak GPT-3 czy Lambda.", "tokens": [51420, 710, 9913, 3057, 2838, 1120, 4151, 6825, 13652, 367, 880, 15397, 14875, 2766, 29968, 1381, 4391, 306, 4207, 26039, 51, 12, 18, 6430, 45691, 13, 51716], "temperature": 0.0, "avg_logprob": -0.17970041795210404, "compression_ratio": 1.35, "no_speech_prob": 0.003409076016396284}, {"id": 188, "seek": 68642, "start": 713.74, "end": 714.42, "text": " A dlaczego?", "tokens": [51730, 316, 37873, 39329, 30, 51764], "temperature": 0.0, "avg_logprob": -0.17970041795210404, "compression_ratio": 1.35, "no_speech_prob": 0.003409076016396284}, {"id": 189, "seek": 71442, "start": 715.42, "end": 718.5, "text": " Prawdopodobn\u0105 przyczyn\u0105 by\u0142a w\u0142a\u015bnie jako\u015b\u0107 paliwa.", "tokens": [50414, 430, 15889, 46684, 996, 13113, 6501, 6522, 13113, 23936, 14234, 17123, 7753, 3984, 72, 4151, 13, 50568], "temperature": 0.0, "avg_logprob": -0.16386358896891276, "compression_ratio": 1.4735202492211839, "no_speech_prob": 0.0009460589499212801}, {"id": 190, "seek": 71442, "start": 718.86, "end": 724.26, "text": " Palm by\u0142 trenowany na lepszych danych z wi\u0119kszym udzia\u0142em kodu i materia\u0142\u00f3w wieloj\u0119zycznych.", "tokens": [50586, 32668, 16673, 23136, 23341, 1667, 476, 1878, 28051, 274, 34644, 710, 29968, 26681, 11727, 89, 36368, 350, 34873, 741, 2389, 8908, 3901, 20570, 78, 11115, 1229, 3689, 9399, 13, 50856], "temperature": 0.0, "avg_logprob": -0.16386358896891276, "compression_ratio": 1.4735202492211839, "no_speech_prob": 0.0009460589499212801}, {"id": 191, "seek": 71442, "start": 724.74, "end": 728.78, "text": " Co wi\u0119cej, gdy jaka\u015b zdolno\u015b\u0107 zostanie ju\u017c odkryta w du\u017cym modelu,", "tokens": [50880, 3066, 26004, 11, 28405, 4207, 64, 1788, 16221, 401, 23293, 31873, 7155, 10678, 3611, 43298, 1328, 261, 21783, 4199, 2316, 84, 11, 51082], "temperature": 0.0, "avg_logprob": -0.16386358896891276, "compression_ratio": 1.4735202492211839, "no_speech_prob": 0.0009460589499212801}, {"id": 192, "seek": 71442, "start": 728.9799999999999, "end": 732.8199999999999, "text": " badacze cz\u0119sto znajduj\u0105 sposoby, by nauczy\u0107 jej mniejsze modele,", "tokens": [51092, 1578, 326, 1381, 34369, 47570, 8555, 20443, 13944, 11, 538, 49103, 27150, 28924, 275, 44258, 4391, 306, 11, 51284], "temperature": 0.0, "avg_logprob": -0.16386358896891276, "compression_ratio": 1.4735202492211839, "no_speech_prob": 0.0009460589499212801}, {"id": 193, "seek": 71442, "start": 733.06, "end": 735.62, "text": " np. przez bardziej celowany fine tuning.", "tokens": [51296, 33808, 13, 14064, 27209, 9277, 23341, 2489, 15164, 13, 51424], "temperature": 0.0, "avg_logprob": -0.16386358896891276, "compression_ratio": 1.4735202492211839, "no_speech_prob": 0.0009460589499212801}, {"id": 194, "seek": 71442, "start": 735.9799999999999, "end": 740.18, "text": " OK, czyli mamy te nieprzewidywalne dobre zdolno\u015bci, kt\u00f3re si\u0119 pojawiaj\u0105.", "tokens": [51442, 2264, 11, 16591, 17335, 535, 2838, 1424, 43551, 327, 27112, 304, 716, 41959, 16221, 401, 16438, 11, 8864, 3244, 30655, 48125, 13, 51652], "temperature": 0.0, "avg_logprob": -0.16386358896891276, "compression_ratio": 1.4735202492211839, "no_speech_prob": 0.0009460589499212801}, {"id": 195, "seek": 71442, "start": 740.5799999999999, "end": 743.2199999999999, "text": " Ale te drzy musz\u0105 si\u0119 otwiera\u0107 w obie strony, prawda?", "tokens": [51672, 9366, 535, 1224, 1229, 1038, 8925, 3244, 4337, 86, 10609, 2162, 261, 1111, 414, 32406, 11, 43607, 30, 51804], "temperature": 0.0, "avg_logprob": -0.16386358896891276, "compression_ratio": 1.4735202492211839, "no_speech_prob": 0.0009460589499212801}, {"id": 196, "seek": 74322, "start": 743.34, "end": 747.3000000000001, "text": " Je\u015bli dobre rzeczy mog\u0105 pojawi\u0107 si\u0119 znik\u0105d, to co zeszkodliwem.", "tokens": [50370, 37086, 41959, 26297, 34123, 30655, 12757, 3244, 710, 13123, 18962, 11, 281, 598, 710, 10430, 74, 378, 2081, 86, 443, 13, 50568], "temperature": 0.0, "avg_logprob": -0.1720073539893944, "compression_ratio": 1.487719298245614, "no_speech_prob": 0.002831551479175687}, {"id": 197, "seek": 74322, "start": 747.5400000000001, "end": 752.82, "text": " To jest druga mroczniejsza strona medalu, kt\u00f3r\u0105 autorzy nazywaj\u0105 emergent risks,", "tokens": [50580, 1407, 3492, 4110, 64, 275, 340, 3689, 30295, 2394, 1056, 4037, 1205, 4929, 11, 37415, 19510, 1229, 20151, 27112, 11133, 4345, 6930, 10888, 11, 50844], "temperature": 0.0, "avg_logprob": -0.1720073539893944, "compression_ratio": 1.487719298245614, "no_speech_prob": 0.002831551479175687}, {"id": 198, "seek": 74322, "start": 752.9, "end": 755.4200000000001, "text": " czyli ryzykami emergentnymi.", "tokens": [50848, 16591, 20791, 1229, 48737, 4345, 6930, 31813, 13, 50974], "temperature": 0.0, "avg_logprob": -0.1720073539893944, "compression_ratio": 1.487719298245614, "no_speech_prob": 0.002831551479175687}, {"id": 199, "seek": 74322, "start": 755.86, "end": 760.7, "text": " Wraz ze skal\u0105 mog\u0105 nieprzewidywalnie rosn\u0105\u0107 problemy takie jak generowanie", "tokens": [50996, 343, 30695, 5277, 16890, 1611, 34123, 2838, 1424, 43551, 327, 27112, 304, 2766, 18953, 13113, 2162, 1154, 88, 15963, 4207, 1337, 22028, 51238], "temperature": 0.0, "avg_logprob": -0.1720073539893944, "compression_ratio": 1.487719298245614, "no_speech_prob": 0.002831551479175687}, {"id": 200, "seek": 74322, "start": 760.74, "end": 766.26, "text": " dezinformacji, wzmacnianie uprzedze\u0144, czyli bias, czy toksyczno\u015b\u0107 j\u0119zyka.", "tokens": [51240, 368, 23584, 837, 13152, 11, 24809, 37065, 77, 952, 414, 493, 81, 11312, 49689, 11, 16591, 12577, 11, 6430, 281, 1694, 17466, 23293, 42309, 40940, 13, 51516], "temperature": 0.0, "avg_logprob": -0.1720073539893944, "compression_ratio": 1.487719298245614, "no_speech_prob": 0.002831551479175687}, {"id": 201, "seek": 74322, "start": 766.5400000000001, "end": 769.62, "text": " Czyli niekt\u00f3re wady modelu nie malej\u0105 ze skal\u0105?", "tokens": [51530, 37099, 2838, 43073, 265, 261, 880, 2316, 84, 2838, 7133, 8555, 5277, 16890, 1611, 30, 51684], "temperature": 0.0, "avg_logprob": -0.1720073539893944, "compression_ratio": 1.487719298245614, "no_speech_prob": 0.002831551479175687}, {"id": 202, "seek": 74322, "start": 769.78, "end": 771.74, "text": " Wr\u0119cz przeciwnie, nasilaj\u0105 si\u0119.", "tokens": [51692, 10159, 1274, 3689, 39622, 14215, 11, 5382, 388, 11133, 3244, 13, 51790], "temperature": 0.0, "avg_logprob": -0.1720073539893944, "compression_ratio": 1.487719298245614, "no_speech_prob": 0.002831551479175687}, {"id": 203, "seek": 77174, "start": 771.98, "end": 775.42, "text": " S\u0105 na to jakie\u015b twarde dowody, podobne do tych z arytmetyk\u0105?", "tokens": [50376, 318, 1611, 1667, 281, 31163, 683, 10866, 9459, 843, 11, 43024, 716, 360, 15180, 710, 594, 4328, 76, 2210, 26304, 30, 50548], "temperature": 0.0, "avg_logprob": -0.16429843073305878, "compression_ratio": 1.3581560283687943, "no_speech_prob": 0.003283441299572587}, {"id": 204, "seek": 77174, "start": 775.5, "end": 780.14, "text": " Tak. W tym samym te\u015bcie Truthful QA, o kt\u00f3rym m\u00f3wili\u015bmy,", "tokens": [50552, 9118, 13, 343, 8107, 3247, 4199, 535, 9815, 20522, 906, 1249, 32, 11, 277, 30120, 13489, 43912, 11, 50784], "temperature": 0.0, "avg_logprob": -0.16429843073305878, "compression_ratio": 1.3581560283687943, "no_speech_prob": 0.003283441299572587}, {"id": 205, "seek": 77174, "start": 780.58, "end": 785.98, "text": " zauwa\u017cono, \u017ce wi\u0119ksze modele GPT-3, chocia\u017c stawa\u0142y j\u0105 inteligentniejsze,", "tokens": [50806, 710, 1459, 27111, 8957, 11, 3561, 29968, 1381, 4391, 306, 26039, 51, 12, 18, 11, 48929, 342, 10449, 6825, 35692, 24777, 25002, 44258, 11, 51076], "temperature": 0.0, "avg_logprob": -0.16429843073305878, "compression_ratio": 1.3581560283687943, "no_speech_prob": 0.003283441299572587}, {"id": 206, "seek": 77174, "start": 786.42, "end": 791.74, "text": " by\u0142y jednocze\u015bnie bardziej sk\u0142onne do na\u015bladowania i powtarzania powszechnych ludzkich fa\u0142sz\u00f3w.", "tokens": [51098, 26366, 5232, 26694, 1381, 12221, 27209, 1110, 1221, 22419, 360, 1667, 1788, 9290, 21308, 741, 3388, 23480, 89, 5609, 280, 1509, 1381, 1377, 16384, 15946, 30154, 480, 2050, 1221, 15453, 3901, 13, 51364], "temperature": 0.0, "avg_logprob": -0.16429843073305878, "compression_ratio": 1.3581560283687943, "no_speech_prob": 0.003283441299572587}, {"id": 207, "seek": 77174, "start": 791.98, "end": 792.7, "text": " Ciekawe.", "tokens": [51376, 383, 414, 2330, 826, 13, 51412], "temperature": 0.0, "avg_logprob": -0.16429843073305878, "compression_ratio": 1.3581560283687943, "no_speech_prob": 0.003283441299572587}, {"id": 208, "seek": 77174, "start": 792.82, "end": 797.82, "text": " Inny benchmark, BBQ, pokaza\u0142, \u017ce w niejednoznacznych kontekstach,", "tokens": [51418, 682, 1634, 18927, 11, 19168, 48, 11, 13010, 12257, 1221, 11, 3561, 261, 2838, 40543, 1771, 22672, 14875, 9399, 14373, 916, 372, 608, 11, 51668], "temperature": 0.0, "avg_logprob": -0.16429843073305878, "compression_ratio": 1.3581560283687943, "no_speech_prob": 0.003283441299572587}, {"id": 209, "seek": 79782, "start": 797.94, "end": 802.7800000000001, "text": " kt\u00f3re mog\u0105 prowokowa\u0107 stereotypowa my\u015blenie uprzedzenia w odpowiedziach modeli,", "tokens": [50370, 8864, 34123, 45553, 453, 11445, 41182, 79, 5528, 48633, 6698, 414, 493, 81, 11312, 14320, 261, 36574, 3992, 608, 2316, 72, 11, 50612], "temperature": 0.0, "avg_logprob": -0.16210574319917861, "compression_ratio": 1.5270758122743682, "no_speech_prob": 0.015025407075881958}, {"id": 210, "seek": 79782, "start": 802.82, "end": 804.4200000000001, "text": " rosn\u0105 wraz z ich skal\u0105.", "tokens": [50614, 18953, 13113, 7843, 89, 710, 1893, 16890, 1611, 13, 50694], "temperature": 0.0, "avg_logprob": -0.16210574319917861, "compression_ratio": 1.5270758122743682, "no_speech_prob": 0.015025407075881958}, {"id": 211, "seek": 79782, "start": 804.7800000000001, "end": 806.0600000000001, "text": " A co z prywatno\u015bci\u0105?", "tokens": [50712, 316, 598, 710, 582, 27112, 267, 16438, 1611, 30, 50776], "temperature": 0.0, "avg_logprob": -0.16210574319917861, "compression_ratio": 1.5270758122743682, "no_speech_prob": 0.015025407075881958}, {"id": 212, "seek": 79782, "start": 806.46, "end": 808.5, "text": " Wi\u0119kszy model to wi\u0119ksza pami\u0119\u0107.", "tokens": [50796, 30127, 1694, 1229, 2316, 281, 29968, 2394, 31088, 2162, 13, 50898], "temperature": 0.0, "avg_logprob": -0.16210574319917861, "compression_ratio": 1.5270758122743682, "no_speech_prob": 0.015025407075881958}, {"id": 213, "seek": 79782, "start": 808.5400000000001, "end": 811.3000000000001, "text": " No i to jest kolejny emergent risk.", "tokens": [50900, 883, 741, 281, 3492, 23749, 1634, 4345, 6930, 3148, 13, 51038], "temperature": 0.0, "avg_logprob": -0.16210574319917861, "compression_ratio": 1.5270758122743682, "no_speech_prob": 0.015025407075881958}, {"id": 214, "seek": 79782, "start": 811.58, "end": 817.1400000000001, "text": " Wi\u0119ksze modele s\u0105 znacznie bardziej podatne na zapami\u0119tywanie i odtwarzanie fragment\u00f3w", "tokens": [51052, 30127, 1694, 1381, 4391, 306, 9015, 15397, 14875, 2766, 27209, 2497, 267, 716, 1667, 14223, 23806, 874, 86, 7155, 741, 3611, 83, 31991, 7155, 26424, 3901, 51330], "temperature": 0.0, "avg_logprob": -0.16210574319917861, "compression_ratio": 1.5270758122743682, "no_speech_prob": 0.015025407075881958}, {"id": 215, "seek": 79782, "start": 817.1800000000001, "end": 818.62, "text": " swoich danych treningowych.", "tokens": [51332, 13291, 480, 274, 34644, 2192, 773, 19605, 13, 51404], "temperature": 0.0, "avg_logprob": -0.16210574319917861, "compression_ratio": 1.5270758122743682, "no_speech_prob": 0.015025407075881958}, {"id": 216, "seek": 79782, "start": 819.1800000000001, "end": 823.98, "text": " Im wi\u0119ksza pami\u0119\u0107, tym wi\u0119ksza szansa, \u017ce model dos\u0142ownie odtworzy, czy i\u015b prywatny e-mail,", "tokens": [51432, 4331, 29968, 2394, 31088, 2162, 11, 8107, 29968, 2394, 7870, 38734, 11, 3561, 2316, 4491, 1221, 648, 414, 3611, 20270, 284, 1229, 11, 6430, 741, 1788, 582, 27112, 267, 1634, 308, 12, 11799, 11, 51672], "temperature": 0.0, "avg_logprob": -0.16210574319917861, "compression_ratio": 1.5270758122743682, "no_speech_prob": 0.015025407075881958}, {"id": 217, "seek": 82398, "start": 823.98, "end": 827.98, "text": " czy dane medyczne, kt\u00f3re przypadkiem znalaz\u0142y si\u0119 w zbiorze treningowym.", "tokens": [50364, 6430, 49206, 1205, 17466, 716, 11, 8864, 33100, 26116, 710, 4660, 921, 6825, 3244, 261, 710, 33362, 1381, 2192, 773, 31691, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1467706750078899, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.06585881114006042}, {"id": 218, "seek": 82398, "start": 828.26, "end": 829.7, "text": " To powa\u017cne zagro\u017cenie.", "tokens": [50578, 1407, 3388, 18264, 716, 27001, 340, 41118, 13, 50650], "temperature": 0.0, "avg_logprob": -0.1467706750078899, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.06585881114006042}, {"id": 219, "seek": 82398, "start": 829.94, "end": 834.74, "text": " Skoro ju\u017c wiemy, \u017ce mapa rozwoju AI, kt\u00f3r\u0105 mia\u0142y\u015bmy, jest b\u0142\u0119dna,", "tokens": [50662, 7324, 10780, 10678, 3355, 2226, 11, 3561, 44025, 9544, 6120, 8954, 7318, 11, 37415, 21290, 6825, 10513, 11, 3492, 272, 1221, 6298, 629, 11, 50902], "temperature": 0.0, "avg_logprob": -0.1467706750078899, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.06585881114006042}, {"id": 220, "seek": 82398, "start": 834.86, "end": 836.7, "text": " to gdzie powinni\u015bmy i\u015b\u0107 dalej?", "tokens": [50908, 281, 18922, 27310, 3722, 10513, 741, 7753, 34257, 30, 51000], "temperature": 0.0, "avg_logprob": -0.1467706750078899, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.06585881114006042}, {"id": 221, "seek": 82398, "start": 836.86, "end": 839.38, "text": " Jakie kierunki bada\u0144 wytycza ten artyku\u0142?", "tokens": [51008, 15029, 414, 38767, 409, 2984, 272, 1538, 5248, 4628, 874, 41524, 2064, 594, 874, 5279, 1221, 30, 51134], "temperature": 0.0, "avg_logprob": -0.1467706750078899, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.06585881114006042}, {"id": 222, "seek": 82398, "start": 839.58, "end": 841.82, "text": " Po pierwsze, oczywi\u015bcie dalsze skalowanie.", "tokens": [51144, 6165, 45994, 11, 23862, 274, 1124, 1381, 16890, 22028, 13, 51256], "temperature": 0.0, "avg_logprob": -0.1467706750078899, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.06585881114006042}, {"id": 223, "seek": 82398, "start": 842.3000000000001, "end": 848.14, "text": " Wci\u0105\u017c jest wiele zdolno\u015bci np. abstrakcyjne rozumowanie potrzebne do gry w szachy na poziomie strategicznym,", "tokens": [51280, 343, 537, 27242, 3492, 33137, 16221, 401, 16438, 33808, 13, 10823, 11272, 42949, 716, 48797, 22028, 37595, 716, 360, 41974, 261, 7870, 608, 88, 1667, 38503, 40120, 10924, 89, 12996, 11, 51572], "temperature": 0.0, "avg_logprob": -0.1467706750078899, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.06585881114006042}, {"id": 224, "seek": 82398, "start": 848.4200000000001, "end": 849.82, "text": " kt\u00f3re jeszcze si\u0119 nie pojawi\u0142y.", "tokens": [51586, 8864, 14168, 3244, 2838, 30655, 72, 6825, 13, 51656], "temperature": 0.0, "avg_logprob": -0.1467706750078899, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.06585881114006042}, {"id": 225, "seek": 82398, "start": 850.0600000000001, "end": 852.26, "text": " By\u0107 mo\u017ce czekaj\u0105 za kolejnym progiem skali.", "tokens": [51668, 3146, 2162, 12034, 6472, 916, 11133, 7949, 23749, 12996, 447, 70, 4907, 1110, 5103, 13, 51778], "temperature": 0.0, "avg_logprob": -0.1467706750078899, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.06585881114006042}, {"id": 226, "seek": 85226, "start": 853.26, "end": 857.9, "text": " Ale co wa\u017cniejsze, poszukiwanie bardziej efektywnych sposob\u00f3w na osi\u0105gni\u0119cie tej skali.", "tokens": [50414, 9366, 598, 27777, 44258, 11, 1366, 89, 11788, 86, 7155, 27209, 31482, 916, 874, 895, 16384, 20443, 996, 3901, 1667, 3003, 11404, 70, 35938, 4260, 12573, 1110, 5103, 13, 50646], "temperature": 0.0, "avg_logprob": -0.1325684514538995, "compression_ratio": 1.3745928338762214, "no_speech_prob": 0.012620905414223671}, {"id": 227, "seek": 85226, "start": 858.34, "end": 860.58, "text": " I tu wchodz\u0105 w gr\u0119 pomys\u0142owe architektury.", "tokens": [50668, 286, 2604, 261, 29914, 8925, 261, 677, 1274, 12991, 39508, 6880, 3912, 642, 2320, 2598, 13, 50780], "temperature": 0.0, "avg_logprob": -0.1325684514538995, "compression_ratio": 1.3745928338762214, "no_speech_prob": 0.012620905414223671}, {"id": 228, "seek": 85226, "start": 861.02, "end": 864.02, "text": " Pomy\u015blmy np. o Sparse Mixture of Experts.", "tokens": [50802, 430, 8488, 19212, 2226, 33808, 13, 277, 1738, 11668, 10204, 8890, 295, 12522, 1373, 13, 50952], "temperature": 0.0, "avg_logprob": -0.1325684514538995, "compression_ratio": 1.3745928338762214, "no_speech_prob": 0.012620905414223671}, {"id": 229, "seek": 85226, "start": 864.18, "end": 864.98, "text": " Jak to dzia\u0142a?", "tokens": [50960, 15029, 281, 37903, 30, 51000], "temperature": 0.0, "avg_logprob": -0.1325684514538995, "compression_ratio": 1.3745928338762214, "no_speech_prob": 0.012620905414223671}, {"id": 230, "seek": 85226, "start": 865.3, "end": 869.74, "text": " Zamiast jednego, gigantycznego m\u00f3zgu, kt\u00f3ry musi zna\u0107 si\u0119 na wszystkim,", "tokens": [51016, 1176, 4526, 525, 5232, 11858, 11, 8741, 394, 17466, 11858, 32515, 89, 2794, 11, 9913, 37587, 710, 629, 2162, 3244, 1667, 30481, 11, 51238], "temperature": 0.0, "avg_logprob": -0.1325684514538995, "compression_ratio": 1.3745928338762214, "no_speech_prob": 0.012620905414223671}, {"id": 231, "seek": 85226, "start": 870.22, "end": 873.74, "text": " to tak jakby\u015bmy mieli zesp\u00f3\u0142 wyspecjalizowanych ekspert\u00f3w.", "tokens": [51262, 281, 991, 28976, 10513, 41214, 710, 13361, 16181, 27062, 494, 66, 22600, 590, 23341, 339, 30724, 15346, 3901, 13, 51438], "temperature": 0.0, "avg_logprob": -0.1325684514538995, "compression_ratio": 1.3745928338762214, "no_speech_prob": 0.012620905414223671}, {"id": 232, "seek": 85226, "start": 874.18, "end": 878.98, "text": " Kiedy pojawia si\u0119 problem, aktywujemy tylko tego jednego, odpowiedniego specjalist\u0119.", "tokens": [51460, 591, 16446, 30655, 654, 3244, 1154, 11, 9308, 874, 86, 21767, 13219, 8627, 5232, 11858, 11, 36574, 2766, 1571, 46433, 468, 1274, 13, 51700], "temperature": 0.0, "avg_logprob": -0.1325684514538995, "compression_ratio": 1.3745928338762214, "no_speech_prob": 0.012620905414223671}, {"id": 233, "seek": 87898, "start": 879.62, "end": 884.14, "text": " To pozwala budowa\u0107 modele z bilionami parametr\u00f3w, kt\u00f3re s\u0105 znacznie ta\u0144sze w u\u017cyciu,", "tokens": [50396, 1407, 40557, 5159, 3265, 11445, 4391, 306, 710, 8588, 313, 4526, 6220, 27965, 3901, 11, 8864, 9015, 15397, 14875, 2766, 1846, 5248, 82, 1381, 261, 34097, 30795, 11, 50622], "temperature": 0.0, "avg_logprob": -0.14365115064255735, "compression_ratio": 1.4033898305084747, "no_speech_prob": 0.01583695225417614}, {"id": 234, "seek": 87898, "start": 884.3000000000001, "end": 887.0600000000001, "text": " bo w danym momencie pracuje tylko ma\u0142ych ich fragment.", "tokens": [50630, 748, 261, 274, 1325, 76, 40883, 22404, 13008, 13219, 463, 47655, 1893, 26424, 13, 50768], "temperature": 0.0, "avg_logprob": -0.14365115064255735, "compression_ratio": 1.4033898305084747, "no_speech_prob": 0.01583695225417614}, {"id": 235, "seek": 87898, "start": 887.34, "end": 893.3000000000001, "text": " Czyli inteligencja rozproszona zamiast monolitycznej, a opr\u00f3cz samej architektury.", "tokens": [50782, 37099, 24777, 3213, 34056, 9544, 1424, 329, 13383, 710, 4526, 525, 1108, 401, 507, 3689, 11794, 11, 257, 999, 11721, 3689, 912, 73, 3912, 642, 2320, 2598, 13, 51080], "temperature": 0.0, "avg_logprob": -0.14365115064255735, "compression_ratio": 1.4033898305084747, "no_speech_prob": 0.01583695225417614}, {"id": 236, "seek": 87898, "start": 893.5, "end": 897.1800000000001, "text": " Dwa pozosta\u0142e kluczowe obszary, to dane i interakcja.", "tokens": [51090, 413, 4151, 21281, 8638, 19827, 9671, 1311, 89, 6880, 3181, 89, 822, 11, 281, 49206, 741, 728, 514, 34056, 13, 51274], "temperature": 0.0, "avg_logprob": -0.14365115064255735, "compression_ratio": 1.4033898305084747, "no_speech_prob": 0.01583695225417614}, {"id": 237, "seek": 87898, "start": 897.7, "end": 901.5, "text": " Potrzebujemy jeszcze wi\u0119kszych i co wa\u017cniejsze czystszych zbior\u00f3w danych.", "tokens": [51300, 9145, 13503, 65, 21767, 14168, 29968, 28051, 741, 598, 27777, 44258, 6430, 372, 45021, 710, 33362, 3901, 274, 34644, 13, 51490], "temperature": 0.0, "avg_logprob": -0.14365115064255735, "compression_ratio": 1.4033898305084747, "no_speech_prob": 0.01583695225417614}, {"id": 238, "seek": 87898, "start": 902.02, "end": 904.54, "text": " I musimy g\u0142\u0119biej zrozumie\u0107 mechanizmy prompting,", "tokens": [51516, 286, 43449, 18117, 1274, 7392, 73, 710, 27857, 449, 414, 2162, 4236, 590, 2226, 12391, 278, 11, 51642], "temperature": 0.0, "avg_logprob": -0.14365115064255735, "compression_ratio": 1.4033898305084747, "no_speech_prob": 0.01583695225417614}, {"id": 239, "seek": 90454, "start": 904.66, "end": 906.98, "text": " zw\u0142aszcza te bardziej z\u0142o\u017cone jak Chain of Thought,", "tokens": [50370, 11873, 1221, 19601, 41524, 535, 27209, 710, 5249, 1427, 546, 4207, 33252, 295, 23058, 11, 50486], "temperature": 0.0, "avg_logprob": -0.13755980232693502, "compression_ratio": 1.4294871794871795, "no_speech_prob": 0.008075556717813015}, {"id": 240, "seek": 90454, "start": 907.26, "end": 911.0999999999999, "text": " \u017ceby m\u00f3c efektywniej wydobywa\u0107 zdolno\u015bci z ju\u017c istniej\u0105cych modeli.", "tokens": [50500, 11316, 32515, 66, 31482, 916, 874, 895, 7764, 25984, 13944, 25234, 16221, 401, 16438, 710, 10678, 1418, 2766, 8555, 31306, 2316, 72, 13, 50692], "temperature": 0.0, "avg_logprob": -0.13755980232693502, "compression_ratio": 1.4294871794871795, "no_speech_prob": 0.008075556717813015}, {"id": 241, "seek": 90454, "start": 911.3399999999999, "end": 914.5799999999999, "text": " My\u015bl\u0119, \u017ce kluczowy wniosek z tej analizy jest taki,", "tokens": [50704, 1222, 28749, 11, 3561, 9671, 1311, 89, 10089, 261, 3722, 541, 74, 710, 12573, 2624, 590, 88, 3492, 20065, 11, 50866], "temperature": 0.0, "avg_logprob": -0.13755980232693502, "compression_ratio": 1.4294871794871795, "no_speech_prob": 0.008075556717813015}, {"id": 242, "seek": 90454, "start": 914.9399999999999, "end": 920.26, "text": " \u017ce droga rozwo\u0142u sztucznej inteligencji nie jest g\u0142adk\u0105, przewidywaln\u0105 autostrad\u0105.", "tokens": [50884, 3561, 3789, 3680, 9544, 6120, 24066, 262, 2682, 1311, 89, 11794, 24777, 3213, 19649, 2838, 3492, 290, 10358, 26304, 11, 39758, 327, 27112, 304, 13113, 1476, 555, 6206, 1611, 13, 51150], "temperature": 0.0, "avg_logprob": -0.13755980232693502, "compression_ratio": 1.4294871794871795, "no_speech_prob": 0.008075556717813015}, {"id": 243, "seek": 90454, "start": 920.66, "end": 927.42, "text": " To bardziej dzika, nieodkryta kraina, pe\u0142na niespodzianek, skok\u00f3w i w\u0142a\u015bnie przej\u015b\u0107 fazowych.", "tokens": [51170, 1407, 27209, 9758, 5439, 11, 2838, 378, 43298, 1328, 28248, 1426, 11, 43205, 629, 48100, 79, 14543, 282, 916, 11, 1110, 453, 3901, 741, 14234, 8325, 44536, 4375, 19605, 13, 51508], "temperature": 0.0, "avg_logprob": -0.13755980232693502, "compression_ratio": 1.4294871794871795, "no_speech_prob": 0.008075556717813015}, {"id": 244, "seek": 90454, "start": 927.8199999999999, "end": 928.54, "text": " Dok\u0142adnie.", "tokens": [51528, 29768, 10358, 2766, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13755980232693502, "compression_ratio": 1.4294871794871795, "no_speech_prob": 0.008075556717813015}, {"id": 245, "seek": 90454, "start": 928.98, "end": 932.5, "text": " W kt\u00f3rych nagle pojawiaj\u0105 si\u0119 zupe\u0142nie nowe mo\u017cliwo\u015bci.", "tokens": [51586, 343, 30382, 297, 15088, 30655, 48125, 3244, 49922, 586, 68, 30854, 36476, 13, 51762], "temperature": 0.0, "avg_logprob": -0.13755980232693502, "compression_ratio": 1.4294871794871795, "no_speech_prob": 0.008075556717813015}, {"id": 246, "seek": 93250, "start": 932.9, "end": 938.86, "text": " Okazuje si\u0119, \u017ce skalowanie modeli to nie tylko kwestia in\u017cynierii i dok\u0142adania mocy obliczeniowej.", "tokens": [50384, 3477, 43317, 3244, 11, 3561, 16890, 22028, 2316, 72, 281, 2838, 13219, 42035, 654, 294, 1427, 2534, 811, 5597, 741, 360, 15317, 5609, 705, 1344, 1111, 1050, 42124, 21091, 13, 50682], "temperature": 0.0, "avg_logprob": -0.18165153144990037, "compression_ratio": 1.5017182130584192, "no_speech_prob": 0.012799680233001709}, {"id": 247, "seek": 93250, "start": 939.3, "end": 943.34, "text": " To tak\u017ce, a mo\u017ce przede wszystkim, proces odkrywania nieznanego.", "tokens": [50704, 1407, 23306, 11, 257, 12034, 44786, 30481, 11, 17565, 3611, 43298, 86, 5609, 2838, 22672, 282, 6308, 13, 50906], "temperature": 0.0, "avg_logprob": -0.18165153144990037, "compression_ratio": 1.5017182130584192, "no_speech_prob": 0.012799680233001709}, {"id": 248, "seek": 93250, "start": 943.58, "end": 948.62, "text": " I jest jeszcze jedna, prowokacyjna my\u015bl na koniec, kt\u00f3ra wykracza poza sam\u0105 technologi\u0119.", "tokens": [50918, 286, 3492, 14168, 5232, 629, 11, 45553, 453, 31285, 629, 452, 19212, 1667, 5897, 35733, 11, 19456, 39287, 12080, 2394, 714, 2394, 3247, 1611, 1537, 1132, 5034, 13, 51170], "temperature": 0.0, "avg_logprob": -0.18165153144990037, "compression_ratio": 1.5017182130584192, "no_speech_prob": 0.012799680233001709}, {"id": 249, "seek": 93250, "start": 949.26, "end": 951.94, "text": " Artyku\u0142 skupia si\u0119 na emergencji w zachowaniu modeli,", "tokens": [51202, 1587, 874, 5279, 1221, 1110, 1010, 654, 3244, 1667, 33983, 19649, 261, 29303, 305, 25849, 2316, 72, 11, 51336], "temperature": 0.0, "avg_logprob": -0.18165153144990037, "compression_ratio": 1.5017182130584192, "no_speech_prob": 0.012799680233001709}, {"id": 250, "seek": 93250, "start": 952.34, "end": 956.62, "text": " ale w dyskusji autorzy wspominaj\u0105 o innym, fascynuj\u0105cym typie emergencji,", "tokens": [51356, 6775, 261, 15243, 35080, 4013, 19510, 1229, 17757, 49217, 8555, 277, 294, 12996, 11, 30632, 1344, 77, 13263, 1344, 76, 2125, 414, 33983, 19649, 11, 51570], "temperature": 0.0, "avg_logprob": -0.18165153144990037, "compression_ratio": 1.5017182130584192, "no_speech_prob": 0.012799680233001709}, {"id": 251, "seek": 93250, "start": 956.82, "end": 958.42, "text": " emergencji socjologicznej.", "tokens": [51580, 33983, 19649, 13598, 73, 1132, 17946, 11794, 13, 51660], "temperature": 0.0, "avg_logprob": -0.18165153144990037, "compression_ratio": 1.5017182130584192, "no_speech_prob": 0.012799680233001709}, {"id": 252, "seek": 93250, "start": 958.66, "end": 959.74, "text": " Sociologicznej?", "tokens": [51672, 407, 537, 1132, 17946, 11794, 30, 51726], "temperature": 0.0, "avg_logprob": -0.18165153144990037, "compression_ratio": 1.5017182130584192, "no_speech_prob": 0.012799680233001709}, {"id": 253, "seek": 95974, "start": 959.86, "end": 965.1800000000001, "text": " Tak. Zwracaj\u0105 uwag\u0119, \u017ce samo pojawienie si\u0119 tych pot\u0119\u017cnych, uniwersalnych modeli j\u0119zykowych", "tokens": [50370, 9118, 13, 1176, 7449, 326, 11133, 43696, 11, 3561, 36422, 30655, 27385, 3244, 15180, 1847, 1274, 1427, 9399, 11, 36435, 5364, 304, 9399, 2316, 72, 49055, 74, 19605, 50636], "temperature": 0.0, "avg_logprob": -0.1380066935221354, "compression_ratio": 1.4796238244514106, "no_speech_prob": 0.004766227211803198}, {"id": 254, "seek": 95974, "start": 965.62, "end": 968.98, "text": " jako\u015bciowo zmieni\u0142o ca\u0142\u0105 dziedzin\u0119 Natural Language Processing.", "tokens": [50658, 17123, 6199, 19941, 17020, 35462, 5249, 1335, 15926, 9758, 15338, 259, 1274, 20137, 24445, 31093, 278, 13, 50826], "temperature": 0.0, "avg_logprob": -0.1380066935221354, "compression_ratio": 1.4796238244514106, "no_speech_prob": 0.004766227211803198}, {"id": 255, "seek": 95974, "start": 969.5, "end": 974.38, "text": " Gwa\u0142townie odeszli\u015bmy od ery ma\u0142ych, wyspecjalizowanych modeli trenowanych do jednego zadania", "tokens": [50852, 460, 44603, 30401, 414, 3611, 10430, 38452, 3611, 1189, 88, 463, 47655, 11, 27062, 494, 66, 22600, 590, 23341, 339, 2316, 72, 23136, 23341, 339, 360, 5232, 11858, 42788, 5609, 51096], "temperature": 0.0, "avg_logprob": -0.1380066935221354, "compression_ratio": 1.4796238244514106, "no_speech_prob": 0.004766227211803198}, {"id": 256, "seek": 95974, "start": 974.66, "end": 978.7, "text": " na rzecz jednego, gigantycznego modelu, kt\u00f3ry ma potencja\u0142 wyrobi\u0107 wszystko.", "tokens": [51110, 1667, 36833, 5232, 11858, 11, 8741, 394, 17466, 11858, 2316, 84, 11, 9913, 463, 1847, 22660, 2938, 1221, 4628, 20027, 2162, 22607, 13, 51312], "temperature": 0.0, "avg_logprob": -0.1380066935221354, "compression_ratio": 1.4796238244514106, "no_speech_prob": 0.004766227211803198}, {"id": 257, "seek": 95974, "start": 979.1800000000001, "end": 982.14, "text": " To jest fundamentalna, emergentna zmiana w samej nauce.", "tokens": [51336, 1407, 3492, 8088, 629, 11, 4345, 6930, 629, 17020, 8497, 261, 912, 73, 35616, 384, 13, 51484], "temperature": 0.0, "avg_logprob": -0.1380066935221354, "compression_ratio": 1.4796238244514106, "no_speech_prob": 0.004766227211803198}, {"id": 258, "seek": 95974, "start": 982.46, "end": 985.7, "text": " I to prowadzi do ostatniego pytania, kt\u00f3re musimy sobie zada\u0107.", "tokens": [51500, 286, 281, 36590, 3992, 360, 32686, 2766, 1571, 25878, 5609, 11, 8864, 43449, 13652, 710, 1538, 2162, 13, 51662], "temperature": 0.0, "avg_logprob": -0.1380066935221354, "compression_ratio": 1.4796238244514106, "no_speech_prob": 0.004766227211803198}, {"id": 259, "seek": 95974, "start": 986.0600000000001, "end": 986.46, "text": " Wiemy.", "tokens": [51680, 9233, 2226, 13, 51700], "temperature": 0.0, "avg_logprob": -0.1380066935221354, "compression_ratio": 1.4796238244514106, "no_speech_prob": 0.004766227211803198}], "language": "pl"}