{"text": " Zaczynamy od pytania, kt\u00f3re w sumie do niedawna wydawa\u0142o si\u0119 ju\u017c zamkni\u0119te. Jak budowa\u0107 te najpot\u0119\u017cniejsze modele j\u0119zykowe? No tak, odpowied\u017a by\u0142a jedna. Jedna i oczywista, transformery w stylu GPT s\u0105 dos\u0142ownie wsz\u0119dzie. Zdominowa\u0142y badania, zasilaj\u0105 te najbardziej znamy produkty, z kt\u00f3rych korzystamy. Ale co je\u015bli ta odpowied\u017a, pomimo ca\u0142ej swojej mocy, prowadzi nas w pewnym sensie w \u015blep\u0105 uliczk\u0119? W\u0142a\u015bnie. W \u015bcie\u017ck\u0119, kt\u00f3ra jest coraz dro\u017csza, wymaga lawinowo rosn\u0105cych koszt\u00f3w, gigantycznego zu\u017cycia energii i tak naprawd\u0119 centralizuje technologie w r\u0119kach kilku firm, kt\u00f3re na to sta\u0107. Dok\u0142adnie. M\u00f3wimy tu o fundamentalnym problemie, kt\u00f3ry technicznie nazywa si\u0119 k\u0142adratyk complexity. Z\u0142o\u017cono\u015b\u0107 kwadratowa. Co to znaczy w praktyce? Wiesz, w architekturze Transformera ka\u017cdy kawa\u0142ek tekstu musi powiedzmy porozmawia\u0107 z ka\u017cdym innym kawa\u0142kiem. Przy dziesi\u0119ciu s\u0142owach to nie jest \u017caden problem. Ale przy dziesi\u0119ciu tysi\u0105cach s\u0142\u00f3w zapotrzebowanie na moc i pami\u0119\u0107 po prostu eksploduje. To jest taki technologiczny mur, w kt\u00f3ry sam czas uderzamy, pr\u00f3buj\u0105c przetwarza\u0107 coraz d\u0142u\u017csze dokumenty, ksi\u0105\u017cki, czy nie wiem, ca\u0142e bazy dane. I co je\u015bli klucz do przebicia tego muru, do stworzenia takiej prawdziwie osobistej, dost\u0119pnej dla ka\u017cdego sztucznej inteligencji, le\u017cy w technologii, kt\u00f3r\u0105 wielu z nas spisa\u0142y ju\u017c na straty. Dzisiaj analizujemy artyku\u0142, kt\u00f3ry pr\u00f3buje dokona\u0107 w\u0142a\u015bnie takiej ma\u0142ej rewolucji wracaj\u0105c do korzeni. Materia\u0142 nosi tytu\u0142 RWKV \u2013 Reinventing RNNs for the Transformer Era. To jest naprawd\u0119 \u015bmia\u0142a teza. Wskrzesi\u0107 stare dobre sieci rekurencyjne i da\u0107 im moc erytransformer\u00f3w. Naszym celem jest zrozumie\u0107, czy to tylko jaka\u015b sprytna sztuczka, czy mo\u017ce faktycznie pocz\u0105tek czego\u015b nowego WEI. To jest fascynuj\u0105ce, bo sieci rekurencyjne, czyli RNN, one z samej swojej natury rozwi\u0105zuj\u0105 ten problem koszt\u00f3w. Przetwarzaj\u0105 informacje sekwencyjnie, wiesz, s\u0142owo po s\u0142owie, wi\u0119c ich koszt ro\u015bnie liniowo, a nie kwadratowo. Czyli tak jak my czytamy. Dok\u0142adnie tak. One by\u0142y podstaw\u0105 NLP przed er\u0105 Transformer\u00f3w. Problem w tym, \u017ce na mia\u0142y swoje w\u0142asne demony. Trudno je by\u0142o trenowa\u0107 na du\u017c\u0105 skal\u0119, zapomina\u0142y informacje z odleg\u0142ej przesz\u0142o\u015bci i co tu du\u017co m\u00f3wi\u0107 nie potrafi\u0142y dor\u00f3wna\u0107 Transformerom pod wzgl\u0119dem czystej wydajno\u015bci. Pytanie wi\u0119c brzmi. Czy autorom RWKV uda\u0142o si\u0119 te demony jako\u015b egzorcyzmowa\u0107? W\u0142a\u015bnie. Dobra, to rozpakujmy to. Je\u015bli oni wskrzeszaj\u0105 RNN-y, to musieli znale\u017a\u0107 jaki\u015b spos\u00f3b, \u017ceby naprawi\u0107 te ich fundamentalne wady. Sama nazwa, RWKV, czyli Receptance Weighted Key Value, brzmi do\u015b\u0107 technicznie. Brzmi strasznie. Co tak naprawd\u0119 dzieje si\u0119 pod mask\u0105 tej architektury? Jakim si\u0119 uda\u0142o po\u0142\u0105czy\u0107 ogie\u0144 z wod\u0105? Najwa\u017cniejsza idea jest moim zdaniem genialna w swojej prostocie. Stworzyli architektur\u0119, kt\u00f3ra jest kameleonem. Ona dzia\u0142a w dw\u00f3ch zupe\u0142nie r\u00f3\u017cnych trybach w zale\u017cno\u015bci od tego, co robi. Podczas treningu, kiedy musimy przetworzy\u0107 gigantyczne ilo\u015bci danych tak szybko, jak to tylko mo\u017cliwe, RWKV zachowuje si\u0119 jak Transformer, a to pozwala na pe\u0142ne zr\u00f3wnoleglenie oblicze\u0144 na setkach tysi\u0105cach procesor\u00f3w graficznych. To by\u0142 klucz do sukcesu Transformer\u00f3w i pi\u0119ta achillesowa starych R&N\u00f3w. Ok, czyli na etapie nauki mamy pe\u0142n\u0105 moc, pe\u0142n\u0105 skalowalno\u015b\u0107, a co z tym drugim trybem? Co si\u0119 dzieje, kiedy ju\u017c go u\u017cywamy? I tu dzieje si\u0119 ca\u0142a magia. Kiedy motel jest ju\u017c wytrenowany i zaczynamy go u\u017cywa\u0107 do generowania tekstu, co w \u017cargonie nazywamy inferencj\u0105, architektura przyk prze\u0142\u0105cza si\u0119 w tryb R&N. I nagle zyskuje wszystkie zalety tego starego podej\u015bcia. Czyli? Sta\u0142e, niskie zu\u017cycie pami\u0119ci i mocy obliczeniowej, niezale\u017cnie od tego, jak d\u0142ugi jest tekst. Przetwarza jedno s\u0142owo, aktualizuje sw\u00f3j wewn\u0119trzny stan i idzie dalej, bez tej kwadratowej eksplozji koszt\u00f3w. Czyli to troch\u0119 tak jakby mie\u0107 samoch\u00f3d, kt\u00f3ry na torze wy\u015bcigowym, czyli podczas treningu, jest bolidem Formu\u0142y 1? Dok\u0142adnie. Zoptymalizowanym pod k\u0105tem maksymalnej pr\u0119dko\u015bci, a na co dzie\u0144 w mie\u015bcie, czyli podczas tej inferencji, to jest superoszcz\u0119dny pojazd elektryczny, kt\u00f3ry zu\u017cywa minimum energii. Idealna analogia. To jest naprawd\u0119 sprytne, ale jak to jest technicznie w og\u00f3le mo\u017cliwe? Jaki jest ten sekretny sk\u0142adnik, kt\u00f3ry pozwala na takie prze\u0142\u0105czanie tryb\u00f3w? Sekretnym sk\u0142adnikiem jest zast\u0105pienie tego klasycznego drogiego mechanizmu E-Tension, kt\u00f3ry wymaga por\u00f3wnowania w ka\u017cdego s\u0142owa z ka\u017cdym, czym\u015b, co autorzy nazywaj\u0105 operatorem WKV, opartym na atencji liniowej. Zamiast tej wszechobecznej rozmowy wszystkich ze wszystkimi, informacje s\u0105 przekazywane sekwencyjnie, krok po kroku. Ale najwi\u0119kszymi prze\u0142omami by\u0142o takie sformu\u0142owanie tego matematycznie, \u017ceby w trybie treningu da\u0142o si\u0119 to w pe\u0142ni zr\u00f3wnolegli\u0107, a w trybie inferencji mo\u017cna by\u0142o to oblicza\u0107 rekurencyjnie. Czyli to unika problemu N do kwadratu, bo nie ma ju\u017c tej siatki po\u0142\u0105cze\u0144 ka\u017cdego z ka\u017cdym? To bardziej jak sztafeta, gdzie pa\u0142eczka z informacjami jest przekazywana od jednego biegacza do drugiego? Tak. Ale zaprojektowana tak, \u017ce podczas treningu mo\u017cemy patrze\u0107 na wszystkich biegaczy naraz? Po jest doskona\u0142a analogia, a sama nazwa RWKV bierze si\u0119 z czterech wektor\u00f3w, kt\u00f3re steruj\u0105 t\u0105 sztafet\u0105 w ka\u017cdym kroku. Mamy R jak receptance, czyli tak\u0105 bramk\u0119, kt\u00f3ra decyduje, ile nowej informacji przyj\u0105\u0107. Mamy W jak weight, czyli wektor zaniku, kt\u00f3ry m\u00f3wi, jak szybko zapomina\u0107 o przesz\u0142o\u015bci. To jest kluczowe, \u017ceby model nieuton\u0105\u0142 w starych danych. Mamy te klasyczn\u0105 par\u0119, K i V, czyli key i value, kt\u00f3re podobnie jak w transformerach, przenosz\u0105 z sam\u0105 tre\u015b\u0107 informacji. To w\u0142a\u015bnie interakcja tych czterech element\u00f3w tworzy ten dynamiczny, hybrydowy mechanizm. Okej, teoria jest bardzo elegancka. Hybrydowy model, kt\u00f3ry \u0142\u0105czy najlepsze cechy obu \u015bwiat\u00f3w. To brzmi \u015bwietnie na papierze. Ale w \u015bwiecie AI teoria to jedno, a dowody to drugie. Wiele pi\u0119knych pomys\u0142\u00f3w po prostu poleg\u0142o w starciu z rzeczywisto\u015bci\u0105. Czy autorze przedstawili jakie\u015b twarde dane, \u017ce to faktycznie dzia\u0142a? Zbudowali model, kt\u00f3ry jest w stanie konkurowa\u0107 z tymi gigantami? I to jest, moim zdaniem, najbardziej imponuj\u0105ca cz\u0119\u015b\u0107 tej ca\u0142ej pracy. Oni nie poprzestali na jakim\u015b ma\u0142ym, koncepcyjnym modeliku. Zbudowali i wytrenowali modele RWKV o skali dochodz\u0105cej do 14 miliard\u00f3w parametr\u00f3w. 14 miliard\u00f3w? Tak. I \u017ceby da\u0107 jaki\u015b kontekst, to czyni go najwi\u0119ksz\u0105, g\u0119st\u0105 sieci\u0105 RNN, jak\u0105 kiedykolwiek w historii uda\u0142o si\u0119 skutecznie wytrenowa\u0107. To ju\u017c samo w sobie jest ogromnym osi\u0105gni\u0119ciem in\u017cynieryjnym. Czyli udawodnili, \u017ce architektura faktycznie si\u0119 skaluje. A co z wynikami? Jak ten 14 miliardowy gigant wypada w por\u00f3wnaniu z transformerami o podobnej wielko\u015bci? Jak modele z rodziny Bloom, Pythia czy OPT? Kiedy spojrz\u0119 si\u0119 na wykres jeden w artykule, wynik jest po prostu zdumiewaj\u0105cy. On por\u00f3wnuje krzyw\u0119 uczenia, czyli to, jak spada b\u0142\u0105d modelu w miar\u0119 zwi\u0119kszania mocy obliczeniowej w\u0142o\u017conej w trening. I te krzywe dla RWKV i dla transformer\u00f3w o por\u00f3wnywalnej wielko\u015bci s\u0105 praktycznie na\u0142o\u017cone na siebie. Niemal identyczne. Tak, r\u00f3\u017cnice s\u0105 minimalne. To pokazuje, \u017ce pod wzgl\u0119dem jako\u015bci, zdolno\u015bci do nauki j\u0119zyka, RWKV osi\u0105ga ten sam poziom, co wsp\u00f3\u0142czesne transformery. Tu nie ma kompromisu. Dobra, ale co z t\u0105 g\u0142\u00f3wn\u0105 obietnic\u0105 wydajno\u015bci\u0105 podczas inferencji? To przecie\u017c by\u0142 ten g\u0142\u00f3wny cel, prawda? I tutaj wida\u0107 prawdziw\u0105 rewolucj\u0119. Wykrez siedem jest chyba najbardziej wymowny. Pokazuje skumulowany czas potrzebny na wygenerowanie d\u0142ugiego tekstu. Dla transformer\u00f3w ta linia startuje p\u0142asko, a potem nagle gwa\u0142townie pnie si\u0119 w g\u00f3r\u0119, prawie pionowo. To jest w\u0142a\u015bnie ta kwadratowa z\u0142o\u017cono\u015b\u0107, kt\u00f3ra uderza w nas jak \u015bciana. A dla RWKV ta linia to jest praktycznie prosta, \u0142agodnie nachylona kreska. Niesamowite. To znaczy, \u017ce generowanie tysi\u0105ca s\u0142\u00f3w czy 100 tysi\u0119cy s\u0142\u00f3w nie stanowi dla niego problemu pod wzgl\u0119dem koszt\u00f3w. Pami\u0119tam, jak kilka lat temu pr\u00f3ba wygenerowania d\u0142u\u017cszego eseju na modelu GPT-2 na moim domowym komputerze ko\u0144czy\u0142a si\u0119 po prostu b\u0142\u0119dem braku pami\u0119ci. Tak, znam to. Tutaj m\u00f3wimy o czym\u015b, co mog\u0142oby dzia\u0142a\u0107 p\u0142ynnie bez \u017cadnego zaj\u0119kni\u0119cia. To jest ten moment, aha, zatem mamy t\u0119 sam\u0105 jako\u015b\u0107, ale przy radykalnie ni\u017cszych kosztach operacyjnych. W artykule wspomniano te\u017c o czym\u015b fundamentalnym, o tak zwanych scaling laws. Co to w\u0142a\u015bciwie znaczy, \u017ce RWK pod\u0105\u017ca za tymi samymi prawami skalowania co transformery? To jest absolutnie kluczowe odkrycie. To obala takie wieloletnie przekonanie, \u017ce RNN-y, nawet te bardziej zaawansowane jak LSTM, nie skaluj\u0105 si\u0119 tak dobrze. Co to znaczy, \u017ce si\u0119 nie skaluj\u0105? To znaczy, \u017ce po przekroczeniu pewnej wielko\u015bci, dok\u0142adanie danych i mocy obliczeniowej przestawa\u0142o przynosi\u0107 proporcjonalne korzy\u015bci. Modele jakby nasyca\u0142y si\u0119, a autorzy RWKV udowodnili, i to wida\u0107 na wykresie cztery, \u017ce ich architektura skaluje si\u0119 logarytmu liniowo, dok\u0142adnie tak samo jak transformery. Czyli nie ma sufitu? Nie ma teoretycznego sufitu, to jest sygna\u0142 dla ca\u0142ej spo\u0142eczno\u015bci badawczej. Mo\u017cecie budowa\u0107 100 miliardowe czy bilionowe modele w tej architekturze, a one b\u0119d\u0105 stawa\u0142y si\u0119 coraz m\u0105drzejsze w przewidywalny spos\u00f3b. To dow\u00f3d, \u017ce to nie jest jaki\u015b niszowy trik, ale fundamentalnie solidna podstawa do budowy przysz\u0142ych i jeszcze wi\u0119kszych modeli. To brzmi prawie zbyt dobrze, \u017ceby by\u0142o prawdziwe. Ta sama jako\u015b\u0107, ta sama skalowalno\u015b\u0107, a przy tym radykalnie ni\u017csze koszty inferencji. W technologii rzadko, kiedy dostajemy co\u015b za darmo, zast\u0105pienie tej pe\u0142nej kwadratowej atencji czym\u015b liniowym, sekwencyjnym musi si\u0119 wi\u0105za\u0107 z jakim\u015b kompromisem. Gdzie on si\u0119 objawia, co tracimy w zamian za t\u0119 niesamowit\u0105 pr\u0119dko\u015b\u0107 i oszcz\u0119dno\u015b\u0107? To jest dok\u0142adnie to pytanie, kt\u00f3re zadali sobie badacze. I s\u0105 w tej kwestii bardzo szczerzy. Kompromis istnieje i jest, co ciekawe, bardzo ciekawy. Najwi\u0119kszym zaskoczeniem, a jednocze\u015bnie s\u0142abo\u015bci\u0105, jest niezwyk\u0142a wra\u017cliwo\u015b\u0107 RWG-KPA na tak zwany prompt engineering. Czyli na to, w jaki spos\u00f3b my formulujemy nasze polecenia do modelu? Dok\u0142adnie. Poniewa\u017c podczas generowania tekstu model dzia\u0142a jak RNN, przetwarza wszystko krok po kroku. On nie mo\u017ce spojrze\u0107 wstecz na pocz\u0105tek promptu z tak\u0105 sam\u0105 swobod\u0105, jak transformer, kt\u00f3ry widzi wszystko naraz. W efekcie kolejno\u015b\u0107 informacji ma absolutnie kluczowe znaczenie. To, co jest na ko\u0144cu promptu, ma na niego najwi\u0119kszy wp\u0142yw. Czy jest w artykule jaki\u015b przyk\u0142ad, kt\u00f3ry to dobrze obrazuje, bo to brzmi do\u015b\u0107 abstrakcyjnie? Jest jeden, kt\u00f3ry pokazuje to doskonale. Testowani model na zadaniu RTE, czyli rozpoznawaniu powi\u0105za\u0144 mi\u0119dzy zdaniami. U\u017cyli takiego standardowego promptu, kt\u00f3ry \u015bwietnie dzia\u0142a\u0142 na modelach typu GPT. W tym promptie najpierw podawano dane, a na samym ko\u0144cu by\u0142o pytanie o te dane. Wynik, jaki uzyska\u0142 RWKV by\u0142 bardzo s\u0142aby, metryka F1 na poziomie 44%. Czyli prawie jak losowe zgadywanie. W\u0142a\u015bnie. Ale wtedy badacze zrobili prost\u0105 rzecz. Odwr\u00f3cili kolejno\u015b\u0107 w promptie. Po prostu zamienili zdania miejscami. Dok\u0142adnie. Najpierw zadali pytanie, a potem podali dane, kt\u00f3re model mia\u0142 przeanalizowa\u0107. Z punktu widzenia RNN to mia\u0142o sens. Najpierw dowiaduje si\u0119 czego szuka\u0107, a potem to znajduje. I jaki by\u0142 wynik? Wynik. Przy tej samej tre\u015bci tylko w innej kolejno\u015bci metryka F1 skoczy\u0142a do prawie 75%. To jest gigantyczna r\u00f3\u017cnica, kt\u00f3ra pokazuje, \u017ce z tymi modelami musimy si\u0119 nauczy\u0107 rozmawia\u0107 w zupe\u0142nie nowy spos\u00f3b. Nie wystarczy przeklei\u0107 prompt\u00f3w z GPT. To fascynuj\u0105ce. Oznacza to, \u017ce wszystkie te intuicje, kt\u00f3re budowali\u015bmy przez lata pracy z transformerami, tutaj mog\u0105 by\u0107 po prostu myl\u0105ce. A to z innymi bardziej fundamentalnymi ograniczeniami. Ta wra\u017cliwo\u015b\u0107 na prompt to jedno. Ale czy s\u0105 zadania, w kt\u00f3rych transformer z swoj\u0105 pe\u0142n\u0105 atencj\u0105 zawsze b\u0119dzie mia\u0142 przewag\u0119? Gdzie ten brak mo\u017cliwo\u015bci spojrzenia wstecz naprawd\u0119 boli? Autorzy sami to przyznaj\u0105. Ta sekwencyjna natura mo\u017ce sprawia\u0107, \u017ce RWKV b\u0119dzie mia\u0142 trudno\u015b\u0107 z przypomnieniem sobie bardzo drobnego, specyficznego detalu z bardzo, bardzo odleg\u0142ego fragmentu tekstu. Wyobra\u017amy sobie takie zadanie typu. Znajd\u017a ig\u0142\u0119 w Stogusianaw z tustro\u0144cowym dokumencie. Na stronie drugiej, w trzecima kapicie jest jakie\u015b nazwisko, o kt\u00f3re pytamy na ko\u0144cu. OK. Transformer w ka\u017cdym kroku generowania odpowiedzi ma bezpo\u015bredni dost\u0119p do tej drugiej strony. RWKV musi polega\u0107 na tym, \u017ce ta informacja jako\u015b przetrwa\u0142a w jego skompresowanym stanie przez ca\u0142\u0105 podr\u00f3\u017c przez reszt\u0119 dokumentu. W takich niszowych, wymagaj\u0105cych bardzo precyzyjnego wyszukiwania zadaniach, klasyczna atencja wci\u0105\u017c mo\u017ce mie\u0107 przewag\u0119. Rozumiem. Czyli do og\u00f3lnego rozumienia tekstu i generowania jest \u015bwietny, ale dla zada\u0144, kt\u00f3re przypominaj\u0105 tak\u0105 wewn\u0119trzn\u0105 wyszukiwark\u0119, mo\u017ce by\u0107 mniej niezawodny. No dobrze, to po\u0142\u0105czmy te wszystkie kropki. Mamy nowostar\u0105 architektur\u0119, kt\u00f3ra jest niemal tak inteligentna jak Transformer, skaluje si\u0119 w ten sam przewidywalny spos\u00f3b, ale jest niewyobra\u017calnie ta\u0144sza i szybsza w u\u017cyciu. Ma przy tym pewne specyficzne dziwactwa i ograniczenia. Co to wszystko oznacza dla przysz\u0142o\u015bci AI? Czy to jest koniec Ery Transformer\u00f3w? Na pewno nie koniec, ale bez w\u0105tpienia pocz\u0105tek bardzo silnej i co wa\u017cne, wydajnej alternatywy. My\u015bl\u0119, \u017ce najwi\u0119ksz\u0105 implikacj\u0105 jest potencjalna demokratyzacja i decentralizacja dost\u0119pu do du\u017cych modeli j\u0119zykowych. Przez ostatnie lata trend by\u0142 jasny. Coraz wi\u0119ksze modele, trenowane w coraz wi\u0119kszych centrach danych, kontrolowane przez coraz mniejsz\u0105 liczb\u0119 firm. RWKV odwraca ten trend. Znacznie ni\u017csze koszty inferencji oznaczaj\u0105, \u017ce pot\u0119\u017cny 14 miliardowy model mo\u017ce zacz\u0105\u0107 dzia\u0142a\u0107 efektywnie na znacznie s\u0142abszym sprz\u0119cie. Czyli ta wizja pot\u0119\u017cnego asystenta AI, kt\u00f3ry dzia\u0142a w pe\u0142ni lokalnie na moim telefonie czy laptopie? Tak. I nie musi wysy\u0142a\u0107 ka\u017cdej mojej my\u015bli i ka\u017cdego zapytania do chmury, staje si\u0119 znacznie bardziej realna. To by\u0142by ogromny prze\u0142om chocia\u017cby z punktu widzenia prywatno\u015bci. Dok\u0142adnie. Prywatno\u015b\u0107, koszty, dost\u0119pno\u015b\u0107 offline, suwerenno\u015b\u0107 danych. To ofiera drzwi do zastosowa\u0144 w tzw. architekturze Edge, czyli na urz\u0105dzeniach ko\u0144cowych, w medycynie, w przemy\u015ble. Wsz\u0119dzie tam, gdzie wysy\u0142anie danych na zewn\u0105trz jest niedopuszczalne. Lub po prostu niepraktyczne. A co do przysz\u0142o\u015bci? Autorzy wskazuj\u0105 kilka naturalnych kierunk\u00f3w. Dalsze doskonalenie samego mechanizmu RWKW. Zastosowanie go w innych typach architektur, na przyk\u0142ad modelach encoder, decoder do t\u0142umaczenia. A nawet, co jest bardzo ciekawe, wykorzystanie tego wewn\u0119trznego stanu RNN do lepszej interpretacji dzia\u0142ania modelu. Ten stand to jest jakby my\u015bl modelu w danym momencie. Do kt\u00f3rej mamy wgl\u0105d? Rozumowuj\u0105c, wygl\u0105da na to, \u017ce RWKW to nie jest tylko kolejna drobna optymalizacja czy jaka\u015b ma\u0142a poprawka. To jest fundamentalne przemy\u015blenie ca\u0142ej architektury, kt\u00f3re z sukcesem \u0142\u0105czy dwa wydawa\u0142oby si\u0119 wykluczaj\u0105ce si\u0119 \u015bwiaty. Brutaln\u0105 moc skalowania transformel\u00f3w, z elegancj\u0105 i niezwyk\u0142\u0105 wydajno\u015bci\u0105 sieci rekur\u0119cznych. To jest przede wszystkim dow\u00f3d na to, \u017ce w nauce czasem warto zrobi\u0107 krok w ty\u0142, \u017ceby p\u00f3j\u015b\u0107 do przodu. Warto wr\u00f3ci\u0107 do starszych, pozornie porzuconych pomys\u0142\u00f3w i spojrze\u0107 na nie przez pryzmat nowych technik, nowych narz\u0119dzi i tej ogromnej skali obliczeniowej, kt\u00f3r\u0105 dzisiaj dysponujemy. RWKW pokazuje, \u017ce era fundamentalnych innowacji w architekturach AI wcale, wcale si\u0119 nie sk\u0105czy\u0142a. Ci\u0105gle jest miejsce na prze\u0142omowe odkrycia. I to zostawia nas z takim prowokuj\u0105cym pytaniem do przemy\u015blenia. Skoro uda\u0142o si\u0119 z tak\u0105 skuteczno\u015bci\u0105 wskrzesi\u0107 i unowocze\u015bni\u0107 RNA, to jakie inne pozornie przestarza\u0142y idee w historii sztucznej inteligencji czekaj\u0105 na sw\u00f3j renesans w tej nowej erze.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.0, "text": " Zaczynamy od pytania, kt\u00f3re w sumie do niedawna wydawa\u0142o si\u0119 ju\u017c zamkni\u0119te.", "tokens": [50364, 1176, 14691, 5378, 88, 3611, 25878, 5609, 11, 8864, 261, 2408, 414, 360, 32488, 1607, 629, 25984, 10449, 5249, 3244, 10678, 19876, 74, 35938, 975, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12927611886638485, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.046586111187934875}, {"id": 1, "seek": 0, "start": 6.0, "end": 9.8, "text": " Jak budowa\u0107 te najpot\u0119\u017cniejsze modele j\u0119zykowe?", "tokens": [50664, 15029, 3265, 11445, 535, 11212, 17698, 1274, 1427, 44258, 4391, 306, 49055, 74, 6880, 30, 50854], "temperature": 0.0, "avg_logprob": -0.12927611886638485, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.046586111187934875}, {"id": 2, "seek": 0, "start": 9.8, "end": 11.9, "text": " No tak, odpowied\u017a by\u0142a jedna.", "tokens": [50854, 883, 991, 11, 36574, 10659, 23936, 5232, 629, 13, 50959], "temperature": 0.0, "avg_logprob": -0.12927611886638485, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.046586111187934875}, {"id": 3, "seek": 0, "start": 11.9, "end": 17.0, "text": " Jedna i oczywista, transformery w stylu GPT s\u0105 dos\u0142ownie wsz\u0119dzie.", "tokens": [50959, 27076, 629, 741, 277, 6522, 86, 5236, 11, 4088, 2109, 261, 7952, 2781, 26039, 51, 9015, 4491, 1221, 648, 414, 38322, 42643, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12927611886638485, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.046586111187934875}, {"id": 4, "seek": 0, "start": 17.0, "end": 21.8, "text": " Zdominowa\u0142y badania, zasilaj\u0105 te najbardziej znamy produkty, z kt\u00f3rych korzystamy.", "tokens": [51214, 1176, 4121, 259, 5528, 6825, 1578, 5609, 11, 710, 13353, 11133, 535, 41857, 710, 5378, 88, 33699, 874, 11, 710, 30382, 14784, 36049, 7804, 13, 51454], "temperature": 0.0, "avg_logprob": -0.12927611886638485, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.046586111187934875}, {"id": 5, "seek": 0, "start": 21.8, "end": 29.400000000000002, "text": " Ale co je\u015bli ta odpowied\u017a, pomimo ca\u0142ej swojej mocy, prowadzi nas w pewnym sensie w \u015blep\u0105 uliczk\u0119?", "tokens": [51454, 9366, 598, 25630, 1846, 36574, 10659, 11, 12991, 6934, 47631, 73, 29489, 73, 705, 1344, 11, 36590, 3992, 5382, 261, 47160, 4199, 2923, 414, 261, 8299, 306, 79, 1611, 344, 1050, 89, 15724, 30, 51834], "temperature": 0.0, "avg_logprob": -0.12927611886638485, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.046586111187934875}, {"id": 6, "seek": 2940, "start": 29.4, "end": 30.099999999999998, "text": " W\u0142a\u015bnie.", "tokens": [50364, 343, 5024, 12221, 13, 50399], "temperature": 0.0, "avg_logprob": -0.10851075514307562, "compression_ratio": 1.43202416918429, "no_speech_prob": 0.02311713993549347}, {"id": 7, "seek": 2940, "start": 30.099999999999998, "end": 37.4, "text": " W \u015bcie\u017ck\u0119, kt\u00f3ra jest coraz dro\u017csza, wymaga lawinowo rosn\u0105cych koszt\u00f3w, gigantycznego zu\u017cycia energii", "tokens": [50399, 343, 8299, 40082, 15724, 11, 19456, 3492, 25899, 3789, 1427, 82, 2394, 11, 29764, 9286, 2101, 259, 19941, 18953, 13113, 31306, 19532, 2682, 3901, 11, 8741, 394, 17466, 11858, 2164, 7735, 2755, 10575, 5597, 50764], "temperature": 0.0, "avg_logprob": -0.10851075514307562, "compression_ratio": 1.43202416918429, "no_speech_prob": 0.02311713993549347}, {"id": 8, "seek": 2940, "start": 37.4, "end": 42.0, "text": " i tak naprawd\u0119 centralizuje technologie w r\u0119kach kilku firm, kt\u00f3re na to sta\u0107.", "tokens": [50764, 741, 991, 20970, 5777, 590, 13008, 1537, 20121, 261, 41197, 41326, 5128, 5279, 6174, 11, 8864, 1667, 281, 11135, 2162, 13, 50994], "temperature": 0.0, "avg_logprob": -0.10851075514307562, "compression_ratio": 1.43202416918429, "no_speech_prob": 0.02311713993549347}, {"id": 9, "seek": 2940, "start": 42.0, "end": 48.4, "text": " Dok\u0142adnie. M\u00f3wimy tu o fundamentalnym problemie, kt\u00f3ry technicznie nazywa si\u0119 k\u0142adratyk complexity.", "tokens": [50994, 29768, 10358, 2766, 13, 376, 3901, 13189, 2604, 277, 8088, 12996, 1154, 414, 11, 9913, 1537, 17946, 2766, 20151, 88, 4151, 3244, 350, 10358, 4481, 46127, 14024, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10851075514307562, "compression_ratio": 1.43202416918429, "no_speech_prob": 0.02311713993549347}, {"id": 10, "seek": 2940, "start": 48.4, "end": 49.9, "text": " Z\u0142o\u017cono\u015b\u0107 kwadratowa.", "tokens": [51314, 1176, 5249, 1427, 8957, 7753, 23846, 345, 4481, 5528, 13, 51389], "temperature": 0.0, "avg_logprob": -0.10851075514307562, "compression_ratio": 1.43202416918429, "no_speech_prob": 0.02311713993549347}, {"id": 11, "seek": 2940, "start": 49.9, "end": 51.4, "text": " Co to znaczy w praktyce?", "tokens": [51389, 3066, 281, 36584, 261, 3206, 74, 874, 384, 30, 51464], "temperature": 0.0, "avg_logprob": -0.10851075514307562, "compression_ratio": 1.43202416918429, "no_speech_prob": 0.02311713993549347}, {"id": 12, "seek": 2940, "start": 51.4, "end": 59.3, "text": " Wiesz, w architekturze Transformera ka\u017cdy kawa\u0142ek tekstu musi powiedzmy porozmawia\u0107 z ka\u017cdym innym kawa\u0142kiem.", "tokens": [51464, 343, 15347, 11, 261, 3912, 642, 2320, 374, 1381, 27938, 1663, 31615, 350, 10449, 1221, 916, 16624, 372, 84, 37587, 27617, 2226, 1515, 15151, 76, 34953, 2162, 710, 31615, 76, 294, 12996, 350, 10449, 1221, 26116, 13, 51859], "temperature": 0.0, "avg_logprob": -0.10851075514307562, "compression_ratio": 1.43202416918429, "no_speech_prob": 0.02311713993549347}, {"id": 13, "seek": 5930, "start": 59.3, "end": 62.3, "text": " Przy dziesi\u0119ciu s\u0142owach to nie jest \u017caden problem.", "tokens": [50364, 39590, 9758, 530, 5034, 30795, 15116, 305, 608, 281, 2838, 3492, 19625, 14771, 1154, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10644411533436876, "compression_ratio": 1.4646464646464648, "no_speech_prob": 0.0006262302049435675}, {"id": 14, "seek": 5930, "start": 62.3, "end": 68.3, "text": " Ale przy dziesi\u0119ciu tysi\u0105cach s\u0142\u00f3w zapotrzebowanie na moc i pami\u0119\u0107 po prostu eksploduje.", "tokens": [50514, 9366, 6501, 9758, 530, 5034, 30795, 38156, 11404, 66, 608, 15116, 3901, 14223, 310, 13503, 8202, 7155, 1667, 34962, 741, 31088, 2162, 714, 19518, 30724, 564, 378, 13008, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10644411533436876, "compression_ratio": 1.4646464646464648, "no_speech_prob": 0.0006262302049435675}, {"id": 15, "seek": 5930, "start": 68.3, "end": 76.3, "text": " To jest taki technologiczny mur, w kt\u00f3ry sam czas uderzamy, pr\u00f3buj\u0105c przetwarza\u0107 coraz d\u0142u\u017csze dokumenty, ksi\u0105\u017cki, czy nie wiem, ca\u0142e bazy dane.", "tokens": [50814, 1407, 3492, 20065, 1537, 1132, 17946, 1634, 5257, 11, 261, 9913, 3247, 13190, 344, 1068, 89, 7804, 11, 8565, 65, 44733, 6541, 302, 6925, 35873, 25899, 274, 24066, 1427, 82, 1381, 40858, 88, 11, 39311, 2984, 11, 6430, 2838, 26522, 11, 47631, 27147, 88, 49206, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10644411533436876, "compression_ratio": 1.4646464646464648, "no_speech_prob": 0.0006262302049435675}, {"id": 16, "seek": 5930, "start": 76.3, "end": 85.3, "text": " I co je\u015bli klucz do przebicia tego muru, do stworzenia takiej prawdziwie osobistej, dost\u0119pnej dla ka\u017cdego sztucznej inteligencji,", "tokens": [51214, 286, 598, 25630, 9671, 1311, 89, 360, 8325, 65, 15341, 8627, 5257, 84, 11, 360, 342, 28321, 14320, 38941, 41175, 3992, 8699, 41518, 8375, 73, 11, 48209, 11794, 12285, 21912, 67, 6308, 262, 2682, 1311, 89, 11794, 24777, 3213, 19649, 11, 51664], "temperature": 0.0, "avg_logprob": -0.10644411533436876, "compression_ratio": 1.4646464646464648, "no_speech_prob": 0.0006262302049435675}, {"id": 17, "seek": 8530, "start": 85.3, "end": 89.3, "text": " le\u017cy w technologii, kt\u00f3r\u0105 wielu z nas spisa\u0142y ju\u017c na straty.", "tokens": [50364, 476, 7735, 261, 1537, 1132, 5597, 11, 37415, 40437, 710, 5382, 637, 3837, 6825, 10678, 1667, 1056, 21398, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10424055934937532, "compression_ratio": 1.3125, "no_speech_prob": 0.11237215995788574}, {"id": 18, "seek": 8530, "start": 89.3, "end": 95.3, "text": " Dzisiaj analizujemy artyku\u0142, kt\u00f3ry pr\u00f3buje dokona\u0107 w\u0142a\u015bnie takiej ma\u0142ej rewolucji wracaj\u0105c do korzeni.", "tokens": [50564, 39448, 22356, 2624, 590, 21767, 594, 874, 5279, 1221, 11, 9913, 8565, 6021, 2884, 25037, 4037, 2162, 14234, 38941, 463, 19827, 73, 319, 48481, 1311, 4013, 928, 326, 38757, 360, 14784, 42124, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10424055934937532, "compression_ratio": 1.3125, "no_speech_prob": 0.11237215995788574}, {"id": 19, "seek": 8530, "start": 95.3, "end": 101.3, "text": " Materia\u0142 nosi tytu\u0142 RWKV \u2013 Reinventing RNNs for the Transformer Era.", "tokens": [50864, 19188, 8908, 3269, 72, 1104, 9179, 1221, 42513, 42, 53, 1662, 42116, 2475, 278, 45702, 45, 82, 337, 264, 27938, 260, 23071, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10424055934937532, "compression_ratio": 1.3125, "no_speech_prob": 0.11237215995788574}, {"id": 20, "seek": 8530, "start": 101.3, "end": 103.3, "text": " To jest naprawd\u0119 \u015bmia\u0142a teza.", "tokens": [51164, 1407, 3492, 20970, 8299, 29958, 5024, 535, 2394, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10424055934937532, "compression_ratio": 1.3125, "no_speech_prob": 0.11237215995788574}, {"id": 21, "seek": 8530, "start": 103.3, "end": 109.3, "text": " Wskrzesi\u0107 stare dobre sieci rekurencyjne i da\u0107 im moc erytransformer\u00f3w.", "tokens": [51264, 343, 5161, 81, 12214, 12757, 22432, 41959, 2804, 537, 33881, 9873, 42949, 716, 741, 1120, 2162, 566, 34962, 1189, 4328, 25392, 837, 260, 3901, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10424055934937532, "compression_ratio": 1.3125, "no_speech_prob": 0.11237215995788574}, {"id": 22, "seek": 10930, "start": 109.3, "end": 115.3, "text": " Naszym celem jest zrozumie\u0107, czy to tylko jaka\u015b sprytna sztuczka, czy mo\u017ce faktycznie pocz\u0105tek czego\u015b nowego WEI.", "tokens": [50364, 16151, 26681, 1769, 10386, 3492, 710, 27857, 449, 414, 2162, 11, 6430, 281, 13219, 4207, 64, 1788, 637, 627, 83, 629, 262, 2682, 1311, 89, 2330, 11, 6430, 12034, 33647, 45586, 34397, 916, 36559, 1788, 586, 6308, 15813, 40, 13, 50664], "temperature": 0.0, "avg_logprob": -0.0909574520893586, "compression_ratio": 1.4415584415584415, "no_speech_prob": 0.07983744144439697}, {"id": 23, "seek": 10930, "start": 115.3, "end": 124.3, "text": " To jest fascynuj\u0105ce, bo sieci rekurencyjne, czyli RNN, one z samej swojej natury rozwi\u0105zuj\u0105 ten problem koszt\u00f3w.", "tokens": [50664, 1407, 3492, 30632, 1344, 77, 13263, 384, 11, 748, 2804, 537, 33881, 9873, 42949, 716, 11, 16591, 45702, 45, 11, 472, 710, 912, 73, 29489, 73, 2249, 2598, 9544, 18234, 89, 13263, 2064, 1154, 19532, 2682, 3901, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0909574520893586, "compression_ratio": 1.4415584415584415, "no_speech_prob": 0.07983744144439697}, {"id": 24, "seek": 10930, "start": 124.3, "end": 132.3, "text": " Przetwarzaj\u0105 informacje sekwencyjnie, wiesz, s\u0142owo po s\u0142owie, wi\u0119c ich koszt ro\u015bnie liniowo, a nie kwadratowo.", "tokens": [51114, 2114, 40399, 31991, 11133, 1356, 29293, 17215, 86, 3020, 73, 2766, 11, 261, 15347, 11, 15116, 19941, 714, 15116, 13998, 11, 16677, 1893, 19532, 2682, 744, 12221, 287, 3812, 19941, 11, 257, 2838, 23846, 345, 4481, 19941, 13, 51514], "temperature": 0.0, "avg_logprob": -0.0909574520893586, "compression_ratio": 1.4415584415584415, "no_speech_prob": 0.07983744144439697}, {"id": 25, "seek": 10930, "start": 132.3, "end": 133.3, "text": " Czyli tak jak my czytamy.", "tokens": [51514, 37099, 991, 4207, 452, 6430, 83, 7804, 13, 51564], "temperature": 0.0, "avg_logprob": -0.0909574520893586, "compression_ratio": 1.4415584415584415, "no_speech_prob": 0.07983744144439697}, {"id": 26, "seek": 10930, "start": 133.3, "end": 134.3, "text": " Dok\u0142adnie tak.", "tokens": [51564, 29768, 10358, 2766, 991, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0909574520893586, "compression_ratio": 1.4415584415584415, "no_speech_prob": 0.07983744144439697}, {"id": 27, "seek": 10930, "start": 134.3, "end": 138.3, "text": " One by\u0142y podstaw\u0105 NLP przed er\u0105 Transformer\u00f3w.", "tokens": [51614, 1485, 26366, 43443, 1611, 426, 45196, 18334, 1189, 1611, 27938, 260, 3901, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0909574520893586, "compression_ratio": 1.4415584415584415, "no_speech_prob": 0.07983744144439697}, {"id": 28, "seek": 13830, "start": 138.3, "end": 141.3, "text": " Problem w tym, \u017ce na mia\u0142y swoje w\u0142asne demony.", "tokens": [50364, 11676, 261, 8107, 11, 3561, 1667, 21290, 6825, 29489, 43572, 716, 1371, 2526, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07859613977629563, "compression_ratio": 1.363265306122449, "no_speech_prob": 0.02358337678015232}, {"id": 29, "seek": 13830, "start": 141.3, "end": 152.3, "text": " Trudno je by\u0142o trenowa\u0107 na du\u017c\u0105 skal\u0119, zapomina\u0142y informacje z odleg\u0142ej przesz\u0142o\u015bci i co tu du\u017co m\u00f3wi\u0107 nie potrafi\u0142y dor\u00f3wna\u0107 Transformerom pod wzgl\u0119dem czystej wydajno\u015bci.", "tokens": [50514, 1765, 532, 1771, 1506, 14811, 23136, 11445, 1667, 21783, 1611, 16890, 1274, 11, 14223, 49217, 6825, 1356, 29293, 710, 277, 2285, 70, 19827, 73, 6541, 10430, 35059, 741, 598, 2604, 26673, 13489, 12757, 2838, 1847, 10437, 72, 6825, 26313, 3901, 629, 2162, 27938, 260, 298, 2497, 48538, 6298, 443, 6430, 2941, 73, 25984, 1805, 16438, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07859613977629563, "compression_ratio": 1.363265306122449, "no_speech_prob": 0.02358337678015232}, {"id": 30, "seek": 13830, "start": 152.3, "end": 154.3, "text": " Pytanie wi\u0119c brzmi.", "tokens": [51064, 430, 4328, 7155, 16677, 738, 89, 3057, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07859613977629563, "compression_ratio": 1.363265306122449, "no_speech_prob": 0.02358337678015232}, {"id": 31, "seek": 13830, "start": 154.3, "end": 159.3, "text": " Czy autorom RWKV uda\u0142o si\u0119 te demony jako\u015b egzorcyzmowa\u0107?", "tokens": [51164, 19832, 19510, 298, 42513, 42, 53, 44544, 5249, 3244, 535, 1371, 2526, 17123, 1788, 24263, 89, 284, 1344, 89, 76, 11445, 30, 51414], "temperature": 0.0, "avg_logprob": -0.07859613977629563, "compression_ratio": 1.363265306122449, "no_speech_prob": 0.02358337678015232}, {"id": 32, "seek": 13830, "start": 159.3, "end": 160.3, "text": " W\u0142a\u015bnie.", "tokens": [51414, 343, 5024, 12221, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07859613977629563, "compression_ratio": 1.363265306122449, "no_speech_prob": 0.02358337678015232}, {"id": 33, "seek": 16030, "start": 161.3, "end": 163.3, "text": " Dobra, to rozpakujmy to.", "tokens": [50414, 413, 24393, 11, 281, 9544, 45944, 4579, 2226, 281, 13, 50514], "temperature": 0.0, "avg_logprob": -0.0888321450416078, "compression_ratio": 1.351170568561873, "no_speech_prob": 0.18641847372055054}, {"id": 34, "seek": 16030, "start": 163.3, "end": 169.3, "text": " Je\u015bli oni wskrzeszaj\u0105 RNN-y, to musieli znale\u017a\u0107 jaki\u015b spos\u00f3b, \u017ceby naprawi\u0107 te ich fundamentalne wady.", "tokens": [50514, 37086, 36317, 261, 5161, 19390, 10430, 11133, 45702, 45, 12, 88, 11, 281, 1038, 23099, 15397, 1220, 10659, 2162, 34721, 22904, 11, 11316, 9296, 5131, 12757, 535, 1893, 8088, 716, 261, 880, 13, 50814], "temperature": 0.0, "avg_logprob": -0.0888321450416078, "compression_ratio": 1.351170568561873, "no_speech_prob": 0.18641847372055054}, {"id": 35, "seek": 16030, "start": 169.3, "end": 176.3, "text": " Sama nazwa, RWKV, czyli Receptance Weighted Key Value, brzmi do\u015b\u0107 technicznie.", "tokens": [50814, 318, 2404, 20151, 4151, 11, 42513, 42, 53, 11, 16591, 1300, 1336, 719, 44464, 292, 12759, 39352, 11, 738, 89, 3057, 49333, 1537, 17946, 2766, 13, 51164], "temperature": 0.0, "avg_logprob": -0.0888321450416078, "compression_ratio": 1.351170568561873, "no_speech_prob": 0.18641847372055054}, {"id": 36, "seek": 16030, "start": 176.3, "end": 177.3, "text": " Brzmi strasznie.", "tokens": [51164, 1603, 89, 3057, 1056, 19601, 2766, 13, 51214], "temperature": 0.0, "avg_logprob": -0.0888321450416078, "compression_ratio": 1.351170568561873, "no_speech_prob": 0.18641847372055054}, {"id": 37, "seek": 16030, "start": 177.3, "end": 180.3, "text": " Co tak naprawd\u0119 dzieje si\u0119 pod mask\u0105 tej architektury?", "tokens": [51214, 3066, 991, 20970, 17953, 2884, 3244, 2497, 6094, 1611, 12573, 3912, 642, 2320, 2598, 30, 51364], "temperature": 0.0, "avg_logprob": -0.0888321450416078, "compression_ratio": 1.351170568561873, "no_speech_prob": 0.18641847372055054}, {"id": 38, "seek": 16030, "start": 180.3, "end": 183.3, "text": " Jakim si\u0119 uda\u0142o po\u0142\u0105czy\u0107 ogie\u0144 z wod\u0105?", "tokens": [51364, 15029, 332, 3244, 44544, 5249, 714, 15926, 33967, 5360, 414, 5248, 710, 47751, 1611, 30, 51514], "temperature": 0.0, "avg_logprob": -0.0888321450416078, "compression_ratio": 1.351170568561873, "no_speech_prob": 0.18641847372055054}, {"id": 39, "seek": 16030, "start": 183.3, "end": 188.3, "text": " Najwa\u017cniejsza idea jest moim zdaniem genialna w swojej prostocie.", "tokens": [51514, 31576, 27111, 30295, 2394, 1558, 3492, 48569, 710, 10312, 4907, 48228, 629, 261, 29489, 73, 10293, 905, 414, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0888321450416078, "compression_ratio": 1.351170568561873, "no_speech_prob": 0.18641847372055054}, {"id": 40, "seek": 18830, "start": 189.3, "end": 192.3, "text": " Stworzyli architektur\u0119, kt\u00f3ra jest kameleonem.", "tokens": [50414, 745, 28321, 1229, 2081, 3912, 642, 2320, 374, 1274, 11, 19456, 3492, 9727, 16884, 266, 443, 13, 50564], "temperature": 0.0, "avg_logprob": -0.056756587823232015, "compression_ratio": 1.3717472118959109, "no_speech_prob": 0.07022295892238617}, {"id": 41, "seek": 18830, "start": 192.3, "end": 197.3, "text": " Ona dzia\u0142a w dw\u00f3ch zupe\u0142nie r\u00f3\u017cnych trybach w zale\u017cno\u015bci od tego, co robi.", "tokens": [50564, 49793, 37903, 261, 27379, 812, 339, 49922, 42602, 853, 32096, 261, 710, 45494, 16438, 3611, 8627, 11, 598, 47380, 13, 50814], "temperature": 0.0, "avg_logprob": -0.056756587823232015, "compression_ratio": 1.3717472118959109, "no_speech_prob": 0.07022295892238617}, {"id": 42, "seek": 18830, "start": 197.3, "end": 204.3, "text": " Podczas treningu, kiedy musimy przetworzy\u0107 gigantyczne ilo\u015bci danych tak szybko, jak to tylko mo\u017cliwe,", "tokens": [50814, 12646, 30989, 2192, 773, 84, 11, 18777, 43449, 6541, 302, 28321, 27150, 8741, 394, 17466, 716, 1930, 44468, 274, 34644, 991, 36456, 4093, 11, 4207, 281, 13219, 30854, 826, 11, 51164], "temperature": 0.0, "avg_logprob": -0.056756587823232015, "compression_ratio": 1.3717472118959109, "no_speech_prob": 0.07022295892238617}, {"id": 43, "seek": 18830, "start": 204.3, "end": 212.3, "text": " RWKV zachowuje si\u0119 jak Transformer, a to pozwala na pe\u0142ne zr\u00f3wnoleglenie oblicze\u0144 na setkach tysi\u0105cach procesor\u00f3w graficznych.", "tokens": [51164, 42513, 42, 53, 29303, 305, 13008, 3244, 4207, 27938, 260, 11, 257, 281, 40557, 5159, 1667, 43205, 716, 710, 11721, 895, 4812, 70, 6698, 414, 1111, 1050, 49689, 1667, 992, 41326, 38156, 11404, 66, 608, 17565, 284, 3901, 1295, 1786, 89, 9399, 13, 51564], "temperature": 0.0, "avg_logprob": -0.056756587823232015, "compression_ratio": 1.3717472118959109, "no_speech_prob": 0.07022295892238617}, {"id": 44, "seek": 21230, "start": 212.3, "end": 218.3, "text": " To by\u0142 klucz do sukcesu Transformer\u00f3w i pi\u0119ta achillesowa starych R&N\u00f3w.", "tokens": [50364, 1407, 16673, 9671, 1311, 89, 360, 46432, 887, 84, 27938, 260, 3901, 741, 32677, 1328, 2800, 14835, 5528, 342, 822, 339, 497, 5, 45, 3901, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08672199510548213, "compression_ratio": 1.4195804195804196, "no_speech_prob": 0.08168443292379379}, {"id": 45, "seek": 21230, "start": 218.3, "end": 225.3, "text": " Ok, czyli na etapie nauki mamy pe\u0142n\u0105 moc, pe\u0142n\u0105 skalowalno\u015b\u0107, a co z tym drugim trybem?", "tokens": [50664, 3477, 11, 16591, 1667, 47634, 414, 35616, 2984, 17335, 43205, 13113, 34962, 11, 43205, 13113, 16890, 305, 304, 23293, 11, 257, 598, 710, 8107, 4110, 332, 853, 65, 443, 30, 51014], "temperature": 0.0, "avg_logprob": -0.08672199510548213, "compression_ratio": 1.4195804195804196, "no_speech_prob": 0.08168443292379379}, {"id": 46, "seek": 21230, "start": 225.3, "end": 227.3, "text": " Co si\u0119 dzieje, kiedy ju\u017c go u\u017cywamy?", "tokens": [51014, 3066, 3244, 17953, 2884, 11, 18777, 10678, 352, 34097, 86, 7804, 30, 51114], "temperature": 0.0, "avg_logprob": -0.08672199510548213, "compression_ratio": 1.4195804195804196, "no_speech_prob": 0.08168443292379379}, {"id": 47, "seek": 21230, "start": 227.3, "end": 229.3, "text": " I tu dzieje si\u0119 ca\u0142a magia.", "tokens": [51114, 286, 2604, 17953, 2884, 3244, 1335, 5024, 2258, 654, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08672199510548213, "compression_ratio": 1.4195804195804196, "no_speech_prob": 0.08168443292379379}, {"id": 48, "seek": 21230, "start": 229.3, "end": 236.3, "text": " Kiedy motel jest ju\u017c wytrenowany i zaczynamy go u\u017cywa\u0107 do generowania tekstu, co w \u017cargonie nazywamy inferencj\u0105,", "tokens": [51214, 591, 16446, 2184, 338, 3492, 10678, 261, 4328, 1095, 23341, 741, 43811, 5378, 88, 352, 34097, 25234, 360, 1337, 21308, 16624, 372, 84, 11, 598, 261, 19625, 289, 10660, 414, 20151, 27112, 7804, 13596, 22660, 8555, 11, 51564], "temperature": 0.0, "avg_logprob": -0.08672199510548213, "compression_ratio": 1.4195804195804196, "no_speech_prob": 0.08168443292379379}, {"id": 49, "seek": 21230, "start": 236.3, "end": 239.3, "text": " architektura przyk prze\u0142\u0105cza si\u0119 w tryb R&N.", "tokens": [51564, 3912, 642, 2320, 2991, 6501, 74, 8325, 15926, 41524, 3244, 261, 853, 65, 497, 5, 45, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08672199510548213, "compression_ratio": 1.4195804195804196, "no_speech_prob": 0.08168443292379379}, {"id": 50, "seek": 23930, "start": 240.3, "end": 243.3, "text": " I nagle zyskuje wszystkie zalety tego starego podej\u015bcia.", "tokens": [50414, 286, 297, 15088, 710, 749, 5279, 2884, 31723, 29599, 2210, 8627, 22432, 1571, 7468, 73, 1788, 2755, 13, 50564], "temperature": 0.0, "avg_logprob": -0.06519686770276958, "compression_ratio": 1.3905723905723906, "no_speech_prob": 0.02139834314584732}, {"id": 51, "seek": 23930, "start": 243.3, "end": 244.3, "text": " Czyli?", "tokens": [50564, 37099, 30, 50614], "temperature": 0.0, "avg_logprob": -0.06519686770276958, "compression_ratio": 1.3905723905723906, "no_speech_prob": 0.02139834314584732}, {"id": 52, "seek": 23930, "start": 244.3, "end": 250.3, "text": " Sta\u0142e, niskie zu\u017cycie pami\u0119ci i mocy obliczeniowej, niezale\u017cnie od tego, jak d\u0142ugi jest tekst.", "tokens": [50614, 16959, 19827, 11, 297, 7797, 414, 2164, 7735, 4260, 31088, 537, 741, 705, 1344, 1111, 1050, 42124, 21091, 11, 33511, 45494, 2766, 3611, 8627, 11, 4207, 44042, 24780, 3492, 16624, 372, 13, 50914], "temperature": 0.0, "avg_logprob": -0.06519686770276958, "compression_ratio": 1.3905723905723906, "no_speech_prob": 0.02139834314584732}, {"id": 53, "seek": 23930, "start": 250.3, "end": 258.3, "text": " Przetwarza jedno s\u0142owo, aktualizuje sw\u00f3j wewn\u0119trzny stan i idzie dalej, bez tej kwadratowej eksplozji koszt\u00f3w.", "tokens": [50914, 2114, 40399, 6925, 2394, 5232, 1771, 15116, 19941, 11, 13680, 901, 590, 13008, 1693, 18999, 321, 895, 1274, 6903, 89, 1634, 27984, 741, 4496, 3283, 34257, 11, 10782, 12573, 23846, 345, 4481, 21091, 30724, 564, 15151, 4013, 19532, 2682, 3901, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06519686770276958, "compression_ratio": 1.3905723905723906, "no_speech_prob": 0.02139834314584732}, {"id": 54, "seek": 23930, "start": 258.3, "end": 265.3, "text": " Czyli to troch\u0119 tak jakby mie\u0107 samoch\u00f3d, kt\u00f3ry na torze wy\u015bcigowym, czyli podczas treningu, jest bolidem Formu\u0142y 1?", "tokens": [51314, 37099, 281, 24926, 991, 28976, 35612, 3247, 8997, 17081, 11, 9913, 1667, 3930, 1381, 4628, 1788, 66, 328, 31691, 11, 16591, 2497, 30989, 2192, 773, 84, 11, 3492, 8986, 327, 443, 10126, 84, 6825, 502, 30, 51664], "temperature": 0.0, "avg_logprob": -0.06519686770276958, "compression_ratio": 1.3905723905723906, "no_speech_prob": 0.02139834314584732}, {"id": 55, "seek": 23930, "start": 265.3, "end": 266.3, "text": " Dok\u0142adnie.", "tokens": [51664, 29768, 10358, 2766, 13, 51714], "temperature": 0.0, "avg_logprob": -0.06519686770276958, "compression_ratio": 1.3905723905723906, "no_speech_prob": 0.02139834314584732}, {"id": 56, "seek": 26630, "start": 266.3, "end": 277.3, "text": " Zoptymalizowanym pod k\u0105tem maksymalnej pr\u0119dko\u015bci, a na co dzie\u0144 w mie\u015bcie, czyli podczas tej inferencji, to jest superoszcz\u0119dny pojazd elektryczny, kt\u00f3ry zu\u017cywa minimum energii.", "tokens": [50364, 1176, 404, 874, 5579, 590, 23341, 76, 2497, 350, 1611, 18275, 963, 3187, 5579, 11794, 582, 6298, 4093, 6199, 11, 257, 1667, 598, 47568, 261, 12597, 9815, 11, 16591, 2497, 30989, 12573, 13596, 268, 19649, 11, 281, 3492, 1687, 329, 43771, 6298, 1634, 714, 34820, 67, 26991, 627, 3689, 1634, 11, 9913, 2164, 7735, 4151, 7285, 10575, 5597, 13, 50914], "temperature": 0.0, "avg_logprob": -0.05310203665393894, "compression_ratio": 1.3968871595330739, "no_speech_prob": 0.037093233317136765}, {"id": 57, "seek": 26630, "start": 277.3, "end": 279.3, "text": " Idealna analogia.", "tokens": [50914, 13090, 304, 629, 16660, 654, 13, 51014], "temperature": 0.0, "avg_logprob": -0.05310203665393894, "compression_ratio": 1.3968871595330739, "no_speech_prob": 0.037093233317136765}, {"id": 58, "seek": 26630, "start": 279.3, "end": 283.3, "text": " To jest naprawd\u0119 sprytne, ale jak to jest technicznie w og\u00f3le mo\u017cliwe?", "tokens": [51014, 1407, 3492, 20970, 637, 627, 83, 716, 11, 6775, 4207, 281, 3492, 1537, 17946, 2766, 261, 29229, 30854, 826, 30, 51214], "temperature": 0.0, "avg_logprob": -0.05310203665393894, "compression_ratio": 1.3968871595330739, "no_speech_prob": 0.037093233317136765}, {"id": 59, "seek": 26630, "start": 283.3, "end": 287.3, "text": " Jaki jest ten sekretny sk\u0142adnik, kt\u00f3ry pozwala na takie prze\u0142\u0105czanie tryb\u00f3w?", "tokens": [51214, 508, 7421, 3492, 2064, 17215, 1505, 1634, 1110, 10358, 13123, 11, 9913, 40557, 5159, 1667, 15963, 8325, 43558, 7155, 853, 65, 3901, 30, 51414], "temperature": 0.0, "avg_logprob": -0.05310203665393894, "compression_ratio": 1.3968871595330739, "no_speech_prob": 0.037093233317136765}, {"id": 60, "seek": 28730, "start": 287.3, "end": 301.3, "text": " Sekretnym sk\u0142adnikiem jest zast\u0105pienie tego klasycznego drogiego mechanizmu E-Tension, kt\u00f3ry wymaga por\u00f3wnowania w ka\u017cdego s\u0142owa z ka\u017cdym, czym\u015b, co autorzy nazywaj\u0105 operatorem WKV, opartym na atencji liniowej.", "tokens": [50364, 24285, 1505, 12996, 1110, 10358, 13123, 4907, 3492, 36746, 1611, 79, 27385, 8627, 9671, 5871, 3689, 11858, 3789, 70, 12200, 4236, 590, 20140, 462, 12, 51, 3378, 11, 9913, 29764, 9286, 1515, 812, 895, 21308, 261, 21912, 67, 6308, 15116, 5528, 710, 31615, 76, 11, 31466, 1788, 11, 598, 19510, 1229, 20151, 27112, 11133, 2208, 267, 37956, 343, 42, 53, 11, 999, 446, 4199, 1667, 412, 268, 19649, 287, 3812, 21091, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12108481557745683, "compression_ratio": 1.2372881355932204, "no_speech_prob": 0.38236936926841736}, {"id": 61, "seek": 30130, "start": 302.3, "end": 309.3, "text": " Zamiast tej wszechobecznej rozmowy wszystkich ze wszystkimi, informacje s\u0105 przekazywane sekwencyjnie, krok po kroku.", "tokens": [50414, 1176, 4526, 525, 12573, 37647, 19439, 78, 8123, 89, 11794, 35234, 10089, 34234, 5277, 14615, 10121, 11, 1356, 29293, 9015, 29785, 921, 27112, 1929, 17215, 86, 3020, 73, 2766, 11, 350, 31621, 714, 45909, 5279, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08567706438211295, "compression_ratio": 1.4414414414414414, "no_speech_prob": 0.7249261140823364}, {"id": 62, "seek": 30130, "start": 309.3, "end": 321.3, "text": " Ale najwi\u0119kszymi prze\u0142omami by\u0142o takie sformu\u0142owanie tego matematycznie, \u017ceby w trybie treningu da\u0142o si\u0119 to w pe\u0142ni zr\u00f3wnolegli\u0107, a w trybie inferencji mo\u017cna by\u0142o to oblicza\u0107 rekurencyjnie.", "tokens": [50764, 9366, 48636, 1694, 1229, 3057, 8325, 1221, 298, 4526, 14811, 15963, 262, 837, 84, 1221, 22028, 8627, 3803, 8615, 17466, 2766, 11, 11316, 261, 853, 7392, 2192, 773, 84, 1120, 5249, 3244, 281, 261, 43205, 3722, 710, 11721, 895, 4812, 70, 2081, 2162, 11, 257, 261, 853, 7392, 13596, 268, 19649, 17790, 14811, 281, 1111, 1050, 35873, 33881, 9873, 42949, 2766, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08567706438211295, "compression_ratio": 1.4414414414414414, "no_speech_prob": 0.7249261140823364}, {"id": 63, "seek": 32130, "start": 321.3, "end": 327.3, "text": " Czyli to unika problemu N do kwadratu, bo nie ma ju\u017c tej siatki po\u0142\u0105cze\u0144 ka\u017cdego z ka\u017cdym?", "tokens": [50364, 37099, 281, 517, 5439, 1154, 84, 426, 360, 23846, 345, 4481, 84, 11, 748, 2838, 463, 10678, 12573, 1511, 267, 2984, 714, 15926, 9680, 5248, 21912, 67, 6308, 710, 31615, 76, 30, 50664], "temperature": 0.0, "avg_logprob": -0.05332075669461449, "compression_ratio": 1.43, "no_speech_prob": 0.4697248041629791}, {"id": 64, "seek": 32130, "start": 327.3, "end": 333.3, "text": " To bardziej jak sztafeta, gdzie pa\u0142eczka z informacjami jest przekazywana od jednego biegacza do drugiego?", "tokens": [50664, 1407, 27209, 4207, 262, 2682, 2792, 7664, 11, 18922, 2502, 33560, 89, 2330, 710, 1356, 326, 73, 4526, 3492, 29785, 921, 27112, 2095, 3611, 5232, 11858, 272, 20408, 326, 2394, 360, 4110, 12200, 30, 50964], "temperature": 0.0, "avg_logprob": -0.05332075669461449, "compression_ratio": 1.43, "no_speech_prob": 0.4697248041629791}, {"id": 65, "seek": 32130, "start": 333.3, "end": 334.3, "text": " Tak.", "tokens": [50964, 9118, 13, 51014], "temperature": 0.0, "avg_logprob": -0.05332075669461449, "compression_ratio": 1.43, "no_speech_prob": 0.4697248041629791}, {"id": 66, "seek": 32130, "start": 334.3, "end": 339.3, "text": " Ale zaprojektowana tak, \u017ce podczas treningu mo\u017cemy patrze\u0107 na wszystkich biegaczy naraz?", "tokens": [51014, 9366, 14223, 340, 14930, 40458, 991, 11, 3561, 2497, 30989, 2192, 773, 84, 26500, 1947, 13503, 2162, 1667, 34234, 272, 20408, 14691, 6714, 921, 30, 51264], "temperature": 0.0, "avg_logprob": -0.05332075669461449, "compression_ratio": 1.43, "no_speech_prob": 0.4697248041629791}, {"id": 67, "seek": 32130, "start": 339.3, "end": 349.3, "text": " Po jest doskona\u0142a analogia, a sama nazwa RWKV bierze si\u0119 z czterech wektor\u00f3w, kt\u00f3re steruj\u0105 t\u0105 sztafet\u0105 w ka\u017cdym kroku.", "tokens": [51264, 6165, 3492, 4491, 74, 4037, 5024, 16660, 654, 11, 257, 17768, 20151, 4151, 42513, 42, 53, 272, 811, 1381, 3244, 710, 269, 2682, 323, 339, 321, 28359, 3901, 11, 8864, 18924, 13263, 32294, 262, 2682, 2792, 302, 1611, 261, 31615, 76, 45909, 5279, 13, 51764], "temperature": 0.0, "avg_logprob": -0.05332075669461449, "compression_ratio": 1.43, "no_speech_prob": 0.4697248041629791}, {"id": 68, "seek": 34930, "start": 349.3, "end": 355.3, "text": " Mamy R jak receptance, czyli tak\u0105 bramk\u0119, kt\u00f3ra decyduje, ile nowej informacji przyj\u0105\u0107.", "tokens": [50364, 376, 7804, 497, 4207, 15263, 719, 11, 16591, 31069, 738, 335, 15724, 11, 19456, 979, 88, 769, 2884, 11, 15465, 586, 40779, 1356, 13152, 6501, 8555, 2162, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11313396453857422, "compression_ratio": 1.4384615384615385, "no_speech_prob": 0.012273306958377361}, {"id": 69, "seek": 34930, "start": 355.3, "end": 362.3, "text": " Mamy W jak weight, czyli wektor zaniku, kt\u00f3ry m\u00f3wi, jak szybko zapomina\u0107 o przesz\u0142o\u015bci.", "tokens": [50664, 376, 7804, 343, 4207, 3364, 11, 16591, 321, 28359, 710, 282, 24320, 11, 9913, 24592, 11, 4207, 36456, 4093, 14223, 49217, 2162, 277, 6541, 10430, 35059, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11313396453857422, "compression_ratio": 1.4384615384615385, "no_speech_prob": 0.012273306958377361}, {"id": 70, "seek": 34930, "start": 362.3, "end": 366.3, "text": " To jest kluczowe, \u017ceby model nieuton\u0105\u0142 w starych danych.", "tokens": [51014, 1407, 3492, 9671, 1311, 89, 6880, 11, 11316, 2316, 2838, 325, 266, 1611, 1221, 261, 342, 822, 339, 274, 34644, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11313396453857422, "compression_ratio": 1.4384615384615385, "no_speech_prob": 0.012273306958377361}, {"id": 71, "seek": 34930, "start": 366.3, "end": 376.3, "text": " Mamy te klasyczn\u0105 par\u0119, K i V, czyli key i value, kt\u00f3re podobnie jak w transformerach, przenosz\u0105 z sam\u0105 tre\u015b\u0107 informacji.", "tokens": [51214, 376, 7804, 535, 9671, 5871, 3689, 13113, 971, 1274, 11, 591, 741, 691, 11, 16591, 2141, 741, 2158, 11, 8864, 43024, 2766, 4207, 261, 31782, 608, 11, 582, 2904, 329, 8925, 710, 3247, 1611, 2192, 7753, 1356, 13152, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11313396453857422, "compression_ratio": 1.4384615384615385, "no_speech_prob": 0.012273306958377361}, {"id": 72, "seek": 37630, "start": 376.3, "end": 381.3, "text": " To w\u0142a\u015bnie interakcja tych czterech element\u00f3w tworzy ten dynamiczny, hybrydowy mechanizm.", "tokens": [50364, 1407, 14234, 728, 514, 34056, 15180, 269, 2682, 323, 339, 4478, 3901, 46288, 1229, 2064, 8546, 89, 1634, 11, 2477, 65, 627, 67, 10089, 4236, 590, 76, 13, 50614], "temperature": 0.0, "avg_logprob": -0.05384789386265714, "compression_ratio": 1.4219269102990033, "no_speech_prob": 0.11917643249034882}, {"id": 73, "seek": 37630, "start": 381.3, "end": 384.3, "text": " Okej, teoria jest bardzo elegancka.", "tokens": [50614, 29094, 73, 11, 535, 8172, 3492, 9034, 1118, 1275, 39342, 13, 50764], "temperature": 0.0, "avg_logprob": -0.05384789386265714, "compression_ratio": 1.4219269102990033, "no_speech_prob": 0.11917643249034882}, {"id": 74, "seek": 37630, "start": 384.3, "end": 388.3, "text": " Hybrydowy model, kt\u00f3ry \u0142\u0105czy najlepsze cechy obu \u015bwiat\u00f3w.", "tokens": [50764, 5701, 65, 627, 67, 10089, 2316, 11, 9913, 220, 15926, 6522, 41903, 1878, 1381, 1769, 28629, 1111, 84, 36425, 3901, 13, 50964], "temperature": 0.0, "avg_logprob": -0.05384789386265714, "compression_ratio": 1.4219269102990033, "no_speech_prob": 0.11917643249034882}, {"id": 75, "seek": 37630, "start": 388.3, "end": 390.3, "text": " To brzmi \u015bwietnie na papierze.", "tokens": [50964, 1407, 738, 89, 3057, 8299, 39083, 2766, 1667, 37410, 1381, 13, 51064], "temperature": 0.0, "avg_logprob": -0.05384789386265714, "compression_ratio": 1.4219269102990033, "no_speech_prob": 0.11917643249034882}, {"id": 76, "seek": 37630, "start": 390.3, "end": 395.3, "text": " Ale w \u015bwiecie AI teoria to jedno, a dowody to drugie.", "tokens": [51064, 9366, 261, 40078, 4260, 7318, 535, 8172, 281, 5232, 1771, 11, 257, 9459, 843, 281, 4110, 414, 13, 51314], "temperature": 0.0, "avg_logprob": -0.05384789386265714, "compression_ratio": 1.4219269102990033, "no_speech_prob": 0.11917643249034882}, {"id": 77, "seek": 37630, "start": 395.3, "end": 399.3, "text": " Wiele pi\u0119knych pomys\u0142\u00f3w po prostu poleg\u0142o w starciu z rzeczywisto\u015bci\u0105.", "tokens": [51314, 9233, 306, 48085, 9399, 12991, 39508, 3901, 714, 19518, 714, 6363, 5249, 261, 3543, 30795, 710, 26297, 86, 9334, 50227, 13, 51514], "temperature": 0.0, "avg_logprob": -0.05384789386265714, "compression_ratio": 1.4219269102990033, "no_speech_prob": 0.11917643249034882}, {"id": 78, "seek": 37630, "start": 399.3, "end": 404.3, "text": " Czy autorze przedstawili jakie\u015b twarde dane, \u017ce to faktycznie dzia\u0142a?", "tokens": [51514, 19832, 19510, 1381, 45616, 2312, 31163, 683, 10866, 49206, 11, 3561, 281, 33647, 45586, 37903, 30, 51764], "temperature": 0.0, "avg_logprob": -0.05384789386265714, "compression_ratio": 1.4219269102990033, "no_speech_prob": 0.11917643249034882}, {"id": 79, "seek": 40430, "start": 404.3, "end": 408.3, "text": " Zbudowali model, kt\u00f3ry jest w stanie konkurowa\u0107 z tymi gigantami?", "tokens": [50364, 1176, 18281, 305, 5103, 2316, 11, 9913, 3492, 261, 40013, 21428, 374, 11445, 710, 1104, 3057, 8741, 394, 4526, 30, 50564], "temperature": 0.0, "avg_logprob": -0.05369325602276725, "compression_ratio": 1.4490445859872612, "no_speech_prob": 0.04786837473511696}, {"id": 80, "seek": 40430, "start": 408.3, "end": 412.3, "text": " I to jest, moim zdaniem, najbardziej imponuj\u0105ca cz\u0119\u015b\u0107 tej ca\u0142ej pracy.", "tokens": [50564, 286, 281, 3492, 11, 48569, 710, 10312, 4907, 11, 41857, 704, 266, 13263, 496, 47149, 12573, 47631, 73, 35591, 13, 50764], "temperature": 0.0, "avg_logprob": -0.05369325602276725, "compression_ratio": 1.4490445859872612, "no_speech_prob": 0.04786837473511696}, {"id": 81, "seek": 40430, "start": 412.3, "end": 416.3, "text": " Oni nie poprzestali na jakim\u015b ma\u0142ym, koncepcyjnym modeliku.", "tokens": [50764, 1282, 72, 2838, 1665, 19390, 377, 5103, 1667, 49410, 1788, 463, 1221, 4199, 11, 5897, 27493, 42949, 12996, 2316, 24320, 13, 50964], "temperature": 0.0, "avg_logprob": -0.05369325602276725, "compression_ratio": 1.4490445859872612, "no_speech_prob": 0.04786837473511696}, {"id": 82, "seek": 40430, "start": 416.3, "end": 422.3, "text": " Zbudowali i wytrenowali modele RWKV o skali dochodz\u0105cej do 14 miliard\u00f3w parametr\u00f3w.", "tokens": [50964, 1176, 18281, 305, 5103, 741, 261, 4328, 1095, 305, 5103, 4391, 306, 42513, 42, 53, 277, 1110, 5103, 9243, 378, 8925, 20811, 360, 3499, 1962, 72, 515, 3901, 6220, 27965, 3901, 13, 51264], "temperature": 0.0, "avg_logprob": -0.05369325602276725, "compression_ratio": 1.4490445859872612, "no_speech_prob": 0.04786837473511696}, {"id": 83, "seek": 40430, "start": 422.3, "end": 424.3, "text": " 14 miliard\u00f3w?", "tokens": [51264, 3499, 1962, 72, 515, 3901, 30, 51364], "temperature": 0.0, "avg_logprob": -0.05369325602276725, "compression_ratio": 1.4490445859872612, "no_speech_prob": 0.04786837473511696}, {"id": 84, "seek": 40430, "start": 424.3, "end": 425.3, "text": " Tak.", "tokens": [51364, 9118, 13, 51414], "temperature": 0.0, "avg_logprob": -0.05369325602276725, "compression_ratio": 1.4490445859872612, "no_speech_prob": 0.04786837473511696}, {"id": 85, "seek": 40430, "start": 425.3, "end": 433.3, "text": " I \u017ceby da\u0107 jaki\u015b kontekst, to czyni go najwi\u0119ksz\u0105, g\u0119st\u0105 sieci\u0105 RNN, jak\u0105 kiedykolwiek w historii uda\u0142o si\u0119 skutecznie wytrenowa\u0107.", "tokens": [51414, 286, 11316, 1120, 2162, 34721, 14373, 916, 372, 11, 281, 6430, 3722, 352, 48636, 1694, 8925, 11, 290, 1274, 372, 1611, 2804, 34381, 45702, 45, 11, 46719, 18777, 36620, 44674, 261, 4058, 5597, 44544, 5249, 3244, 1110, 1169, 19923, 261, 4328, 1095, 11445, 13, 51814], "temperature": 0.0, "avg_logprob": -0.05369325602276725, "compression_ratio": 1.4490445859872612, "no_speech_prob": 0.04786837473511696}, {"id": 86, "seek": 43330, "start": 433.3, "end": 436.3, "text": " To ju\u017c samo w sobie jest ogromnym osi\u0105gni\u0119ciem in\u017cynieryjnym.", "tokens": [50364, 1407, 10678, 36422, 261, 13652, 3492, 34416, 298, 12996, 3003, 11404, 70, 35938, 4260, 76, 294, 1427, 2534, 811, 88, 73, 12996, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07037607298956977, "compression_ratio": 1.4269005847953216, "no_speech_prob": 0.007563305553048849}, {"id": 87, "seek": 43330, "start": 436.3, "end": 440.3, "text": " Czyli udawodnili, \u017ce architektura faktycznie si\u0119 skaluje.", "tokens": [50514, 37099, 11727, 1607, 378, 77, 2312, 11, 3561, 3912, 642, 2320, 2991, 33647, 45586, 3244, 16890, 13008, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07037607298956977, "compression_ratio": 1.4269005847953216, "no_speech_prob": 0.007563305553048849}, {"id": 88, "seek": 43330, "start": 440.3, "end": 442.3, "text": " A co z wynikami?", "tokens": [50714, 316, 598, 710, 31936, 1035, 4526, 30, 50814], "temperature": 0.0, "avg_logprob": -0.07037607298956977, "compression_ratio": 1.4269005847953216, "no_speech_prob": 0.007563305553048849}, {"id": 89, "seek": 43330, "start": 442.3, "end": 448.3, "text": " Jak ten 14 miliardowy gigant wypada w por\u00f3wnaniu z transformerami o podobnej wielko\u015bci?", "tokens": [50814, 15029, 2064, 3499, 1962, 72, 515, 10089, 8741, 394, 46392, 1538, 261, 1515, 812, 895, 25849, 710, 31782, 4526, 277, 43024, 11794, 20570, 4093, 6199, 30, 51114], "temperature": 0.0, "avg_logprob": -0.07037607298956977, "compression_ratio": 1.4269005847953216, "no_speech_prob": 0.007563305553048849}, {"id": 90, "seek": 43330, "start": 448.3, "end": 451.3, "text": " Jak modele z rodziny Bloom, Pythia czy OPT?", "tokens": [51114, 15029, 4391, 306, 710, 28607, 3519, 25927, 11, 9953, 392, 654, 6430, 23324, 51, 30, 51264], "temperature": 0.0, "avg_logprob": -0.07037607298956977, "compression_ratio": 1.4269005847953216, "no_speech_prob": 0.007563305553048849}, {"id": 91, "seek": 43330, "start": 451.3, "end": 455.3, "text": " Kiedy spojrz\u0119 si\u0119 na wykres jeden w artykule, wynik jest po prostu zdumiewaj\u0105cy.", "tokens": [51264, 591, 16446, 8243, 73, 81, 11052, 3244, 1667, 39287, 495, 12906, 261, 594, 874, 74, 2271, 11, 31936, 1035, 3492, 714, 19518, 16221, 449, 1093, 11133, 1344, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07037607298956977, "compression_ratio": 1.4269005847953216, "no_speech_prob": 0.007563305553048849}, {"id": 92, "seek": 43330, "start": 455.3, "end": 462.3, "text": " On por\u00f3wnuje krzyw\u0119 uczenia, czyli to, jak spada b\u0142\u0105d modelu w miar\u0119 zwi\u0119kszania mocy obliczeniowej w\u0142o\u017conej w trening.", "tokens": [51464, 1282, 1515, 812, 895, 13008, 350, 13047, 86, 1274, 344, 38517, 11, 16591, 281, 11, 4207, 637, 1538, 272, 15926, 67, 2316, 84, 261, 2752, 289, 1274, 11873, 5034, 1694, 89, 5609, 705, 1344, 1111, 1050, 42124, 21091, 261, 5249, 1427, 546, 73, 261, 2192, 773, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07037607298956977, "compression_ratio": 1.4269005847953216, "no_speech_prob": 0.007563305553048849}, {"id": 93, "seek": 46230, "start": 462.3, "end": 468.3, "text": " I te krzywe dla RWKV i dla transformer\u00f3w o por\u00f3wnywalnej wielko\u015bci s\u0105 praktycznie na\u0142o\u017cone na siebie.", "tokens": [50364, 286, 535, 350, 13047, 826, 12285, 42513, 42, 53, 741, 12285, 31782, 3901, 277, 1515, 812, 895, 27112, 304, 11794, 20570, 4093, 6199, 9015, 3206, 74, 45586, 1667, 5249, 1427, 546, 1667, 39137, 13, 50664], "temperature": 0.0, "avg_logprob": -0.05738766128952439, "compression_ratio": 1.4573378839590443, "no_speech_prob": 0.011211893521249294}, {"id": 94, "seek": 46230, "start": 468.3, "end": 470.3, "text": " Niemal identyczne.", "tokens": [50664, 426, 4907, 304, 2473, 17466, 716, 13, 50764], "temperature": 0.0, "avg_logprob": -0.05738766128952439, "compression_ratio": 1.4573378839590443, "no_speech_prob": 0.011211893521249294}, {"id": 95, "seek": 46230, "start": 470.3, "end": 472.3, "text": " Tak, r\u00f3\u017cnice s\u0105 minimalne.", "tokens": [50764, 9118, 11, 19637, 77, 573, 9015, 13206, 716, 13, 50864], "temperature": 0.0, "avg_logprob": -0.05738766128952439, "compression_ratio": 1.4573378839590443, "no_speech_prob": 0.011211893521249294}, {"id": 96, "seek": 46230, "start": 472.3, "end": 481.3, "text": " To pokazuje, \u017ce pod wzgl\u0119dem jako\u015bci, zdolno\u015bci do nauki j\u0119zyka, RWKV osi\u0105ga ten sam poziom, co wsp\u00f3\u0142czesne transformery.", "tokens": [50864, 1407, 13010, 43317, 11, 3561, 2497, 48538, 6298, 443, 17123, 6199, 11, 16221, 401, 16438, 360, 35616, 2984, 42309, 40940, 11, 42513, 42, 53, 3003, 11404, 3680, 2064, 3247, 38503, 298, 11, 598, 39069, 3689, 279, 716, 4088, 2109, 13, 51314], "temperature": 0.0, "avg_logprob": -0.05738766128952439, "compression_ratio": 1.4573378839590443, "no_speech_prob": 0.011211893521249294}, {"id": 97, "seek": 46230, "start": 481.3, "end": 483.3, "text": " Tu nie ma kompromisu.", "tokens": [51314, 7836, 2838, 463, 5207, 28722, 25871, 13, 51414], "temperature": 0.0, "avg_logprob": -0.05738766128952439, "compression_ratio": 1.4573378839590443, "no_speech_prob": 0.011211893521249294}, {"id": 98, "seek": 46230, "start": 483.3, "end": 490.3, "text": " Dobra, ale co z t\u0105 g\u0142\u00f3wn\u0105 obietnic\u0105 wydajno\u015bci\u0105 podczas inferencji? To przecie\u017c by\u0142 ten g\u0142\u00f3wny cel, prawda?", "tokens": [51414, 413, 24393, 11, 6775, 598, 710, 32294, 18117, 812, 895, 1611, 1111, 1684, 7692, 1611, 25984, 1805, 16438, 1611, 2497, 30989, 13596, 268, 19649, 30, 1407, 8325, 40082, 16673, 2064, 18117, 812, 43682, 9277, 11, 43607, 30, 51764], "temperature": 0.0, "avg_logprob": -0.05738766128952439, "compression_ratio": 1.4573378839590443, "no_speech_prob": 0.011211893521249294}, {"id": 99, "seek": 49030, "start": 490.3, "end": 495.3, "text": " I tutaj wida\u0107 prawdziw\u0105 rewolucj\u0119. Wykrez siedem jest chyba najbardziej wymowny.", "tokens": [50364, 286, 12749, 261, 46898, 41175, 3992, 86, 1611, 319, 48481, 1311, 11115, 13, 14458, 27885, 89, 262, 1091, 443, 3492, 31532, 41857, 29764, 648, 88, 13, 50614], "temperature": 0.0, "avg_logprob": -0.051281826168883084, "compression_ratio": 1.3993506493506493, "no_speech_prob": 0.09862995147705078}, {"id": 100, "seek": 49030, "start": 495.3, "end": 499.3, "text": " Pokazuje skumulowany czas potrzebny na wygenerowanie d\u0142ugiego tekstu.", "tokens": [50614, 14958, 43317, 1110, 449, 425, 23341, 13190, 37595, 1634, 1667, 4628, 21848, 22028, 274, 34077, 12200, 16624, 372, 84, 13, 50814], "temperature": 0.0, "avg_logprob": -0.051281826168883084, "compression_ratio": 1.3993506493506493, "no_speech_prob": 0.09862995147705078}, {"id": 101, "seek": 49030, "start": 499.3, "end": 506.3, "text": " Dla transformer\u00f3w ta linia startuje p\u0142asko, a potem nagle gwa\u0142townie pnie si\u0119 w g\u00f3r\u0119, prawie pionowo.", "tokens": [50814, 413, 875, 31782, 3901, 1846, 22896, 654, 722, 13008, 28695, 3863, 78, 11, 257, 36513, 297, 15088, 290, 44603, 30401, 414, 280, 2766, 3244, 261, 290, 15614, 1274, 11, 3206, 8699, 280, 313, 19941, 13, 51164], "temperature": 0.0, "avg_logprob": -0.051281826168883084, "compression_ratio": 1.3993506493506493, "no_speech_prob": 0.09862995147705078}, {"id": 102, "seek": 49030, "start": 506.3, "end": 511.3, "text": " To jest w\u0142a\u015bnie ta kwadratowa z\u0142o\u017cono\u015b\u0107, kt\u00f3ra uderza w nas jak \u015bciana.", "tokens": [51164, 1407, 3492, 14234, 1846, 23846, 345, 4481, 5528, 710, 5249, 1427, 8957, 7753, 11, 19456, 344, 1068, 2394, 261, 5382, 4207, 220, 6199, 2095, 13, 51414], "temperature": 0.0, "avg_logprob": -0.051281826168883084, "compression_ratio": 1.3993506493506493, "no_speech_prob": 0.09862995147705078}, {"id": 103, "seek": 49030, "start": 511.3, "end": 516.3, "text": " A dla RWKV ta linia to jest praktycznie prosta, \u0142agodnie nachylona kreska.", "tokens": [51414, 316, 12285, 42513, 42, 53, 1846, 22896, 654, 281, 3492, 3206, 74, 45586, 582, 8638, 11, 25387, 559, 378, 2766, 5168, 5088, 4037, 350, 495, 2330, 13, 51664], "temperature": 0.0, "avg_logprob": -0.051281826168883084, "compression_ratio": 1.3993506493506493, "no_speech_prob": 0.09862995147705078}, {"id": 104, "seek": 49030, "start": 516.3, "end": 517.3, "text": " Niesamowite.", "tokens": [51664, 426, 530, 335, 305, 642, 13, 51714], "temperature": 0.0, "avg_logprob": -0.051281826168883084, "compression_ratio": 1.3993506493506493, "no_speech_prob": 0.09862995147705078}, {"id": 105, "seek": 51730, "start": 517.3, "end": 524.3, "text": " To znaczy, \u017ce generowanie tysi\u0105ca s\u0142\u00f3w czy 100 tysi\u0119cy s\u0142\u00f3w nie stanowi dla niego problemu pod wzgl\u0119dem koszt\u00f3w.", "tokens": [50364, 1407, 36584, 11, 3561, 1337, 22028, 38156, 11404, 496, 15116, 3901, 6430, 2319, 38156, 47303, 15116, 3901, 2838, 27984, 24503, 12285, 49615, 1154, 84, 2497, 48538, 6298, 443, 19532, 2682, 3901, 13, 50714], "temperature": 0.0, "avg_logprob": -0.06464929888325353, "compression_ratio": 1.4803625377643506, "no_speech_prob": 0.07101774960756302}, {"id": 106, "seek": 51730, "start": 524.3, "end": 533.3, "text": " Pami\u0119tam, jak kilka lat temu pr\u00f3ba wygenerowania d\u0142u\u017cszego eseju na modelu GPT-2 na moim domowym komputerze ko\u0144czy\u0142a si\u0119 po prostu b\u0142\u0119dem braku pami\u0119ci.", "tokens": [50714, 430, 23806, 37323, 11, 4207, 36466, 4465, 33346, 8565, 4231, 4628, 21848, 21308, 274, 24066, 1427, 15453, 6308, 10167, 8954, 1667, 2316, 84, 26039, 51, 12, 17, 1667, 48569, 3285, 31691, 5207, 13849, 1381, 26470, 6522, 5024, 3244, 714, 19518, 272, 1221, 6298, 443, 1548, 5279, 31088, 537, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06464929888325353, "compression_ratio": 1.4803625377643506, "no_speech_prob": 0.07101774960756302}, {"id": 107, "seek": 51730, "start": 533.3, "end": 535.3, "text": " Tak, znam to.", "tokens": [51164, 9118, 11, 710, 5378, 281, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06464929888325353, "compression_ratio": 1.4803625377643506, "no_speech_prob": 0.07101774960756302}, {"id": 108, "seek": 51730, "start": 535.3, "end": 539.3, "text": " Tutaj m\u00f3wimy o czym\u015b, co mog\u0142oby dzia\u0142a\u0107 p\u0142ynnie bez \u017cadnego zaj\u0119kni\u0119cia.", "tokens": [51264, 41819, 13489, 13189, 277, 31466, 1788, 11, 598, 13172, 1221, 13944, 37903, 2162, 28695, 2534, 2766, 10782, 39628, 11858, 33729, 1274, 74, 35938, 2755, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06464929888325353, "compression_ratio": 1.4803625377643506, "no_speech_prob": 0.07101774960756302}, {"id": 109, "seek": 51730, "start": 539.3, "end": 546.3, "text": " To jest ten moment, aha, zatem mamy t\u0119 sam\u0105 jako\u015b\u0107, ale przy radykalnie ni\u017cszych kosztach operacyjnych.", "tokens": [51464, 1407, 3492, 2064, 1623, 11, 47340, 11, 710, 26851, 17335, 32489, 3247, 1611, 17123, 7753, 11, 6775, 6501, 367, 880, 19990, 2766, 28502, 45021, 19532, 2682, 608, 2208, 31285, 9399, 13, 51814], "temperature": 0.0, "avg_logprob": -0.06464929888325353, "compression_ratio": 1.4803625377643506, "no_speech_prob": 0.07101774960756302}, {"id": 110, "seek": 54630, "start": 546.3, "end": 552.3, "text": " W artykule wspomniano te\u017c o czym\u015b fundamentalnym, o tak zwanych scaling laws.", "tokens": [50364, 343, 594, 874, 74, 2271, 17757, 38131, 6254, 9516, 277, 31466, 1788, 8088, 12996, 11, 277, 991, 11873, 34644, 21589, 6064, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06049089108483266, "compression_ratio": 1.434108527131783, "no_speech_prob": 0.016857624053955078}, {"id": 111, "seek": 54630, "start": 552.3, "end": 558.3, "text": " Co to w\u0142a\u015bciwie znaczy, \u017ce RWK pod\u0105\u017ca za tymi samymi prawami skalowania co transformery?", "tokens": [50664, 3066, 281, 50108, 36584, 11, 3561, 42513, 42, 2497, 27242, 64, 7949, 1104, 3057, 3247, 88, 3057, 22508, 4526, 16890, 21308, 598, 4088, 2109, 30, 50964], "temperature": 0.0, "avg_logprob": -0.06049089108483266, "compression_ratio": 1.434108527131783, "no_speech_prob": 0.016857624053955078}, {"id": 112, "seek": 54630, "start": 558.3, "end": 560.3, "text": " To jest absolutnie kluczowe odkrycie.", "tokens": [50964, 1407, 3492, 18757, 2766, 9671, 1311, 89, 6880, 3611, 43298, 4260, 13, 51064], "temperature": 0.0, "avg_logprob": -0.06049089108483266, "compression_ratio": 1.434108527131783, "no_speech_prob": 0.016857624053955078}, {"id": 113, "seek": 54630, "start": 560.3, "end": 568.3, "text": " To obala takie wieloletnie przekonanie, \u017ce RNN-y, nawet te bardziej zaawansowane jak LSTM, nie skaluj\u0105 si\u0119 tak dobrze.", "tokens": [51064, 1407, 1111, 5159, 15963, 20570, 401, 302, 2766, 29785, 266, 7155, 11, 3561, 45702, 45, 12, 88, 11, 22696, 535, 27209, 7949, 1607, 599, 23066, 4207, 441, 6840, 44, 11, 2838, 16890, 13263, 3244, 991, 28335, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06049089108483266, "compression_ratio": 1.434108527131783, "no_speech_prob": 0.016857624053955078}, {"id": 114, "seek": 54630, "start": 568.3, "end": 570.3, "text": " Co to znaczy, \u017ce si\u0119 nie skaluj\u0105?", "tokens": [51464, 3066, 281, 36584, 11, 3561, 3244, 2838, 16890, 13263, 30, 51564], "temperature": 0.0, "avg_logprob": -0.06049089108483266, "compression_ratio": 1.434108527131783, "no_speech_prob": 0.016857624053955078}, {"id": 115, "seek": 57030, "start": 570.3, "end": 575.3, "text": " To znaczy, \u017ce po przekroczeniu pewnej wielko\u015bci, dok\u0142adanie danych i mocy obliczeniowej", "tokens": [50364, 1407, 36584, 11, 3561, 714, 29785, 24174, 39651, 25889, 11794, 20570, 4093, 6199, 11, 45864, 7155, 274, 34644, 741, 705, 1344, 1111, 1050, 42124, 21091, 50614], "temperature": 0.0, "avg_logprob": -0.10843098163604736, "compression_ratio": 1.4652777777777777, "no_speech_prob": 0.18099690973758698}, {"id": 116, "seek": 57030, "start": 575.3, "end": 578.3, "text": " przestawa\u0142o przynosi\u0107 proporcjonalne korzy\u015bci.", "tokens": [50614, 44264, 10449, 5249, 6501, 16751, 12757, 2365, 36003, 15735, 304, 716, 14784, 1229, 6199, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10843098163604736, "compression_ratio": 1.4652777777777777, "no_speech_prob": 0.18099690973758698}, {"id": 117, "seek": 57030, "start": 578.3, "end": 585.3, "text": " Modele jakby nasyca\u0142y si\u0119, a autorzy RWKV udowodnili, i to wida\u0107 na wykresie cztery,", "tokens": [50764, 20500, 306, 28976, 5382, 88, 496, 6825, 3244, 11, 257, 19510, 1229, 42513, 42, 53, 11727, 305, 378, 77, 2312, 11, 741, 281, 261, 46898, 1667, 39287, 495, 414, 6472, 12733, 11, 51114], "temperature": 0.0, "avg_logprob": -0.10843098163604736, "compression_ratio": 1.4652777777777777, "no_speech_prob": 0.18099690973758698}, {"id": 118, "seek": 57030, "start": 585.3, "end": 590.3, "text": " \u017ce ich architektura skaluje si\u0119 logarytmu liniowo, dok\u0142adnie tak samo jak transformery.", "tokens": [51114, 3561, 1893, 3912, 642, 2320, 2991, 16890, 13008, 3244, 41473, 4328, 20140, 287, 3812, 19941, 11, 45864, 2766, 991, 36422, 4207, 4088, 2109, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10843098163604736, "compression_ratio": 1.4652777777777777, "no_speech_prob": 0.18099690973758698}, {"id": 119, "seek": 57030, "start": 590.3, "end": 596.3, "text": " Czyli nie ma sufitu? Nie ma teoretycznego sufitu, to jest sygna\u0142 dla ca\u0142ej spo\u0142eczno\u015bci badawczej.", "tokens": [51364, 37099, 2838, 463, 459, 6845, 84, 30, 12016, 463, 535, 418, 874, 3689, 11858, 459, 6845, 84, 11, 281, 3492, 943, 70, 629, 1221, 12285, 47631, 73, 36851, 89, 16438, 272, 1538, 86, 9680, 73, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10843098163604736, "compression_ratio": 1.4652777777777777, "no_speech_prob": 0.18099690973758698}, {"id": 120, "seek": 59630, "start": 597.3, "end": 601.3, "text": " Mo\u017cecie budowa\u0107 100 miliardowe czy bilionowe modele w tej architekturze,", "tokens": [50414, 43774, 4260, 3265, 11445, 2319, 1962, 72, 515, 6880, 6430, 8588, 313, 6880, 4391, 306, 261, 12573, 3912, 642, 2320, 374, 1381, 11, 50614], "temperature": 0.0, "avg_logprob": -0.058920275555909986, "compression_ratio": 1.4807017543859649, "no_speech_prob": 0.27438098192214966}, {"id": 121, "seek": 59630, "start": 601.3, "end": 605.3, "text": " a one b\u0119d\u0105 stawa\u0142y si\u0119 coraz m\u0105drzejsze w przewidywalny spos\u00f3b.", "tokens": [50614, 257, 472, 26239, 342, 10449, 6825, 3244, 25899, 275, 18962, 13503, 25530, 1381, 261, 39758, 327, 27112, 304, 1634, 22904, 13, 50814], "temperature": 0.0, "avg_logprob": -0.058920275555909986, "compression_ratio": 1.4807017543859649, "no_speech_prob": 0.27438098192214966}, {"id": 122, "seek": 59630, "start": 605.3, "end": 612.3, "text": " To dow\u00f3d, \u017ce to nie jest jaki\u015b niszowy trik, ale fundamentalnie solidna podstawa do budowy przysz\u0142ych i jeszcze wi\u0119kszych modeli.", "tokens": [50814, 1407, 9459, 17081, 11, 3561, 281, 2838, 3492, 34721, 297, 23848, 10089, 1376, 74, 11, 6775, 8088, 2766, 5100, 629, 2497, 372, 10449, 360, 3265, 10089, 44018, 47655, 741, 14168, 29968, 28051, 2316, 72, 13, 51164], "temperature": 0.0, "avg_logprob": -0.058920275555909986, "compression_ratio": 1.4807017543859649, "no_speech_prob": 0.27438098192214966}, {"id": 123, "seek": 59630, "start": 612.3, "end": 614.3, "text": " To brzmi prawie zbyt dobrze, \u017ceby by\u0142o prawdziwe.", "tokens": [51164, 1407, 738, 89, 3057, 3206, 8699, 710, 2322, 83, 28335, 11, 11316, 14811, 41175, 3992, 826, 13, 51264], "temperature": 0.0, "avg_logprob": -0.058920275555909986, "compression_ratio": 1.4807017543859649, "no_speech_prob": 0.27438098192214966}, {"id": 124, "seek": 59630, "start": 614.3, "end": 620.3, "text": " Ta sama jako\u015b\u0107, ta sama skalowalno\u015b\u0107, a przy tym radykalnie ni\u017csze koszty inferencji.", "tokens": [51264, 6551, 17768, 17123, 7753, 11, 1846, 17768, 16890, 305, 304, 23293, 11, 257, 6501, 8107, 367, 880, 19990, 2766, 28502, 82, 1381, 19532, 89, 874, 13596, 268, 19649, 13, 51564], "temperature": 0.0, "avg_logprob": -0.058920275555909986, "compression_ratio": 1.4807017543859649, "no_speech_prob": 0.27438098192214966}, {"id": 125, "seek": 62030, "start": 621.3, "end": 623.3, "text": " W technologii rzadko, kiedy dostajemy co\u015b za darmo,", "tokens": [50414, 343, 1537, 1132, 5597, 367, 89, 345, 4093, 11, 18777, 20568, 1805, 3633, 19241, 7949, 4072, 3280, 11, 50514], "temperature": 0.0, "avg_logprob": -0.07074283731394801, "compression_ratio": 1.4577464788732395, "no_speech_prob": 0.10554035753011703}, {"id": 126, "seek": 62030, "start": 623.3, "end": 630.3, "text": " zast\u0105pienie tej pe\u0142nej kwadratowej atencji czym\u015b liniowym, sekwencyjnym musi si\u0119 wi\u0105za\u0107 z jakim\u015b kompromisem.", "tokens": [50514, 36746, 1611, 79, 27385, 12573, 43205, 11794, 23846, 345, 4481, 21091, 21723, 19649, 31466, 1788, 287, 3812, 31691, 11, 17215, 86, 3020, 73, 12996, 37587, 3244, 261, 11404, 35873, 710, 49410, 1788, 5207, 28722, 271, 443, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07074283731394801, "compression_ratio": 1.4577464788732395, "no_speech_prob": 0.10554035753011703}, {"id": 127, "seek": 62030, "start": 630.3, "end": 635.3, "text": " Gdzie on si\u0119 objawia, co tracimy w zamian za t\u0119 niesamowit\u0105 pr\u0119dko\u015b\u0107 i oszcz\u0119dno\u015b\u0107?", "tokens": [50864, 460, 13096, 322, 3244, 1111, 22199, 654, 11, 598, 504, 326, 13189, 261, 19876, 952, 7949, 32489, 48100, 335, 305, 270, 1611, 582, 6298, 4093, 7753, 741, 3003, 43771, 6298, 23293, 30, 51114], "temperature": 0.0, "avg_logprob": -0.07074283731394801, "compression_ratio": 1.4577464788732395, "no_speech_prob": 0.10554035753011703}, {"id": 128, "seek": 62030, "start": 635.3, "end": 638.3, "text": " To jest dok\u0142adnie to pytanie, kt\u00f3re zadali sobie badacze.", "tokens": [51114, 1407, 3492, 45864, 2766, 281, 36610, 11, 8864, 42788, 5103, 13652, 1578, 326, 1381, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07074283731394801, "compression_ratio": 1.4577464788732395, "no_speech_prob": 0.10554035753011703}, {"id": 129, "seek": 62030, "start": 638.3, "end": 640.3, "text": " I s\u0105 w tej kwestii bardzo szczerzy.", "tokens": [51264, 286, 9015, 261, 12573, 42035, 5597, 9034, 22090, 260, 1229, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07074283731394801, "compression_ratio": 1.4577464788732395, "no_speech_prob": 0.10554035753011703}, {"id": 130, "seek": 62030, "start": 640.3, "end": 644.3, "text": " Kompromis istnieje i jest, co ciekawe, bardzo ciekawy.", "tokens": [51364, 14286, 28722, 271, 1418, 2766, 2884, 741, 3492, 11, 598, 30596, 2330, 826, 11, 9034, 46419, 41961, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07074283731394801, "compression_ratio": 1.4577464788732395, "no_speech_prob": 0.10554035753011703}, {"id": 131, "seek": 64430, "start": 644.3, "end": 652.3, "text": " Najwi\u0119kszym zaskoczeniem, a jednocze\u015bnie s\u0142abo\u015bci\u0105, jest niezwyk\u0142a wra\u017cliwo\u015b\u0107 RWG-KPA na tak zwany prompt engineering.", "tokens": [50364, 31576, 22423, 1694, 26681, 710, 3863, 905, 2904, 4907, 11, 257, 5232, 26694, 1381, 12221, 15116, 41265, 50227, 11, 3492, 33511, 9726, 74, 5024, 7843, 1427, 2081, 48847, 42513, 38, 12, 42, 10297, 1667, 991, 11873, 1325, 12391, 7043, 13, 50764], "temperature": 0.0, "avg_logprob": -0.0893743127808535, "compression_ratio": 1.4026402640264026, "no_speech_prob": 0.0022738156840205193}, {"id": 132, "seek": 64430, "start": 652.3, "end": 656.3, "text": " Czyli na to, w jaki spos\u00f3b my formulujemy nasze polecenia do modelu?", "tokens": [50764, 37099, 1667, 281, 11, 261, 24492, 22904, 452, 1254, 425, 21767, 43394, 13208, 13037, 654, 360, 2316, 84, 30, 50964], "temperature": 0.0, "avg_logprob": -0.0893743127808535, "compression_ratio": 1.4026402640264026, "no_speech_prob": 0.0022738156840205193}, {"id": 133, "seek": 64430, "start": 656.3, "end": 662.3, "text": " Dok\u0142adnie. Poniewa\u017c podczas generowania tekstu model dzia\u0142a jak RNN, przetwarza wszystko krok po kroku.", "tokens": [50964, 29768, 10358, 2766, 13, 31756, 27806, 2497, 30989, 1337, 21308, 16624, 372, 84, 2316, 37903, 4207, 45702, 45, 11, 6541, 302, 6925, 2394, 22607, 350, 31621, 714, 45909, 5279, 13, 51264], "temperature": 0.0, "avg_logprob": -0.0893743127808535, "compression_ratio": 1.4026402640264026, "no_speech_prob": 0.0022738156840205193}, {"id": 134, "seek": 64430, "start": 662.3, "end": 669.3, "text": " On nie mo\u017ce spojrze\u0107 wstecz na pocz\u0105tek promptu z tak\u0105 sam\u0105 swobod\u0105, jak transformer, kt\u00f3ry widzi wszystko naraz.", "tokens": [51264, 1282, 2838, 12034, 8243, 73, 13503, 2162, 261, 2941, 3689, 1667, 34397, 916, 12391, 84, 710, 31069, 3247, 1611, 1693, 996, 378, 1611, 11, 4207, 31782, 11, 9913, 5274, 3992, 22607, 6714, 921, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0893743127808535, "compression_ratio": 1.4026402640264026, "no_speech_prob": 0.0022738156840205193}, {"id": 135, "seek": 66930, "start": 670.3, "end": 675.3, "text": " W efekcie kolejno\u015b\u0107 informacji ma absolutnie kluczowe znaczenie.", "tokens": [50414, 343, 31482, 916, 4260, 23749, 23293, 1356, 13152, 463, 18757, 2766, 9671, 1311, 89, 6880, 15397, 326, 16778, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06765110181725544, "compression_ratio": 1.3873517786561265, "no_speech_prob": 0.014933647587895393}, {"id": 136, "seek": 66930, "start": 675.3, "end": 679.3, "text": " To, co jest na ko\u0144cu promptu, ma na niego najwi\u0119kszy wp\u0142yw.", "tokens": [50664, 1407, 11, 598, 3492, 1667, 26470, 12032, 12391, 84, 11, 463, 1667, 49615, 48636, 1694, 1229, 32444, 6825, 86, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06765110181725544, "compression_ratio": 1.3873517786561265, "no_speech_prob": 0.014933647587895393}, {"id": 137, "seek": 66930, "start": 679.3, "end": 685.3, "text": " Czy jest w artykule jaki\u015b przyk\u0142ad, kt\u00f3ry to dobrze obrazuje, bo to brzmi do\u015b\u0107 abstrakcyjnie?", "tokens": [50864, 19832, 3492, 261, 594, 874, 74, 2271, 34721, 23144, 11, 9913, 281, 28335, 22798, 11728, 2884, 11, 748, 281, 738, 89, 3057, 49333, 10823, 11272, 42949, 2766, 30, 51164], "temperature": 0.0, "avg_logprob": -0.06765110181725544, "compression_ratio": 1.3873517786561265, "no_speech_prob": 0.014933647587895393}, {"id": 138, "seek": 66930, "start": 685.3, "end": 688.3, "text": " Jest jeden, kt\u00f3ry pokazuje to doskonale.", "tokens": [51164, 24918, 12906, 11, 9913, 13010, 43317, 281, 4491, 18295, 1220, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06765110181725544, "compression_ratio": 1.3873517786561265, "no_speech_prob": 0.014933647587895393}, {"id": 139, "seek": 66930, "start": 688.3, "end": 693.3, "text": " Testowani model na zadaniu RTE, czyli rozpoznawaniu powi\u0105za\u0144 mi\u0119dzy zdaniami.", "tokens": [51314, 9279, 305, 3782, 2316, 1667, 42788, 25849, 497, 13639, 11, 16591, 9544, 2259, 35458, 86, 25849, 3388, 11404, 2394, 5248, 33964, 710, 10312, 15568, 13, 51564], "temperature": 0.0, "avg_logprob": -0.06765110181725544, "compression_ratio": 1.3873517786561265, "no_speech_prob": 0.014933647587895393}, {"id": 140, "seek": 69330, "start": 694.3, "end": 699.3, "text": " U\u017cyli takiego standardowego promptu, kt\u00f3ry \u015bwietnie dzia\u0142a\u0142 na modelach typu GPT.", "tokens": [50414, 624, 7735, 2081, 32296, 3832, 26576, 12391, 84, 11, 9913, 8299, 39083, 2766, 37903, 1221, 1667, 2316, 608, 2125, 84, 26039, 51, 13, 50664], "temperature": 0.0, "avg_logprob": -0.0694894229664522, "compression_ratio": 1.3993055555555556, "no_speech_prob": 0.06844765692949295}, {"id": 141, "seek": 69330, "start": 699.3, "end": 704.3, "text": " W tym promptie najpierw podawano dane, a na samym ko\u0144cu by\u0142o pytanie o te dane.", "tokens": [50664, 343, 8107, 12391, 414, 11212, 45119, 86, 2497, 1607, 3730, 49206, 11, 257, 1667, 3247, 4199, 26470, 12032, 14811, 36610, 277, 535, 49206, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0694894229664522, "compression_ratio": 1.3993055555555556, "no_speech_prob": 0.06844765692949295}, {"id": 142, "seek": 69330, "start": 704.3, "end": 711.3, "text": " Wynik, jaki uzyska\u0142 RWKV by\u0142 bardzo s\u0142aby, metryka F1 na poziomie 44%.", "tokens": [50914, 343, 2534, 1035, 11, 24492, 16851, 749, 2330, 1221, 42513, 42, 53, 16673, 9034, 15116, 2509, 11, 1131, 627, 2330, 479, 16, 1667, 38503, 40120, 16408, 6856, 51264], "temperature": 0.0, "avg_logprob": -0.0694894229664522, "compression_ratio": 1.3993055555555556, "no_speech_prob": 0.06844765692949295}, {"id": 143, "seek": 69330, "start": 711.3, "end": 713.3, "text": " Czyli prawie jak losowe zgadywanie.", "tokens": [51264, 37099, 3206, 8699, 4207, 1750, 6880, 40948, 880, 86, 7155, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0694894229664522, "compression_ratio": 1.3993055555555556, "no_speech_prob": 0.06844765692949295}, {"id": 144, "seek": 69330, "start": 713.3, "end": 715.3, "text": " W\u0142a\u015bnie.", "tokens": [51364, 343, 5024, 12221, 13, 51464], "temperature": 0.0, "avg_logprob": -0.0694894229664522, "compression_ratio": 1.3993055555555556, "no_speech_prob": 0.06844765692949295}, {"id": 145, "seek": 69330, "start": 715.3, "end": 718.3, "text": " Ale wtedy badacze zrobili prost\u0105 rzecz.", "tokens": [51464, 9366, 26959, 1578, 326, 1381, 44399, 2312, 10293, 1611, 36833, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0694894229664522, "compression_ratio": 1.3993055555555556, "no_speech_prob": 0.06844765692949295}, {"id": 146, "seek": 69330, "start": 718.3, "end": 722.3, "text": " Odwr\u00f3cili kolejno\u015b\u0107 w promptie. Po prostu zamienili zdania miejscami.", "tokens": [51614, 12210, 7449, 40993, 2312, 23749, 23293, 261, 12391, 414, 13, 6165, 19518, 19876, 1053, 2312, 16221, 5609, 32754, 4526, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0694894229664522, "compression_ratio": 1.3993055555555556, "no_speech_prob": 0.06844765692949295}, {"id": 147, "seek": 72230, "start": 722.3, "end": 723.3, "text": " Dok\u0142adnie.", "tokens": [50364, 29768, 10358, 2766, 13, 50414], "temperature": 0.0, "avg_logprob": -0.0602425920202377, "compression_ratio": 1.4219269102990033, "no_speech_prob": 0.009718828834593296}, {"id": 148, "seek": 72230, "start": 723.3, "end": 728.3, "text": " Najpierw zadali pytanie, a potem podali dane, kt\u00f3re model mia\u0142 przeanalizowa\u0107.", "tokens": [50414, 31576, 45119, 86, 42788, 5103, 36610, 11, 257, 36513, 2497, 5103, 49206, 11, 8864, 2316, 27989, 8325, 29702, 590, 11445, 13, 50664], "temperature": 0.0, "avg_logprob": -0.0602425920202377, "compression_ratio": 1.4219269102990033, "no_speech_prob": 0.009718828834593296}, {"id": 149, "seek": 72230, "start": 728.3, "end": 731.3, "text": " Z punktu widzenia RNN to mia\u0142o sens.", "tokens": [50664, 1176, 39561, 84, 5274, 14320, 45702, 45, 281, 21290, 5249, 2923, 13, 50814], "temperature": 0.0, "avg_logprob": -0.0602425920202377, "compression_ratio": 1.4219269102990033, "no_speech_prob": 0.009718828834593296}, {"id": 150, "seek": 72230, "start": 731.3, "end": 735.3, "text": " Najpierw dowiaduje si\u0119 czego szuka\u0107, a potem to znajduje.", "tokens": [50814, 31576, 45119, 86, 9459, 38069, 13008, 3244, 36559, 7870, 13599, 2162, 11, 257, 36513, 281, 47570, 2884, 13, 51014], "temperature": 0.0, "avg_logprob": -0.0602425920202377, "compression_ratio": 1.4219269102990033, "no_speech_prob": 0.009718828834593296}, {"id": 151, "seek": 72230, "start": 735.3, "end": 736.3, "text": " I jaki by\u0142 wynik?", "tokens": [51014, 286, 24492, 16673, 31936, 1035, 30, 51064], "temperature": 0.0, "avg_logprob": -0.0602425920202377, "compression_ratio": 1.4219269102990033, "no_speech_prob": 0.009718828834593296}, {"id": 152, "seek": 72230, "start": 736.3, "end": 737.3, "text": " Wynik.", "tokens": [51064, 343, 2534, 1035, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0602425920202377, "compression_ratio": 1.4219269102990033, "no_speech_prob": 0.009718828834593296}, {"id": 153, "seek": 72230, "start": 737.3, "end": 743.3, "text": " Przy tej samej tre\u015bci tylko w innej kolejno\u015bci metryka F1 skoczy\u0142a do prawie 75%.", "tokens": [51114, 39590, 12573, 912, 73, 2192, 6199, 13219, 261, 294, 11794, 23749, 16438, 1131, 627, 2330, 479, 16, 1110, 905, 1229, 5024, 360, 3206, 8699, 9562, 6856, 51414], "temperature": 0.0, "avg_logprob": -0.0602425920202377, "compression_ratio": 1.4219269102990033, "no_speech_prob": 0.009718828834593296}, {"id": 154, "seek": 72230, "start": 743.3, "end": 750.3, "text": " To jest gigantyczna r\u00f3\u017cnica, kt\u00f3ra pokazuje, \u017ce z tymi modelami musimy si\u0119 nauczy\u0107 rozmawia\u0107 w zupe\u0142nie nowy spos\u00f3b.", "tokens": [51414, 1407, 3492, 8741, 394, 17466, 629, 19637, 32687, 11, 19456, 13010, 43317, 11, 3561, 710, 1104, 3057, 2316, 4526, 43449, 3244, 49103, 27150, 35234, 34953, 2162, 261, 49922, 586, 88, 22904, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0602425920202377, "compression_ratio": 1.4219269102990033, "no_speech_prob": 0.009718828834593296}, {"id": 155, "seek": 75030, "start": 750.3, "end": 752.3, "text": " Nie wystarczy przeklei\u0107 prompt\u00f3w z GPT.", "tokens": [50364, 12016, 4628, 9710, 6522, 29785, 306, 12757, 12391, 3901, 710, 26039, 51, 13, 50464], "temperature": 0.0, "avg_logprob": -0.0652969386301884, "compression_ratio": 1.470404984423676, "no_speech_prob": 0.018846098333597183}, {"id": 156, "seek": 75030, "start": 752.3, "end": 757.3, "text": " To fascynuj\u0105ce. Oznacza to, \u017ce wszystkie te intuicje, kt\u00f3re budowali\u015bmy przez lata pracy z transformerami,", "tokens": [50464, 1407, 30632, 1344, 77, 13263, 384, 13, 422, 22672, 326, 2394, 281, 11, 3561, 31723, 535, 560, 84, 299, 2884, 11, 8864, 3265, 305, 33955, 14064, 46722, 35591, 710, 31782, 4526, 11, 50714], "temperature": 0.0, "avg_logprob": -0.0652969386301884, "compression_ratio": 1.470404984423676, "no_speech_prob": 0.018846098333597183}, {"id": 157, "seek": 75030, "start": 757.3, "end": 759.3, "text": " tutaj mog\u0105 by\u0107 po prostu myl\u0105ce.", "tokens": [50714, 12749, 34123, 15069, 714, 19518, 452, 75, 1611, 384, 13, 50814], "temperature": 0.0, "avg_logprob": -0.0652969386301884, "compression_ratio": 1.470404984423676, "no_speech_prob": 0.018846098333597183}, {"id": 158, "seek": 75030, "start": 759.3, "end": 762.3, "text": " A to z innymi bardziej fundamentalnymi ograniczeniami.", "tokens": [50814, 316, 281, 710, 294, 31813, 27209, 8088, 31813, 34416, 30732, 2904, 15568, 13, 50964], "temperature": 0.0, "avg_logprob": -0.0652969386301884, "compression_ratio": 1.470404984423676, "no_speech_prob": 0.018846098333597183}, {"id": 159, "seek": 75030, "start": 762.3, "end": 764.3, "text": " Ta wra\u017cliwo\u015b\u0107 na prompt to jedno.", "tokens": [50964, 6551, 7843, 1427, 2081, 48847, 1667, 12391, 281, 5232, 1771, 13, 51064], "temperature": 0.0, "avg_logprob": -0.0652969386301884, "compression_ratio": 1.470404984423676, "no_speech_prob": 0.018846098333597183}, {"id": 160, "seek": 75030, "start": 764.3, "end": 769.3, "text": " Ale czy s\u0105 zadania, w kt\u00f3rych transformer z swoj\u0105 pe\u0142n\u0105 atencj\u0105 zawsze b\u0119dzie mia\u0142 przewag\u0119?", "tokens": [51064, 9366, 6430, 9015, 42788, 5609, 11, 261, 30382, 31782, 710, 49194, 43205, 13113, 21723, 66, 8555, 30964, 10562, 27989, 39758, 40748, 30, 51314], "temperature": 0.0, "avg_logprob": -0.0652969386301884, "compression_ratio": 1.470404984423676, "no_speech_prob": 0.018846098333597183}, {"id": 161, "seek": 75030, "start": 769.3, "end": 773.3, "text": " Gdzie ten brak mo\u017cliwo\u015bci spojrzenia wstecz naprawd\u0119 boli?", "tokens": [51314, 460, 13096, 2064, 1548, 74, 30854, 36476, 8243, 73, 81, 14320, 261, 2941, 3689, 20970, 8986, 72, 30, 51514], "temperature": 0.0, "avg_logprob": -0.0652969386301884, "compression_ratio": 1.470404984423676, "no_speech_prob": 0.018846098333597183}, {"id": 162, "seek": 75030, "start": 773.3, "end": 775.3, "text": " Autorzy sami to przyznaj\u0105.", "tokens": [51514, 6049, 284, 1229, 3247, 72, 281, 6501, 35458, 8555, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0652969386301884, "compression_ratio": 1.470404984423676, "no_speech_prob": 0.018846098333597183}, {"id": 163, "seek": 77530, "start": 775.3, "end": 782.3, "text": " Ta sekwencyjna natura mo\u017ce sprawia\u0107, \u017ce RWKV b\u0119dzie mia\u0142 trudno\u015b\u0107 z przypomnieniem sobie bardzo drobnego,", "tokens": [50364, 6551, 17215, 15615, 42949, 629, 2249, 2991, 12034, 22734, 654, 2162, 11, 3561, 42513, 42, 53, 10562, 27989, 32007, 23293, 710, 41780, 38131, 1053, 4907, 13652, 9034, 3789, 65, 11858, 11, 50714], "temperature": 0.0, "avg_logprob": -0.08464748608438592, "compression_ratio": 1.4320987654320987, "no_speech_prob": 0.05809597298502922}, {"id": 164, "seek": 77530, "start": 782.3, "end": 786.3, "text": " specyficznego detalu z bardzo, bardzo odleg\u0142ego fragmentu tekstu.", "tokens": [50714, 768, 1344, 1786, 89, 11858, 1141, 4929, 710, 9034, 11, 9034, 277, 2285, 70, 1221, 6308, 26424, 84, 16624, 372, 84, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08464748608438592, "compression_ratio": 1.4320987654320987, "no_speech_prob": 0.05809597298502922}, {"id": 165, "seek": 77530, "start": 786.3, "end": 788.3, "text": " Wyobra\u017amy sobie takie zadanie typu.", "tokens": [50914, 14458, 24393, 10659, 2226, 13652, 15963, 42788, 7155, 2125, 84, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08464748608438592, "compression_ratio": 1.4320987654320987, "no_speech_prob": 0.05809597298502922}, {"id": 166, "seek": 77530, "start": 788.3, "end": 791.3, "text": " Znajd\u017a ig\u0142\u0119 w Stogusianaw z tustro\u0144cowym dokumencie.", "tokens": [51014, 1176, 20981, 67, 10659, 8508, 46564, 261, 745, 664, 301, 952, 1607, 710, 2604, 27616, 5248, 66, 31691, 25037, 16988, 4260, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08464748608438592, "compression_ratio": 1.4320987654320987, "no_speech_prob": 0.05809597298502922}, {"id": 167, "seek": 77530, "start": 791.3, "end": 796.3, "text": " Na stronie drugiej, w trzecima kapicie jest jakie\u015b nazwisko, o kt\u00f3re pytamy na ko\u0144cu.", "tokens": [51164, 6056, 1056, 32242, 47373, 11, 261, 22266, 66, 4775, 13816, 28434, 3492, 31163, 20151, 86, 43442, 11, 277, 8864, 25878, 7804, 1667, 26470, 12032, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08464748608438592, "compression_ratio": 1.4320987654320987, "no_speech_prob": 0.05809597298502922}, {"id": 168, "seek": 77530, "start": 796.3, "end": 797.3, "text": " OK.", "tokens": [51414, 2264, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08464748608438592, "compression_ratio": 1.4320987654320987, "no_speech_prob": 0.05809597298502922}, {"id": 169, "seek": 77530, "start": 797.3, "end": 802.3, "text": " Transformer w ka\u017cdym kroku generowania odpowiedzi ma bezpo\u015bredni dost\u0119p do tej drugiej strony.", "tokens": [51464, 27938, 260, 261, 31615, 76, 45909, 5279, 1337, 21308, 36574, 3992, 463, 10782, 2259, 1788, 986, 3722, 48209, 360, 12573, 47373, 32406, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08464748608438592, "compression_ratio": 1.4320987654320987, "no_speech_prob": 0.05809597298502922}, {"id": 170, "seek": 80230, "start": 802.3, "end": 810.3, "text": " RWKV musi polega\u0107 na tym, \u017ce ta informacja jako\u015b przetrwa\u0142a w jego skompresowanym stanie przez ca\u0142\u0105 podr\u00f3\u017c przez reszt\u0119 dokumentu.", "tokens": [50364, 42513, 42, 53, 37587, 13208, 3680, 2162, 1667, 8107, 11, 3561, 1846, 1356, 23395, 17123, 1788, 6541, 27965, 4151, 5024, 261, 26542, 1110, 8586, 495, 23341, 76, 40013, 14064, 1335, 15926, 2497, 11721, 1427, 14064, 725, 2682, 1274, 40858, 84, 13, 50764], "temperature": 0.0, "avg_logprob": -0.052101579430985125, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.00709975091740489}, {"id": 171, "seek": 80230, "start": 810.3, "end": 817.3, "text": " W takich niszowych, wymagaj\u0105cych bardzo precyzyjnego wyszukiwania zadaniach, klasyczna atencja wci\u0105\u017c mo\u017ce mie\u0107 przewag\u0119.", "tokens": [50764, 343, 29607, 297, 23848, 19605, 11, 29764, 559, 11133, 31306, 9034, 659, 1344, 1229, 73, 11858, 261, 20589, 11788, 86, 5609, 42788, 3782, 608, 11, 9671, 5871, 3689, 629, 21723, 34056, 261, 537, 27242, 12034, 35612, 39758, 40748, 13, 51114], "temperature": 0.0, "avg_logprob": -0.052101579430985125, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.00709975091740489}, {"id": 172, "seek": 80230, "start": 817.3, "end": 818.3, "text": " Rozumiem.", "tokens": [51114, 43313, 449, 4907, 13, 51164], "temperature": 0.0, "avg_logprob": -0.052101579430985125, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.00709975091740489}, {"id": 173, "seek": 80230, "start": 818.3, "end": 827.3, "text": " Czyli do og\u00f3lnego rozumienia tekstu i generowania jest \u015bwietny, ale dla zada\u0144, kt\u00f3re przypominaj\u0105 tak\u0105 wewn\u0119trzn\u0105 wyszukiwark\u0119,", "tokens": [51164, 37099, 360, 5360, 15741, 11858, 48797, 18811, 16624, 372, 84, 741, 1337, 21308, 3492, 8299, 39083, 1634, 11, 6775, 12285, 710, 1538, 5248, 11, 8864, 41780, 49217, 8555, 31069, 321, 895, 1274, 6903, 89, 13113, 261, 20589, 11788, 86, 809, 1274, 11, 51614], "temperature": 0.0, "avg_logprob": -0.052101579430985125, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.00709975091740489}, {"id": 174, "seek": 80230, "start": 827.3, "end": 829.3, "text": " mo\u017ce by\u0107 mniej niezawodny.", "tokens": [51614, 12034, 15069, 39513, 33511, 1607, 378, 1634, 13, 51714], "temperature": 0.0, "avg_logprob": -0.052101579430985125, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.00709975091740489}, {"id": 175, "seek": 82930, "start": 829.3, "end": 832.3, "text": " No dobrze, to po\u0142\u0105czmy te wszystkie kropki.", "tokens": [50364, 883, 28335, 11, 281, 714, 43558, 2226, 535, 31723, 350, 1513, 2984, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07376211408584837, "compression_ratio": 1.3909774436090225, "no_speech_prob": 0.02740412764251232}, {"id": 176, "seek": 82930, "start": 832.3, "end": 841.3, "text": " Mamy nowostar\u0105 architektur\u0119, kt\u00f3ra jest niemal tak inteligentna jak Transformer, skaluje si\u0119 w ten sam przewidywalny spos\u00f3b,", "tokens": [50514, 376, 7804, 586, 555, 289, 1611, 3912, 642, 2320, 374, 1274, 11, 19456, 3492, 2838, 5579, 991, 24777, 25002, 629, 4207, 27938, 260, 11, 1110, 304, 13008, 3244, 261, 2064, 3247, 39758, 327, 27112, 304, 1634, 22904, 11, 50964], "temperature": 0.0, "avg_logprob": -0.07376211408584837, "compression_ratio": 1.3909774436090225, "no_speech_prob": 0.02740412764251232}, {"id": 177, "seek": 82930, "start": 841.3, "end": 845.3, "text": " ale jest niewyobra\u017calnie ta\u0144sza i szybsza w u\u017cyciu.", "tokens": [50964, 6775, 3492, 43622, 88, 24393, 1427, 304, 2766, 1846, 5248, 82, 2394, 741, 30526, 929, 2394, 261, 34097, 30795, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07376211408584837, "compression_ratio": 1.3909774436090225, "no_speech_prob": 0.02740412764251232}, {"id": 178, "seek": 82930, "start": 845.3, "end": 849.3, "text": " Ma przy tym pewne specyficzne dziwactwa i ograniczenia.", "tokens": [51164, 4042, 6501, 8107, 25889, 716, 768, 1344, 1786, 43077, 31981, 86, 578, 4151, 741, 34416, 30732, 14320, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07376211408584837, "compression_ratio": 1.3909774436090225, "no_speech_prob": 0.02740412764251232}, {"id": 179, "seek": 82930, "start": 849.3, "end": 852.3, "text": " Co to wszystko oznacza dla przysz\u0142o\u015bci AI?", "tokens": [51364, 3066, 281, 22607, 277, 22672, 326, 2394, 12285, 44018, 35059, 7318, 30, 51514], "temperature": 0.0, "avg_logprob": -0.07376211408584837, "compression_ratio": 1.3909774436090225, "no_speech_prob": 0.02740412764251232}, {"id": 180, "seek": 82930, "start": 852.3, "end": 854.3, "text": " Czy to jest koniec Ery Transformer\u00f3w?", "tokens": [51514, 19832, 281, 3492, 5897, 35733, 462, 627, 27938, 260, 3901, 30, 51614], "temperature": 0.0, "avg_logprob": -0.07376211408584837, "compression_ratio": 1.3909774436090225, "no_speech_prob": 0.02740412764251232}, {"id": 181, "seek": 85430, "start": 854.3, "end": 862.3, "text": " Na pewno nie koniec, ale bez w\u0105tpienia pocz\u0105tek bardzo silnej i co wa\u017cne, wydajnej alternatywy.", "tokens": [50364, 6056, 33002, 2838, 5897, 35733, 11, 6775, 10782, 261, 23430, 79, 18811, 34397, 916, 9034, 3425, 11794, 741, 598, 46110, 11, 25984, 1805, 11794, 5400, 21398, 9726, 13, 50764], "temperature": 0.0, "avg_logprob": -0.0687568327959846, "compression_ratio": 1.4921875, "no_speech_prob": 0.2028103470802307}, {"id": 182, "seek": 85430, "start": 862.3, "end": 871.3, "text": " My\u015bl\u0119, \u017ce najwi\u0119ksz\u0105 implikacj\u0105 jest potencjalna demokratyzacja i decentralizacja dost\u0119pu do du\u017cych modeli j\u0119zykowych.", "tokens": [50764, 1222, 28749, 11, 3561, 48636, 1694, 8925, 8484, 1035, 326, 8555, 3492, 1847, 22660, 22600, 629, 49432, 37433, 23395, 741, 368, 2207, 2155, 590, 23395, 48209, 84, 360, 1581, 7735, 339, 2316, 72, 49055, 74, 19605, 13, 51214], "temperature": 0.0, "avg_logprob": -0.0687568327959846, "compression_ratio": 1.4921875, "no_speech_prob": 0.2028103470802307}, {"id": 183, "seek": 85430, "start": 871.3, "end": 874.3, "text": " Przez ostatnie lata trend by\u0142 jasny.", "tokens": [51214, 2114, 1381, 89, 32686, 2766, 46722, 6028, 16673, 361, 296, 1634, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0687568327959846, "compression_ratio": 1.4921875, "no_speech_prob": 0.2028103470802307}, {"id": 184, "seek": 85430, "start": 874.3, "end": 882.3, "text": " Coraz wi\u0119ksze modele, trenowane w coraz wi\u0119kszych centrach danych, kontrolowane przez coraz mniejsz\u0105 liczb\u0119 firm.", "tokens": [51364, 3925, 921, 29968, 1381, 4391, 306, 11, 23136, 23066, 261, 25899, 29968, 28051, 32199, 608, 274, 34644, 11, 14373, 6623, 23066, 14064, 25899, 275, 30295, 8925, 6169, 89, 65, 1274, 6174, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0687568327959846, "compression_ratio": 1.4921875, "no_speech_prob": 0.2028103470802307}, {"id": 185, "seek": 88230, "start": 882.3, "end": 885.3, "text": " RWKV odwraca ten trend.", "tokens": [50364, 42513, 42, 53, 3611, 7449, 6628, 2064, 6028, 13, 50514], "temperature": 0.0, "avg_logprob": -0.05710196323531995, "compression_ratio": 1.4157706093189963, "no_speech_prob": 0.002033133991062641}, {"id": 186, "seek": 88230, "start": 885.3, "end": 895.3, "text": " Znacznie ni\u017csze koszty inferencji oznaczaj\u0105, \u017ce pot\u0119\u017cny 14 miliardowy model mo\u017ce zacz\u0105\u0107 dzia\u0142a\u0107 efektywnie na znacznie s\u0142abszym sprz\u0119cie.", "tokens": [50514, 1176, 77, 14875, 2766, 28502, 82, 1381, 19532, 89, 874, 13596, 268, 19649, 277, 22672, 14875, 11133, 11, 3561, 1847, 1274, 1427, 1634, 3499, 1962, 72, 515, 10089, 2316, 12034, 34430, 8925, 2162, 37903, 2162, 31482, 916, 874, 14215, 1667, 15397, 14875, 2766, 15116, 455, 7706, 76, 6103, 11052, 4260, 13, 51014], "temperature": 0.0, "avg_logprob": -0.05710196323531995, "compression_ratio": 1.4157706093189963, "no_speech_prob": 0.002033133991062641}, {"id": 187, "seek": 88230, "start": 895.3, "end": 902.3, "text": " Czyli ta wizja pot\u0119\u017cnego asystenta AI, kt\u00f3ry dzia\u0142a w pe\u0142ni lokalnie na moim telefonie czy laptopie?", "tokens": [51014, 37099, 1846, 40808, 2938, 1847, 1274, 1427, 11858, 382, 38593, 8938, 7318, 11, 9913, 37903, 261, 43205, 3722, 450, 19990, 2766, 1667, 48569, 26812, 414, 6430, 10732, 414, 30, 51364], "temperature": 0.0, "avg_logprob": -0.05710196323531995, "compression_ratio": 1.4157706093189963, "no_speech_prob": 0.002033133991062641}, {"id": 188, "seek": 88230, "start": 902.3, "end": 903.3, "text": " Tak.", "tokens": [51364, 9118, 13, 51414], "temperature": 0.0, "avg_logprob": -0.05710196323531995, "compression_ratio": 1.4157706093189963, "no_speech_prob": 0.002033133991062641}, {"id": 189, "seek": 88230, "start": 903.3, "end": 910.3, "text": " I nie musi wysy\u0142a\u0107 ka\u017cdej mojej my\u015bli i ka\u017cdego zapytania do chmury, staje si\u0119 znacznie bardziej realna.", "tokens": [51414, 286, 2838, 37587, 27062, 88, 5024, 2162, 21912, 1479, 73, 36383, 73, 452, 15350, 741, 21912, 67, 6308, 14223, 4328, 5609, 360, 417, 76, 2598, 11, 342, 11153, 3244, 15397, 14875, 2766, 27209, 957, 629, 13, 51764], "temperature": 0.0, "avg_logprob": -0.05710196323531995, "compression_ratio": 1.4157706093189963, "no_speech_prob": 0.002033133991062641}, {"id": 190, "seek": 91030, "start": 910.3, "end": 914.3, "text": " To by\u0142by ogromny prze\u0142om chocia\u017cby z punktu widzenia prywatno\u015bci.", "tokens": [50364, 1407, 16673, 2322, 34416, 298, 1634, 8325, 1221, 298, 48929, 2322, 710, 39561, 84, 5274, 14320, 582, 27112, 267, 16438, 13, 50564], "temperature": 0.0, "avg_logprob": -0.061069834232330325, "compression_ratio": 1.4212218649517685, "no_speech_prob": 0.030240299180150032}, {"id": 191, "seek": 91030, "start": 914.3, "end": 915.3, "text": " Dok\u0142adnie.", "tokens": [50564, 29768, 10358, 2766, 13, 50614], "temperature": 0.0, "avg_logprob": -0.061069834232330325, "compression_ratio": 1.4212218649517685, "no_speech_prob": 0.030240299180150032}, {"id": 192, "seek": 91030, "start": 915.3, "end": 919.3, "text": " Prywatno\u015b\u0107, koszty, dost\u0119pno\u015b\u0107 offline, suwerenno\u015b\u0107 danych.", "tokens": [50614, 430, 627, 44824, 23293, 11, 19532, 89, 874, 11, 48209, 23293, 21857, 11, 459, 1554, 268, 23293, 274, 34644, 13, 50814], "temperature": 0.0, "avg_logprob": -0.061069834232330325, "compression_ratio": 1.4212218649517685, "no_speech_prob": 0.030240299180150032}, {"id": 193, "seek": 91030, "start": 919.3, "end": 927.3, "text": " To ofiera drzwi do zastosowa\u0144 w tzw. architekturze Edge, czyli na urz\u0105dzeniach ko\u0144cowych, w medycynie, w przemy\u015ble.", "tokens": [50814, 1407, 295, 10609, 1224, 89, 6253, 360, 36746, 329, 5528, 5248, 261, 256, 14406, 13, 3912, 642, 2320, 374, 1381, 19328, 11, 16591, 1667, 4038, 23876, 42124, 608, 26470, 66, 19605, 11, 261, 1205, 88, 1344, 2766, 11, 261, 6541, 3633, 1788, 306, 13, 51214], "temperature": 0.0, "avg_logprob": -0.061069834232330325, "compression_ratio": 1.4212218649517685, "no_speech_prob": 0.030240299180150032}, {"id": 194, "seek": 91030, "start": 927.3, "end": 931.3, "text": " Wsz\u0119dzie tam, gdzie wysy\u0142anie danych na zewn\u0105trz jest niedopuszczalne.", "tokens": [51214, 343, 15453, 42643, 7677, 11, 18922, 27062, 88, 1221, 7155, 274, 34644, 1667, 5277, 895, 1611, 6903, 89, 3492, 32488, 404, 22378, 3689, 304, 716, 13, 51414], "temperature": 0.0, "avg_logprob": -0.061069834232330325, "compression_ratio": 1.4212218649517685, "no_speech_prob": 0.030240299180150032}, {"id": 195, "seek": 91030, "start": 931.3, "end": 933.3, "text": " Lub po prostu niepraktyczne.", "tokens": [51414, 43781, 714, 19518, 2838, 79, 11272, 874, 38491, 13, 51514], "temperature": 0.0, "avg_logprob": -0.061069834232330325, "compression_ratio": 1.4212218649517685, "no_speech_prob": 0.030240299180150032}, {"id": 196, "seek": 91030, "start": 933.3, "end": 937.3, "text": " A co do przysz\u0142o\u015bci? Autorzy wskazuj\u0105 kilka naturalnych kierunk\u00f3w.", "tokens": [51514, 316, 598, 360, 44018, 35059, 30, 6049, 284, 1229, 261, 5161, 921, 13263, 36466, 3303, 9399, 38767, 3197, 3901, 13, 51714], "temperature": 0.0, "avg_logprob": -0.061069834232330325, "compression_ratio": 1.4212218649517685, "no_speech_prob": 0.030240299180150032}, {"id": 197, "seek": 93730, "start": 937.3, "end": 940.3, "text": " Dalsze doskonalenie samego mechanizmu RWKW.", "tokens": [50364, 413, 1124, 1381, 4491, 18295, 21745, 414, 912, 1571, 4236, 590, 20140, 42513, 42, 54, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10848134860657808, "compression_ratio": 1.3775100401606426, "no_speech_prob": 0.020918944850564003}, {"id": 198, "seek": 93730, "start": 940.3, "end": 946.3, "text": " Zastosowanie go w innych typach architektur, na przyk\u0142ad modelach encoder, decoder do t\u0142umaczenia.", "tokens": [50514, 1176, 525, 329, 22028, 352, 261, 36286, 2125, 608, 3912, 642, 2320, 374, 11, 1667, 23144, 2316, 608, 2058, 19866, 11, 979, 19866, 360, 256, 49166, 326, 14320, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10848134860657808, "compression_ratio": 1.3775100401606426, "no_speech_prob": 0.020918944850564003}, {"id": 199, "seek": 93730, "start": 946.3, "end": 954.3, "text": " A nawet, co jest bardzo ciekawe, wykorzystanie tego wewn\u0119trznego stanu RNN do lepszej interpretacji dzia\u0142ania modelu.", "tokens": [50814, 316, 22696, 11, 598, 3492, 9034, 30596, 2330, 826, 11, 43606, 36049, 7155, 8627, 321, 895, 1274, 6903, 89, 11858, 27984, 84, 45702, 45, 360, 476, 1878, 16920, 7302, 13152, 27121, 5609, 2316, 84, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10848134860657808, "compression_ratio": 1.3775100401606426, "no_speech_prob": 0.020918944850564003}, {"id": 200, "seek": 93730, "start": 954.3, "end": 957.3, "text": " Ten stand to jest jakby my\u015bl modelu w danym momencie.", "tokens": [51214, 9380, 1463, 281, 3492, 28976, 452, 19212, 2316, 84, 261, 274, 1325, 76, 40883, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10848134860657808, "compression_ratio": 1.3775100401606426, "no_speech_prob": 0.020918944850564003}, {"id": 201, "seek": 93730, "start": 957.3, "end": 959.3, "text": " Do kt\u00f3rej mamy wgl\u0105d?", "tokens": [51364, 1144, 36023, 17335, 261, 7191, 18962, 30, 51464], "temperature": 0.0, "avg_logprob": -0.10848134860657808, "compression_ratio": 1.3775100401606426, "no_speech_prob": 0.020918944850564003}, {"id": 202, "seek": 95930, "start": 959.3, "end": 966.3, "text": " Rozumowuj\u0105c, wygl\u0105da na to, \u017ce RWKW to nie jest tylko kolejna drobna optymalizacja czy jaka\u015b ma\u0142a poprawka.", "tokens": [50364, 43313, 449, 305, 44733, 11, 32015, 1667, 281, 11, 3561, 42513, 42, 54, 281, 2838, 3492, 13219, 23749, 629, 3789, 65, 629, 2427, 4199, 304, 590, 23395, 6430, 4207, 64, 1788, 463, 5024, 1665, 5131, 2330, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08152011846074995, "compression_ratio": 1.453968253968254, "no_speech_prob": 0.0650026947259903}, {"id": 203, "seek": 95930, "start": 966.3, "end": 974.3, "text": " To jest fundamentalne przemy\u015blenie ca\u0142ej architektury, kt\u00f3re z sukcesem \u0142\u0105czy dwa wydawa\u0142oby si\u0119 wykluczaj\u0105ce si\u0119 \u015bwiaty.", "tokens": [50714, 1407, 3492, 8088, 716, 6541, 3633, 1788, 6698, 414, 47631, 73, 3912, 642, 2320, 2598, 11, 8864, 710, 46432, 887, 443, 220, 15926, 6522, 35045, 25984, 10449, 1221, 13944, 3244, 4628, 7837, 1311, 89, 11133, 384, 3244, 21485, 21398, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08152011846074995, "compression_ratio": 1.453968253968254, "no_speech_prob": 0.0650026947259903}, {"id": 204, "seek": 95930, "start": 974.3, "end": 981.3, "text": " Brutaln\u0105 moc skalowania transformel\u00f3w, z elegancj\u0105 i niezwyk\u0142\u0105 wydajno\u015bci\u0105 sieci rekur\u0119cznych.", "tokens": [51114, 1603, 325, 304, 13113, 34962, 16890, 21308, 4088, 338, 3901, 11, 710, 1118, 1275, 66, 8555, 741, 33511, 9726, 74, 15926, 25984, 1805, 16438, 1611, 2804, 537, 319, 33503, 1274, 3689, 9399, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08152011846074995, "compression_ratio": 1.453968253968254, "no_speech_prob": 0.0650026947259903}, {"id": 205, "seek": 95930, "start": 981.3, "end": 987.3, "text": " To jest przede wszystkim dow\u00f3d na to, \u017ce w nauce czasem warto zrobi\u0107 krok w ty\u0142, \u017ceby p\u00f3j\u015b\u0107 do przodu.", "tokens": [51464, 1407, 3492, 44786, 30481, 9459, 17081, 1667, 281, 11, 3561, 261, 35616, 384, 13190, 443, 31830, 31785, 350, 31621, 261, 1104, 1221, 11, 11316, 280, 18999, 7753, 360, 6541, 34873, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08152011846074995, "compression_ratio": 1.453968253968254, "no_speech_prob": 0.0650026947259903}, {"id": 206, "seek": 98730, "start": 987.3, "end": 998.3, "text": " Warto wr\u00f3ci\u0107 do starszych, pozornie porzuconych pomys\u0142\u00f3w i spojrze\u0107 na nie przez pryzmat nowych technik, nowych narz\u0119dzi i tej ogromnej skali obliczeniowej, kt\u00f3r\u0105 dzisiaj dysponujemy.", "tokens": [50364, 343, 15864, 928, 812, 39162, 360, 6105, 28051, 11, 21281, 1865, 414, 1515, 89, 1311, 2526, 339, 12991, 39508, 3901, 741, 8243, 73, 13503, 2162, 1667, 2838, 14064, 582, 37433, 15677, 586, 16384, 1537, 1035, 11, 586, 16384, 6714, 89, 6298, 3992, 741, 12573, 34416, 298, 11794, 1110, 5103, 1111, 1050, 42124, 21091, 11, 37415, 25772, 15243, 79, 266, 21767, 13, 50914], "temperature": 0.0, "avg_logprob": -0.046621000166419596, "compression_ratio": 1.403448275862069, "no_speech_prob": 0.01203590165823698}, {"id": 207, "seek": 98730, "start": 998.3, "end": 1006.3, "text": " RWKW pokazuje, \u017ce era fundamentalnych innowacji w architekturach AI wcale, wcale si\u0119 nie sk\u0105czy\u0142a.", "tokens": [50914, 42513, 42, 54, 13010, 43317, 11, 3561, 4249, 8088, 9399, 294, 3785, 13152, 261, 3912, 642, 2320, 374, 608, 7318, 261, 37088, 11, 261, 37088, 3244, 2838, 1110, 1611, 6522, 5024, 13, 51314], "temperature": 0.0, "avg_logprob": -0.046621000166419596, "compression_ratio": 1.403448275862069, "no_speech_prob": 0.01203590165823698}, {"id": 208, "seek": 98730, "start": 1006.3, "end": 1009.3, "text": " Ci\u0105gle jest miejsce na prze\u0142omowe odkrycia.", "tokens": [51314, 383, 11404, 22631, 3492, 38122, 1667, 8325, 1221, 298, 6880, 3611, 43298, 2755, 13, 51464], "temperature": 0.0, "avg_logprob": -0.046621000166419596, "compression_ratio": 1.403448275862069, "no_speech_prob": 0.01203590165823698}, {"id": 209, "seek": 98730, "start": 1009.3, "end": 1012.3, "text": " I to zostawia nas z takim prowokuj\u0105cym pytaniem do przemy\u015blenia.", "tokens": [51464, 286, 281, 31873, 34953, 5382, 710, 31732, 45553, 453, 13263, 1344, 76, 25878, 282, 4907, 360, 6541, 3633, 1788, 6698, 654, 13, 51614], "temperature": 0.0, "avg_logprob": -0.046621000166419596, "compression_ratio": 1.403448275862069, "no_speech_prob": 0.01203590165823698}, {"id": 210, "seek": 101230, "start": 1012.3, "end": 1025.3, "text": " Skoro uda\u0142o si\u0119 z tak\u0105 skuteczno\u015bci\u0105 wskrzesi\u0107 i unowocze\u015bni\u0107 RNA, to jakie inne pozornie przestarza\u0142y idee w historii sztucznej inteligencji czekaj\u0105 na sw\u00f3j renesans w tej nowej erze.", "tokens": [50414, 7324, 10780, 44544, 5249, 3244, 710, 31069, 1110, 1169, 3689, 16438, 1611, 261, 5161, 81, 12214, 12757, 741, 517, 305, 905, 1381, 1788, 3722, 2162, 22484, 11, 281, 22124, 24170, 21281, 1865, 414, 6541, 39791, 2394, 6825, 49742, 261, 4058, 5597, 262, 2682, 1311, 89, 11794, 24777, 3213, 19649, 6472, 916, 11133, 1667, 1693, 18999, 319, 4081, 599, 261, 12573, 586, 40779, 1189, 1381, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1331793280208812, "compression_ratio": 1.2341772151898733, "no_speech_prob": 0.7431038618087769}], "language": "pl"}