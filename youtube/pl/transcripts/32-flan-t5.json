{"text": " Ka\u017cdy kto, kto bawi\u0142 si\u0119 troch\u0119 du\u017cymi modelami j\u0119zykowymi, na pewno zna to uczucie. Zadajesz proste, wydawa\u0142oby si\u0119 pytanie, a model zamiast odpowiedzie\u0107, zaczyna je powtarza\u0107 w k\u00f3\u0142ko albo, albo kompletnie gubi w\u0105tek. No w\u0142a\u015bnie. Generuje tekst, kt\u00f3ry wygl\u0105da jak, nie wiem, losowy zbi\u00f3r s\u0142\u00f3w. To tak jakby mie\u0107 dost\u0119p do ca\u0142ej wiedzy \u015bwiata, ale przez interfejs, kt\u00f3ry czasem no po prostu dostaje czkawki. To jest idealne okre\u015blenie. Mamy do czynienia z takimi cyfrowymi geniuszami, kt\u00f3rzy potrafi\u0105 napisa\u0107 sonet, wyt\u0142umaczy\u0107 mechanik\u0119 kwantow\u0105. Tak. Ale jak ich poprosisz o co\u015b prostego w jaki\u015b konkretny spos\u00f3b, to potrafi\u0105 si\u0119 kompletnie pogubi\u0107. To troch\u0119 tak jakby genialny profesor nie potrafi\u0142 odpowiedzie\u0107, kt\u00f3ra jest godzina, tylko zaczyna\u0142 wyk\u0142ad o historii czasu. W\u0142a\u015bnie, brakuje im tej jednej fundamentalnej umiej\u0119tno\u015bci, post\u0119powania zgodnie z instrukcj\u0105. I rozumiem, \u017ce w\u0142a\u015bnie ten problem, ta przepa\u015b\u0107 mi\u0119dzy ogromn\u0105 wiedz\u0105, a praktyczn\u0105 u\u017cyteczno\u015bci\u0105 sta\u0142 si\u0119 punktem wyj\u015bcia dla pracy, kt\u00f3rej dzisiaj si\u0119 przyjrzymy. Mam tu przed sob\u0105 scaling instruction fine-tuned language models. Tak. Tytu\u0142 brzmi bardzo technicznie, ale idea za nim jest chyba, no, bardzo intuicyjna. Niezwykle autorzy postawili prost\u0105, ale, wiesz, pot\u0119\u017cn\u0105 hipotez\u0119. To znaczy. Co je\u015bli we\u017amiemy tego genialnego, ale troch\u0119 chaotycznego, cyfrowego studenta i zamiast wpycha\u0107 w niego jeszcze wi\u0119cej ksi\u0105\u017cek, wy\u015blemy go na taki intensywny kurs pod tytu\u0142em Jak skutecznie s\u0142ucha\u0107 i wykonywa\u0107 polecenia. Ciekawe podej\u015bcie. Ja sugerowali, \u017ce mo\u017cna odblokowa\u0107 ukryty potencje o tych modeli niekoniecznie przez budowanie jeszcze wi\u0119kszych, ale przez nauczenie ich jak lepiej korzysta\u0107 z tego, co ju\u017c wiedz\u0105. Ok, to brzmi obiecuj\u0105co. Zatem naszym celem dzisiaj jest. No, roz\u0142o\u017cenie tego podej\u015bcia na czynniki pierwsze. Chcemy zrozumie\u0107, czym jest ten intensywny kurs dla modeli, czyli instruction fine-tuning. Jak go przeprowadzono na niespotykan\u0105 skal\u0119 i co najwa\u017cniejsze, czy to faktycznie dzia\u0142a. Czy mo\u017cna niewielkim kosztem zamieni\u0107 chaotycznego geniusza w pomocnego eksperta? Dok\u0142adnie. A odpowied\u017a, jak zaraz zobaczymy, okaza\u0142a si\u0119 chyba bardziej zaskakuj\u0105ca ni\u017c sami autorzy si\u0119 spodziewali. No to zacznijmy od podstaw. Dobrze. Wi\u0119c na czym polega ten proces? W najwi\u0119kszym skr\u00f3cie bierzemy ju\u017c wst\u0119pnie wytremowany pot\u0119\u017cny model, taki jak palm czy T5. Ten, co ju\u017c poch\u0142on\u0105\u0142 p\u00f3\u0142 internetu. W\u0142a\u015bnie. Ma w sobie gigantyczn\u0105 wiedz\u0119. I zamiast uczy\u0107 go od zera, poddajemy go o dodatkowej, bardzo specyficznej sesji treningowej. I ta sesja to jest w\u0142a\u015bnie instruction fine-tuning. Ale czym to si\u0119 r\u00f3\u017cni od zwyk\u0142ego treningu? R\u00f3\u017cnica jest kluczowa. Zamiast pokazywa\u0107 mu miliardy zda\u0144 i kaza\u0107 przewidywa\u0107 nast\u0119pne s\u0142owo, pokazujemy mu konkretne przyk\u0142ady. Przyk\u0142ady czego? Instrukcji i po\u017c\u0105danych odpowiedzi. To s\u0105 zadania sformu\u0142owane w naturalnym j\u0119zyku. Niesuche zapytania do bazy danych, ale polecenia, jakie wyda\u0142by cz\u0142owiek. Czyli na przyk\u0142ad. Podsumuj ten tekst w trzech zdaniach. Dok\u0142adnie. Albo przet\u0142umacz to zdanie na niemiecki, napisz wiersz o jesieni. Model uczy si\u0119 samego formatu polecenie wykonanie. To faktycznie fundamentalna zmiana. Pami\u0119tam ten przyk\u0142ad z pracy, kt\u00f3ry \u015bwietnie to ilustruje. Ten z hajku? Tak. Pytanie brzmia\u0142o. Czy w jednym t\u0142icie zmie\u015bci si\u0119 ca\u0142e hajku? A model mog\u0142by odpowiedzie\u0107 po prostu tak albo no wygenerowa\u0107 co\u015b losowego. W\u0142a\u015bnie. A model po Instruction Fine Tuning nazwany w pracy Flan Palm uczy si\u0119 generowa\u0107 odpowied\u017a, kt\u00f3ra jest nie tylko poprawna, ale te\u017c pomocna. I zawiera uzasadnienie. Dok\u0142adnie. Co\u015b w stylu hajku to japo\u0144ski wiersz. Jest bardzo kr\u00f3tkie. Limit znak\u00f3w na Twitterze to 288. Hajku bez problemu si\u0119 w nim zmie\u015bci. Odpowied\u017a brzmi tak. To jest zupe\u0142nie inny poziom interakcji. Zdecydowanie. Model nie tylko podaje fakt, ale pokazuje, \u017ce zrozumia\u0142 intencje pytania. A skala tego eksperymentu no robi wra\u017cenie. Autorzy nie bazowali na jednym ma\u0142ym zbiorze danych. Zdecydowanie nie. Wzi\u0119li cztery pot\u0119\u017cne kolekcje zada\u0144. Muffin T0SF NIV2 i co oka\u017ce si\u0119 kluczowe zbiory z adnotacjami Chain of Thought. I po\u0142\u0105czyli to wszystko w jedn\u0105 gigantyczn\u0105 mieszank\u0119. Tak. W sumie da\u0142o to 18-36 unikalnych typ\u00f3w zada\u0144. I przetestowali to podej\u015bcie na ca\u0142ej gamie modeli. Od tych ma\u0142ych do gigant\u00f3w. Tak. Od stosunkowo ma\u0142ego Flante 5 Small z 80 milionami parametr\u00f3w, a\u017c po no prawdziwego behemota flan palmem z 540 miliardami parametr\u00f3w. Chcieli sprawdzi\u0107, czy ta metoda dzia\u0142a uniwersalnie? Tak. Niezale\u017cnie od rozmiaru m\u00f3zgu modelu. Ale tutaj dochodzimy do czego\u015b, co w tej pracy jest dla mnie no absolutnym szokiem. Koszt obliczeniowy. A tak. To jest niesamowite. Kiedy my\u015blimy o trenowaniu modeli z setkami miliard\u00f3w parametr\u00f3w, to wyobra\u017camy sobie farm serwer\u00f3w pracuj\u0105ce miesi\u0105cami. I rachunki za pr\u0105d id\u0105ce w miliony. Dok\u0142adnie. Tymczasem tabela 2 z tej pracy pokazuje, \u017ce w przypadku najwi\u0119kszego modelu palm ca\u0142y ten proces Instruction Fine Tuning zu\u017cy\u0142 zaledwie 0,2% mocy obliczeniowej. Tak. Czy ja to dobrze czytam? Tylko 20%. Czytasz to doskonale i to jest jeden z najwa\u017cniejszych wniosk\u00f3w tej pracy. To pokazuje, jak niezwykle efektywna jest ta metoda. To nie jest budowanie nowej rakiety od zera. W\u0142a\u015bnie. To jest wzi\u0119cie ju\u017c istniej\u0105cej, pot\u0119\u017cnej rakiety i za u\u0142amet koszt\u00f3w w granie jej nowego oprogramowania do systemu naprowadzania. Kt\u00f3re czyni j\u0105 orz\u0119dy wielko\u015bci bardziej precyzyjn\u0105? Dok\u0142adnie. W gigantycznym zwr\u00f3cie z inwestycji, je\u015bli chodzi o zasoby obliczeniowe. To kompletnie zmienia posta\u0107 rzeczy, bo oznacza, \u017ce poprawa jako\u015bci AI nie musi oznacza\u0107 wyk\u0142ad niczego wzrostu koszt\u00f3w. Mo\u017cemy pracowa\u0107 m\u0105drzej, a nie ci\u0119\u017cej. No dobrze. Mamy wi\u0119c do czynienia z niezwykle wydajn\u0105 metod\u0105. To imponuj\u0105ce, ale sama wydajno\u015b\u0107 to nie wszystko. Najwa\u017cniejsze jest, czy to faktycznie dzia\u0142a. Czy te dostrojone modele naprawd\u0119 sta\u0142y si\u0119 m\u0105drzejsze? W\u0142a\u015bnie. No i wyniki s\u0105 jednoznaczne. Po pierwsze, potwierdzi\u0142a si\u0119 og\u00f3lna zasada skalowania. Im wi\u0119kszy model i im wi\u0119cej zada\u0144 w procesie fine tuning, tym lepsze wyniki. To by\u0142o do przewidzenia. Ale. Ale wykresy, a konkretnie figure 4 pokazuj\u0105 co\u015b znacznie ciekawszego. Co\u015b bardziej subtelnego. Mianowicie. Okazuje si\u0119, \u017ce najwi\u0119kszy skok jako\u015bciowy nast\u0119puje do momentu u\u017cycia oko\u0142o 282 zada\u0144. P\u00f3\u017aniej dok\u0142adanie kolejnych setek, nawet tysi\u0119cy zada\u0144, wci\u0105\u017c poprawia wyniki, ale. Ale przyrosty s\u0105 ju\u017c znacznie mniejsze. Tak. Bardziej stopniowe. To fascynuj\u0105ce. Co to w\u0142a\u015bciwie oznacza? \u017be model nauczy\u0142 si\u0119 ju\u017c wszystkiego, co mia\u0142 si\u0119 nauczy\u0107? To sugeruje co\u015b g\u0142\u0119bszego. Model nie tyle uczy si\u0119 nowych fakt\u00f3w, bo wi\u0119kszo\u015b\u0107 wiedzy ju\u017c posiad\u0142 podczas tego masowego, wst\u0119pnego treningu. Czyli pre-training? Tak. On raczej uczy si\u0119 wzorca. Uczy si\u0119 generalizowa\u0107, jak post\u0119powa\u0107 zgodnie z instrukcj\u0105. To tak, jakby\u015bmy dali komu\u015b do przeczytania ca\u0142\u0105 bibliotek\u0119. To jest pre-training. Dok\u0142adnie. A potem przeprowadzili z nim seri\u0119 \u0107wicze\u0144, kt\u00f3re ucz\u0105 go, jak szybko i trafnie znajdowa\u0107 w tej bibliotece odpowiedzi na konkretne pytania. I to jest Instruction Find Uning. Rozumiem. Po pewnym czasie on ju\u017c \u0142apie, na czym polega proces szukania odpowiedzi. W\u0142a\u015bnie. I kolejne przyk\u0142ady tylko nieznacznie go w tym doskonalaj\u0105. Ok, czyli nie chodzi o ilo\u015b\u0107 wiedzy, ale o umiej\u0119tno\u015b\u0107 jej wykorzystania. Ale wspomnia\u0142a\u015b wcze\u015bniej, \u017ce w tej mieszance zada\u0144 by\u0142 jeden, jak si\u0119 okaza\u0142o, magiczny sk\u0142adnik. Co\u015b, co si\u0119 nazywa Chain of Thot. Tak. I to jest absolutnie kluczowe. To chyba najwa\u017cniejsze odkrycie tej pracy. Czym jest ten Chain of Thot? Brzmi tajemniczo. A w zasadzie to jest uczenie modelu, by my\u015bla\u0142 na g\u0142os. Czyli? Zamiast od razu podawa\u0107 finaln\u0105 odpowied\u017a na z\u0142o\u017cony problem, model jest trenowany na przyk\u0142adach, kt\u00f3re pokazuj\u0105 ca\u0142y proces my\u015blowy. Krok po kroku prowadz\u0105cy do rozwi\u0105zania. Czyli zamiast uczy\u0107 go tylko pary, pytanie, odpowied\u017a, uczymy go samego procesu dochodzenia do odpowiedzi. Masz jaki\u015b konkretny przyk\u0142ad? W pracy jest \u015bwietny, prosty przyk\u0142ad z jab\u0142kami. Pytanie. W sto\u0142\u00f3wce by\u0142y 23 jab\u0142ka, u\u017cyto 20 do lanczu i dokupiono 6. Ile jab\u0142k jest teraz? OK. Model bez treningu COT mog\u0142oby odpowiedzie\u0107 po prostu 9. Model uczony w trybie Chain of Thot generuje co\u015b takiego. Na pocz\u0105tku by\u0142y 23 jab\u0142ka, u\u017cyto 20, wi\u0119c zosta\u0142o 23, 20 r\u00f3wna si\u0119 3. Pokazuje obliczenia? Tak. Dokupiono 6 jab\u0142ek, wi\u0119c teraz jest 3 plus 6 r\u00f3wna si\u0119 9. Ostateczna odpowied\u017a to 9. On pokazuje ca\u0142\u0105 \u015bcie\u017ck\u0119 rozumowania. Rozumiem. To wydaje si\u0119 logiczne, \u017ce uczenie modelu w ten spos\u00f3b poprawi jego zdolno\u015bci w zadaniach, nie wiem, matematycznych czy logicznych. Ale. Ale. Tutaj dochodzimy do czego\u015b, co autorzy opisuj\u0105 jako absolutnie nieintuicyjne odkrycie. Tak. Prawdzili, co si\u0119 stanie, je\u015bli przeprowadz\u0105 Instruction Fine Tuning na tej ogromnej lidbie zada\u0144, ale celowo pomin\u0105 te przyk\u0142ady Chain of Thot. I logika podpowiada\u0142aby, \u017ce model i tak stanie si\u0119 znacznie lepszy, prawda? No tak, uczy si\u0119 na tysi\u0105cach innych przyk\u0142ad\u00f3w. A sta\u0142o si\u0119 co\u015b dok\u0142adnie odwrotnego, co\u015b, co zaniepokoi\u0142o badaczy. Czyli co konkretnie? Okaza\u0142o si\u0119, \u017ce trenowanie modelu wy\u0142\u0105cznie na standardowych zadaniach typu pytanie-odpowied\u017a pogarsza\u0142o jego wrodzone zdolno\u015bci do rozumowania. Jak to pogarsza\u0142o? Na benchmarkach wymagaj\u0105cych wieloetapowego my\u015blenia model po takim treningu radzi\u0142 sobie gorzej ni\u017c model wyj\u015bciowy, ten przed Fine Tuning. Chwila, chwila, czyli pr\u00f3buj\u0105c go ulepszy\u0107 w pewnym sensie go og\u0142upili. Mo\u017cna tak powiedzie\u0107, to brzmi paradoksalnie, wiem. To jakby ucze\u0144, kt\u00f3ry rozwi\u0105zuje w k\u00f3\u0142ko testy jednokrotnego wyboru, nagle traci\u0142 zdolno\u015b\u0107 pisania wypracowa\u0144. To jest idealna analogia. Model optymalizowa\u0142 si\u0119 pod k\u0105tem prostych, szybkich odpowiedzi i niejako zapomina\u0142, jak my\u015ble\u0107 w spos\u00f3b z\u0142o\u017cony. To krytyczna obserwacja dla ca\u0142ej dziedziny. Ogromnie. Pokazuje, \u017ce spos\u00f3b w jaki trenujemy modele ma ogromny wp\u0142yw na ich fundamentalne zdolno\u015bci poznawcze. Na szcz\u0119\u015bcie rozwi\u0105zanie tego problemu okaza\u0142o si\u0119 zaskakuj\u0105co proste. To znaczy? Wystarczy\u0142o doda\u0107 zaledwie 9 zbior\u00f3w danych z anotacjami COT do ca\u0142ej licz\u0105cej ponad 1800 zada\u0144 mieszanki treningowej. Tylko 9? Tak. To by\u0142 procentowo niewielki dodatek, ale efekt by\u0142 kolosalny. Ten ma\u0142y dodatek nie tylko zatrzyma\u0142 t\u0119 detradacj\u0119 zdolno\u015bci rozumowania, ale znacz\u0105co poprawi\u0142 wyniki we wszystkich kategoriach zada\u0144. Niesamowite. Czyli te dane chain of thought dzia\u0142aj\u0105 jak jaka\u015b szczepionka przeciwko my\u015bleniu na skr\u00f3ty? Dok\u0142adnie. Ale to nie wszystko. Co jeszcze? Ten trening odblokowa\u0142 co\u015b jeszcze. Co\u015b co jest cz\u0119sto postrzewane jako \u015bwi\u0119ty gra w interakcji z AI. Czyli rozumowanie zero shot? W\u0142a\u015bnie. To jest zdolno\u015b\u0107 do rozwi\u0105zania zupe\u0142nie nowego problemu bez wcze\u015bniejszego pokazania modelowi przyk\u0142adu jak to zrobi\u0107. Zwyk\u0142y palm, gdy dosta\u0142 w poleceniu prost\u0105 fraz\u0119 let's think step by step. Pomy\u015blmy krok po kroku. Kompletnie nie wiedzia\u0142 co z tym zrobi\u0107. Traktowa\u0142 to jak zwyk\u0142y tekst. A model po treningu z COT, czyli flan palm? Dla niego ta sama fraza stawa\u0142a si\u0119 magicznym wyzwalaczem. Model rozumia\u0142, \u017ce ma teraz samodzielnie wygenerowa\u0107 logiczny ci\u0105g my\u015blowy, rozbi\u0107 problem na cz\u0119\u015bci i rozwi\u0105za\u0107 go krok po kroku. Nawet je\u015bli nigdy wcze\u015bniej nie widzia\u0142 dok\u0142adnie takiego zadania. Dok\u0142adnie. Nauczy\u0142 si\u0119 nie tylko odpowiedzi, ale samej metody rozwi\u0105zywania problem\u00f3w. To jest gigantyczny prze\u0142om. To wszystko brzmi imponuj\u0105co w teori\u0119, ale prze\u0142\u00f3\u017cmy to na twarde dane. Jak te ulepszone modele wypad\u0142y w konkretnych testach? No w\u0142a\u015bnie, wyniki. Wspomina si\u0119 tu o benchmarku MMLU. Co to jest i jakie by\u0142y wyniki? MMLU to jeden z najtrudniejszych test\u00f3w dla modeli j\u0119zykowych. Sprawdza wiedz\u0119 i rozumowanie w 57 r\u00f3\u017cnych dziedzinach. Od fizyki, medycyny, przez prawo, a\u017c po histori\u0119 sztuki. Taka matura dla AI. Co\u015b w tym stylu. I tutaj wyniki m\u00f3wi\u0105 same za siebie. Flan Palm, ten najwi\u0119kszy, osi\u0105gn\u0105\u0142 w tym te\u015bcie wynik 75,2%. Ok, 75,2%. Ale co to oznacza w praktyce? To du\u017co? Jak to si\u0119 ma do wcze\u015bniejszych modeli? To jest kosmiczny skok, \u017ceby da\u0107 kontekst. Poprzednia wersja tego samego modelu, zwyk\u0142y palm, osi\u0105gn\u0105\u0142a 69,3%. A wcze\u015bniejszy gigant model, o kt\u00f3rym wszyscy m\u00f3wili? GPT-3. Tak, GPT-3 mia\u0142 w tym samym te\u015bcie wynik 43,9%. M\u00f3wimy wi\u0119c o przeskoczeniu z poziomu studenta ledwo zdaj\u0105cego egzamin na poziom, no, prymusa. R\u00f3\u017cnica jest faktycznie ogromna. Ale najbardziej zdumiewaj\u0105ce jest chyba to, co sta\u0142o si\u0119 z mniejszymi modelami. Tak, i to jest drugi wielki wniosek tej pracy, zwi\u0105zany z demokratyzacj\u0105 AI. Mianowicie? Ta technika okaza\u0142a si\u0119 tak pot\u0119\u017cna, \u017ce nawet mniejsze, publicznie dost\u0119pne modele, ogromnie na niej skorzysta\u0142y. Przyk\u0142ad, Flante 5 XL, kt\u00f3ry ma zaledwie 3 miliardy parametr\u00f3w. Zaledwie. Osi\u0105gn\u0105\u0142 w te\u015bcie MMLU wynik lepszy ni\u017c 175 miliardowy GPT-3. Chwila, powt\u00f3rzmy to. Model ponad 50 razy mniejszy, dzi\u0119ki tej technice Fine Tuning, pokona\u0142 jednego z najwi\u0119kszych graczy na rynku. Dok\u0142adnie tak. To ca\u0142kowicie zmienia zasady gry. Oznacza, \u017ce dost\u0119p do pot\u0119\u017cnej AI przestaje by\u0107 domen\u0105 tylko kilku najwi\u0119kszych korporacji. Zdecydowanie. Mniejsze gracze, startopy, uniwersytety mog\u0105 teraz osi\u0105ga\u0107 podobne rezultaty przy znacznie ni\u017cszych kosztach. Ale benchmarki to jedno. Prawdziwym testem jest interakcja z cz\u0142owiekiem. W\u0142a\u015bnie. Jak model radzi sobie przy otwartych, kreatywnych zadaniach? I tu autorzy przeprowadzili ocen\u0119 ludzk\u0105, por\u00f3wnuj\u0105c odpowiedzi zwyk\u0142ego palm z flan palm. Prosili ludzi, by ocenili, kt\u00f3ra odpowied\u017a jest lepsza, bardziej pomocna. I co wysz\u0142o? Wyniki pokazane na Figure 8 s\u0105 mia\u017cd\u017c\u0105ce. W przypadku pyta\u0144 wymagaj\u0105cych kreatywno\u015bci, czy z\u0142o\u017conego planowania, odpowiedzi modelu po Instruction Find Tuning by\u0142y proferowane przez ludzi, a\u017c w 79% przypadk\u00f3w. Prawie 80%. Tak. To pokazuje, \u017ce ta metoda nie tylko poprawia wyniki w testach, ale realnie uczy model, jak by\u0107 lepszym partnerem do rozmowy. Model wreszcie zaczyna rozumie\u0107, czego oczekuje od niego cz\u0142owiek. A praca wspomina jeszcze o jednym niezwykle wa\u017cnym efekcie ubocznym. Tak, i jest to bardzo pozytywne zaskoczenie. Okazuje si\u0119, \u017ce Instruction Find Tuning znacz\u0105co zredukowa\u0142 sk\u0142o\u0144no\u015b\u0107 modeli do generowania toksycznych tre\u015bci. A to ciekawe. Zmniejsz\u0105 te\u017c niekt\u00f3re uprzedzenia, tak zwane bajesy. Nie jest to idealne rozwi\u0105zanie, ale wyra\u017any krok w dobrym kierunku, je\u015bli chodzi o odpowiedzialny rozw\u00f3j AI. Czyli ucz\u0105c model by by\u0142 pomocny, przy okazji uczymy go, by by\u0142 bardziej cywilizowany. W\u0142a\u015bnie tak. Dobrze, spr\u00f3bujmy to wszystko zebra\u0107. Gdyby\u015bmy mieli wyci\u0105gn\u0105\u0107 jeden kluczowy wniosek z tej prze\u0142omowej pracy. Co by to by\u0142o? My\u015bl\u0119, \u017ce by\u0142oby to stwierdzenie, \u017ce Instruction Find Tuning, zw\u0142aszcza wzbogacony o dane Chain of Thought, to jest relatywnie tania, uniwersalna i niezwykle skuteczna metoda. Kt\u00f3ra sprawia, \u017ce modele staj\u0105 si\u0119 nie tylko troch\u0119 lepsze? Ale fundamentalnie inteligentniejsze i bardziej u\u017cyteczne w praktyce. To przej\u015bcie od pot\u0119\u017cnej, ale nieokrzesanej si\u0142y obliczeniowej do responsywnego narz\u0119dzia, kt\u00f3re rozumie intencje. Tak. To sedno sprawy. Ale doda\u0142bym jeszcze jeden wymiar. To nie tylko sprawia, \u017ce modele s\u0105 m\u0105drzejsze, ale te\u017c bardziej przejrzyste. O, to bardzo wa\u017cna uwaga. Kiedy model pokazuje swoje rozmowanie w Chain of Thought, my jako u\u017cytkownicy mo\u017cemy zobaczy\u0107, gdzie ewentualnie pope\u0142ni\u0142 b\u0142\u0105d logiczny. Absolutnie. To ogromna zmiana w por\u00f3wnaniu do czarnej skrzynki, kt\u00f3ra po prostu wypluwa odpowied\u017a. To buduje zaufanie i otwiera drog\u0119 do debagowania. Na koniec chcia\u0142bym zostawi\u0107 naszych s\u0142uchaczy z jedn\u0105 my\u015bl\u0105 do rozwa\u017cenia. Praca pokaza\u0142a, \u017ce korzy\u015bci z dodawania coraz wi\u0119kszej liczby zada\u0144 w pewnym momencie malej\u0105. Autorzy sugeruj\u0105, \u017ce mo\u017ce to oznacza\u0107, i\u017c model uczy si\u0119 g\u0142\u00f3wnie tego, jak uzyska\u0107 dost\u0119p do posiadanej ju\u017c wiedzy, a niekoniecznie zdobywanow\u0105. To jest bardzo prowokuj\u0105ca i wa\u017cna my\u015bl. Je\u015bli to prawda, to co to m\u00f3wi o przysz\u0142o\u015bci rozwoju AI? No w\u0142a\u015bnie. Czy kolejny wielki prze\u0142om nadejdzie niezbudowy jeszcze wi\u0119kszych modeli, kt\u00f3re poch\u0142on\u0105 jeszcze wi\u0119cej danych energii, ale z odkrywania sprytniejszych i bardziej efektywnych instrukcji obs\u0142ugi dla tych, kt\u00f3re ju\u017c mamy? Dok\u0142adnie. By\u0107 mo\u017ce wchodzimy w ERE, w kt\u00f3rej na\u017cniejsze odbudowania coraz wi\u0119kszych cyfrowych m\u00f3zg\u00f3w stanie si\u0119 bycie dla nich po prostu lepszymi nauczycielem. Mo\u017ce klucz nie le\u017cy w rozmiarze, ale w pedagogice. W znalezieniu idealnego zestawu instrukcji, kt\u00f3ry odblokuje ten pe\u0142en u\u015bpiony potencja\u0142. A ta praca jest chyba pierwszym gigantycznym krokiem w tym w\u0142a\u015bnie kierunku.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.6000000000000005, "text": " Ka\u017cdy kto, kto bawi\u0142 si\u0119 troch\u0119 du\u017cymi modelami j\u0119zykowymi, na pewno zna to uczucie.", "tokens": [50364, 10988, 1427, 3173, 23780, 11, 23780, 272, 38402, 1221, 3244, 24926, 1581, 7735, 3057, 2316, 4526, 49055, 74, 10089, 3057, 11, 1667, 33002, 710, 629, 281, 35403, 1311, 414, 13, 50644], "temperature": 0.0, "avg_logprob": -0.19936166587450826, "compression_ratio": 1.435215946843854, "no_speech_prob": 0.011552244424819946}, {"id": 1, "seek": 0, "start": 6.4, "end": 16.0, "text": " Zadajesz proste, wydawa\u0142oby si\u0119 pytanie, a model zamiast odpowiedzie\u0107, zaczyna je powtarza\u0107 w k\u00f3\u0142ko albo, albo kompletnie gubi w\u0105tek.", "tokens": [50684, 1176, 1538, 73, 10430, 10293, 68, 11, 25984, 10449, 1221, 13944, 3244, 36610, 11, 257, 2316, 710, 4526, 525, 24314, 22078, 11, 43811, 629, 1506, 3388, 23480, 35873, 261, 350, 16181, 4093, 22622, 11, 22622, 5207, 14657, 2766, 695, 5614, 261, 23430, 916, 13, 51164], "temperature": 0.0, "avg_logprob": -0.19936166587450826, "compression_ratio": 1.435215946843854, "no_speech_prob": 0.011552244424819946}, {"id": 2, "seek": 0, "start": 16.0, "end": 16.7, "text": " No w\u0142a\u015bnie.", "tokens": [51164, 883, 14234, 13, 51199], "temperature": 0.0, "avg_logprob": -0.19936166587450826, "compression_ratio": 1.435215946843854, "no_speech_prob": 0.011552244424819946}, {"id": 3, "seek": 0, "start": 16.7, "end": 20.3, "text": " Generuje tekst, kt\u00f3ry wygl\u0105da jak, nie wiem, losowy zbi\u00f3r s\u0142\u00f3w.", "tokens": [51199, 15409, 13008, 16624, 372, 11, 9913, 32015, 4207, 11, 2838, 26522, 11, 1750, 10089, 710, 5614, 15614, 15116, 3901, 13, 51379], "temperature": 0.0, "avg_logprob": -0.19936166587450826, "compression_ratio": 1.435215946843854, "no_speech_prob": 0.011552244424819946}, {"id": 4, "seek": 0, "start": 20.8, "end": 28.2, "text": " To tak jakby mie\u0107 dost\u0119p do ca\u0142ej wiedzy \u015bwiata, ale przez interfejs, kt\u00f3ry czasem no po prostu dostaje czkawki.", "tokens": [51404, 1407, 991, 28976, 35612, 48209, 360, 47631, 73, 46894, 1229, 21485, 3274, 11, 6775, 14064, 728, 2106, 25530, 11, 9913, 13190, 443, 572, 714, 19518, 20568, 11153, 6472, 74, 1607, 2984, 13, 51774], "temperature": 0.0, "avg_logprob": -0.19936166587450826, "compression_ratio": 1.435215946843854, "no_speech_prob": 0.011552244424819946}, {"id": 5, "seek": 2820, "start": 28.2, "end": 30.2, "text": " To jest idealne okre\u015blenie.", "tokens": [50364, 1407, 3492, 7157, 716, 3133, 265, 1788, 6698, 414, 13, 50464], "temperature": 0.0, "avg_logprob": -0.08459345702153102, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.009495570324361324}, {"id": 6, "seek": 2820, "start": 30.2, "end": 37.2, "text": " Mamy do czynienia z takimi cyfrowymi geniuszami, kt\u00f3rzy potrafi\u0105 napisa\u0107 sonet, wyt\u0142umaczy\u0107 mechanik\u0119 kwantow\u0105.", "tokens": [50464, 376, 7804, 360, 6430, 77, 18811, 710, 991, 10121, 3185, 69, 1892, 88, 3057, 14017, 89, 4526, 11, 25382, 1847, 10437, 11404, 9296, 3837, 2162, 1872, 302, 11, 261, 4328, 49166, 14691, 2162, 4236, 1035, 1274, 23846, 394, 30297, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08459345702153102, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.009495570324361324}, {"id": 7, "seek": 2820, "start": 37.2, "end": 37.7, "text": " Tak.", "tokens": [50814, 9118, 13, 50839], "temperature": 0.0, "avg_logprob": -0.08459345702153102, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.009495570324361324}, {"id": 8, "seek": 2820, "start": 37.7, "end": 43.2, "text": " Ale jak ich poprosisz o co\u015b prostego w jaki\u015b konkretny spos\u00f3b, to potrafi\u0105 si\u0119 kompletnie pogubi\u0107.", "tokens": [50839, 9366, 4207, 1893, 1665, 2635, 23848, 277, 19241, 10293, 6308, 261, 34721, 36500, 1634, 22904, 11, 281, 1847, 10437, 11404, 3244, 5207, 14657, 2766, 32037, 836, 12757, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08459345702153102, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.009495570324361324}, {"id": 9, "seek": 2820, "start": 43.2, "end": 50.2, "text": " To troch\u0119 tak jakby genialny profesor nie potrafi\u0142 odpowiedzie\u0107, kt\u00f3ra jest godzina, tylko zaczyna\u0142 wyk\u0142ad o historii czasu.", "tokens": [51114, 1407, 24926, 991, 28976, 48228, 1634, 22912, 284, 2838, 1847, 10437, 40622, 24314, 22078, 11, 19456, 3492, 3044, 89, 1426, 11, 13219, 43811, 629, 1221, 4628, 15317, 277, 4058, 5597, 40860, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08459345702153102, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.009495570324361324}, {"id": 10, "seek": 2820, "start": 50.2, "end": 57.2, "text": " W\u0142a\u015bnie, brakuje im tej jednej fundamentalnej umiej\u0119tno\u015bci, post\u0119powania zgodnie z instrukcj\u0105.", "tokens": [51464, 343, 5024, 12221, 11, 1548, 5279, 2884, 566, 12573, 5232, 11794, 8088, 11794, 1105, 7764, 46788, 16438, 11, 2183, 1274, 14701, 5609, 710, 21787, 2766, 710, 1058, 25126, 66, 8555, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08459345702153102, "compression_ratio": 1.4684684684684686, "no_speech_prob": 0.009495570324361324}, {"id": 11, "seek": 5720, "start": 57.2, "end": 70.2, "text": " I rozumiem, \u017ce w\u0142a\u015bnie ten problem, ta przepa\u015b\u0107 mi\u0119dzy ogromn\u0105 wiedz\u0105, a praktyczn\u0105 u\u017cyteczno\u015bci\u0105 sta\u0142 si\u0119 punktem wyj\u015bcia dla pracy, kt\u00f3rej dzisiaj si\u0119 przyjrzymy.", "tokens": [50364, 286, 48797, 4907, 11, 3561, 14234, 2064, 1154, 11, 1846, 30829, 64, 7753, 33964, 34416, 298, 13113, 46894, 8925, 11, 257, 3206, 74, 874, 3689, 13113, 34097, 975, 3689, 16438, 1611, 11135, 1221, 3244, 39561, 443, 4628, 73, 1788, 2755, 12285, 35591, 11, 36023, 25772, 3244, 6501, 73, 13047, 2226, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08820434751964751, "compression_ratio": 1.3755102040816327, "no_speech_prob": 0.0009328522719442844}, {"id": 12, "seek": 5720, "start": 70.2, "end": 76.2, "text": " Mam tu przed sob\u0105 scaling instruction fine-tuned language models.", "tokens": [51014, 19899, 2604, 18334, 18253, 1611, 21589, 10951, 2489, 12, 83, 43703, 2856, 5245, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08820434751964751, "compression_ratio": 1.3755102040816327, "no_speech_prob": 0.0009328522719442844}, {"id": 13, "seek": 5720, "start": 76.2, "end": 76.7, "text": " Tak.", "tokens": [51314, 9118, 13, 51339], "temperature": 0.0, "avg_logprob": -0.08820434751964751, "compression_ratio": 1.3755102040816327, "no_speech_prob": 0.0009328522719442844}, {"id": 14, "seek": 5720, "start": 76.7, "end": 82.2, "text": " Tytu\u0142 brzmi bardzo technicznie, ale idea za nim jest chyba, no, bardzo intuicyjna.", "tokens": [51339, 314, 4328, 84, 1221, 738, 89, 3057, 9034, 1537, 17946, 2766, 11, 6775, 1558, 7949, 24887, 3492, 31532, 11, 572, 11, 9034, 560, 84, 2632, 73, 629, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08820434751964751, "compression_ratio": 1.3755102040816327, "no_speech_prob": 0.0009328522719442844}, {"id": 15, "seek": 8220, "start": 82.2, "end": 87.2, "text": " Niezwykle autorzy postawili prost\u0105, ale, wiesz, pot\u0119\u017cn\u0105 hipotez\u0119.", "tokens": [50364, 12016, 89, 9726, 14677, 19510, 1229, 2183, 1607, 2312, 10293, 1611, 11, 6775, 11, 261, 15347, 11, 1847, 1274, 1427, 13113, 8103, 1370, 11052, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08964195251464843, "compression_ratio": 1.3729508196721312, "no_speech_prob": 0.5187180042266846}, {"id": 16, "seek": 8220, "start": 87.2, "end": 88.2, "text": " To znaczy.", "tokens": [50614, 1407, 36584, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08964195251464843, "compression_ratio": 1.3729508196721312, "no_speech_prob": 0.5187180042266846}, {"id": 17, "seek": 8220, "start": 88.2, "end": 103.2, "text": " Co je\u015bli we\u017amiemy tego genialnego, ale troch\u0119 chaotycznego, cyfrowego studenta i zamiast wpycha\u0107 w niego jeszcze wi\u0119cej ksi\u0105\u017cek, wy\u015blemy go na taki intensywny kurs pod tytu\u0142em Jak skutecznie s\u0142ucha\u0107 i wykonywa\u0107 polecenia.", "tokens": [50664, 3066, 25630, 321, 10659, 25210, 2226, 8627, 48228, 11858, 11, 6775, 24926, 6294, 6737, 3689, 11858, 11, 3185, 69, 1892, 6308, 3107, 64, 741, 710, 4526, 525, 261, 8200, 4413, 2162, 261, 49615, 14168, 26004, 39311, 916, 11, 4628, 1788, 306, 2226, 352, 1667, 20065, 14056, 88, 43682, 350, 2156, 2497, 1104, 9179, 11126, 15029, 1110, 1169, 19923, 15116, 26042, 2162, 741, 39287, 2526, 25234, 13208, 13037, 654, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08964195251464843, "compression_ratio": 1.3729508196721312, "no_speech_prob": 0.5187180042266846}, {"id": 18, "seek": 8220, "start": 103.2, "end": 105.2, "text": " Ciekawe podej\u015bcie.", "tokens": [51414, 383, 414, 2330, 826, 7468, 73, 9815, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08964195251464843, "compression_ratio": 1.3729508196721312, "no_speech_prob": 0.5187180042266846}, {"id": 19, "seek": 10520, "start": 105.2, "end": 117.2, "text": " Ja sugerowali, \u017ce mo\u017cna odblokowa\u0107 ukryty potencje o tych modeli niekoniecznie przez budowanie jeszcze wi\u0119kszych, ale przez nauczenie ich jak lepiej korzysta\u0107 z tego, co ju\u017c wiedz\u0105.", "tokens": [50364, 3530, 459, 1321, 305, 5103, 11, 3561, 17790, 3611, 5199, 453, 11445, 26769, 627, 874, 1847, 22660, 2884, 277, 15180, 2316, 72, 2838, 18295, 414, 19923, 14064, 3265, 22028, 14168, 29968, 28051, 11, 6775, 14064, 49103, 16778, 1893, 4207, 476, 39699, 14784, 49590, 2162, 710, 8627, 11, 598, 10678, 46894, 8925, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10690528389037125, "compression_ratio": 1.437956204379562, "no_speech_prob": 0.31479141116142273}, {"id": 20, "seek": 10520, "start": 117.2, "end": 119.2, "text": " Ok, to brzmi obiecuj\u0105co.", "tokens": [50964, 3477, 11, 281, 738, 89, 3057, 1111, 35733, 13263, 1291, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10690528389037125, "compression_ratio": 1.437956204379562, "no_speech_prob": 0.31479141116142273}, {"id": 21, "seek": 10520, "start": 119.2, "end": 122.2, "text": " Zatem naszym celem dzisiaj jest.", "tokens": [51064, 1176, 26851, 48094, 1769, 10386, 25772, 3492, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10690528389037125, "compression_ratio": 1.437956204379562, "no_speech_prob": 0.31479141116142273}, {"id": 22, "seek": 10520, "start": 122.2, "end": 126.2, "text": " No, roz\u0142o\u017cenie tego podej\u015bcia na czynniki pierwsze.", "tokens": [51214, 883, 11, 9544, 5249, 41118, 8627, 7468, 73, 1788, 2755, 1667, 6430, 26384, 9850, 45994, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10690528389037125, "compression_ratio": 1.437956204379562, "no_speech_prob": 0.31479141116142273}, {"id": 23, "seek": 10520, "start": 126.2, "end": 133.2, "text": " Chcemy zrozumie\u0107, czym jest ten intensywny kurs dla modeli, czyli instruction fine-tuning.", "tokens": [51414, 761, 384, 2226, 710, 27857, 449, 414, 2162, 11, 31466, 3492, 2064, 14056, 88, 43682, 350, 2156, 12285, 2316, 72, 11, 16591, 10951, 2489, 12, 83, 37726, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10690528389037125, "compression_ratio": 1.437956204379562, "no_speech_prob": 0.31479141116142273}, {"id": 24, "seek": 13320, "start": 133.2, "end": 139.2, "text": " Jak go przeprowadzono na niespotykan\u0105 skal\u0119 i co najwa\u017cniejsze, czy to faktycznie dzia\u0142a.", "tokens": [50364, 15029, 352, 30829, 1892, 345, 89, 8957, 1667, 48100, 79, 6737, 5225, 1611, 16890, 1274, 741, 598, 11212, 27111, 44258, 11, 6430, 281, 33647, 45586, 37903, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06082482528686523, "compression_ratio": 1.4106463878326996, "no_speech_prob": 0.028461892157793045}, {"id": 25, "seek": 13320, "start": 139.2, "end": 144.2, "text": " Czy mo\u017cna niewielkim kosztem zamieni\u0107 chaotycznego geniusza w pomocnego eksperta?", "tokens": [50664, 19832, 17790, 43622, 1187, 25112, 19532, 2682, 443, 19876, 1053, 12757, 6294, 6737, 3689, 11858, 14017, 2394, 261, 48962, 11858, 30724, 610, 1328, 30, 50914], "temperature": 0.0, "avg_logprob": -0.06082482528686523, "compression_ratio": 1.4106463878326996, "no_speech_prob": 0.028461892157793045}, {"id": 26, "seek": 13320, "start": 144.2, "end": 145.2, "text": " Dok\u0142adnie.", "tokens": [50914, 29768, 10358, 2766, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06082482528686523, "compression_ratio": 1.4106463878326996, "no_speech_prob": 0.028461892157793045}, {"id": 27, "seek": 13320, "start": 145.2, "end": 151.2, "text": " A odpowied\u017a, jak zaraz zobaczymy, okaza\u0142a si\u0119 chyba bardziej zaskakuj\u0105ca ni\u017c sami autorzy si\u0119 spodziewali.", "tokens": [50964, 316, 36574, 10659, 11, 4207, 22675, 921, 37273, 2226, 11, 3133, 12257, 5024, 3244, 31532, 27209, 710, 3863, 514, 13263, 496, 28502, 3247, 72, 19510, 1229, 3244, 637, 378, 89, 1093, 5103, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06082482528686523, "compression_ratio": 1.4106463878326996, "no_speech_prob": 0.028461892157793045}, {"id": 28, "seek": 13320, "start": 151.2, "end": 153.2, "text": " No to zacznijmy od podstaw.", "tokens": [51264, 883, 281, 710, 14875, 77, 1718, 2226, 3611, 43443, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06082482528686523, "compression_ratio": 1.4106463878326996, "no_speech_prob": 0.028461892157793045}, {"id": 29, "seek": 13320, "start": 153.2, "end": 154.2, "text": " Dobrze.", "tokens": [51364, 29679, 13503, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06082482528686523, "compression_ratio": 1.4106463878326996, "no_speech_prob": 0.028461892157793045}, {"id": 30, "seek": 13320, "start": 154.2, "end": 156.2, "text": " Wi\u0119c na czym polega ten proces?", "tokens": [51414, 32508, 1667, 31466, 13208, 3680, 2064, 17565, 30, 51514], "temperature": 0.0, "avg_logprob": -0.06082482528686523, "compression_ratio": 1.4106463878326996, "no_speech_prob": 0.028461892157793045}, {"id": 31, "seek": 15620, "start": 156.2, "end": 163.2, "text": " W najwi\u0119kszym skr\u00f3cie bierzemy ju\u017c wst\u0119pnie wytremowany pot\u0119\u017cny model, taki jak palm czy T5.", "tokens": [50364, 343, 48636, 1694, 26681, 1110, 11721, 4260, 272, 34602, 3633, 10678, 261, 372, 18085, 2766, 261, 4328, 2579, 23341, 1847, 1274, 1427, 1634, 2316, 11, 20065, 4207, 17018, 6430, 314, 20, 13, 50714], "temperature": 0.0, "avg_logprob": -0.05716537211063134, "compression_ratio": 1.3741007194244603, "no_speech_prob": 0.021907353773713112}, {"id": 32, "seek": 15620, "start": 163.2, "end": 165.2, "text": " Ten, co ju\u017c poch\u0142on\u0105\u0142 p\u00f3\u0142 internetu.", "tokens": [50714, 9380, 11, 598, 10678, 714, 339, 1221, 266, 1611, 1221, 47907, 4705, 84, 13, 50814], "temperature": 0.0, "avg_logprob": -0.05716537211063134, "compression_ratio": 1.3741007194244603, "no_speech_prob": 0.021907353773713112}, {"id": 33, "seek": 15620, "start": 165.2, "end": 166.2, "text": " W\u0142a\u015bnie.", "tokens": [50814, 343, 5024, 12221, 13, 50864], "temperature": 0.0, "avg_logprob": -0.05716537211063134, "compression_ratio": 1.3741007194244603, "no_speech_prob": 0.021907353773713112}, {"id": 34, "seek": 15620, "start": 166.2, "end": 168.2, "text": " Ma w sobie gigantyczn\u0105 wiedz\u0119.", "tokens": [50864, 4042, 261, 13652, 8741, 394, 17466, 13113, 46894, 11052, 13, 50964], "temperature": 0.0, "avg_logprob": -0.05716537211063134, "compression_ratio": 1.3741007194244603, "no_speech_prob": 0.021907353773713112}, {"id": 35, "seek": 15620, "start": 168.2, "end": 174.2, "text": " I zamiast uczy\u0107 go od zera, poddajemy go o dodatkowej, bardzo specyficznej sesji treningowej.", "tokens": [50964, 286, 710, 4526, 525, 344, 33967, 352, 3611, 710, 1663, 11, 2497, 67, 1805, 3633, 352, 277, 13886, 33525, 21091, 11, 9034, 768, 1344, 1786, 89, 11794, 5385, 4013, 2192, 773, 21091, 13, 51264], "temperature": 0.0, "avg_logprob": -0.05716537211063134, "compression_ratio": 1.3741007194244603, "no_speech_prob": 0.021907353773713112}, {"id": 36, "seek": 15620, "start": 174.2, "end": 177.2, "text": " I ta sesja to jest w\u0142a\u015bnie instruction fine-tuning.", "tokens": [51264, 286, 1846, 5385, 2938, 281, 3492, 14234, 10951, 2489, 12, 83, 37726, 13, 51414], "temperature": 0.0, "avg_logprob": -0.05716537211063134, "compression_ratio": 1.3741007194244603, "no_speech_prob": 0.021907353773713112}, {"id": 37, "seek": 15620, "start": 177.2, "end": 180.2, "text": " Ale czym to si\u0119 r\u00f3\u017cni od zwyk\u0142ego treningu?", "tokens": [51414, 9366, 31466, 281, 3244, 19637, 3722, 3611, 43436, 74, 1221, 6308, 2192, 773, 84, 30, 51564], "temperature": 0.0, "avg_logprob": -0.05716537211063134, "compression_ratio": 1.3741007194244603, "no_speech_prob": 0.021907353773713112}, {"id": 38, "seek": 18020, "start": 180.2, "end": 182.2, "text": " R\u00f3\u017cnica jest kluczowa.", "tokens": [50364, 497, 812, 1427, 32687, 3492, 9671, 1311, 89, 5528, 13, 50464], "temperature": 0.0, "avg_logprob": -0.05094287378324879, "compression_ratio": 1.3948339483394834, "no_speech_prob": 0.021226506680250168}, {"id": 39, "seek": 18020, "start": 182.2, "end": 190.2, "text": " Zamiast pokazywa\u0107 mu miliardy zda\u0144 i kaza\u0107 przewidywa\u0107 nast\u0119pne s\u0142owo, pokazujemy mu konkretne przyk\u0142ady.", "tokens": [50464, 1176, 4526, 525, 13010, 33235, 25234, 2992, 1962, 72, 515, 88, 710, 2675, 5248, 741, 350, 12257, 2162, 39758, 38836, 25234, 39662, 716, 15116, 19941, 11, 13010, 921, 21767, 2992, 36500, 716, 6501, 74, 1221, 880, 13, 50864], "temperature": 0.0, "avg_logprob": -0.05094287378324879, "compression_ratio": 1.3948339483394834, "no_speech_prob": 0.021226506680250168}, {"id": 40, "seek": 18020, "start": 190.2, "end": 191.2, "text": " Przyk\u0142ady czego?", "tokens": [50864, 39590, 74, 1221, 880, 36559, 30, 50914], "temperature": 0.0, "avg_logprob": -0.05094287378324879, "compression_ratio": 1.3948339483394834, "no_speech_prob": 0.021226506680250168}, {"id": 41, "seek": 18020, "start": 191.2, "end": 194.2, "text": " Instrukcji i po\u017c\u0105danych odpowiedzi.", "tokens": [50914, 2730, 25126, 19649, 741, 714, 1427, 18962, 34644, 36574, 3992, 13, 51064], "temperature": 0.0, "avg_logprob": -0.05094287378324879, "compression_ratio": 1.3948339483394834, "no_speech_prob": 0.021226506680250168}, {"id": 42, "seek": 18020, "start": 194.2, "end": 197.2, "text": " To s\u0105 zadania sformu\u0142owane w naturalnym j\u0119zyku.", "tokens": [51064, 1407, 9015, 42788, 5609, 262, 837, 84, 1221, 23066, 261, 3303, 12996, 49055, 5279, 13, 51214], "temperature": 0.0, "avg_logprob": -0.05094287378324879, "compression_ratio": 1.3948339483394834, "no_speech_prob": 0.021226506680250168}, {"id": 43, "seek": 18020, "start": 197.2, "end": 202.2, "text": " Niesuche zapytania do bazy danych, ale polecenia, jakie wyda\u0142by cz\u0142owiek.", "tokens": [51214, 426, 530, 17545, 14223, 4328, 5609, 360, 27147, 88, 274, 34644, 11, 6775, 13208, 13037, 654, 11, 22124, 4628, 2675, 34635, 36282, 74, 13, 51464], "temperature": 0.0, "avg_logprob": -0.05094287378324879, "compression_ratio": 1.3948339483394834, "no_speech_prob": 0.021226506680250168}, {"id": 44, "seek": 18020, "start": 202.2, "end": 204.2, "text": " Czyli na przyk\u0142ad.", "tokens": [51464, 37099, 1667, 23144, 13, 51564], "temperature": 0.0, "avg_logprob": -0.05094287378324879, "compression_ratio": 1.3948339483394834, "no_speech_prob": 0.021226506680250168}, {"id": 45, "seek": 18020, "start": 204.2, "end": 206.2, "text": " Podsumuj ten tekst w trzech zdaniach.", "tokens": [51564, 12646, 82, 449, 4579, 2064, 16624, 372, 261, 504, 19439, 16221, 3782, 608, 13, 51664], "temperature": 0.0, "avg_logprob": -0.05094287378324879, "compression_ratio": 1.3948339483394834, "no_speech_prob": 0.021226506680250168}, {"id": 46, "seek": 20620, "start": 206.2, "end": 207.2, "text": " Dok\u0142adnie.", "tokens": [50364, 29768, 10358, 2766, 13, 50414], "temperature": 0.0, "avg_logprob": -0.09169110384854404, "compression_ratio": 1.3755458515283843, "no_speech_prob": 0.41227981448173523}, {"id": 47, "seek": 20620, "start": 207.2, "end": 212.2, "text": " Albo przet\u0142umacz to zdanie na niemiecki, napisz wiersz o jesieni.", "tokens": [50414, 967, 1763, 6541, 302, 49166, 14875, 281, 16221, 7155, 1667, 2838, 25210, 547, 72, 11, 9296, 23848, 261, 4890, 89, 277, 361, 279, 35462, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09169110384854404, "compression_ratio": 1.3755458515283843, "no_speech_prob": 0.41227981448173523}, {"id": 48, "seek": 20620, "start": 212.2, "end": 216.2, "text": " Model uczy si\u0119 samego formatu polecenie wykonanie.", "tokens": [50664, 17105, 344, 6522, 3244, 912, 1571, 7877, 84, 13208, 13037, 414, 46702, 7155, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09169110384854404, "compression_ratio": 1.3755458515283843, "no_speech_prob": 0.41227981448173523}, {"id": 49, "seek": 20620, "start": 216.2, "end": 218.2, "text": " To faktycznie fundamentalna zmiana.", "tokens": [50864, 1407, 33647, 45586, 8088, 629, 17020, 8497, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09169110384854404, "compression_ratio": 1.3755458515283843, "no_speech_prob": 0.41227981448173523}, {"id": 50, "seek": 20620, "start": 218.2, "end": 223.2, "text": " Pami\u0119tam ten przyk\u0142ad z pracy, kt\u00f3ry \u015bwietnie to ilustruje.", "tokens": [50964, 430, 23806, 37323, 2064, 23144, 710, 35591, 11, 9913, 8299, 39083, 2766, 281, 1930, 381, 894, 2884, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09169110384854404, "compression_ratio": 1.3755458515283843, "no_speech_prob": 0.41227981448173523}, {"id": 51, "seek": 20620, "start": 223.2, "end": 224.2, "text": " Ten z hajku?", "tokens": [51214, 9380, 710, 324, 73, 5279, 30, 51264], "temperature": 0.0, "avg_logprob": -0.09169110384854404, "compression_ratio": 1.3755458515283843, "no_speech_prob": 0.41227981448173523}, {"id": 52, "seek": 20620, "start": 224.2, "end": 225.2, "text": " Tak.", "tokens": [51264, 9118, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09169110384854404, "compression_ratio": 1.3755458515283843, "no_speech_prob": 0.41227981448173523}, {"id": 53, "seek": 20620, "start": 225.2, "end": 227.2, "text": " Pytanie brzmia\u0142o.", "tokens": [51314, 430, 4328, 7155, 738, 89, 29958, 5249, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09169110384854404, "compression_ratio": 1.3755458515283843, "no_speech_prob": 0.41227981448173523}, {"id": 54, "seek": 20620, "start": 227.2, "end": 231.2, "text": " Czy w jednym t\u0142icie zmie\u015bci si\u0119 ca\u0142e hajku?", "tokens": [51414, 19832, 261, 5232, 12996, 256, 1221, 28434, 17020, 414, 6199, 3244, 47631, 324, 73, 5279, 30, 51614], "temperature": 0.0, "avg_logprob": -0.09169110384854404, "compression_ratio": 1.3755458515283843, "no_speech_prob": 0.41227981448173523}, {"id": 55, "seek": 23120, "start": 231.2, "end": 236.2, "text": " A model mog\u0142by odpowiedzie\u0107 po prostu tak albo no wygenerowa\u0107 co\u015b losowego.", "tokens": [50364, 316, 2316, 13172, 34635, 24314, 22078, 714, 19518, 991, 22622, 572, 4628, 21848, 11445, 19241, 1750, 26576, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11183355736920214, "compression_ratio": 1.3542435424354244, "no_speech_prob": 0.5237764716148376}, {"id": 56, "seek": 23120, "start": 236.2, "end": 237.2, "text": " W\u0142a\u015bnie.", "tokens": [50614, 343, 5024, 12221, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11183355736920214, "compression_ratio": 1.3542435424354244, "no_speech_prob": 0.5237764716148376}, {"id": 57, "seek": 23120, "start": 237.2, "end": 244.2, "text": " A model po Instruction Fine Tuning nazwany w pracy Flan Palm uczy si\u0119 generowa\u0107 odpowied\u017a,", "tokens": [50664, 316, 2316, 714, 2730, 3826, 12024, 21363, 278, 20151, 86, 1325, 261, 35591, 3235, 282, 32668, 344, 6522, 3244, 1337, 11445, 36574, 10659, 11, 51014], "temperature": 0.0, "avg_logprob": -0.11183355736920214, "compression_ratio": 1.3542435424354244, "no_speech_prob": 0.5237764716148376}, {"id": 58, "seek": 23120, "start": 244.2, "end": 247.2, "text": " kt\u00f3ra jest nie tylko poprawna, ale te\u017c pomocna.", "tokens": [51014, 19456, 3492, 2838, 13219, 1665, 5131, 629, 11, 6775, 9516, 48962, 629, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11183355736920214, "compression_ratio": 1.3542435424354244, "no_speech_prob": 0.5237764716148376}, {"id": 59, "seek": 23120, "start": 247.2, "end": 249.2, "text": " I zawiera uzasadnienie.", "tokens": [51164, 286, 28165, 10609, 16851, 296, 345, 77, 27385, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11183355736920214, "compression_ratio": 1.3542435424354244, "no_speech_prob": 0.5237764716148376}, {"id": 60, "seek": 23120, "start": 249.2, "end": 250.2, "text": " Dok\u0142adnie.", "tokens": [51264, 29768, 10358, 2766, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11183355736920214, "compression_ratio": 1.3542435424354244, "no_speech_prob": 0.5237764716148376}, {"id": 61, "seek": 23120, "start": 250.2, "end": 253.2, "text": " Co\u015b w stylu hajku to japo\u0144ski wiersz.", "tokens": [51314, 3066, 1788, 261, 7952, 2781, 324, 73, 5279, 281, 361, 37615, 5248, 18020, 261, 4890, 89, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11183355736920214, "compression_ratio": 1.3542435424354244, "no_speech_prob": 0.5237764716148376}, {"id": 62, "seek": 23120, "start": 253.2, "end": 255.2, "text": " Jest bardzo kr\u00f3tkie.", "tokens": [51464, 24918, 9034, 42366, 83, 22872, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11183355736920214, "compression_ratio": 1.3542435424354244, "no_speech_prob": 0.5237764716148376}, {"id": 63, "seek": 23120, "start": 255.2, "end": 259.2, "text": " Limit znak\u00f3w na Twitterze to 288.", "tokens": [51564, 16406, 270, 15397, 514, 3901, 1667, 5794, 1381, 281, 7562, 23, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11183355736920214, "compression_ratio": 1.3542435424354244, "no_speech_prob": 0.5237764716148376}, {"id": 64, "seek": 25920, "start": 260.2, "end": 262.2, "text": " Hajku bez problemu si\u0119 w nim zmie\u015bci.", "tokens": [50414, 4064, 73, 5279, 10782, 1154, 84, 3244, 261, 24887, 17020, 414, 6199, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07144331409983391, "compression_ratio": 1.4136546184738956, "no_speech_prob": 0.12939295172691345}, {"id": 65, "seek": 25920, "start": 262.2, "end": 264.2, "text": " Odpowied\u017a brzmi tak.", "tokens": [50514, 12210, 14701, 1091, 10659, 738, 89, 3057, 991, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07144331409983391, "compression_ratio": 1.4136546184738956, "no_speech_prob": 0.12939295172691345}, {"id": 66, "seek": 25920, "start": 264.2, "end": 266.2, "text": " To jest zupe\u0142nie inny poziom interakcji.", "tokens": [50614, 1407, 3492, 49922, 294, 1634, 38503, 298, 728, 514, 19649, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07144331409983391, "compression_ratio": 1.4136546184738956, "no_speech_prob": 0.12939295172691345}, {"id": 67, "seek": 25920, "start": 266.2, "end": 267.2, "text": " Zdecydowanie.", "tokens": [50714, 1176, 1479, 1344, 67, 22028, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07144331409983391, "compression_ratio": 1.4136546184738956, "no_speech_prob": 0.12939295172691345}, {"id": 68, "seek": 25920, "start": 267.2, "end": 272.2, "text": " Model nie tylko podaje fakt, ale pokazuje, \u017ce zrozumia\u0142 intencje pytania.", "tokens": [50764, 17105, 2838, 13219, 2497, 11153, 21310, 11, 6775, 13010, 43317, 11, 3561, 710, 27857, 449, 8908, 560, 22660, 2884, 25878, 5609, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07144331409983391, "compression_ratio": 1.4136546184738956, "no_speech_prob": 0.12939295172691345}, {"id": 69, "seek": 25920, "start": 272.2, "end": 276.2, "text": " A skala tego eksperymentu no robi wra\u017cenie.", "tokens": [51014, 316, 1110, 5159, 8627, 30724, 610, 88, 518, 84, 572, 47380, 7843, 41118, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07144331409983391, "compression_ratio": 1.4136546184738956, "no_speech_prob": 0.12939295172691345}, {"id": 70, "seek": 25920, "start": 276.2, "end": 279.2, "text": " Autorzy nie bazowali na jednym ma\u0142ym zbiorze danych.", "tokens": [51214, 6049, 284, 1229, 2838, 27147, 305, 5103, 1667, 5232, 12996, 463, 1221, 4199, 710, 33362, 1381, 274, 34644, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07144331409983391, "compression_ratio": 1.4136546184738956, "no_speech_prob": 0.12939295172691345}, {"id": 71, "seek": 25920, "start": 279.2, "end": 280.2, "text": " Zdecydowanie nie.", "tokens": [51364, 1176, 1479, 1344, 67, 22028, 2838, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07144331409983391, "compression_ratio": 1.4136546184738956, "no_speech_prob": 0.12939295172691345}, {"id": 72, "seek": 25920, "start": 280.2, "end": 283.2, "text": " Wzi\u0119li cztery pot\u0119\u017cne kolekcje zada\u0144.", "tokens": [51414, 343, 16706, 2081, 6472, 12733, 1847, 1274, 1427, 716, 18303, 74, 44261, 710, 1538, 5248, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07144331409983391, "compression_ratio": 1.4136546184738956, "no_speech_prob": 0.12939295172691345}, {"id": 73, "seek": 28320, "start": 283.2, "end": 291.2, "text": " Muffin T0SF NIV2 i co oka\u017ce si\u0119 kluczowe zbiory z adnotacjami Chain of Thought.", "tokens": [50364, 376, 1245, 259, 314, 15, 50, 37, 426, 10375, 17, 741, 598, 277, 2330, 2875, 3244, 9671, 1311, 89, 6880, 710, 5614, 827, 710, 614, 2247, 326, 73, 4526, 33252, 295, 23058, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12954528089882672, "compression_ratio": 1.3169811320754716, "no_speech_prob": 0.33222612738609314}, {"id": 74, "seek": 28320, "start": 291.2, "end": 295.2, "text": " I po\u0142\u0105czyli to wszystko w jedn\u0105 gigantyczn\u0105 mieszank\u0119.", "tokens": [50764, 286, 714, 15926, 6522, 2081, 281, 22607, 261, 5232, 13113, 8741, 394, 17466, 13113, 33039, 657, 1274, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12954528089882672, "compression_ratio": 1.3169811320754716, "no_speech_prob": 0.33222612738609314}, {"id": 75, "seek": 28320, "start": 295.2, "end": 296.2, "text": " Tak.", "tokens": [50964, 9118, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12954528089882672, "compression_ratio": 1.3169811320754716, "no_speech_prob": 0.33222612738609314}, {"id": 76, "seek": 28320, "start": 296.2, "end": 300.2, "text": " W sumie da\u0142o to 18-36 unikalnych typ\u00f3w zada\u0144.", "tokens": [51014, 343, 2408, 414, 1120, 5249, 281, 2443, 12, 11309, 517, 41216, 9399, 2125, 3901, 710, 1538, 5248, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12954528089882672, "compression_ratio": 1.3169811320754716, "no_speech_prob": 0.33222612738609314}, {"id": 77, "seek": 28320, "start": 300.2, "end": 303.2, "text": " I przetestowali to podej\u015bcie na ca\u0142ej gamie modeli.", "tokens": [51214, 286, 6541, 302, 377, 305, 5103, 281, 7468, 73, 9815, 1667, 47631, 73, 8019, 414, 2316, 72, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12954528089882672, "compression_ratio": 1.3169811320754716, "no_speech_prob": 0.33222612738609314}, {"id": 78, "seek": 28320, "start": 303.2, "end": 305.2, "text": " Od tych ma\u0142ych do gigant\u00f3w.", "tokens": [51364, 12210, 15180, 463, 47655, 360, 8741, 394, 3901, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12954528089882672, "compression_ratio": 1.3169811320754716, "no_speech_prob": 0.33222612738609314}, {"id": 79, "seek": 28320, "start": 305.2, "end": 306.2, "text": " Tak.", "tokens": [51464, 9118, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12954528089882672, "compression_ratio": 1.3169811320754716, "no_speech_prob": 0.33222612738609314}, {"id": 80, "seek": 28320, "start": 306.2, "end": 311.2, "text": " Od stosunkowo ma\u0142ego Flante 5 Small z 80 milionami parametr\u00f3w,", "tokens": [51514, 12210, 43581, 3197, 19941, 463, 1221, 6308, 3235, 2879, 1025, 15287, 710, 4688, 1962, 313, 4526, 6220, 27965, 3901, 11, 51764], "temperature": 0.0, "avg_logprob": -0.12954528089882672, "compression_ratio": 1.3169811320754716, "no_speech_prob": 0.33222612738609314}, {"id": 81, "seek": 31120, "start": 311.2, "end": 318.2, "text": " a\u017c po no prawdziwego behemota flan palmem z 540 miliardami parametr\u00f3w.", "tokens": [50364, 48134, 714, 572, 41175, 3992, 826, 1571, 1540, 443, 5377, 932, 282, 3984, 17886, 710, 1025, 5254, 1962, 72, 515, 4526, 6220, 27965, 3901, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07249583047011803, "compression_ratio": 1.3246753246753247, "no_speech_prob": 0.024053508415818214}, {"id": 82, "seek": 31120, "start": 318.2, "end": 322.2, "text": " Chcieli sprawdzi\u0107, czy ta metoda dzia\u0142a uniwersalnie?", "tokens": [50714, 761, 537, 10148, 46192, 28496, 11, 6430, 1846, 1131, 13449, 37903, 36435, 5364, 304, 2766, 30, 50914], "temperature": 0.0, "avg_logprob": -0.07249583047011803, "compression_ratio": 1.3246753246753247, "no_speech_prob": 0.024053508415818214}, {"id": 83, "seek": 31120, "start": 322.2, "end": 323.2, "text": " Tak.", "tokens": [50914, 9118, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07249583047011803, "compression_ratio": 1.3246753246753247, "no_speech_prob": 0.024053508415818214}, {"id": 84, "seek": 31120, "start": 323.2, "end": 326.2, "text": " Niezale\u017cnie od rozmiaru m\u00f3zgu modelu.", "tokens": [50964, 12016, 89, 45494, 2766, 3611, 9544, 3057, 16870, 32515, 89, 2794, 2316, 84, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07249583047011803, "compression_ratio": 1.3246753246753247, "no_speech_prob": 0.024053508415818214}, {"id": 85, "seek": 31120, "start": 326.2, "end": 331.2, "text": " Ale tutaj dochodzimy do czego\u015b, co w tej pracy jest dla mnie no absolutnym szokiem.", "tokens": [51114, 9366, 12749, 9243, 378, 89, 13189, 360, 36559, 1788, 11, 598, 261, 12573, 35591, 3492, 12285, 17661, 572, 18757, 12996, 7870, 453, 4907, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07249583047011803, "compression_ratio": 1.3246753246753247, "no_speech_prob": 0.024053508415818214}, {"id": 86, "seek": 31120, "start": 331.2, "end": 333.2, "text": " Koszt obliczeniowy.", "tokens": [51364, 36909, 2682, 1111, 1050, 42124, 10089, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07249583047011803, "compression_ratio": 1.3246753246753247, "no_speech_prob": 0.024053508415818214}, {"id": 87, "seek": 31120, "start": 333.2, "end": 335.2, "text": " A tak.", "tokens": [51464, 316, 991, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07249583047011803, "compression_ratio": 1.3246753246753247, "no_speech_prob": 0.024053508415818214}, {"id": 88, "seek": 31120, "start": 335.2, "end": 337.2, "text": " To jest niesamowite.", "tokens": [51564, 1407, 3492, 48100, 335, 305, 642, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07249583047011803, "compression_ratio": 1.3246753246753247, "no_speech_prob": 0.024053508415818214}, {"id": 89, "seek": 33720, "start": 337.2, "end": 341.2, "text": " Kiedy my\u015blimy o trenowaniu modeli z setkami miliard\u00f3w parametr\u00f3w,", "tokens": [50364, 591, 16446, 48633, 4197, 88, 277, 23136, 305, 25849, 2316, 72, 710, 992, 48737, 1962, 72, 515, 3901, 6220, 27965, 3901, 11, 50564], "temperature": 0.0, "avg_logprob": -0.078614090844024, "compression_ratio": 1.3508771929824561, "no_speech_prob": 0.4027896821498871}, {"id": 90, "seek": 33720, "start": 341.2, "end": 344.2, "text": " to wyobra\u017camy sobie farm serwer\u00f3w pracuj\u0105ce miesi\u0105cami.", "tokens": [50564, 281, 4628, 24393, 1427, 7804, 13652, 5421, 816, 1554, 3901, 22404, 13263, 384, 41543, 11404, 66, 4526, 13, 50714], "temperature": 0.0, "avg_logprob": -0.078614090844024, "compression_ratio": 1.3508771929824561, "no_speech_prob": 0.4027896821498871}, {"id": 91, "seek": 33720, "start": 344.2, "end": 347.2, "text": " I rachunki za pr\u0105d id\u0105ce w miliony.", "tokens": [50714, 286, 367, 608, 409, 2984, 7949, 582, 18962, 4496, 1611, 384, 261, 1962, 46184, 13, 50864], "temperature": 0.0, "avg_logprob": -0.078614090844024, "compression_ratio": 1.3508771929824561, "no_speech_prob": 0.4027896821498871}, {"id": 92, "seek": 33720, "start": 347.2, "end": 348.2, "text": " Dok\u0142adnie.", "tokens": [50864, 29768, 10358, 2766, 13, 50914], "temperature": 0.0, "avg_logprob": -0.078614090844024, "compression_ratio": 1.3508771929824561, "no_speech_prob": 0.4027896821498871}, {"id": 93, "seek": 33720, "start": 348.2, "end": 353.2, "text": " Tymczasem tabela 2 z tej pracy pokazuje, \u017ce w przypadku najwi\u0119kszego modelu palm", "tokens": [50914, 314, 4199, 30989, 443, 4421, 4053, 568, 710, 12573, 35591, 13010, 43317, 11, 3561, 261, 41955, 48636, 1694, 27725, 2316, 84, 17018, 51164], "temperature": 0.0, "avg_logprob": -0.078614090844024, "compression_ratio": 1.3508771929824561, "no_speech_prob": 0.4027896821498871}, {"id": 94, "seek": 33720, "start": 353.2, "end": 361.2, "text": " ca\u0142y ten proces Instruction Fine Tuning zu\u017cy\u0142 zaledwie 0,2% mocy obliczeniowej.", "tokens": [51164, 35226, 2064, 17565, 2730, 3826, 12024, 21363, 278, 2164, 7735, 1221, 710, 5573, 8699, 1958, 11, 17, 4, 705, 1344, 1111, 1050, 42124, 21091, 13, 51564], "temperature": 0.0, "avg_logprob": -0.078614090844024, "compression_ratio": 1.3508771929824561, "no_speech_prob": 0.4027896821498871}, {"id": 95, "seek": 33720, "start": 361.2, "end": 362.2, "text": " Tak.", "tokens": [51564, 9118, 13, 51614], "temperature": 0.0, "avg_logprob": -0.078614090844024, "compression_ratio": 1.3508771929824561, "no_speech_prob": 0.4027896821498871}, {"id": 96, "seek": 33720, "start": 362.2, "end": 363.2, "text": " Czy ja to dobrze czytam?", "tokens": [51614, 19832, 2784, 281, 28335, 6430, 37323, 30, 51664], "temperature": 0.0, "avg_logprob": -0.078614090844024, "compression_ratio": 1.3508771929824561, "no_speech_prob": 0.4027896821498871}, {"id": 97, "seek": 33720, "start": 363.2, "end": 365.2, "text": " Tylko 20%.", "tokens": [51664, 49286, 4093, 945, 6856, 51764], "temperature": 0.0, "avg_logprob": -0.078614090844024, "compression_ratio": 1.3508771929824561, "no_speech_prob": 0.4027896821498871}, {"id": 98, "seek": 36520, "start": 365.2, "end": 369.2, "text": " Czytasz to doskonale i to jest jeden z najwa\u017cniejszych wniosk\u00f3w tej pracy.", "tokens": [50364, 19832, 83, 19601, 281, 4491, 18295, 1220, 741, 281, 3492, 12906, 710, 11212, 27111, 10402, 45021, 45368, 2717, 23849, 12573, 35591, 13, 50564], "temperature": 0.0, "avg_logprob": -0.06522628622995295, "compression_ratio": 1.4609665427509293, "no_speech_prob": 0.016114600002765656}, {"id": 99, "seek": 36520, "start": 369.2, "end": 373.2, "text": " To pokazuje, jak niezwykle efektywna jest ta metoda.", "tokens": [50564, 1407, 13010, 43317, 11, 4207, 33511, 9726, 14677, 31482, 916, 874, 86, 629, 3492, 1846, 1131, 13449, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06522628622995295, "compression_ratio": 1.4609665427509293, "no_speech_prob": 0.016114600002765656}, {"id": 100, "seek": 36520, "start": 373.2, "end": 376.2, "text": " To nie jest budowanie nowej rakiety od zera.", "tokens": [50764, 1407, 2838, 3492, 3265, 22028, 586, 40779, 35544, 4014, 3611, 710, 1663, 13, 50914], "temperature": 0.0, "avg_logprob": -0.06522628622995295, "compression_ratio": 1.4609665427509293, "no_speech_prob": 0.016114600002765656}, {"id": 101, "seek": 36520, "start": 376.2, "end": 377.2, "text": " W\u0142a\u015bnie.", "tokens": [50914, 343, 5024, 12221, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06522628622995295, "compression_ratio": 1.4609665427509293, "no_speech_prob": 0.016114600002765656}, {"id": 102, "seek": 36520, "start": 377.2, "end": 381.2, "text": " To jest wzi\u0119cie ju\u017c istniej\u0105cej, pot\u0119\u017cnej rakiety", "tokens": [50964, 1407, 3492, 261, 16706, 4260, 10678, 1418, 2766, 8555, 20811, 11, 1847, 1274, 1427, 11794, 35544, 4014, 51164], "temperature": 0.0, "avg_logprob": -0.06522628622995295, "compression_ratio": 1.4609665427509293, "no_speech_prob": 0.016114600002765656}, {"id": 103, "seek": 36520, "start": 381.2, "end": 387.2, "text": " i za u\u0142amet koszt\u00f3w w granie jej nowego oprogramowania do systemu naprowadzania.", "tokens": [51164, 741, 7949, 344, 20177, 302, 19532, 2682, 3901, 261, 677, 7155, 28924, 586, 6308, 999, 340, 1342, 21308, 360, 1185, 84, 9296, 1892, 345, 89, 5609, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06522628622995295, "compression_ratio": 1.4609665427509293, "no_speech_prob": 0.016114600002765656}, {"id": 104, "seek": 36520, "start": 387.2, "end": 391.2, "text": " Kt\u00f3re czyni j\u0105 orz\u0119dy wielko\u015bci bardziej precyzyjn\u0105?", "tokens": [51464, 591, 4547, 265, 6430, 3722, 35692, 420, 11052, 3173, 20570, 4093, 6199, 27209, 659, 1344, 1229, 73, 13113, 30, 51664], "temperature": 0.0, "avg_logprob": -0.06522628622995295, "compression_ratio": 1.4609665427509293, "no_speech_prob": 0.016114600002765656}, {"id": 105, "seek": 36520, "start": 391.2, "end": 392.2, "text": " Dok\u0142adnie.", "tokens": [51664, 29768, 10358, 2766, 13, 51714], "temperature": 0.0, "avg_logprob": -0.06522628622995295, "compression_ratio": 1.4609665427509293, "no_speech_prob": 0.016114600002765656}, {"id": 106, "seek": 39220, "start": 392.2, "end": 396.2, "text": " W gigantycznym zwr\u00f3cie z inwestycji, je\u015bli chodzi o zasoby obliczeniowe.", "tokens": [50364, 343, 8741, 394, 17466, 12996, 49111, 812, 4260, 710, 294, 86, 7819, 19649, 11, 25630, 23998, 277, 26530, 13944, 1111, 1050, 42124, 6880, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07529755880141399, "compression_ratio": 1.48159509202454, "no_speech_prob": 0.14788004755973816}, {"id": 107, "seek": 39220, "start": 396.2, "end": 398.2, "text": " To kompletnie zmienia posta\u0107 rzeczy,", "tokens": [50564, 1407, 5207, 14657, 2766, 17020, 18811, 2183, 43379, 26297, 11, 50664], "temperature": 0.0, "avg_logprob": -0.07529755880141399, "compression_ratio": 1.48159509202454, "no_speech_prob": 0.14788004755973816}, {"id": 108, "seek": 39220, "start": 398.2, "end": 403.2, "text": " bo oznacza, \u017ce poprawa jako\u015bci AI nie musi oznacza\u0107 wyk\u0142ad niczego wzrostu koszt\u00f3w.", "tokens": [50664, 748, 277, 22672, 326, 2394, 11, 3561, 1665, 424, 4151, 17123, 6199, 7318, 2838, 37587, 277, 22672, 326, 35873, 4628, 15317, 6201, 27725, 24809, 27494, 84, 19532, 2682, 3901, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07529755880141399, "compression_ratio": 1.48159509202454, "no_speech_prob": 0.14788004755973816}, {"id": 109, "seek": 39220, "start": 403.2, "end": 406.2, "text": " Mo\u017cemy pracowa\u0107 m\u0105drzej, a nie ci\u0119\u017cej.", "tokens": [50914, 44736, 3633, 22404, 11445, 275, 18962, 13503, 73, 11, 257, 2838, 35484, 38493, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07529755880141399, "compression_ratio": 1.48159509202454, "no_speech_prob": 0.14788004755973816}, {"id": 110, "seek": 39220, "start": 406.2, "end": 407.2, "text": " No dobrze.", "tokens": [51064, 883, 28335, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07529755880141399, "compression_ratio": 1.48159509202454, "no_speech_prob": 0.14788004755973816}, {"id": 111, "seek": 39220, "start": 407.2, "end": 410.2, "text": " Mamy wi\u0119c do czynienia z niezwykle wydajn\u0105 metod\u0105.", "tokens": [51114, 376, 7804, 16677, 360, 6430, 77, 18811, 710, 33511, 9726, 14677, 25984, 1805, 13113, 1131, 378, 1611, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07529755880141399, "compression_ratio": 1.48159509202454, "no_speech_prob": 0.14788004755973816}, {"id": 112, "seek": 39220, "start": 410.2, "end": 414.2, "text": " To imponuj\u0105ce, ale sama wydajno\u015b\u0107 to nie wszystko.", "tokens": [51264, 1407, 704, 266, 13263, 384, 11, 6775, 17768, 25984, 1805, 23293, 281, 2838, 22607, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07529755880141399, "compression_ratio": 1.48159509202454, "no_speech_prob": 0.14788004755973816}, {"id": 113, "seek": 39220, "start": 414.2, "end": 417.2, "text": " Najwa\u017cniejsze jest, czy to faktycznie dzia\u0142a.", "tokens": [51464, 31576, 27111, 44258, 3492, 11, 6430, 281, 33647, 45586, 37903, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07529755880141399, "compression_ratio": 1.48159509202454, "no_speech_prob": 0.14788004755973816}, {"id": 114, "seek": 39220, "start": 417.2, "end": 420.2, "text": " Czy te dostrojone modele naprawd\u0119 sta\u0142y si\u0119 m\u0105drzejsze?", "tokens": [51614, 19832, 535, 20568, 340, 73, 546, 4391, 306, 20970, 11135, 6825, 3244, 275, 18962, 13503, 25530, 1381, 30, 51764], "temperature": 0.0, "avg_logprob": -0.07529755880141399, "compression_ratio": 1.48159509202454, "no_speech_prob": 0.14788004755973816}, {"id": 115, "seek": 39220, "start": 420.2, "end": 421.2, "text": " W\u0142a\u015bnie.", "tokens": [51764, 343, 5024, 12221, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07529755880141399, "compression_ratio": 1.48159509202454, "no_speech_prob": 0.14788004755973816}, {"id": 116, "seek": 42120, "start": 421.2, "end": 423.2, "text": " No i wyniki s\u0105 jednoznaczne.", "tokens": [50364, 883, 741, 31936, 9850, 9015, 5232, 1771, 22672, 14875, 716, 13, 50464], "temperature": 0.0, "avg_logprob": -0.0651235647604499, "compression_ratio": 1.424561403508772, "no_speech_prob": 0.009311173111200333}, {"id": 117, "seek": 42120, "start": 423.2, "end": 427.2, "text": " Po pierwsze, potwierdzi\u0142a si\u0119 og\u00f3lna zasada skalowania.", "tokens": [50464, 6165, 45994, 11, 1847, 40717, 67, 3992, 5024, 3244, 5360, 15741, 629, 26530, 1538, 16890, 21308, 13, 50664], "temperature": 0.0, "avg_logprob": -0.0651235647604499, "compression_ratio": 1.424561403508772, "no_speech_prob": 0.009311173111200333}, {"id": 118, "seek": 42120, "start": 427.2, "end": 432.2, "text": " Im wi\u0119kszy model i im wi\u0119cej zada\u0144 w procesie fine tuning, tym lepsze wyniki.", "tokens": [50664, 4331, 29968, 1229, 2316, 741, 566, 26004, 710, 1538, 5248, 261, 17565, 414, 2489, 15164, 11, 8107, 476, 1878, 1381, 31936, 9850, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0651235647604499, "compression_ratio": 1.424561403508772, "no_speech_prob": 0.009311173111200333}, {"id": 119, "seek": 42120, "start": 432.2, "end": 433.2, "text": " To by\u0142o do przewidzenia.", "tokens": [50914, 1407, 14811, 360, 39758, 327, 14320, 13, 50964], "temperature": 0.0, "avg_logprob": -0.0651235647604499, "compression_ratio": 1.424561403508772, "no_speech_prob": 0.009311173111200333}, {"id": 120, "seek": 42120, "start": 433.2, "end": 434.2, "text": " Ale.", "tokens": [50964, 9366, 13, 51014], "temperature": 0.0, "avg_logprob": -0.0651235647604499, "compression_ratio": 1.424561403508772, "no_speech_prob": 0.009311173111200333}, {"id": 121, "seek": 42120, "start": 434.2, "end": 439.2, "text": " Ale wykresy, a konkretnie figure 4 pokazuj\u0105 co\u015b znacznie ciekawszego.", "tokens": [51014, 9366, 39287, 495, 88, 11, 257, 36500, 2766, 2573, 1017, 13010, 921, 13263, 19241, 15397, 14875, 2766, 46419, 1607, 15453, 6308, 13, 51264], "temperature": 0.0, "avg_logprob": -0.0651235647604499, "compression_ratio": 1.424561403508772, "no_speech_prob": 0.009311173111200333}, {"id": 122, "seek": 42120, "start": 439.2, "end": 441.2, "text": " Co\u015b bardziej subtelnego.", "tokens": [51264, 3066, 1788, 27209, 7257, 338, 11858, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0651235647604499, "compression_ratio": 1.424561403508772, "no_speech_prob": 0.009311173111200333}, {"id": 123, "seek": 42120, "start": 441.2, "end": 442.2, "text": " Mianowicie.", "tokens": [51364, 376, 952, 305, 28434, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0651235647604499, "compression_ratio": 1.424561403508772, "no_speech_prob": 0.009311173111200333}, {"id": 124, "seek": 42120, "start": 442.2, "end": 450.2, "text": " Okazuje si\u0119, \u017ce najwi\u0119kszy skok jako\u015bciowy nast\u0119puje do momentu u\u017cycia oko\u0142o 282 zada\u0144.", "tokens": [51414, 3477, 43317, 3244, 11, 3561, 48636, 1694, 1229, 1110, 453, 17123, 6199, 10089, 39662, 13008, 360, 1623, 84, 34097, 2755, 45730, 5249, 7562, 17, 710, 1538, 5248, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0651235647604499, "compression_ratio": 1.424561403508772, "no_speech_prob": 0.009311173111200333}, {"id": 125, "seek": 45020, "start": 450.2, "end": 456.2, "text": " P\u00f3\u017aniej dok\u0142adanie kolejnych setek, nawet tysi\u0119cy zada\u0144, wci\u0105\u017c poprawia wyniki, ale.", "tokens": [50364, 430, 812, 33405, 45864, 7155, 23749, 9399, 992, 916, 11, 22696, 38156, 47303, 710, 1538, 5248, 11, 261, 537, 27242, 1665, 5131, 654, 31936, 9850, 11, 6775, 13, 50664], "temperature": 0.0, "avg_logprob": -0.05656967844281878, "compression_ratio": 1.4598765432098766, "no_speech_prob": 0.005589563399553299}, {"id": 126, "seek": 45020, "start": 456.2, "end": 458.2, "text": " Ale przyrosty s\u0105 ju\u017c znacznie mniejsze.", "tokens": [50664, 9366, 6501, 27494, 88, 9015, 10678, 15397, 14875, 2766, 275, 44258, 13, 50764], "temperature": 0.0, "avg_logprob": -0.05656967844281878, "compression_ratio": 1.4598765432098766, "no_speech_prob": 0.005589563399553299}, {"id": 127, "seek": 45020, "start": 458.2, "end": 459.2, "text": " Tak.", "tokens": [50764, 9118, 13, 50814], "temperature": 0.0, "avg_logprob": -0.05656967844281878, "compression_ratio": 1.4598765432098766, "no_speech_prob": 0.005589563399553299}, {"id": 128, "seek": 45020, "start": 459.2, "end": 460.2, "text": " Bardziej stopniowe.", "tokens": [50814, 26841, 19554, 1590, 3722, 6880, 13, 50864], "temperature": 0.0, "avg_logprob": -0.05656967844281878, "compression_ratio": 1.4598765432098766, "no_speech_prob": 0.005589563399553299}, {"id": 129, "seek": 45020, "start": 460.2, "end": 461.2, "text": " To fascynuj\u0105ce.", "tokens": [50864, 1407, 30632, 1344, 77, 13263, 384, 13, 50914], "temperature": 0.0, "avg_logprob": -0.05656967844281878, "compression_ratio": 1.4598765432098766, "no_speech_prob": 0.005589563399553299}, {"id": 130, "seek": 45020, "start": 461.2, "end": 462.2, "text": " Co to w\u0142a\u015bciwie oznacza?", "tokens": [50914, 3066, 281, 50108, 277, 22672, 326, 2394, 30, 50964], "temperature": 0.0, "avg_logprob": -0.05656967844281878, "compression_ratio": 1.4598765432098766, "no_speech_prob": 0.005589563399553299}, {"id": 131, "seek": 45020, "start": 462.2, "end": 465.2, "text": " \u017be model nauczy\u0142 si\u0119 ju\u017c wszystkiego, co mia\u0142 si\u0119 nauczy\u0107?", "tokens": [50964, 46864, 2316, 49103, 1229, 1221, 3244, 10678, 14615, 12200, 11, 598, 27989, 3244, 49103, 27150, 30, 51114], "temperature": 0.0, "avg_logprob": -0.05656967844281878, "compression_ratio": 1.4598765432098766, "no_speech_prob": 0.005589563399553299}, {"id": 132, "seek": 45020, "start": 465.2, "end": 467.2, "text": " To sugeruje co\u015b g\u0142\u0119bszego.", "tokens": [51114, 1407, 459, 1321, 13008, 19241, 18117, 1274, 929, 27725, 13, 51214], "temperature": 0.0, "avg_logprob": -0.05656967844281878, "compression_ratio": 1.4598765432098766, "no_speech_prob": 0.005589563399553299}, {"id": 133, "seek": 45020, "start": 467.2, "end": 474.2, "text": " Model nie tyle uczy si\u0119 nowych fakt\u00f3w, bo wi\u0119kszo\u015b\u0107 wiedzy ju\u017c posiad\u0142 podczas tego masowego, wst\u0119pnego treningu.", "tokens": [51214, 17105, 2838, 39293, 344, 6522, 3244, 586, 16384, 21310, 3901, 11, 748, 29968, 4765, 7753, 46894, 1229, 10678, 1366, 38069, 1221, 2497, 30989, 8627, 2300, 26576, 11, 261, 372, 18085, 11858, 2192, 773, 84, 13, 51564], "temperature": 0.0, "avg_logprob": -0.05656967844281878, "compression_ratio": 1.4598765432098766, "no_speech_prob": 0.005589563399553299}, {"id": 134, "seek": 45020, "start": 474.2, "end": 475.2, "text": " Czyli pre-training?", "tokens": [51564, 37099, 659, 12, 17227, 1760, 30, 51614], "temperature": 0.0, "avg_logprob": -0.05656967844281878, "compression_ratio": 1.4598765432098766, "no_speech_prob": 0.005589563399553299}, {"id": 135, "seek": 45020, "start": 475.2, "end": 476.2, "text": " Tak.", "tokens": [51614, 9118, 13, 51664], "temperature": 0.0, "avg_logprob": -0.05656967844281878, "compression_ratio": 1.4598765432098766, "no_speech_prob": 0.005589563399553299}, {"id": 136, "seek": 45020, "start": 476.2, "end": 478.2, "text": " On raczej uczy si\u0119 wzorca.", "tokens": [51664, 1282, 4129, 16920, 344, 6522, 3244, 24809, 284, 496, 13, 51764], "temperature": 0.0, "avg_logprob": -0.05656967844281878, "compression_ratio": 1.4598765432098766, "no_speech_prob": 0.005589563399553299}, {"id": 137, "seek": 47820, "start": 478.2, "end": 482.2, "text": " Uczy si\u0119 generalizowa\u0107, jak post\u0119powa\u0107 zgodnie z instrukcj\u0105.", "tokens": [50364, 624, 6522, 3244, 2674, 590, 11445, 11, 4207, 2183, 18085, 11445, 710, 21787, 2766, 710, 1058, 25126, 66, 8555, 13, 50564], "temperature": 0.0, "avg_logprob": -0.06113535398012632, "compression_ratio": 1.4684385382059801, "no_speech_prob": 0.032307907938957214}, {"id": 138, "seek": 47820, "start": 482.2, "end": 486.2, "text": " To tak, jakby\u015bmy dali komu\u015b do przeczytania ca\u0142\u0105 bibliotek\u0119.", "tokens": [50564, 1407, 991, 11, 28976, 10513, 274, 5103, 5207, 84, 1788, 360, 8325, 6522, 83, 5609, 1335, 15926, 34344, 310, 916, 1274, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06113535398012632, "compression_ratio": 1.4684385382059801, "no_speech_prob": 0.032307907938957214}, {"id": 139, "seek": 47820, "start": 486.2, "end": 487.2, "text": " To jest pre-training.", "tokens": [50764, 1407, 3492, 659, 12, 17227, 1760, 13, 50814], "temperature": 0.0, "avg_logprob": -0.06113535398012632, "compression_ratio": 1.4684385382059801, "no_speech_prob": 0.032307907938957214}, {"id": 140, "seek": 47820, "start": 487.2, "end": 488.2, "text": " Dok\u0142adnie.", "tokens": [50814, 29768, 10358, 2766, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06113535398012632, "compression_ratio": 1.4684385382059801, "no_speech_prob": 0.032307907938957214}, {"id": 141, "seek": 47820, "start": 488.2, "end": 497.2, "text": " A potem przeprowadzili z nim seri\u0119 \u0107wicze\u0144, kt\u00f3re ucz\u0105 go, jak szybko i trafnie znajdowa\u0107 w tej bibliotece odpowiedzi na konkretne pytania.", "tokens": [50864, 316, 36513, 30829, 1892, 345, 89, 2312, 710, 24887, 816, 5034, 45854, 22295, 49689, 11, 8864, 35403, 1611, 352, 11, 4207, 36456, 4093, 741, 944, 69, 2766, 27318, 67, 11445, 261, 12573, 34344, 1370, 384, 36574, 3992, 1667, 36500, 716, 25878, 5609, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06113535398012632, "compression_ratio": 1.4684385382059801, "no_speech_prob": 0.032307907938957214}, {"id": 142, "seek": 47820, "start": 497.2, "end": 500.2, "text": " I to jest Instruction Find Uning.", "tokens": [51314, 286, 281, 3492, 2730, 3826, 11809, 1156, 278, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06113535398012632, "compression_ratio": 1.4684385382059801, "no_speech_prob": 0.032307907938957214}, {"id": 143, "seek": 47820, "start": 500.2, "end": 501.2, "text": " Rozumiem.", "tokens": [51464, 43313, 449, 4907, 13, 51514], "temperature": 0.0, "avg_logprob": -0.06113535398012632, "compression_ratio": 1.4684385382059801, "no_speech_prob": 0.032307907938957214}, {"id": 144, "seek": 47820, "start": 501.2, "end": 505.2, "text": " Po pewnym czasie on ju\u017c \u0142apie, na czym polega proces szukania odpowiedzi.", "tokens": [51514, 6165, 47160, 4199, 42667, 322, 10678, 25387, 569, 414, 11, 1667, 31466, 13208, 3680, 17565, 7870, 2034, 5609, 36574, 3992, 13, 51714], "temperature": 0.0, "avg_logprob": -0.06113535398012632, "compression_ratio": 1.4684385382059801, "no_speech_prob": 0.032307907938957214}, {"id": 145, "seek": 47820, "start": 505.2, "end": 506.2, "text": " W\u0142a\u015bnie.", "tokens": [51714, 343, 5024, 12221, 13, 51764], "temperature": 0.0, "avg_logprob": -0.06113535398012632, "compression_ratio": 1.4684385382059801, "no_speech_prob": 0.032307907938957214}, {"id": 146, "seek": 50620, "start": 506.2, "end": 510.2, "text": " I kolejne przyk\u0142ady tylko nieznacznie go w tym doskonalaj\u0105.", "tokens": [50364, 286, 23749, 716, 6501, 74, 1221, 880, 13219, 2838, 22672, 14875, 2766, 352, 261, 8107, 4491, 18295, 304, 11133, 13, 50564], "temperature": 0.0, "avg_logprob": -0.054613925315238336, "compression_ratio": 1.437062937062937, "no_speech_prob": 0.01571023277938366}, {"id": 147, "seek": 50620, "start": 510.2, "end": 515.2, "text": " Ok, czyli nie chodzi o ilo\u015b\u0107 wiedzy, ale o umiej\u0119tno\u015b\u0107 jej wykorzystania.", "tokens": [50564, 3477, 11, 16591, 2838, 23998, 277, 1930, 78, 7753, 46894, 1229, 11, 6775, 277, 1105, 7764, 46788, 23293, 28924, 43606, 36049, 5609, 13, 50814], "temperature": 0.0, "avg_logprob": -0.054613925315238336, "compression_ratio": 1.437062937062937, "no_speech_prob": 0.01571023277938366}, {"id": 148, "seek": 50620, "start": 515.2, "end": 521.2, "text": " Ale wspomnia\u0142a\u015b wcze\u015bniej, \u017ce w tej mieszance zada\u0144 by\u0142 jeden, jak si\u0119 okaza\u0142o, magiczny sk\u0142adnik.", "tokens": [50814, 9366, 17757, 45438, 5024, 1788, 40785, 11, 3561, 261, 12573, 33039, 719, 710, 1538, 5248, 16673, 12906, 11, 4207, 3244, 3133, 12257, 5249, 11, 5585, 89, 1634, 1110, 10358, 13123, 13, 51114], "temperature": 0.0, "avg_logprob": -0.054613925315238336, "compression_ratio": 1.437062937062937, "no_speech_prob": 0.01571023277938366}, {"id": 149, "seek": 50620, "start": 521.2, "end": 523.2, "text": " Co\u015b, co si\u0119 nazywa Chain of Thot.", "tokens": [51114, 3066, 1788, 11, 598, 3244, 20151, 88, 4151, 33252, 295, 334, 310, 13, 51214], "temperature": 0.0, "avg_logprob": -0.054613925315238336, "compression_ratio": 1.437062937062937, "no_speech_prob": 0.01571023277938366}, {"id": 150, "seek": 50620, "start": 523.2, "end": 524.2, "text": " Tak.", "tokens": [51214, 9118, 13, 51264], "temperature": 0.0, "avg_logprob": -0.054613925315238336, "compression_ratio": 1.437062937062937, "no_speech_prob": 0.01571023277938366}, {"id": 151, "seek": 50620, "start": 524.2, "end": 526.2, "text": " I to jest absolutnie kluczowe.", "tokens": [51264, 286, 281, 3492, 18757, 2766, 9671, 1311, 89, 6880, 13, 51364], "temperature": 0.0, "avg_logprob": -0.054613925315238336, "compression_ratio": 1.437062937062937, "no_speech_prob": 0.01571023277938366}, {"id": 152, "seek": 50620, "start": 526.2, "end": 529.2, "text": " To chyba najwa\u017cniejsze odkrycie tej pracy.", "tokens": [51364, 1407, 31532, 11212, 27111, 44258, 3611, 43298, 4260, 12573, 35591, 13, 51514], "temperature": 0.0, "avg_logprob": -0.054613925315238336, "compression_ratio": 1.437062937062937, "no_speech_prob": 0.01571023277938366}, {"id": 153, "seek": 50620, "start": 529.2, "end": 531.2, "text": " Czym jest ten Chain of Thot?", "tokens": [51514, 19832, 76, 3492, 2064, 33252, 295, 334, 310, 30, 51614], "temperature": 0.0, "avg_logprob": -0.054613925315238336, "compression_ratio": 1.437062937062937, "no_speech_prob": 0.01571023277938366}, {"id": 154, "seek": 50620, "start": 531.2, "end": 532.2, "text": " Brzmi tajemniczo.", "tokens": [51614, 1603, 89, 3057, 256, 1805, 443, 7692, 4765, 13, 51664], "temperature": 0.0, "avg_logprob": -0.054613925315238336, "compression_ratio": 1.437062937062937, "no_speech_prob": 0.01571023277938366}, {"id": 155, "seek": 53220, "start": 532.2, "end": 536.2, "text": " A w zasadzie to jest uczenie modelu, by my\u015bla\u0142 na g\u0142os.", "tokens": [50364, 316, 261, 44585, 3283, 281, 3492, 344, 39043, 2316, 84, 11, 538, 48633, 875, 1221, 1667, 43767, 13, 50564], "temperature": 0.0, "avg_logprob": -0.05173836435590472, "compression_ratio": 1.5342465753424657, "no_speech_prob": 0.06731046736240387}, {"id": 156, "seek": 53220, "start": 536.2, "end": 537.2, "text": " Czyli?", "tokens": [50564, 37099, 30, 50614], "temperature": 0.0, "avg_logprob": -0.05173836435590472, "compression_ratio": 1.5342465753424657, "no_speech_prob": 0.06731046736240387}, {"id": 157, "seek": 53220, "start": 537.2, "end": 545.2, "text": " Zamiast od razu podawa\u0107 finaln\u0105 odpowied\u017a na z\u0142o\u017cony problem, model jest trenowany na przyk\u0142adach, kt\u00f3re pokazuj\u0105 ca\u0142y proces my\u015blowy.", "tokens": [50614, 1176, 4526, 525, 3611, 367, 8813, 2497, 10449, 2162, 2572, 13113, 36574, 10659, 1667, 710, 5249, 1427, 2526, 1154, 11, 2316, 3492, 23136, 23341, 1667, 23144, 608, 11, 8864, 13010, 921, 13263, 35226, 17565, 452, 19212, 10089, 13, 51014], "temperature": 0.0, "avg_logprob": -0.05173836435590472, "compression_ratio": 1.5342465753424657, "no_speech_prob": 0.06731046736240387}, {"id": 158, "seek": 53220, "start": 545.2, "end": 548.2, "text": " Krok po kroku prowadz\u0105cy do rozwi\u0105zania.", "tokens": [51014, 591, 31621, 714, 45909, 5279, 36590, 8925, 1344, 360, 9544, 22620, 5609, 13, 51164], "temperature": 0.0, "avg_logprob": -0.05173836435590472, "compression_ratio": 1.5342465753424657, "no_speech_prob": 0.06731046736240387}, {"id": 159, "seek": 53220, "start": 548.2, "end": 555.2, "text": " Czyli zamiast uczy\u0107 go tylko pary, pytanie, odpowied\u017a, uczymy go samego procesu dochodzenia do odpowiedzi.", "tokens": [51164, 37099, 710, 4526, 525, 344, 33967, 352, 13219, 280, 822, 11, 36610, 11, 36574, 10659, 11, 344, 6522, 2226, 352, 912, 1571, 17565, 84, 9243, 378, 14320, 360, 36574, 3992, 13, 51514], "temperature": 0.0, "avg_logprob": -0.05173836435590472, "compression_ratio": 1.5342465753424657, "no_speech_prob": 0.06731046736240387}, {"id": 160, "seek": 53220, "start": 555.2, "end": 557.2, "text": " Masz jaki\u015b konkretny przyk\u0142ad?", "tokens": [51514, 5224, 89, 34721, 36500, 1634, 23144, 30, 51614], "temperature": 0.0, "avg_logprob": -0.05173836435590472, "compression_ratio": 1.5342465753424657, "no_speech_prob": 0.06731046736240387}, {"id": 161, "seek": 53220, "start": 557.2, "end": 561.2, "text": " W pracy jest \u015bwietny, prosty przyk\u0142ad z jab\u0142kami.", "tokens": [51614, 343, 35591, 3492, 8299, 39083, 1634, 11, 10293, 88, 23144, 710, 33475, 1221, 48737, 13, 51814], "temperature": 0.0, "avg_logprob": -0.05173836435590472, "compression_ratio": 1.5342465753424657, "no_speech_prob": 0.06731046736240387}, {"id": 162, "seek": 56120, "start": 561.2, "end": 562.2, "text": " Pytanie.", "tokens": [50364, 430, 4328, 7155, 13, 50414], "temperature": 0.0, "avg_logprob": -0.099057473427008, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.01941060647368431}, {"id": 163, "seek": 56120, "start": 562.2, "end": 568.2, "text": " W sto\u0142\u00f3wce by\u0142y 23 jab\u0142ka, u\u017cyto 20 do lanczu i dokupiono 6.", "tokens": [50414, 343, 22784, 1221, 3901, 384, 26366, 6673, 33475, 1221, 2330, 11, 34097, 1353, 945, 360, 287, 4463, 11728, 741, 25037, 1010, 49020, 1386, 13, 50714], "temperature": 0.0, "avg_logprob": -0.099057473427008, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.01941060647368431}, {"id": 164, "seek": 56120, "start": 568.2, "end": 570.2, "text": " Ile jab\u0142k jest teraz?", "tokens": [50714, 286, 306, 33475, 1221, 74, 3492, 16854, 30, 50814], "temperature": 0.0, "avg_logprob": -0.099057473427008, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.01941060647368431}, {"id": 165, "seek": 56120, "start": 570.2, "end": 571.2, "text": " OK.", "tokens": [50814, 2264, 13, 50864], "temperature": 0.0, "avg_logprob": -0.099057473427008, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.01941060647368431}, {"id": 166, "seek": 56120, "start": 571.2, "end": 575.2, "text": " Model bez treningu COT mog\u0142oby odpowiedzie\u0107 po prostu 9.", "tokens": [50864, 17105, 10782, 2192, 773, 84, 3002, 51, 13172, 1221, 13944, 24314, 22078, 714, 19518, 1722, 13, 51064], "temperature": 0.0, "avg_logprob": -0.099057473427008, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.01941060647368431}, {"id": 167, "seek": 56120, "start": 575.2, "end": 579.2, "text": " Model uczony w trybie Chain of Thot generuje co\u015b takiego.", "tokens": [51064, 17105, 35403, 2526, 261, 853, 7392, 33252, 295, 334, 310, 1337, 13008, 19241, 32296, 13, 51264], "temperature": 0.0, "avg_logprob": -0.099057473427008, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.01941060647368431}, {"id": 168, "seek": 56120, "start": 579.2, "end": 585.2, "text": " Na pocz\u0105tku by\u0142y 23 jab\u0142ka, u\u017cyto 20, wi\u0119c zosta\u0142o 23, 20 r\u00f3wna si\u0119 3.", "tokens": [51264, 6056, 43959, 26366, 6673, 33475, 1221, 2330, 11, 34097, 1353, 945, 11, 16677, 23154, 5249, 6673, 11, 945, 367, 3901, 629, 3244, 805, 13, 51564], "temperature": 0.0, "avg_logprob": -0.099057473427008, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.01941060647368431}, {"id": 169, "seek": 56120, "start": 585.2, "end": 587.2, "text": " Pokazuje obliczenia?", "tokens": [51564, 14958, 43317, 1111, 1050, 14320, 30, 51664], "temperature": 0.0, "avg_logprob": -0.099057473427008, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.01941060647368431}, {"id": 170, "seek": 56120, "start": 587.2, "end": 588.2, "text": " Tak.", "tokens": [51664, 9118, 13, 51714], "temperature": 0.0, "avg_logprob": -0.099057473427008, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.01941060647368431}, {"id": 171, "seek": 58820, "start": 588.2, "end": 593.2, "text": " Dokupiono 6 jab\u0142ek, wi\u0119c teraz jest 3 plus 6 r\u00f3wna si\u0119 9.", "tokens": [50364, 29768, 1010, 49020, 1386, 33475, 1221, 916, 11, 16677, 16854, 3492, 805, 1804, 1386, 367, 3901, 629, 3244, 1722, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07984310432716653, "compression_ratio": 1.3840579710144927, "no_speech_prob": 0.051587365567684174}, {"id": 172, "seek": 58820, "start": 593.2, "end": 595.2, "text": " Ostateczna odpowied\u017a to 9.", "tokens": [50614, 422, 15406, 3689, 629, 36574, 10659, 281, 1722, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07984310432716653, "compression_ratio": 1.3840579710144927, "no_speech_prob": 0.051587365567684174}, {"id": 173, "seek": 58820, "start": 595.2, "end": 598.2, "text": " On pokazuje ca\u0142\u0105 \u015bcie\u017ck\u0119 rozumowania.", "tokens": [50714, 1282, 13010, 43317, 1335, 15926, 8299, 40082, 15724, 48797, 21308, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07984310432716653, "compression_ratio": 1.3840579710144927, "no_speech_prob": 0.051587365567684174}, {"id": 174, "seek": 58820, "start": 598.2, "end": 599.2, "text": " Rozumiem.", "tokens": [50864, 43313, 449, 4907, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07984310432716653, "compression_ratio": 1.3840579710144927, "no_speech_prob": 0.051587365567684174}, {"id": 175, "seek": 58820, "start": 599.2, "end": 606.2, "text": " To wydaje si\u0119 logiczne, \u017ce uczenie modelu w ten spos\u00f3b poprawi jego zdolno\u015bci w zadaniach, nie wiem, matematycznych czy logicznych.", "tokens": [50914, 1407, 49165, 3244, 9952, 43077, 11, 3561, 344, 39043, 2316, 84, 261, 2064, 22904, 1665, 5131, 72, 26542, 16221, 401, 16438, 261, 42788, 3782, 608, 11, 2838, 26522, 11, 3803, 8615, 17466, 9399, 6430, 9952, 89, 9399, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07984310432716653, "compression_ratio": 1.3840579710144927, "no_speech_prob": 0.051587365567684174}, {"id": 176, "seek": 58820, "start": 606.2, "end": 607.2, "text": " Ale.", "tokens": [51264, 9366, 13, 51314], "temperature": 0.0, "avg_logprob": -0.07984310432716653, "compression_ratio": 1.3840579710144927, "no_speech_prob": 0.051587365567684174}, {"id": 177, "seek": 58820, "start": 607.2, "end": 608.2, "text": " Ale.", "tokens": [51314, 9366, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07984310432716653, "compression_ratio": 1.3840579710144927, "no_speech_prob": 0.051587365567684174}, {"id": 178, "seek": 58820, "start": 608.2, "end": 612.2, "text": " Tutaj dochodzimy do czego\u015b, co autorzy opisuj\u0105 jako absolutnie nieintuicyjne odkrycie.", "tokens": [51364, 41819, 9243, 378, 89, 13189, 360, 36559, 1788, 11, 598, 19510, 1229, 45477, 13263, 17123, 18757, 2766, 2838, 686, 84, 2632, 73, 716, 3611, 43298, 4260, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07984310432716653, "compression_ratio": 1.3840579710144927, "no_speech_prob": 0.051587365567684174}, {"id": 179, "seek": 58820, "start": 612.2, "end": 613.2, "text": " Tak.", "tokens": [51564, 9118, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07984310432716653, "compression_ratio": 1.3840579710144927, "no_speech_prob": 0.051587365567684174}, {"id": 180, "seek": 61320, "start": 613.2, "end": 622.2, "text": " Prawdzili, co si\u0119 stanie, je\u015bli przeprowadz\u0105 Instruction Fine Tuning na tej ogromnej lidbie zada\u0144, ale celowo pomin\u0105 te przyk\u0142ady Chain of Thot.", "tokens": [50364, 430, 15889, 89, 2312, 11, 598, 3244, 40013, 11, 25630, 30829, 1892, 345, 8925, 2730, 3826, 12024, 21363, 278, 1667, 12573, 34416, 298, 11794, 10252, 7392, 710, 1538, 5248, 11, 6775, 9277, 19941, 280, 6981, 1611, 535, 6501, 74, 1221, 880, 33252, 295, 334, 310, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07747333011929951, "compression_ratio": 1.4119850187265917, "no_speech_prob": 0.136806458234787}, {"id": 181, "seek": 61320, "start": 622.2, "end": 626.2, "text": " I logika podpowiada\u0142aby, \u017ce model i tak stanie si\u0119 znacznie lepszy, prawda?", "tokens": [50814, 286, 3565, 5439, 2497, 14701, 39018, 1221, 2509, 11, 3561, 2316, 741, 991, 40013, 3244, 15397, 14875, 2766, 476, 1878, 1229, 11, 43607, 30, 51014], "temperature": 0.0, "avg_logprob": -0.07747333011929951, "compression_ratio": 1.4119850187265917, "no_speech_prob": 0.136806458234787}, {"id": 182, "seek": 61320, "start": 626.2, "end": 629.2, "text": " No tak, uczy si\u0119 na tysi\u0105cach innych przyk\u0142ad\u00f3w.", "tokens": [51014, 883, 991, 11, 344, 6522, 3244, 1667, 38156, 11404, 66, 608, 36286, 23144, 3901, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07747333011929951, "compression_ratio": 1.4119850187265917, "no_speech_prob": 0.136806458234787}, {"id": 183, "seek": 61320, "start": 629.2, "end": 634.2, "text": " A sta\u0142o si\u0119 co\u015b dok\u0142adnie odwrotnego, co\u015b, co zaniepokoi\u0142o badaczy.", "tokens": [51164, 316, 11135, 5249, 3244, 19241, 45864, 2766, 3611, 7449, 310, 11858, 11, 19241, 11, 598, 710, 7155, 79, 453, 4869, 5249, 1578, 14691, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07747333011929951, "compression_ratio": 1.4119850187265917, "no_speech_prob": 0.136806458234787}, {"id": 184, "seek": 61320, "start": 634.2, "end": 635.2, "text": " Czyli co konkretnie?", "tokens": [51414, 37099, 598, 36500, 2766, 30, 51464], "temperature": 0.0, "avg_logprob": -0.07747333011929951, "compression_ratio": 1.4119850187265917, "no_speech_prob": 0.136806458234787}, {"id": 185, "seek": 63520, "start": 635.2, "end": 644.2, "text": " Okaza\u0142o si\u0119, \u017ce trenowanie modelu wy\u0142\u0105cznie na standardowych zadaniach typu pytanie-odpowied\u017a pogarsza\u0142o jego wrodzone zdolno\u015bci do rozumowania.", "tokens": [50364, 3477, 12257, 5249, 3244, 11, 3561, 23136, 22028, 2316, 84, 4628, 15926, 19923, 1667, 3832, 19605, 42788, 3782, 608, 2125, 84, 36610, 12, 378, 14701, 1091, 10659, 32037, 685, 2394, 5249, 26542, 261, 11452, 16896, 16221, 401, 16438, 360, 48797, 21308, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07696095440122816, "compression_ratio": 1.4405144694533762, "no_speech_prob": 0.6410040855407715}, {"id": 186, "seek": 63520, "start": 644.2, "end": 645.2, "text": " Jak to pogarsza\u0142o?", "tokens": [50814, 15029, 281, 32037, 685, 2394, 5249, 30, 50864], "temperature": 0.0, "avg_logprob": -0.07696095440122816, "compression_ratio": 1.4405144694533762, "no_speech_prob": 0.6410040855407715}, {"id": 187, "seek": 63520, "start": 645.2, "end": 654.2, "text": " Na benchmarkach wymagaj\u0105cych wieloetapowego my\u015blenia model po takim treningu radzi\u0142 sobie gorzej ni\u017c model wyj\u015bciowy, ten przed Fine Tuning.", "tokens": [50864, 6056, 18927, 608, 29764, 559, 11133, 31306, 20570, 78, 302, 569, 26576, 48633, 6698, 654, 2316, 714, 31732, 2192, 773, 84, 2843, 3992, 1221, 13652, 24012, 16920, 28502, 2316, 4628, 73, 6199, 10089, 11, 2064, 18334, 12024, 21363, 278, 13, 51314], "temperature": 0.0, "avg_logprob": -0.07696095440122816, "compression_ratio": 1.4405144694533762, "no_speech_prob": 0.6410040855407715}, {"id": 188, "seek": 63520, "start": 654.2, "end": 659.2, "text": " Chwila, chwila, czyli pr\u00f3buj\u0105c go ulepszy\u0107 w pewnym sensie go og\u0142upili.", "tokens": [51314, 761, 86, 7371, 11, 26237, 7371, 11, 16591, 8565, 65, 44733, 352, 344, 306, 1878, 27150, 261, 47160, 4199, 2923, 414, 352, 5360, 1221, 1010, 2312, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07696095440122816, "compression_ratio": 1.4405144694533762, "no_speech_prob": 0.6410040855407715}, {"id": 189, "seek": 63520, "start": 659.2, "end": 662.2, "text": " Mo\u017cna tak powiedzie\u0107, to brzmi paradoksalnie, wiem.", "tokens": [51564, 44736, 629, 991, 27886, 11, 281, 738, 89, 3057, 13480, 453, 15142, 2766, 11, 26522, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07696095440122816, "compression_ratio": 1.4405144694533762, "no_speech_prob": 0.6410040855407715}, {"id": 190, "seek": 66220, "start": 662.2, "end": 668.2, "text": " To jakby ucze\u0144, kt\u00f3ry rozwi\u0105zuje w k\u00f3\u0142ko testy jednokrotnego wyboru, nagle traci\u0142 zdolno\u015b\u0107 pisania wypracowa\u0144.", "tokens": [50364, 1407, 28976, 344, 9680, 5248, 11, 9913, 9544, 22620, 13008, 261, 350, 16181, 4093, 1500, 88, 5232, 77, 453, 10536, 11858, 4628, 3918, 84, 11, 297, 15088, 504, 22086, 1221, 16221, 401, 23293, 26584, 5609, 4628, 1424, 326, 5528, 5248, 13, 50664], "temperature": 0.0, "avg_logprob": -0.061679054189611365, "compression_ratio": 1.3361702127659574, "no_speech_prob": 0.1260330080986023}, {"id": 191, "seek": 66220, "start": 668.2, "end": 679.2, "text": " To jest idealna analogia. Model optymalizowa\u0142 si\u0119 pod k\u0105tem prostych, szybkich odpowiedzi i niejako zapomina\u0142, jak my\u015ble\u0107 w spos\u00f3b z\u0142o\u017cony.", "tokens": [50664, 1407, 3492, 7157, 629, 16660, 654, 13, 17105, 2427, 4199, 304, 590, 30105, 3244, 2497, 350, 1611, 18275, 10293, 16384, 11, 36456, 48349, 36574, 3992, 741, 2838, 73, 18501, 14223, 49217, 1221, 11, 4207, 48633, 306, 2162, 261, 22904, 710, 5249, 1427, 2526, 13, 51214], "temperature": 0.0, "avg_logprob": -0.061679054189611365, "compression_ratio": 1.3361702127659574, "no_speech_prob": 0.1260330080986023}, {"id": 192, "seek": 66220, "start": 679.2, "end": 682.2, "text": " To krytyczna obserwacja dla ca\u0142ej dziedziny.", "tokens": [51214, 1407, 34847, 874, 3689, 629, 12887, 86, 23395, 12285, 47631, 73, 9758, 15338, 3519, 13, 51364], "temperature": 0.0, "avg_logprob": -0.061679054189611365, "compression_ratio": 1.3361702127659574, "no_speech_prob": 0.1260330080986023}, {"id": 193, "seek": 68220, "start": 683.2, "end": 692.2, "text": " Ogromnie. Pokazuje, \u017ce spos\u00f3b w jaki trenujemy modele ma ogromny wp\u0142yw na ich fundamentalne zdolno\u015bci poznawcze.", "tokens": [50414, 422, 861, 298, 2766, 13, 14958, 43317, 11, 3561, 22904, 261, 24492, 23136, 21767, 4391, 306, 463, 34416, 298, 1634, 32444, 6825, 86, 1667, 1893, 8088, 716, 16221, 401, 16438, 21281, 629, 86, 9680, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07958445378712245, "compression_ratio": 1.300395256916996, "no_speech_prob": 0.3285691440105438}, {"id": 194, "seek": 68220, "start": 692.2, "end": 697.2, "text": " Na szcz\u0119\u015bcie rozwi\u0105zanie tego problemu okaza\u0142o si\u0119 zaskakuj\u0105co proste.", "tokens": [50864, 6056, 22090, 1274, 9815, 9544, 22620, 7155, 8627, 1154, 84, 3133, 12257, 5249, 3244, 710, 3863, 514, 13263, 1291, 10293, 68, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07958445378712245, "compression_ratio": 1.300395256916996, "no_speech_prob": 0.3285691440105438}, {"id": 195, "seek": 68220, "start": 697.2, "end": 698.2, "text": " To znaczy?", "tokens": [51114, 1407, 36584, 30, 51164], "temperature": 0.0, "avg_logprob": -0.07958445378712245, "compression_ratio": 1.300395256916996, "no_speech_prob": 0.3285691440105438}, {"id": 196, "seek": 68220, "start": 698.2, "end": 710.2, "text": " Wystarczy\u0142o doda\u0107 zaledwie 9 zbior\u00f3w danych z anotacjami COT do ca\u0142ej licz\u0105cej ponad 1800 zada\u0144 mieszanki treningowej.", "tokens": [51164, 14458, 9710, 6522, 5249, 360, 2675, 2162, 710, 5573, 8699, 1722, 710, 33362, 3901, 274, 34644, 710, 364, 310, 326, 73, 4526, 3002, 51, 360, 47631, 73, 6169, 8925, 20811, 9224, 345, 24327, 710, 1538, 5248, 33039, 27203, 2192, 773, 21091, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07958445378712245, "compression_ratio": 1.300395256916996, "no_speech_prob": 0.3285691440105438}, {"id": 197, "seek": 71020, "start": 711.2, "end": 712.2, "text": " Tylko 9?", "tokens": [50414, 49286, 4093, 1722, 30, 50464], "temperature": 0.0, "avg_logprob": -0.07188016035425382, "compression_ratio": 1.408239700374532, "no_speech_prob": 0.0663072019815445}, {"id": 198, "seek": 71020, "start": 712.2, "end": 717.2, "text": " Tak. To by\u0142 procentowo niewielki dodatek, ale efekt by\u0142 kolosalny.", "tokens": [50464, 9118, 13, 1407, 16673, 38826, 19941, 43622, 1187, 2984, 13886, 473, 74, 11, 6775, 31482, 8192, 16673, 17818, 329, 304, 1634, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07188016035425382, "compression_ratio": 1.408239700374532, "no_speech_prob": 0.0663072019815445}, {"id": 199, "seek": 71020, "start": 717.2, "end": 725.2, "text": " Ten ma\u0142y dodatek nie tylko zatrzyma\u0142 t\u0119 detradacj\u0119 zdolno\u015bci rozumowania, ale znacz\u0105co poprawi\u0142 wyniki we wszystkich kategoriach zada\u0144.", "tokens": [50714, 9380, 463, 6825, 13886, 473, 74, 2838, 13219, 35802, 13047, 1696, 1221, 32489, 1141, 6206, 29924, 16221, 401, 16438, 48797, 21308, 11, 6775, 15397, 326, 8925, 1291, 1665, 5131, 40622, 31936, 9850, 321, 34234, 350, 2968, 7386, 608, 710, 1538, 5248, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07188016035425382, "compression_ratio": 1.408239700374532, "no_speech_prob": 0.0663072019815445}, {"id": 200, "seek": 71020, "start": 725.2, "end": 733.2, "text": " Niesamowite. Czyli te dane chain of thought dzia\u0142aj\u0105 jak jaka\u015b szczepionka przeciwko my\u015bleniu na skr\u00f3ty?", "tokens": [51114, 426, 530, 335, 305, 642, 13, 37099, 535, 49206, 5021, 295, 1194, 27121, 11133, 4207, 4207, 64, 1788, 22090, 595, 313, 2330, 39622, 86, 4093, 48633, 6698, 5951, 1667, 1110, 11721, 874, 30, 51514], "temperature": 0.0, "avg_logprob": -0.07188016035425382, "compression_ratio": 1.408239700374532, "no_speech_prob": 0.0663072019815445}, {"id": 201, "seek": 71020, "start": 733.2, "end": 735.2, "text": " Dok\u0142adnie. Ale to nie wszystko.", "tokens": [51514, 29768, 10358, 2766, 13, 9366, 281, 2838, 22607, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07188016035425382, "compression_ratio": 1.408239700374532, "no_speech_prob": 0.0663072019815445}, {"id": 202, "seek": 71020, "start": 735.2, "end": 736.2, "text": " Co jeszcze?", "tokens": [51614, 3066, 14168, 30, 51664], "temperature": 0.0, "avg_logprob": -0.07188016035425382, "compression_ratio": 1.408239700374532, "no_speech_prob": 0.0663072019815445}, {"id": 203, "seek": 73620, "start": 736.2, "end": 745.2, "text": " Ten trening odblokowa\u0142 co\u015b jeszcze. Co\u015b co jest cz\u0119sto postrzewane jako \u015bwi\u0119ty gra w interakcji z AI.", "tokens": [50364, 9380, 2192, 773, 3611, 5199, 453, 30105, 19241, 14168, 13, 3066, 1788, 598, 3492, 34369, 2183, 19390, 1023, 1929, 17123, 8299, 22423, 874, 1295, 261, 728, 514, 19649, 710, 7318, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10073041107694981, "compression_ratio": 1.3948339483394834, "no_speech_prob": 0.0761188268661499}, {"id": 204, "seek": 73620, "start": 745.2, "end": 747.2, "text": " Czyli rozumowanie zero shot?", "tokens": [50814, 37099, 48797, 22028, 4018, 3347, 30, 50914], "temperature": 0.0, "avg_logprob": -0.10073041107694981, "compression_ratio": 1.3948339483394834, "no_speech_prob": 0.0761188268661499}, {"id": 205, "seek": 73620, "start": 747.2, "end": 757.2, "text": " W\u0142a\u015bnie. To jest zdolno\u015b\u0107 do rozwi\u0105zania zupe\u0142nie nowego problemu bez wcze\u015bniejszego pokazania modelowi przyk\u0142adu jak to zrobi\u0107.", "tokens": [50914, 343, 5024, 12221, 13, 1407, 3492, 16221, 401, 23293, 360, 9544, 22620, 5609, 49922, 586, 6308, 1154, 84, 10782, 40785, 15453, 6308, 13010, 921, 5609, 2316, 24503, 23144, 84, 4207, 281, 31785, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10073041107694981, "compression_ratio": 1.3948339483394834, "no_speech_prob": 0.0761188268661499}, {"id": 206, "seek": 73620, "start": 757.2, "end": 762.2, "text": " Zwyk\u0142y palm, gdy dosta\u0142 w poleceniu prost\u0105 fraz\u0119 let's think step by step.", "tokens": [51414, 1176, 9726, 74, 6825, 3984, 76, 11, 28405, 274, 8638, 1221, 261, 13208, 13037, 5951, 10293, 1611, 6600, 11052, 718, 311, 519, 1823, 538, 1823, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10073041107694981, "compression_ratio": 1.3948339483394834, "no_speech_prob": 0.0761188268661499}, {"id": 207, "seek": 73620, "start": 762.2, "end": 764.2, "text": " Pomy\u015blmy krok po kroku.", "tokens": [51664, 430, 8488, 19212, 2226, 350, 31621, 714, 45909, 5279, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10073041107694981, "compression_ratio": 1.3948339483394834, "no_speech_prob": 0.0761188268661499}, {"id": 208, "seek": 76420, "start": 764.2, "end": 768.2, "text": " Kompletnie nie wiedzia\u0142 co z tym zrobi\u0107. Traktowa\u0142 to jak zwyk\u0142y tekst.", "tokens": [50364, 14286, 14657, 2766, 2838, 261, 15338, 8908, 598, 710, 8107, 31785, 13, 5403, 2320, 30105, 281, 4207, 43436, 74, 6825, 16624, 372, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07155822262619481, "compression_ratio": 1.4480286738351253, "no_speech_prob": 0.028940875083208084}, {"id": 209, "seek": 76420, "start": 768.2, "end": 772.2, "text": " A model po treningu z COT, czyli flan palm?", "tokens": [50564, 316, 2316, 714, 2192, 773, 84, 710, 3002, 51, 11, 16591, 932, 282, 3984, 76, 30, 50764], "temperature": 0.0, "avg_logprob": -0.07155822262619481, "compression_ratio": 1.4480286738351253, "no_speech_prob": 0.028940875083208084}, {"id": 210, "seek": 76420, "start": 772.2, "end": 776.2, "text": " Dla niego ta sama fraza stawa\u0142a si\u0119 magicznym wyzwalaczem.", "tokens": [50764, 413, 875, 49615, 1846, 17768, 6600, 2394, 342, 10449, 5024, 3244, 5585, 89, 12996, 4628, 14406, 304, 14875, 443, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07155822262619481, "compression_ratio": 1.4480286738351253, "no_speech_prob": 0.028940875083208084}, {"id": 211, "seek": 76420, "start": 776.2, "end": 785.2, "text": " Model rozumia\u0142, \u017ce ma teraz samodzielnie wygenerowa\u0107 logiczny ci\u0105g my\u015blowy, rozbi\u0107 problem na cz\u0119\u015bci i rozwi\u0105za\u0107 go krok po kroku.", "tokens": [50964, 17105, 48797, 8908, 11, 3561, 463, 16854, 3247, 378, 42280, 2766, 4628, 21848, 11445, 9952, 89, 1634, 42398, 70, 452, 19212, 10089, 11, 9544, 5614, 2162, 1154, 1667, 41314, 741, 9544, 18234, 35873, 352, 350, 31621, 714, 45909, 5279, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07155822262619481, "compression_ratio": 1.4480286738351253, "no_speech_prob": 0.028940875083208084}, {"id": 212, "seek": 76420, "start": 785.2, "end": 788.2, "text": " Nawet je\u015bli nigdy wcze\u015bniej nie widzia\u0142 dok\u0142adnie takiego zadania.", "tokens": [51414, 40315, 302, 25630, 26996, 3173, 40785, 2838, 27486, 8908, 45864, 2766, 32296, 42788, 5609, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07155822262619481, "compression_ratio": 1.4480286738351253, "no_speech_prob": 0.028940875083208084}, {"id": 213, "seek": 76420, "start": 788.2, "end": 789.2, "text": " Dok\u0142adnie.", "tokens": [51564, 29768, 10358, 2766, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07155822262619481, "compression_ratio": 1.4480286738351253, "no_speech_prob": 0.028940875083208084}, {"id": 214, "seek": 78920, "start": 789.2, "end": 795.2, "text": " Nauczy\u0142 si\u0119 nie tylko odpowiedzi, ale samej metody rozwi\u0105zywania problem\u00f3w. To jest gigantyczny prze\u0142om.", "tokens": [50364, 6056, 1311, 1229, 1221, 3244, 2838, 13219, 36574, 3992, 11, 6775, 912, 73, 1131, 843, 9544, 18234, 1229, 86, 5609, 1154, 3901, 13, 1407, 3492, 8741, 394, 17466, 1634, 8325, 1221, 298, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08022143159593854, "compression_ratio": 1.3373493975903614, "no_speech_prob": 0.20381391048431396}, {"id": 215, "seek": 78920, "start": 795.2, "end": 803.2, "text": " To wszystko brzmi imponuj\u0105co w teori\u0119, ale prze\u0142\u00f3\u017cmy to na twarde dane. Jak te ulepszone modele wypad\u0142y w konkretnych testach?", "tokens": [50664, 1407, 22607, 738, 89, 3057, 704, 266, 13263, 1291, 261, 40238, 5034, 11, 6775, 8325, 1221, 812, 1427, 2226, 281, 1667, 683, 10866, 49206, 13, 15029, 535, 344, 306, 1878, 16896, 4391, 306, 4628, 13647, 6825, 261, 36500, 9399, 1500, 608, 30, 51064], "temperature": 0.0, "avg_logprob": -0.08022143159593854, "compression_ratio": 1.3373493975903614, "no_speech_prob": 0.20381391048431396}, {"id": 216, "seek": 78920, "start": 803.2, "end": 805.2, "text": " No w\u0142a\u015bnie, wyniki.", "tokens": [51064, 883, 14234, 11, 31936, 9850, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08022143159593854, "compression_ratio": 1.3373493975903614, "no_speech_prob": 0.20381391048431396}, {"id": 217, "seek": 78920, "start": 805.2, "end": 811.2, "text": " Wspomina si\u0119 tu o benchmarku MMLU. Co to jest i jakie by\u0142y wyniki?", "tokens": [51164, 343, 4952, 49217, 3244, 2604, 277, 18927, 84, 376, 12683, 52, 13, 3066, 281, 3492, 741, 22124, 26366, 31936, 9850, 30, 51464], "temperature": 0.0, "avg_logprob": -0.08022143159593854, "compression_ratio": 1.3373493975903614, "no_speech_prob": 0.20381391048431396}, {"id": 218, "seek": 81120, "start": 811.2, "end": 817.2, "text": " MMLU to jeden z najtrudniejszych test\u00f3w dla modeli j\u0119zykowych.", "tokens": [50364, 376, 12683, 52, 281, 12906, 710, 11212, 6903, 532, 10402, 45021, 1500, 3901, 12285, 2316, 72, 49055, 74, 19605, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07598075364765368, "compression_ratio": 1.2392344497607655, "no_speech_prob": 0.4381422996520996}, {"id": 219, "seek": 81120, "start": 817.2, "end": 822.2, "text": " Sprawdza wiedz\u0119 i rozumowanie w 57 r\u00f3\u017cnych dziedzinach.", "tokens": [50664, 1738, 15889, 2394, 46894, 11052, 741, 48797, 22028, 261, 21423, 42602, 9758, 15338, 259, 608, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07598075364765368, "compression_ratio": 1.2392344497607655, "no_speech_prob": 0.4381422996520996}, {"id": 220, "seek": 81120, "start": 822.2, "end": 828.2, "text": " Od fizyki, medycyny, przez prawo, a\u017c po histori\u0119 sztuki.", "tokens": [50914, 12210, 21000, 88, 2984, 11, 1205, 88, 1344, 1634, 11, 14064, 3206, 6120, 11, 48134, 714, 4058, 5034, 262, 2682, 11788, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07598075364765368, "compression_ratio": 1.2392344497607655, "no_speech_prob": 0.4381422996520996}, {"id": 221, "seek": 81120, "start": 828.2, "end": 830.2, "text": " Taka matura dla AI.", "tokens": [51214, 314, 7849, 3803, 2991, 12285, 7318, 13, 51314], "temperature": 0.0, "avg_logprob": -0.07598075364765368, "compression_ratio": 1.2392344497607655, "no_speech_prob": 0.4381422996520996}, {"id": 222, "seek": 81120, "start": 830.2, "end": 831.2, "text": " Co\u015b w tym stylu.", "tokens": [51314, 3066, 1788, 261, 8107, 7952, 2781, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07598075364765368, "compression_ratio": 1.2392344497607655, "no_speech_prob": 0.4381422996520996}, {"id": 223, "seek": 81120, "start": 831.2, "end": 834.2, "text": " I tutaj wyniki m\u00f3wi\u0105 same za siebie.", "tokens": [51364, 286, 12749, 31936, 9850, 46591, 912, 7949, 39137, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07598075364765368, "compression_ratio": 1.2392344497607655, "no_speech_prob": 0.4381422996520996}, {"id": 224, "seek": 83420, "start": 834.2, "end": 841.2, "text": " Flan Palm, ten najwi\u0119kszy, osi\u0105gn\u0105\u0142 w tym te\u015bcie wynik 75,2%.", "tokens": [50364, 3235, 282, 32668, 11, 2064, 48636, 1694, 1229, 11, 3003, 11404, 4568, 1611, 1221, 261, 8107, 535, 9815, 31936, 1035, 9562, 11, 17, 6856, 50714], "temperature": 0.0, "avg_logprob": -0.1042167904140713, "compression_ratio": 1.2545454545454546, "no_speech_prob": 0.02901901863515377}, {"id": 225, "seek": 83420, "start": 841.2, "end": 844.2, "text": " Ok, 75,2%.", "tokens": [50714, 3477, 11, 9562, 11, 17, 6856, 50864], "temperature": 0.0, "avg_logprob": -0.1042167904140713, "compression_ratio": 1.2545454545454546, "no_speech_prob": 0.02901901863515377}, {"id": 226, "seek": 83420, "start": 844.2, "end": 849.2, "text": " Ale co to oznacza w praktyce? To du\u017co? Jak to si\u0119 ma do wcze\u015bniejszych modeli?", "tokens": [50864, 9366, 598, 281, 277, 22672, 326, 2394, 261, 3206, 74, 874, 384, 30, 1407, 26673, 30, 15029, 281, 3244, 463, 360, 40785, 45021, 2316, 72, 30, 51114], "temperature": 0.0, "avg_logprob": -0.1042167904140713, "compression_ratio": 1.2545454545454546, "no_speech_prob": 0.02901901863515377}, {"id": 227, "seek": 83420, "start": 849.2, "end": 852.2, "text": " To jest kosmiczny skok, \u017ceby da\u0107 kontekst.", "tokens": [51114, 1407, 3492, 19532, 13195, 89, 1634, 1110, 453, 11, 11316, 1120, 2162, 14373, 916, 372, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1042167904140713, "compression_ratio": 1.2545454545454546, "no_speech_prob": 0.02901901863515377}, {"id": 228, "seek": 83420, "start": 852.2, "end": 860.2, "text": " Poprzednia wersja tego samego modelu, zwyk\u0142y palm, osi\u0105gn\u0105\u0142a 69,3%.", "tokens": [51264, 10215, 81, 11312, 12679, 261, 433, 2938, 8627, 912, 1571, 2316, 84, 11, 43436, 74, 6825, 17018, 11, 3003, 11404, 4568, 1611, 5024, 28267, 11, 18, 6856, 51664], "temperature": 0.0, "avg_logprob": -0.1042167904140713, "compression_ratio": 1.2545454545454546, "no_speech_prob": 0.02901901863515377}, {"id": 229, "seek": 86020, "start": 860.2, "end": 864.2, "text": " A wcze\u015bniejszy gigant model, o kt\u00f3rym wszyscy m\u00f3wili?", "tokens": [50364, 316, 40785, 7706, 8741, 394, 2316, 11, 277, 30120, 44232, 13489, 2312, 30, 50564], "temperature": 0.0, "avg_logprob": -0.09801191442153033, "compression_ratio": 1.3266129032258065, "no_speech_prob": 0.007901297882199287}, {"id": 230, "seek": 86020, "start": 864.2, "end": 865.2, "text": " GPT-3.", "tokens": [50564, 26039, 51, 12, 18, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09801191442153033, "compression_ratio": 1.3266129032258065, "no_speech_prob": 0.007901297882199287}, {"id": 231, "seek": 86020, "start": 865.2, "end": 871.2, "text": " Tak, GPT-3 mia\u0142 w tym samym te\u015bcie wynik 43,9%.", "tokens": [50614, 9118, 11, 26039, 51, 12, 18, 27989, 261, 8107, 3247, 4199, 535, 9815, 31936, 1035, 17914, 11, 24, 6856, 50914], "temperature": 0.0, "avg_logprob": -0.09801191442153033, "compression_ratio": 1.3266129032258065, "no_speech_prob": 0.007901297882199287}, {"id": 232, "seek": 86020, "start": 871.2, "end": 879.2, "text": " M\u00f3wimy wi\u0119c o przeskoczeniu z poziomu studenta ledwo zdaj\u0105cego egzamin na poziom, no, prymusa.", "tokens": [50914, 376, 3901, 13189, 16677, 277, 6541, 279, 74, 905, 39651, 710, 38503, 298, 84, 3107, 64, 4684, 6120, 16221, 11133, 384, 1571, 24263, 89, 7428, 1667, 38503, 298, 11, 572, 11, 41902, 18761, 64, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09801191442153033, "compression_ratio": 1.3266129032258065, "no_speech_prob": 0.007901297882199287}, {"id": 233, "seek": 86020, "start": 879.2, "end": 882.2, "text": " R\u00f3\u017cnica jest faktycznie ogromna.", "tokens": [51314, 497, 812, 1427, 32687, 3492, 33647, 45586, 34416, 298, 629, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09801191442153033, "compression_ratio": 1.3266129032258065, "no_speech_prob": 0.007901297882199287}, {"id": 234, "seek": 86020, "start": 882.2, "end": 887.2, "text": " Ale najbardziej zdumiewaj\u0105ce jest chyba to, co sta\u0142o si\u0119 z mniejszymi modelami.", "tokens": [51464, 9366, 41857, 16221, 449, 1093, 11133, 384, 3492, 31532, 281, 11, 598, 11135, 5249, 3244, 710, 39513, 7706, 3057, 2316, 4526, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09801191442153033, "compression_ratio": 1.3266129032258065, "no_speech_prob": 0.007901297882199287}, {"id": 235, "seek": 88720, "start": 887.2, "end": 893.2, "text": " Tak, i to jest drugi wielki wniosek tej pracy, zwi\u0105zany z demokratyzacj\u0105 AI.", "tokens": [50364, 9118, 11, 741, 281, 3492, 4110, 72, 20570, 2984, 261, 3722, 541, 74, 12573, 35591, 11, 27741, 1325, 710, 1371, 453, 81, 21398, 89, 326, 8555, 7318, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09798936567444733, "compression_ratio": 1.290909090909091, "no_speech_prob": 0.08763034641742706}, {"id": 236, "seek": 88720, "start": 893.2, "end": 894.2, "text": " Mianowicie?", "tokens": [50664, 376, 952, 305, 28434, 30, 50714], "temperature": 0.0, "avg_logprob": -0.09798936567444733, "compression_ratio": 1.290909090909091, "no_speech_prob": 0.08763034641742706}, {"id": 237, "seek": 88720, "start": 894.2, "end": 902.2, "text": " Ta technika okaza\u0142a si\u0119 tak pot\u0119\u017cna, \u017ce nawet mniejsze, publicznie dost\u0119pne modele, ogromnie na niej skorzysta\u0142y.", "tokens": [50714, 6551, 1537, 5439, 3133, 12257, 5024, 3244, 991, 1847, 1274, 1427, 629, 11, 3561, 22696, 275, 44258, 11, 1908, 89, 2766, 48209, 716, 4391, 306, 11, 34416, 298, 2766, 1667, 2838, 73, 1110, 284, 49590, 6825, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09798936567444733, "compression_ratio": 1.290909090909091, "no_speech_prob": 0.08763034641742706}, {"id": 238, "seek": 88720, "start": 902.2, "end": 908.2, "text": " Przyk\u0142ad, Flante 5 XL, kt\u00f3ry ma zaledwie 3 miliardy parametr\u00f3w.", "tokens": [51114, 39590, 15317, 11, 3235, 2879, 1025, 37210, 11, 9913, 463, 710, 5573, 8699, 805, 1962, 72, 515, 88, 6220, 27965, 3901, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09798936567444733, "compression_ratio": 1.290909090909091, "no_speech_prob": 0.08763034641742706}, {"id": 239, "seek": 88720, "start": 908.2, "end": 909.2, "text": " Zaledwie.", "tokens": [51414, 1176, 5573, 8699, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09798936567444733, "compression_ratio": 1.290909090909091, "no_speech_prob": 0.08763034641742706}, {"id": 240, "seek": 88720, "start": 909.2, "end": 915.2, "text": " Osi\u0105gn\u0105\u0142 w te\u015bcie MMLU wynik lepszy ni\u017c 175 miliardowy GPT-3.", "tokens": [51464, 422, 7691, 1611, 4568, 1611, 1221, 261, 535, 9815, 376, 12683, 52, 31936, 1035, 476, 1878, 1229, 28502, 41165, 1962, 72, 515, 10089, 26039, 51, 12, 18, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09798936567444733, "compression_ratio": 1.290909090909091, "no_speech_prob": 0.08763034641742706}, {"id": 241, "seek": 91520, "start": 915.2, "end": 917.2, "text": " Chwila, powt\u00f3rzmy to.", "tokens": [50364, 761, 86, 7371, 11, 3388, 4547, 19390, 2226, 281, 13, 50464], "temperature": 0.0, "avg_logprob": -0.051323830286661785, "compression_ratio": 1.4207119741100325, "no_speech_prob": 0.12535418570041656}, {"id": 242, "seek": 91520, "start": 917.2, "end": 924.2, "text": " Model ponad 50 razy mniejszy, dzi\u0119ki tej technice Fine Tuning, pokona\u0142 jednego z najwi\u0119kszych graczy na rynku.", "tokens": [50464, 17105, 9224, 345, 2625, 9639, 88, 39513, 7706, 11, 45003, 12573, 1537, 573, 12024, 21363, 278, 11, 13010, 4037, 1221, 5232, 11858, 710, 48636, 1694, 28051, 11625, 1229, 1667, 367, 2534, 5279, 13, 50814], "temperature": 0.0, "avg_logprob": -0.051323830286661785, "compression_ratio": 1.4207119741100325, "no_speech_prob": 0.12535418570041656}, {"id": 243, "seek": 91520, "start": 924.2, "end": 925.2, "text": " Dok\u0142adnie tak.", "tokens": [50814, 29768, 10358, 2766, 991, 13, 50864], "temperature": 0.0, "avg_logprob": -0.051323830286661785, "compression_ratio": 1.4207119741100325, "no_speech_prob": 0.12535418570041656}, {"id": 244, "seek": 91520, "start": 925.2, "end": 927.2, "text": " To ca\u0142kowicie zmienia zasady gry.", "tokens": [50864, 1407, 35224, 74, 305, 28434, 17020, 18811, 26530, 880, 41974, 13, 50964], "temperature": 0.0, "avg_logprob": -0.051323830286661785, "compression_ratio": 1.4207119741100325, "no_speech_prob": 0.12535418570041656}, {"id": 245, "seek": 91520, "start": 927.2, "end": 933.2, "text": " Oznacza, \u017ce dost\u0119p do pot\u0119\u017cnej AI przestaje by\u0107 domen\u0105 tylko kilku najwi\u0119kszych korporacji.", "tokens": [50964, 422, 22672, 326, 2394, 11, 3561, 48209, 360, 1847, 1274, 1427, 11794, 7318, 44264, 11153, 15069, 3285, 268, 1611, 13219, 5128, 5279, 48636, 1694, 28051, 14784, 2816, 13152, 13, 51264], "temperature": 0.0, "avg_logprob": -0.051323830286661785, "compression_ratio": 1.4207119741100325, "no_speech_prob": 0.12535418570041656}, {"id": 246, "seek": 91520, "start": 933.2, "end": 942.2, "text": " Zdecydowanie. Mniejsze gracze, startopy, uniwersytety mog\u0105 teraz osi\u0105ga\u0107 podobne rezultaty przy znacznie ni\u017cszych kosztach.", "tokens": [51264, 1176, 1479, 1344, 67, 22028, 13, 376, 44258, 11625, 1381, 11, 722, 19680, 11, 36435, 5364, 4328, 2210, 34123, 16854, 3003, 11404, 3680, 2162, 43024, 716, 48060, 723, 21398, 6501, 15397, 14875, 2766, 28502, 45021, 19532, 2682, 608, 13, 51714], "temperature": 0.0, "avg_logprob": -0.051323830286661785, "compression_ratio": 1.4207119741100325, "no_speech_prob": 0.12535418570041656}, {"id": 247, "seek": 91520, "start": 942.2, "end": 944.2, "text": " Ale benchmarki to jedno.", "tokens": [51714, 9366, 18927, 72, 281, 5232, 1771, 13, 51814], "temperature": 0.0, "avg_logprob": -0.051323830286661785, "compression_ratio": 1.4207119741100325, "no_speech_prob": 0.12535418570041656}, {"id": 248, "seek": 94420, "start": 944.2, "end": 947.2, "text": " Prawdziwym testem jest interakcja z cz\u0142owiekiem.", "tokens": [50364, 430, 15889, 3992, 86, 4199, 1500, 443, 3492, 728, 514, 34056, 710, 36282, 26116, 13, 50514], "temperature": 0.0, "avg_logprob": -0.05906292796134949, "compression_ratio": 1.3524904214559388, "no_speech_prob": 0.012613953091204166}, {"id": 249, "seek": 94420, "start": 947.2, "end": 952.2, "text": " W\u0142a\u015bnie. Jak model radzi sobie przy otwartych, kreatywnych zadaniach?", "tokens": [50514, 343, 5024, 12221, 13, 15029, 2316, 2843, 3992, 13652, 6501, 4337, 29587, 16384, 11, 350, 620, 88, 895, 16384, 42788, 3782, 608, 30, 50764], "temperature": 0.0, "avg_logprob": -0.05906292796134949, "compression_ratio": 1.3524904214559388, "no_speech_prob": 0.012613953091204166}, {"id": 250, "seek": 94420, "start": 952.2, "end": 958.2, "text": " I tu autorzy przeprowadzili ocen\u0119 ludzk\u0105, por\u00f3wnuj\u0105c odpowiedzi zwyk\u0142ego palm z flan palm.", "tokens": [50764, 286, 2604, 19510, 1229, 30829, 1892, 345, 89, 2312, 10409, 268, 1274, 15946, 89, 26304, 11, 1515, 812, 895, 44733, 36574, 3992, 43436, 74, 1221, 6308, 17018, 710, 932, 282, 17018, 13, 51064], "temperature": 0.0, "avg_logprob": -0.05906292796134949, "compression_ratio": 1.3524904214559388, "no_speech_prob": 0.012613953091204166}, {"id": 251, "seek": 94420, "start": 958.2, "end": 963.2, "text": " Prosili ludzi, by ocenili, kt\u00f3ra odpowied\u017a jest lepsza, bardziej pomocna.", "tokens": [51064, 26024, 2312, 29586, 11, 538, 10409, 268, 2312, 11, 19456, 36574, 10659, 3492, 476, 1878, 2394, 11, 27209, 48962, 629, 13, 51314], "temperature": 0.0, "avg_logprob": -0.05906292796134949, "compression_ratio": 1.3524904214559388, "no_speech_prob": 0.012613953091204166}, {"id": 252, "seek": 94420, "start": 963.2, "end": 964.2, "text": " I co wysz\u0142o?", "tokens": [51314, 286, 598, 261, 20589, 5249, 30, 51364], "temperature": 0.0, "avg_logprob": -0.05906292796134949, "compression_ratio": 1.3524904214559388, "no_speech_prob": 0.012613953091204166}, {"id": 253, "seek": 94420, "start": 964.2, "end": 968.2, "text": " Wyniki pokazane na Figure 8 s\u0105 mia\u017cd\u017c\u0105ce.", "tokens": [51364, 343, 2534, 9850, 13010, 921, 1929, 1667, 43225, 1649, 9015, 21290, 1427, 67, 1427, 1611, 384, 13, 51564], "temperature": 0.0, "avg_logprob": -0.05906292796134949, "compression_ratio": 1.3524904214559388, "no_speech_prob": 0.012613953091204166}, {"id": 254, "seek": 96820, "start": 968.2, "end": 972.2, "text": " W przypadku pyta\u0144 wymagaj\u0105cych kreatywno\u015bci, czy z\u0142o\u017conego planowania,", "tokens": [50364, 343, 41955, 10664, 1328, 5248, 29764, 559, 11133, 31306, 350, 620, 88, 20944, 6199, 11, 6430, 710, 5249, 1427, 546, 1571, 1393, 21308, 11, 50564], "temperature": 0.0, "avg_logprob": -0.05290604382753372, "compression_ratio": 1.3674911660777385, "no_speech_prob": 0.041493725031614304}, {"id": 255, "seek": 96820, "start": 972.2, "end": 980.2, "text": " odpowiedzi modelu po Instruction Find Tuning by\u0142y proferowane przez ludzi, a\u017c w 79% przypadk\u00f3w.", "tokens": [50564, 36574, 3992, 2316, 84, 714, 2730, 3826, 11809, 21363, 278, 26366, 447, 612, 23066, 14064, 29586, 11, 48134, 261, 32803, 4, 33100, 23849, 13, 50964], "temperature": 0.0, "avg_logprob": -0.05290604382753372, "compression_ratio": 1.3674911660777385, "no_speech_prob": 0.041493725031614304}, {"id": 256, "seek": 96820, "start": 980.2, "end": 982.2, "text": " Prawie 80%.", "tokens": [50964, 430, 5131, 414, 4688, 6856, 51064], "temperature": 0.0, "avg_logprob": -0.05290604382753372, "compression_ratio": 1.3674911660777385, "no_speech_prob": 0.041493725031614304}, {"id": 257, "seek": 96820, "start": 982.2, "end": 986.2, "text": " Tak. To pokazuje, \u017ce ta metoda nie tylko poprawia wyniki w testach,", "tokens": [51064, 9118, 13, 1407, 13010, 43317, 11, 3561, 1846, 1131, 13449, 2838, 13219, 1665, 5131, 654, 31936, 9850, 261, 1500, 608, 11, 51264], "temperature": 0.0, "avg_logprob": -0.05290604382753372, "compression_ratio": 1.3674911660777385, "no_speech_prob": 0.041493725031614304}, {"id": 258, "seek": 96820, "start": 986.2, "end": 990.2, "text": " ale realnie uczy model, jak by\u0107 lepszym partnerem do rozmowy.", "tokens": [51264, 6775, 957, 2766, 344, 6522, 2316, 11, 4207, 15069, 476, 1878, 26681, 644, 77, 7333, 360, 35234, 10089, 13, 51464], "temperature": 0.0, "avg_logprob": -0.05290604382753372, "compression_ratio": 1.3674911660777385, "no_speech_prob": 0.041493725031614304}, {"id": 259, "seek": 96820, "start": 990.2, "end": 994.2, "text": " Model wreszcie zaczyna rozumie\u0107, czego oczekuje od niego cz\u0142owiek.", "tokens": [51464, 17105, 261, 495, 89, 4260, 43811, 629, 48797, 414, 2162, 11, 36559, 277, 3689, 916, 13008, 3611, 49615, 36282, 74, 13, 51664], "temperature": 0.0, "avg_logprob": -0.05290604382753372, "compression_ratio": 1.3674911660777385, "no_speech_prob": 0.041493725031614304}, {"id": 260, "seek": 99420, "start": 994.2, "end": 998.2, "text": " A praca wspomina jeszcze o jednym niezwykle wa\u017cnym efekcie ubocznym.", "tokens": [50364, 316, 582, 6628, 17757, 49217, 14168, 277, 5232, 12996, 33511, 9726, 14677, 27777, 12996, 31482, 916, 4260, 26709, 905, 89, 12996, 13, 50564], "temperature": 0.0, "avg_logprob": -0.06952804696970973, "compression_ratio": 1.3770491803278688, "no_speech_prob": 0.0164664126932621}, {"id": 261, "seek": 99420, "start": 998.2, "end": 1001.2, "text": " Tak, i jest to bardzo pozytywne zaskoczenie.", "tokens": [50564, 9118, 11, 741, 3492, 281, 9034, 49358, 874, 86, 716, 710, 3863, 905, 16778, 13, 50714], "temperature": 0.0, "avg_logprob": -0.06952804696970973, "compression_ratio": 1.3770491803278688, "no_speech_prob": 0.0164664126932621}, {"id": 262, "seek": 99420, "start": 1001.2, "end": 1009.2, "text": " Okazuje si\u0119, \u017ce Instruction Find Tuning znacz\u0105co zredukowa\u0142 sk\u0142o\u0144no\u015b\u0107 modeli do generowania toksycznych tre\u015bci.", "tokens": [50714, 3477, 43317, 3244, 11, 3561, 2730, 3826, 11809, 21363, 278, 15397, 326, 8925, 1291, 710, 265, 769, 74, 30105, 1110, 5249, 5248, 23293, 2316, 72, 360, 1337, 21308, 281, 1694, 17466, 9399, 2192, 6199, 13, 51114], "temperature": 0.0, "avg_logprob": -0.06952804696970973, "compression_ratio": 1.3770491803278688, "no_speech_prob": 0.0164664126932621}, {"id": 263, "seek": 99420, "start": 1009.2, "end": 1011.2, "text": " A to ciekawe.", "tokens": [51114, 316, 281, 30596, 2330, 826, 13, 51214], "temperature": 0.0, "avg_logprob": -0.06952804696970973, "compression_ratio": 1.3770491803278688, "no_speech_prob": 0.0164664126932621}, {"id": 264, "seek": 99420, "start": 1011.2, "end": 1014.2, "text": " Zmniejsz\u0105 te\u017c niekt\u00f3re uprzedzenia, tak zwane bajesy.", "tokens": [51214, 1176, 76, 30295, 8925, 9516, 2838, 43073, 265, 493, 81, 11312, 14320, 11, 991, 11873, 1929, 23589, 17823, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06952804696970973, "compression_ratio": 1.3770491803278688, "no_speech_prob": 0.0164664126932621}, {"id": 265, "seek": 99420, "start": 1014.2, "end": 1022.2, "text": " Nie jest to idealne rozwi\u0105zanie, ale wyra\u017any krok w dobrym kierunku, je\u015bli chodzi o odpowiedzialny rozw\u00f3j AI.", "tokens": [51364, 12016, 3492, 281, 7157, 716, 9544, 22620, 7155, 11, 6775, 4628, 424, 10659, 1634, 350, 31621, 261, 35884, 76, 38767, 49910, 11, 25630, 23998, 277, 24314, 15338, 831, 1634, 9544, 86, 18999, 7318, 13, 51764], "temperature": 0.0, "avg_logprob": -0.06952804696970973, "compression_ratio": 1.3770491803278688, "no_speech_prob": 0.0164664126932621}, {"id": 266, "seek": 102220, "start": 1022.2, "end": 1028.2, "text": " Czyli ucz\u0105c model by by\u0142 pomocny, przy okazji uczymy go, by by\u0142 bardziej cywilizowany.", "tokens": [50364, 37099, 35403, 1611, 66, 2316, 538, 16673, 48962, 1634, 11, 6501, 3133, 921, 4013, 344, 6522, 2226, 352, 11, 538, 16673, 27209, 3185, 86, 47043, 23341, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06575617071700422, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.046057235449552536}, {"id": 267, "seek": 102220, "start": 1028.2, "end": 1030.2, "text": " W\u0142a\u015bnie tak.", "tokens": [50664, 343, 5024, 12221, 991, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06575617071700422, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.046057235449552536}, {"id": 268, "seek": 102220, "start": 1030.2, "end": 1032.2, "text": " Dobrze, spr\u00f3bujmy to wszystko zebra\u0107.", "tokens": [50764, 29679, 13503, 11, 6103, 14216, 4579, 2226, 281, 22607, 47060, 2162, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06575617071700422, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.046057235449552536}, {"id": 269, "seek": 102220, "start": 1032.2, "end": 1038.2, "text": " Gdyby\u015bmy mieli wyci\u0105gn\u0105\u0107 jeden kluczowy wniosek z tej prze\u0142omowej pracy. Co by to by\u0142o?", "tokens": [50864, 460, 3173, 2322, 10513, 41214, 4628, 34381, 4568, 36374, 12906, 9671, 1311, 89, 10089, 261, 3722, 541, 74, 710, 12573, 8325, 1221, 298, 21091, 35591, 13, 3066, 538, 281, 14811, 30, 51164], "temperature": 0.0, "avg_logprob": -0.06575617071700422, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.046057235449552536}, {"id": 270, "seek": 102220, "start": 1038.2, "end": 1042.2, "text": " My\u015bl\u0119, \u017ce by\u0142oby to stwierdzenie, \u017ce Instruction Find Tuning,", "tokens": [51164, 1222, 28749, 11, 3561, 16673, 13944, 281, 342, 40717, 67, 16778, 11, 3561, 2730, 3826, 11809, 21363, 278, 11, 51364], "temperature": 0.0, "avg_logprob": -0.06575617071700422, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.046057235449552536}, {"id": 271, "seek": 102220, "start": 1042.2, "end": 1050.2, "text": " zw\u0142aszcza wzbogacony o dane Chain of Thought, to jest relatywnie tania, uniwersalna i niezwykle skuteczna metoda.", "tokens": [51364, 11873, 1221, 19601, 41524, 24809, 65, 664, 326, 2526, 277, 49206, 33252, 295, 23058, 11, 281, 3492, 1039, 21398, 14215, 256, 5609, 11, 36435, 5364, 304, 629, 741, 33511, 9726, 14677, 1110, 1169, 3689, 629, 1131, 13449, 13, 51764], "temperature": 0.0, "avg_logprob": -0.06575617071700422, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.046057235449552536}, {"id": 272, "seek": 105020, "start": 1050.2, "end": 1053.2, "text": " Kt\u00f3ra sprawia, \u017ce modele staj\u0105 si\u0119 nie tylko troch\u0119 lepsze?", "tokens": [50364, 591, 4547, 424, 22734, 654, 11, 3561, 4391, 306, 342, 11133, 3244, 2838, 13219, 24926, 476, 1878, 1381, 30, 50514], "temperature": 0.0, "avg_logprob": -0.05746279971700319, "compression_ratio": 1.5179856115107915, "no_speech_prob": 0.1098456010222435}, {"id": 273, "seek": 105020, "start": 1053.2, "end": 1058.2, "text": " Ale fundamentalnie inteligentniejsze i bardziej u\u017cyteczne w praktyce.", "tokens": [50514, 9366, 8088, 2766, 24777, 25002, 44258, 741, 27209, 34097, 975, 38491, 261, 3206, 74, 874, 384, 13, 50764], "temperature": 0.0, "avg_logprob": -0.05746279971700319, "compression_ratio": 1.5179856115107915, "no_speech_prob": 0.1098456010222435}, {"id": 274, "seek": 105020, "start": 1058.2, "end": 1067.2, "text": " To przej\u015bcie od pot\u0119\u017cnej, ale nieokrzesanej si\u0142y obliczeniowej do responsywnego narz\u0119dzia, kt\u00f3re rozumie intencje.", "tokens": [50764, 1407, 8325, 73, 9815, 3611, 1847, 1274, 1427, 11794, 11, 6775, 2838, 453, 19390, 279, 1929, 73, 1511, 6825, 1111, 1050, 42124, 21091, 360, 2914, 27112, 11858, 6714, 89, 6298, 40395, 11, 8864, 48797, 414, 560, 22660, 2884, 13, 51214], "temperature": 0.0, "avg_logprob": -0.05746279971700319, "compression_ratio": 1.5179856115107915, "no_speech_prob": 0.1098456010222435}, {"id": 275, "seek": 105020, "start": 1067.2, "end": 1069.2, "text": " Tak. To sedno sprawy.", "tokens": [51214, 9118, 13, 1407, 9643, 1771, 22734, 88, 13, 51314], "temperature": 0.0, "avg_logprob": -0.05746279971700319, "compression_ratio": 1.5179856115107915, "no_speech_prob": 0.1098456010222435}, {"id": 276, "seek": 105020, "start": 1069.2, "end": 1071.2, "text": " Ale doda\u0142bym jeszcze jeden wymiar.", "tokens": [51314, 9366, 360, 2675, 43579, 14168, 12906, 4628, 3057, 289, 13, 51414], "temperature": 0.0, "avg_logprob": -0.05746279971700319, "compression_ratio": 1.5179856115107915, "no_speech_prob": 0.1098456010222435}, {"id": 277, "seek": 105020, "start": 1071.2, "end": 1076.2, "text": " To nie tylko sprawia, \u017ce modele s\u0105 m\u0105drzejsze, ale te\u017c bardziej przejrzyste.", "tokens": [51414, 1407, 2838, 13219, 22734, 654, 11, 3561, 4391, 306, 9015, 275, 18962, 13503, 25530, 1381, 11, 6775, 9516, 27209, 8325, 73, 13047, 2941, 13, 51664], "temperature": 0.0, "avg_logprob": -0.05746279971700319, "compression_ratio": 1.5179856115107915, "no_speech_prob": 0.1098456010222435}, {"id": 278, "seek": 105020, "start": 1076.2, "end": 1078.2, "text": " O, to bardzo wa\u017cna uwaga.", "tokens": [51664, 422, 11, 281, 9034, 27777, 629, 23147, 9286, 13, 51764], "temperature": 0.0, "avg_logprob": -0.05746279971700319, "compression_ratio": 1.5179856115107915, "no_speech_prob": 0.1098456010222435}, {"id": 279, "seek": 107820, "start": 1078.2, "end": 1085.2, "text": " Kiedy model pokazuje swoje rozmowanie w Chain of Thought, my jako u\u017cytkownicy mo\u017cemy zobaczy\u0107, gdzie ewentualnie pope\u0142ni\u0142 b\u0142\u0105d logiczny.", "tokens": [50364, 591, 16446, 2316, 13010, 43317, 29489, 35234, 22028, 261, 33252, 295, 23058, 11, 452, 17123, 344, 1427, 4328, 74, 648, 2632, 26500, 37273, 2162, 11, 18922, 308, 34798, 901, 2766, 42248, 1221, 3722, 1221, 272, 15926, 67, 9952, 89, 1634, 13, 50714], "temperature": 0.0, "avg_logprob": -0.03280770929553841, "compression_ratio": 1.4201183431952662, "no_speech_prob": 0.007381657604128122}, {"id": 280, "seek": 107820, "start": 1085.2, "end": 1091.2, "text": " Absolutnie. To ogromna zmiana w por\u00f3wnaniu do czarnej skrzynki, kt\u00f3ra po prostu wypluwa odpowied\u017a.", "tokens": [50714, 5813, 2308, 2766, 13, 1407, 34416, 298, 629, 17020, 8497, 261, 1515, 812, 895, 25849, 360, 6472, 289, 11794, 1110, 13047, 77, 2984, 11, 19456, 714, 19518, 4628, 564, 84, 4151, 36574, 10659, 13, 51014], "temperature": 0.0, "avg_logprob": -0.03280770929553841, "compression_ratio": 1.4201183431952662, "no_speech_prob": 0.007381657604128122}, {"id": 281, "seek": 107820, "start": 1091.2, "end": 1094.2, "text": " To buduje zaufanie i otwiera drog\u0119 do debagowania.", "tokens": [51014, 1407, 3265, 13008, 710, 9507, 7155, 741, 4337, 86, 10609, 3789, 70, 1274, 360, 3001, 559, 21308, 13, 51164], "temperature": 0.0, "avg_logprob": -0.03280770929553841, "compression_ratio": 1.4201183431952662, "no_speech_prob": 0.007381657604128122}, {"id": 282, "seek": 107820, "start": 1094.2, "end": 1098.2, "text": " Na koniec chcia\u0142bym zostawi\u0107 naszych s\u0142uchaczy z jedn\u0105 my\u015bl\u0105 do rozwa\u017cenia.", "tokens": [51164, 6056, 5897, 35733, 26497, 43579, 31873, 1607, 12757, 45002, 15116, 625, 14691, 710, 5232, 13113, 452, 19212, 1611, 360, 9544, 4151, 48830, 13, 51364], "temperature": 0.0, "avg_logprob": -0.03280770929553841, "compression_ratio": 1.4201183431952662, "no_speech_prob": 0.007381657604128122}, {"id": 283, "seek": 107820, "start": 1098.2, "end": 1104.2, "text": " Praca pokaza\u0142a, \u017ce korzy\u015bci z dodawania coraz wi\u0119kszej liczby zada\u0144 w pewnym momencie malej\u0105.", "tokens": [51364, 2114, 6628, 13010, 12257, 5024, 11, 3561, 14784, 1229, 6199, 710, 13886, 1607, 5609, 25899, 29968, 16920, 6169, 89, 2322, 710, 1538, 5248, 261, 47160, 4199, 40883, 7133, 8555, 13, 51664], "temperature": 0.0, "avg_logprob": -0.03280770929553841, "compression_ratio": 1.4201183431952662, "no_speech_prob": 0.007381657604128122}, {"id": 284, "seek": 110420, "start": 1105.2, "end": 1114.2, "text": " Autorzy sugeruj\u0105, \u017ce mo\u017ce to oznacza\u0107, i\u017c model uczy si\u0119 g\u0142\u00f3wnie tego, jak uzyska\u0107 dost\u0119p do posiadanej ju\u017c wiedzy, a niekoniecznie zdobywanow\u0105.", "tokens": [50414, 6049, 284, 1229, 459, 1321, 13263, 11, 3561, 12034, 281, 277, 22672, 326, 35873, 11, 741, 1427, 2316, 344, 6522, 3244, 18117, 812, 14215, 8627, 11, 4207, 16851, 749, 2330, 2162, 48209, 360, 1366, 38069, 1929, 73, 10678, 46894, 1229, 11, 257, 2838, 18295, 414, 19923, 16221, 13944, 7916, 30297, 13, 50864], "temperature": 0.0, "avg_logprob": -0.05338415645417713, "compression_ratio": 1.4050179211469533, "no_speech_prob": 0.028288178145885468}, {"id": 285, "seek": 110420, "start": 1114.2, "end": 1117.2, "text": " To jest bardzo prowokuj\u0105ca i wa\u017cna my\u015bl.", "tokens": [50864, 1407, 3492, 9034, 45553, 453, 13263, 496, 741, 27777, 629, 452, 19212, 13, 51014], "temperature": 0.0, "avg_logprob": -0.05338415645417713, "compression_ratio": 1.4050179211469533, "no_speech_prob": 0.028288178145885468}, {"id": 286, "seek": 110420, "start": 1117.2, "end": 1122.2, "text": " Je\u015bli to prawda, to co to m\u00f3wi o przysz\u0142o\u015bci rozwoju AI?", "tokens": [51014, 37086, 281, 43607, 11, 281, 598, 281, 24592, 277, 44018, 35059, 9544, 6120, 8954, 7318, 30, 51264], "temperature": 0.0, "avg_logprob": -0.05338415645417713, "compression_ratio": 1.4050179211469533, "no_speech_prob": 0.028288178145885468}, {"id": 287, "seek": 110420, "start": 1122.2, "end": 1123.2, "text": " No w\u0142a\u015bnie.", "tokens": [51264, 883, 14234, 13, 51314], "temperature": 0.0, "avg_logprob": -0.05338415645417713, "compression_ratio": 1.4050179211469533, "no_speech_prob": 0.028288178145885468}, {"id": 288, "seek": 110420, "start": 1123.2, "end": 1130.2, "text": " Czy kolejny wielki prze\u0142om nadejdzie niezbudowy jeszcze wi\u0119kszych modeli, kt\u00f3re poch\u0142on\u0105 jeszcze wi\u0119cej danych", "tokens": [51314, 19832, 23749, 1634, 20570, 2984, 8325, 1221, 298, 297, 762, 73, 13096, 33511, 18281, 10089, 14168, 29968, 28051, 2316, 72, 11, 8864, 714, 339, 1221, 266, 1611, 14168, 26004, 274, 34644, 51664], "temperature": 0.0, "avg_logprob": -0.05338415645417713, "compression_ratio": 1.4050179211469533, "no_speech_prob": 0.028288178145885468}, {"id": 289, "seek": 113020, "start": 1130.2, "end": 1137.2, "text": " energii, ale z odkrywania sprytniejszych i bardziej efektywnych instrukcji obs\u0142ugi dla tych, kt\u00f3re ju\u017c mamy?", "tokens": [50364, 10575, 5597, 11, 6775, 710, 3611, 43298, 86, 5609, 637, 627, 83, 10402, 45021, 741, 27209, 31482, 916, 874, 895, 16384, 1058, 25126, 19649, 3181, 1221, 24780, 12285, 15180, 11, 8864, 10678, 17335, 30, 50714], "temperature": 0.0, "avg_logprob": -0.06722602588218331, "compression_ratio": 1.4474576271186441, "no_speech_prob": 0.12046690285205841}, {"id": 290, "seek": 113020, "start": 1137.2, "end": 1138.2, "text": " Dok\u0142adnie.", "tokens": [50714, 29768, 10358, 2766, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06722602588218331, "compression_ratio": 1.4474576271186441, "no_speech_prob": 0.12046690285205841}, {"id": 291, "seek": 113020, "start": 1138.2, "end": 1143.2, "text": " By\u0107 mo\u017ce wchodzimy w ERE, w kt\u00f3rej na\u017cniejsze odbudowania coraz wi\u0119kszych cyfrowych m\u00f3zg\u00f3w", "tokens": [50764, 3146, 2162, 12034, 261, 29914, 89, 13189, 261, 462, 3850, 11, 261, 36023, 1667, 1427, 44258, 3611, 18281, 21308, 25899, 29968, 28051, 3185, 69, 1892, 16384, 32515, 89, 70, 3901, 51014], "temperature": 0.0, "avg_logprob": -0.06722602588218331, "compression_ratio": 1.4474576271186441, "no_speech_prob": 0.12046690285205841}, {"id": 292, "seek": 113020, "start": 1143.2, "end": 1147.2, "text": " stanie si\u0119 bycie dla nich po prostu lepszymi nauczycielem.", "tokens": [51014, 40013, 3244, 538, 4260, 12285, 25570, 714, 19518, 476, 1878, 1229, 3057, 49103, 1229, 4260, 10386, 13, 51214], "temperature": 0.0, "avg_logprob": -0.06722602588218331, "compression_ratio": 1.4474576271186441, "no_speech_prob": 0.12046690285205841}, {"id": 293, "seek": 113020, "start": 1147.2, "end": 1150.2, "text": " Mo\u017ce klucz nie le\u017cy w rozmiarze, ale w pedagogice.", "tokens": [51214, 43774, 9671, 1311, 89, 2838, 476, 7735, 261, 9544, 3057, 289, 1381, 11, 6775, 261, 5670, 31599, 573, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06722602588218331, "compression_ratio": 1.4474576271186441, "no_speech_prob": 0.12046690285205841}, {"id": 294, "seek": 113020, "start": 1150.2, "end": 1156.2, "text": " W znalezieniu idealnego zestawu instrukcji, kt\u00f3ry odblokuje ten pe\u0142en u\u015bpiony potencja\u0142.", "tokens": [51364, 343, 15397, 37646, 1053, 5951, 7157, 11858, 37889, 1607, 84, 1058, 25126, 19649, 11, 9913, 3611, 5199, 453, 13008, 2064, 43205, 268, 344, 1788, 79, 46184, 1847, 22660, 2938, 1221, 13, 51664], "temperature": 0.0, "avg_logprob": -0.06722602588218331, "compression_ratio": 1.4474576271186441, "no_speech_prob": 0.12046690285205841}, {"id": 295, "seek": 115620, "start": 1156.2, "end": 1160.2, "text": " A ta praca jest chyba pierwszym gigantycznym krokiem w tym w\u0142a\u015bnie kierunku.", "tokens": [50364, 316, 1846, 582, 6628, 3492, 31532, 34016, 76, 8741, 394, 17466, 12996, 45909, 26116, 261, 8107, 14234, 38767, 49910, 13, 50564], "temperature": 0.0, "avg_logprob": -0.056071478387583855, "compression_ratio": 0.975, "no_speech_prob": 0.1556806117296219}], "language": "pl"}