{"text": " Witajcie. Mamy dzisiaj na stole problem, kt\u00f3ry chyba sp\u0119dza sen z powiek ca\u0142ej bran\u017cy technologicznej. Chyba nie ma w tym przesady. No w\u0142a\u015bnie, bo z jednej strony mamy te gigantyczne modele j\u0119zykowe jak GPT-3, kt\u00f3re, no, kompletnie zmieni\u0142y zasady gry. Ale z drugiej ich dalsze skalowanie jakby uderzy\u0142o w mur, taki mur zbudowany z pieni\u0119dzy i, co wa\u017cniejsze, z energii. Zdecydowanie. Ka\u017cda kolejna, pot\u0119\u017cniejsza wersja wymaga zasob\u00f3w por\u00f3wnywalnych z zasileniem ma\u0142ego miasta. I pojawia si\u0119 pytanie, czy to ju\u017c koniec, czy jest jaki\u015b m\u0105drzejszy spos\u00f3b na budowanie AI bez budowania dla niej osobnej elektrowni? To pytanie jest idealnym punktem wy\u015bcia. W\u0142a\u015bnie dlatego dzisiaj bierzemy na warsztat artyku\u0142, kt\u00f3ry, no, w momencie publikacji wywo\u0142a\u0142 prawdziw\u0105 burz\u0119. A konkretnie chodzi o prac\u0119 przedstawiaj\u0105c\u0105 rodzin\u0119 modeli o nazwie GLAM, czyli Generalist Language Model. I obietnica, kt\u00f3r\u0105 sk\u0142adaj\u0105 tw\u00f3rcy, jest, no, niezwykle kusz\u0105ca. Czyli osi\u0105gi na poziomie absolutnej \u015bwiatowej czo\u0142\u00f3wki, ale przy u\u0142amku koszt\u00f3w. I to zar\u00f3wno tych obliczeniowych, jak i energetycznych. Wi\u0119c naszym celem jest zrozumie\u0107, na czym polega ta innowacja. Sprawdzi\u0107, czy te dowody faktycznie si\u0119 broni\u0105. Dok\u0142adnie. I co to wszystko oznacza dla przysz\u0142o\u015bci? Czy to nowa droga, czy mo\u017ce tylko taka, wiesz, ciekawa, ale \u015blepa uliczka. Dobra, rozpakujmy to. \u017beby zrozumie\u0107, co GLAM robi inaczej, musimy chyba najpierw dotkn\u0105\u0107 sedna problemu z tymi dotychczasowymi gigantami, jak GPT-Free. One s\u0105 okre\u015blane jako g\u0119ste, czyli dense. Jak to najpro\u015bciej zobrazowa\u0107? Wyobra\u017cmy sobie taki model jako jednego, genialnego eksperta od wszystkiego. Nazwijmy go mo\u017ce panem g\u0119stym. Pan g\u0119sty, podoba mi si\u0119? Pytasz go o astrofizyk\u0119, histori\u0119 renesansu, przepis na ciasto on zna odpowied\u017a. Ale, i to jest kluczowe, ma jedn\u0105 fundamentaln\u0105 wad\u0119. Jak\u0105? Niewa\u017cne, czy zadajesz mu pytanie o sens \u017cycia, czy prosisz o dodanie dwa do dw\u00f3ch. Pan g\u0119sty musi zaanga\u017cowa\u0107 ka\u017cd\u0105 kom\u00f3rk\u0119 swojego m\u00f3zgu. Ca\u0142a jego wiedza jest aktywowana do najprostszego zadania. Aha, czyli to jest pot\u0119\u017cne, ale absurdalnie nieefektywne. Dok\u0142adnie. I tu wchodzi GLAM, kt\u00f3ry jest przedstawicielem architektury Sparsely Activated, czyli rzadko aktywowanej. Czyli koniec z jednym przepracowanym geniuszem. W\u0142a\u015bnie, to jest podej\u015bcie oparty na koncepcji Mixture of Experts w skr\u00f3cie MOE. Zostaj\u0105c przy naszej analogii, GLAM to nie jeden ekspert, to ogromna korporacja ekspert\u00f3w. Okej. Masz tam setki specjalist\u00f3w od poezji, od paitona, od biologii molekularnej. Ale kluczowy jest tu nowy pracownik. Taka inteligentna recepcja. W technicznej terminologii to jest ta gating function. Tak. Dok\u0142adnie. Kiedy do modelu trafia jakie\u015b zadanie, powiedzmy jedno s\u0142owo, czyli token, ta recepcja w u\u0142amku sekundy decyduje. Dobra, do tego s\u0142owa, w tym kontek\u015bcie, potrzebuje naszego link wisty i speca od historii. Reszta zespo\u0142u mo\u017ce pi\u0107 kaw\u0119. Czyli sednem jest to, \u017ce chocia\u017c ca\u0142a ta korporacja wiedzy jest gigantyczna? To do wykonania konkretnej pracy anga\u017cowana jest tylko male\u0144ka wyspecjalizowana cz\u0119\u015b\u0107. To brzmi rewolucyjnie. Jak to wygl\u0105da w liczbach? Bo s\u0142ysza\u0142em, \u017ce one robi\u0105 najwi\u0119ksze wra\u017cenie. I s\u0142usznie. Najwi\u0119kszy model GLAM ma 1,2 biliona parametr\u00f3w. Parametry to wiesz, w takim du\u017cym uproszczeniu pokr\u0119t\u0142a i suwaki, kt\u00f3re model ustawia podczas treningu, \u017ceby si\u0119 uczy\u0107. To w nich jest ca\u0142a jego wiedza? Tak. I teraz por\u00f3wnajmy to z GPT-3, kt\u00f3ry wtedy by\u0142 gigantem, mia\u0142 175 miliard\u00f3w parametr\u00f3w. GLAM jest wi\u0119c prawie siedem razy wi\u0119kszy. Chwila, chwila. Czyli jest siedem razy wi\u0119kszy, a ma by\u0107 ta\u0144szy? To brzmi w bref intuicji. Gdzie jest haczyk? Haczek jest w\u0142a\u015bnie w tej leniwej aktywacji. Bo chocia\u017c GLAM ma ten 1,2 biliona parametr\u00f3w, to podczas przetwarzania jednego tokenu, aktywuje zaledwie 96,6 miliarda z nich. To jest jakie\u015b 8% ca\u0142o\u015bci? Dok\u0142adnie. Czyli masz do dyspozycji wiedz\u0119 z siedmym encyklopedii, ale w danym momencie czytasz tylko dwa najbardziej trafne akapity. St\u0105d to wydajno\u015b\u0107. Ok. Czyli oszcz\u0119dno\u015b\u0107 jest gigantyczna, ale to nic nie znaczy, je\u015bli wyniki s\u0105 gorsze. Jak GLAM wypada w realnych testach w por\u00f3wnaniu z GPT-3? Czy ta oszcz\u0119dno\u015b\u0107 nie odbywa si\u0119 kosztem jako\u015bci? To jest pytanie za milion dolar\u00f3w i autorze artyku\u0142u doskonale o tym wiedzieli. Dlatego zrobili bezpo\u015brednie por\u00f3wnanie, a wyniki s\u0105 no mia\u017cd\u017c\u0105ce. Zacznijmy od koszt\u00f3w. Dobra. Training GLAM-u zu\u017cy\u0142 oko\u0142o 1,3 energii potrzebnej do wytrenowania GPT-3. 1,3? Tak. M\u00f3wimy o setkach megawatogodzin r\u00f3\u017cnicy. A co z dzia\u0142aniem, czyli tak zwanym wnioskowaniem? Tutaj dzilem potrzebuje o prawie 50% mniej mocy obliczeniowej na ka\u017cdy token. A co to jest tam moc obliczeniowa? W artykule pojawia si\u0119 termin flops. Flops, czyli floating point operations per second? M\u00f3wi\u0105c po ludzku, to liczba operacji matematycznych, kt\u00f3re komputer musi wykona\u0107. Mniej flops oznacza, \u017ce model jest po prostu rzejszy do uruchomienia. Jasne. Dobrze, jest taniej, jest wydajniej, ale wracam do pytania o jako\u015b\u0107. Czy dzilem jest m\u0105drzejszy? Okazuje si\u0119, \u017ce tak. I to w spos\u00f3b znacz\u0105cy. Przetestowano go na 29 r\u00f3\u017cnych benchmarkach i \u015brednio uzyska\u0142 lepsze wyniki. W tych r\u00f3\u017cnych trybach, typu zero shot, one shot. Dok\u0142adnie. We wszystkich tych kategoriach dzilem okaza\u0142 si\u0119 lepsze od GPT-3. Jest jaki\u015b konkretny przyk\u0142ad, kt\u00f3ry szczeg\u00f3lnie pokazuje t\u0119 przewag\u0119? Co\u015b, co wiesz, naprawd\u0119 wbija w fotel? Zdecydowanie. Benchmark o nazwie trivia QA to jest taki test wiedzy og\u00f3lnej, pytania typu, kto by\u0142 drugim cz\u0142owiekiem na ksi\u0119\u017cycu. I tu wydarzy\u0142o si\u0119 co\u015b spektakularnego. Glam w trybie one shot, czyli po zobaczeniu tylko jednego przyk\u0142adu, osi\u0105gn\u0105\u0142 75-8% dok\u0142adno\u015bci. Wow. A jak wypad\u0142 GPT-3? I tu jest ca\u0142a magia. GPT-3, \u017ceby w og\u00f3le zbli\u017cy\u0107 si\u0119 do tego wyniku, potrzebowa\u0142 a\u017c 64 przyk\u0142ad\u00f3w, czyli fused. A\u017c tylu? Tak. A i tak uzyska\u0142 tylko 71-2%. Ale to nie wszystko. Wynik Glam pobi\u0142 nawet poprzedni najlepszy model w tym benchmarku, kt\u00f3ry by\u0142 specjalnie feintuned. Czyli dostrajany, przygotowywany wy\u0142\u0105cznie do tego jednego zadania. Dok\u0142adnie. By\u0142 dostrajany przez wiele godzin, tylko do trivia QA. Niesamowite. To tak jakby amator przyszed\u0142 i wygra\u0142 z zawodowcem na jego w\u0142asnym boisku. \u015apiona pojemno\u015b\u0107 Glamu, te wszystkie nieaktywne parametry, to nie jest martwy balast. Tylko gigantyczne rezerwo war wiedzy. Do kt\u00f3rego model potrafi si\u0119gn\u0105\u0107, kiedy trzeba. Dzi\u0119ki specjalizacji znajduje w\u0142a\u015bciw\u0105 informacj\u0119 znacznie precyzyjnie ni\u017c g\u0119sty model. To prowadzi mnie do naturalnego pytania. Czy za tym sukcesem stoi wy\u0142\u0105cznie tak genialna architektura, a co z paliwem, czyli danymi? Mo\u017ce po prostu zalali go lepszymi danymi i st\u0105d te wyniki? To jest kluczowa kwestia i na szcz\u0119\u015bcie autorze postanowili to sprawdzi\u0107. Przeprowadzili eksperyment, kt\u00f3ry moim zdaniem jest jedn\u0105 z najwa\u017cniejszych lekcji p\u0142yn\u0105cych z ca\u0142ej tej pracy. Czyli? Wzi\u0119li dwa identyczne, mniejsze modele Glamu. Jeden wytrenowali na gigantycznym, ale niefiltrowanym zbiorze danych z internetu oko\u0142o 7 bilion\u00f3w token\u00f3w. Potw\u00f3r. A drugi model dosta\u0142 znacznie mniejszy, ale starannie wyselekcjonowany, oczyszczony, w wysokiej jako\u015bci zbi\u00f3r. Mia\u0142 zaledwie 143 miliardy token\u00f3w. Chwila, chwila. To brzmi kompletnie wbrew intuicji. M\u00f3wisz, \u017ce jeden model dosta\u0142 prawie 50 razy mniej danych ni\u017c drugi. Dok\u0142adnie tak. No to z g\u00f3ry wiadomo kto powinien wygra\u0107. Ilo\u015b\u0107 kontrajako\u015b\u0107 te\u017c bym tak pomy\u015bla\u0142a, ale wynik by\u0142 jednoznaczny. Jako\u015b\u0107 wygra\u0142a przez knockout serio? Model trenowany na mniejszym, ale czystszym zbiorze osi\u0105gn\u0105\u0142 znacznie lepsze wyniki we wszystkich testowanych zadaniach. Wniosek jest pot\u0119\u017cny. Jako\u015b\u0107 danych jest wa\u017cniejsza ni\u017c ich ilo\u015b\u0107. Karmienie modelu \u015bmieciami prowadzi do \u015bmieciowych rezultat\u00f3w. To jest fascynuj\u0105ce, bo intuicja podpowiada\u0142aby, \u017ce przy tak gigantycznej skali model sam powinien odfiltrowa\u0107 te \u015bmieci. Dlaczego tak si\u0119 nie dzieje? Czy te \u015bmieciowe dane aktywnie go og\u0142upiaj\u0105? Dok\u0142adnie tak. Model uczy si\u0119 na wzorcach statystycznych. Je\u015bli w danych jest mn\u00f3stwo szumu, b\u0142\u0119d\u00f3w, teorii spiskowych, on to traktuje jako prawomocny sygna\u0142. Uczy si\u0119 tych nieprawid\u0142owych korelacji. Aha. To tak jakby\u015b pr\u00f3bowa\u0142a nauczy\u0107 si\u0119 j\u0119zyka obcego tylko z for\u00f3w internetowych pe\u0142nych b\u0142\u0119d\u00f3w. Nauczysz si\u0119 mn\u00f3stwa s\u0142\u00f3w, ale twoja gramatyka b\u0119dzie fatalna. Rozumiem. Czyste dane ucz\u0105 go o solidniejszych podstaw. W\u0142a\u015bnie, wi\u0119c co to wszystko oznacza? Mamy model ta\u0144szy w treningu, wyda\u0144niejszy w dzia\u0142aniu, z lekszymi wynikami, a do tego udowadnia, \u017ce kluczem jest kura tela danych. Brzmi jak rewolucja, ale zawsze gdy co\u015b brzmi zbyt dobrze, pojawia si\u0119 pytanie, gdzie jest haczyk? I s\u0142usznie, bo kompromis istnieje i jest bardzo istotny. Nie ma darmowych lunczy, nawet w AI. Mimo, \u017ce GLAM jest wyda\u0144niejszy podczas wnioskowania, jego ca\u0142kowita liczba parametr\u00f3w jest, jak ustalili\u015bmy, przeogromna. Co to oznacza w praktyce? To oznacza, \u017ce sam fakt za\u0142adowania tego 1,2 bilionowego potwora do pami\u0119ci i utrzymywanie go w gotowo\u015bci wymaga gigantycznych zasob\u00f3w sprz\u0119towych. Mo\u017cesz to wyja\u015bni\u0107 na jakim\u015b przyk\u0142adzie? Jasne. Pomy\u015bl o tym tak. Model g\u0119sty, jak GPT-3, to podr\u0119czna, ale bardzo gruba encyklopedia. Zmie\u015bcisz j\u0105 na jednym solidnym regale. GLAM to ca\u0142a biblioteka narodowa. Ok, to dobra analogia. Odpowied\u017a na ka\u017cde pytanie jest gdzie\u015b w \u015brodku, a wyspecjalizowany bibliotekarz, nasza gating function, znajdzie Ci j\u0105 b\u0142yskawicznie. Problem w tym, \u017ce musisz mie\u0107 budynek wielko\u015bci pa\u0142acu, \u017ceby w og\u00f3le t\u0119 bibliotek\u0119 zmie\u015bci\u0107. Czyli to rozwi\u0105zanie nie jest dla ka\u017cdego. Komu w takim razie najbardziej op\u0142aca si\u0119 budowa takiego pa\u0142acu? To jest architektura stworzona dla najwi\u0119kszych graczy. Dla firm, kt\u00f3re obs\u0142uguj\u0105 miliardy zapyta\u0144 dziennie. W takiej skali oszcz\u0119dno\u015b\u0107 na ka\u017cdym zapytaniu jest tak ogromna, \u017ce uzasadnia gigantyczny sta\u0142y koszt utrzymania infrastruktury. A dla mniejszej firmy? Dla startupu, kt\u00f3ry ma sporadyczny ruch, utrzymywanie takiego giganta w gotowo\u015bci by\u0142oby skralnie nieop\u0142acalne. Lepiej mie\u0107 mniejszy, g\u0119sty model, kt\u00f3ry mo\u017ce i jest wolniejszy, ale nie wymaga tak pot\u0119\u017cnej infrastruktury. Rozumiem, czyli to nie uniwersalny zamiennik, ale nowe, pot\u0119\u017cne narz\u0119dzie do zastosowa\u0144 na masow\u0105 skal\u0119. Zmienia zasady gry, ale nie dla wszystkich. Dok\u0142adnie. Podsumowuj\u0105c g\u0142\u00f3wna lekcja z Glam wydaje si\u0119 prosta. Wi\u0119cej nie zawsze znaczy lepiej, a przynajmniej nie wi\u0119cej wszystkiego naraz. Tak, zamiast budowa\u0107 coraz wi\u0119ksze monolityczne m\u00f3zgi, kt\u00f3re zu\u017cywaj\u0105 ca\u0142\u0105 energi\u0119 na ka\u017cde zadanie, przysz\u0142o\u015b\u0107 mo\u017ce le\u017cy\u0107 w inteligentnej specjalizacji. Leniwe obliczania, jak w tej architekturze MiX czy RoF Experts. Dok\u0142adnie. Gdzie do pracy anga\u017cowani s\u0105 tylko niezb\u0119dni specjali\u015bci. To mo\u017ce by\u0107 klucz do bardziej wydajnej i, co wa\u017cne, bardziej zr\u00f3wnowa\u017conej przysz\u0142o\u015bci AI. I to sk\u0142ania do pewnej intryguj\u0105cej my\u015bli na koniec? Tak. Skoro specjalizacja na tak niskim poziomie pojedynczych s\u0142\u00f3w przynosi tak spektakularne korzy\u015bci, to co by si\u0119 sta\u0142o, gdyby\u015bmy zastosowali t\u0105 sam\u0105 logik\u0119 na wy\u017cszym poziomie abstrakcji? Czyli? Co je\u015bli przysz\u0142o\u015b\u0107 AI to nie jeden wszechwiedz\u0105cy model, ale raczej spo\u0142eczno\u015b\u0107 mniejszych wyspecjalizowanych modeli, kt\u00f3re dynamicznie ze sob\u0105 wsp\u00f3\u0142pracuj\u0105? Model lekarz konsultuj\u0105cy si\u0119 z modelem prawnikiem. Dok\u0142adnie. \u017beby rozwi\u0105za\u0107 jaki\u015b skomplikowany problem. GIM pokazuje, \u017ce idea zespo\u0142u ekspert\u00f3w nawet na poziomie mikro dzia\u0142a. By\u0107 mo\u017ce to jest w\u0142a\u015bnie kierunek. Nie jeden super m\u00f3zg, ale ca\u0142e spo\u0142ecze\u0144stwo inteligentnych agent\u00f3w.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.2, "text": " Witajcie. Mamy dzisiaj na stole problem, kt\u00f3ry chyba sp\u0119dza sen z powiek ca\u0142ej bran\u017cy technologicznej.", "tokens": [50364, 42299, 47276, 13, 376, 7804, 25772, 1667, 16326, 1154, 11, 9913, 31532, 637, 6298, 2394, 3151, 710, 3388, 19487, 47631, 73, 12029, 7735, 1537, 1132, 17946, 11794, 13, 50774], "temperature": 0.0, "avg_logprob": -0.20606296763700596, "compression_ratio": 1.273170731707317, "no_speech_prob": 0.017669260501861572}, {"id": 1, "seek": 0, "start": 8.700000000000001, "end": 10.1, "text": " Chyba nie ma w tym przesady.", "tokens": [50799, 761, 28375, 2838, 463, 261, 8107, 6541, 279, 880, 13, 50869], "temperature": 0.0, "avg_logprob": -0.20606296763700596, "compression_ratio": 1.273170731707317, "no_speech_prob": 0.017669260501861572}, {"id": 2, "seek": 0, "start": 10.3, "end": 19.5, "text": " No w\u0142a\u015bnie, bo z jednej strony mamy te gigantyczne modele j\u0119zykowe jak GPT-3, kt\u00f3re, no, kompletnie zmieni\u0142y zasady gry.", "tokens": [50879, 883, 14234, 11, 748, 710, 5232, 11794, 32406, 17335, 535, 8741, 394, 17466, 716, 4391, 306, 49055, 74, 6880, 4207, 26039, 51, 12, 18, 11, 8864, 11, 572, 11, 5207, 14657, 2766, 17020, 35462, 6825, 26530, 880, 41974, 13, 51339], "temperature": 0.0, "avg_logprob": -0.20606296763700596, "compression_ratio": 1.273170731707317, "no_speech_prob": 0.017669260501861572}, {"id": 3, "seek": 1950, "start": 20.5, "end": 30.0, "text": " Ale z drugiej ich dalsze skalowanie jakby uderzy\u0142o w mur, taki mur zbudowany z pieni\u0119dzy i, co wa\u017cniejsze, z energii.", "tokens": [50414, 9366, 710, 47373, 1893, 274, 1124, 1381, 16890, 22028, 28976, 344, 1068, 1229, 5249, 261, 5257, 11, 20065, 5257, 710, 18281, 23341, 710, 26274, 49485, 741, 11, 598, 27777, 44258, 11, 710, 10575, 5597, 13, 50889], "temperature": 0.0, "avg_logprob": -0.12061205142881812, "compression_ratio": 1.4307692307692308, "no_speech_prob": 0.8794944882392883}, {"id": 4, "seek": 1950, "start": 30.5, "end": 38.5, "text": " Zdecydowanie. Ka\u017cda kolejna, pot\u0119\u017cniejsza wersja wymaga zasob\u00f3w por\u00f3wnywalnych z zasileniem ma\u0142ego miasta.", "tokens": [50914, 1176, 1479, 1344, 67, 22028, 13, 10988, 1427, 2675, 23749, 629, 11, 1847, 1274, 1427, 30295, 2394, 261, 433, 2938, 29764, 9286, 26530, 996, 3901, 1515, 812, 895, 27112, 304, 9399, 710, 710, 13353, 268, 4907, 463, 1221, 6308, 2752, 12468, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12061205142881812, "compression_ratio": 1.4307692307692308, "no_speech_prob": 0.8794944882392883}, {"id": 5, "seek": 1950, "start": 39.0, "end": 49.0, "text": " I pojawia si\u0119 pytanie, czy to ju\u017c koniec, czy jest jaki\u015b m\u0105drzejszy spos\u00f3b na budowanie AI bez budowania dla niej osobnej elektrowni?", "tokens": [51339, 286, 30655, 654, 3244, 36610, 11, 6430, 281, 10678, 5897, 35733, 11, 6430, 3492, 34721, 275, 18962, 13503, 73, 7706, 22904, 1667, 3265, 22028, 7318, 10782, 3265, 21308, 12285, 2838, 73, 41518, 11794, 26991, 81, 648, 72, 30, 51839], "temperature": 0.0, "avg_logprob": -0.12061205142881812, "compression_ratio": 1.4307692307692308, "no_speech_prob": 0.8794944882392883}, {"id": 6, "seek": 4900, "start": 49.0, "end": 59.0, "text": " To pytanie jest idealnym punktem wy\u015bcia. W\u0142a\u015bnie dlatego dzisiaj bierzemy na warsztat artyku\u0142, kt\u00f3ry, no, w momencie publikacji wywo\u0142a\u0142 prawdziw\u0105 burz\u0119.", "tokens": [50364, 1407, 36610, 3492, 7157, 12996, 39561, 443, 4628, 1788, 2755, 13, 343, 5024, 12221, 32205, 25772, 272, 34602, 3633, 1667, 13718, 2682, 267, 594, 874, 5279, 1221, 11, 9913, 11, 572, 11, 261, 40883, 11227, 1035, 13152, 4628, 6120, 5024, 1221, 41175, 3992, 86, 1611, 2779, 11052, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09912521750838668, "compression_ratio": 1.3611111111111112, "no_speech_prob": 0.006832783576101065}, {"id": 7, "seek": 4900, "start": 59.5, "end": 67.5, "text": " A konkretnie chodzi o prac\u0119 przedstawiaj\u0105c\u0105 rodzin\u0119 modeli o nazwie GLAM, czyli Generalist Language Model.", "tokens": [50889, 316, 36500, 2766, 23998, 277, 22404, 1274, 45616, 48125, 32557, 8685, 23584, 1274, 2316, 72, 277, 20151, 8699, 16225, 2865, 11, 16591, 6996, 468, 24445, 17105, 13, 51289], "temperature": 0.0, "avg_logprob": -0.09912521750838668, "compression_ratio": 1.3611111111111112, "no_speech_prob": 0.006832783576101065}, {"id": 8, "seek": 4900, "start": 68.0, "end": 73.5, "text": " I obietnica, kt\u00f3r\u0105 sk\u0142adaj\u0105 tw\u00f3rcy, jest, no, niezwykle kusz\u0105ca.", "tokens": [51314, 286, 1111, 1684, 32687, 11, 37415, 1110, 46217, 8555, 683, 15614, 1344, 11, 3492, 11, 572, 11, 33511, 9726, 14677, 350, 301, 8925, 496, 13, 51589], "temperature": 0.0, "avg_logprob": -0.09912521750838668, "compression_ratio": 1.3611111111111112, "no_speech_prob": 0.006832783576101065}, {"id": 9, "seek": 7350, "start": 73.5, "end": 80.0, "text": " Czyli osi\u0105gi na poziomie absolutnej \u015bwiatowej czo\u0142\u00f3wki, ale przy u\u0142amku koszt\u00f3w.", "tokens": [50364, 37099, 3003, 11404, 7834, 1667, 38503, 40120, 18757, 11794, 36425, 21091, 269, 4765, 1221, 3901, 2984, 11, 6775, 6501, 344, 20177, 5279, 19532, 2682, 3901, 13, 50689], "temperature": 0.0, "avg_logprob": -0.08185995596426504, "compression_ratio": 1.410071942446043, "no_speech_prob": 0.06362633407115936}, {"id": 10, "seek": 7350, "start": 80.5, "end": 85.0, "text": " I to zar\u00f3wno tych obliczeniowych, jak i energetycznych.", "tokens": [50714, 286, 281, 22675, 812, 20944, 15180, 1111, 1050, 42124, 19605, 11, 4207, 741, 2043, 847, 17466, 9399, 13, 50939], "temperature": 0.0, "avg_logprob": -0.08185995596426504, "compression_ratio": 1.410071942446043, "no_speech_prob": 0.06362633407115936}, {"id": 11, "seek": 7350, "start": 85.5, "end": 88.5, "text": " Wi\u0119c naszym celem jest zrozumie\u0107, na czym polega ta innowacja.", "tokens": [50964, 32508, 48094, 1769, 10386, 3492, 710, 27857, 449, 414, 2162, 11, 1667, 31466, 13208, 3680, 1846, 294, 3785, 23395, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08185995596426504, "compression_ratio": 1.410071942446043, "no_speech_prob": 0.06362633407115936}, {"id": 12, "seek": 7350, "start": 89.0, "end": 91.5, "text": " Sprawdzi\u0107, czy te dowody faktycznie si\u0119 broni\u0105.", "tokens": [51139, 1738, 15889, 28496, 11, 6430, 535, 9459, 843, 33647, 45586, 3244, 16586, 11404, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08185995596426504, "compression_ratio": 1.410071942446043, "no_speech_prob": 0.06362633407115936}, {"id": 13, "seek": 7350, "start": 92.0, "end": 95.5, "text": " Dok\u0142adnie. I co to wszystko oznacza dla przysz\u0142o\u015bci?", "tokens": [51289, 29768, 10358, 2766, 13, 286, 598, 281, 22607, 277, 22672, 326, 2394, 12285, 44018, 35059, 30, 51464], "temperature": 0.0, "avg_logprob": -0.08185995596426504, "compression_ratio": 1.410071942446043, "no_speech_prob": 0.06362633407115936}, {"id": 14, "seek": 7350, "start": 96.0, "end": 100.0, "text": " Czy to nowa droga, czy mo\u017ce tylko taka, wiesz, ciekawa, ale \u015blepa uliczka.", "tokens": [51489, 19832, 281, 586, 64, 3789, 3680, 11, 6430, 12034, 13219, 28017, 11, 261, 15347, 11, 46419, 10449, 11, 6775, 8299, 306, 4306, 344, 1050, 89, 2330, 13, 51689], "temperature": 0.0, "avg_logprob": -0.08185995596426504, "compression_ratio": 1.410071942446043, "no_speech_prob": 0.06362633407115936}, {"id": 15, "seek": 10000, "start": 100.0, "end": 101.0, "text": " Dobra, rozpakujmy to.", "tokens": [50364, 413, 24393, 11, 9544, 45944, 4579, 2226, 281, 13, 50414], "temperature": 0.0, "avg_logprob": -0.09021644820710142, "compression_ratio": 1.406153846153846, "no_speech_prob": 0.00558449886739254}, {"id": 16, "seek": 10000, "start": 101.5, "end": 109.5, "text": " \u017beby zrozumie\u0107, co GLAM robi inaczej, musimy chyba najpierw dotkn\u0105\u0107 sedna problemu z tymi dotychczasowymi gigantami, jak GPT-Free.", "tokens": [50439, 46864, 2322, 710, 27857, 449, 414, 2162, 11, 598, 16225, 2865, 47380, 33230, 16920, 11, 43449, 31532, 11212, 45119, 86, 5893, 5457, 36374, 9643, 629, 1154, 84, 710, 1104, 3057, 5893, 16384, 30989, 10089, 3057, 8741, 394, 4526, 11, 4207, 26039, 51, 12, 45479, 13, 50839], "temperature": 0.0, "avg_logprob": -0.09021644820710142, "compression_ratio": 1.406153846153846, "no_speech_prob": 0.00558449886739254}, {"id": 17, "seek": 10000, "start": 110.0, "end": 114.5, "text": " One s\u0105 okre\u015blane jako g\u0119ste, czyli dense. Jak to najpro\u015bciej zobrazowa\u0107?", "tokens": [50864, 1485, 9015, 3133, 265, 19212, 1929, 17123, 290, 1274, 2941, 11, 16591, 18011, 13, 15029, 281, 11212, 4318, 9815, 73, 710, 24393, 89, 11445, 30, 51089], "temperature": 0.0, "avg_logprob": -0.09021644820710142, "compression_ratio": 1.406153846153846, "no_speech_prob": 0.00558449886739254}, {"id": 18, "seek": 10000, "start": 115.0, "end": 119.5, "text": " Wyobra\u017cmy sobie taki model jako jednego, genialnego eksperta od wszystkiego.", "tokens": [51114, 14458, 24393, 1427, 2226, 13652, 20065, 2316, 17123, 5232, 11858, 11, 48228, 11858, 30724, 610, 1328, 3611, 14615, 12200, 13, 51339], "temperature": 0.0, "avg_logprob": -0.09021644820710142, "compression_ratio": 1.406153846153846, "no_speech_prob": 0.00558449886739254}, {"id": 19, "seek": 10000, "start": 120.0, "end": 121.5, "text": " Nazwijmy go mo\u017ce panem g\u0119stym.", "tokens": [51364, 11870, 36652, 2226, 352, 12034, 2462, 443, 290, 1274, 372, 4199, 13, 51439], "temperature": 0.0, "avg_logprob": -0.09021644820710142, "compression_ratio": 1.406153846153846, "no_speech_prob": 0.00558449886739254}, {"id": 20, "seek": 10000, "start": 122.0, "end": 123.5, "text": " Pan g\u0119sty, podoba mi si\u0119?", "tokens": [51464, 7557, 290, 1274, 25134, 11, 2497, 19481, 2752, 3244, 30, 51539], "temperature": 0.0, "avg_logprob": -0.09021644820710142, "compression_ratio": 1.406153846153846, "no_speech_prob": 0.00558449886739254}, {"id": 21, "seek": 10000, "start": 124.0, "end": 129.0, "text": " Pytasz go o astrofizyk\u0119, histori\u0119 renesansu, przepis na ciasto on zna odpowied\u017a.", "tokens": [51564, 430, 4328, 19601, 352, 277, 5357, 340, 69, 590, 88, 15724, 11, 4058, 5034, 319, 4081, 599, 84, 11, 30829, 271, 1667, 6983, 33869, 322, 710, 629, 36574, 10659, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09021644820710142, "compression_ratio": 1.406153846153846, "no_speech_prob": 0.00558449886739254}, {"id": 22, "seek": 12900, "start": 129.0, "end": 133.5, "text": " Ale, i to jest kluczowe, ma jedn\u0105 fundamentaln\u0105 wad\u0119.", "tokens": [50364, 9366, 11, 741, 281, 3492, 9671, 1311, 89, 6880, 11, 463, 5232, 13113, 8088, 13113, 261, 345, 1274, 13, 50589], "temperature": 0.0, "avg_logprob": -0.09659563240252043, "compression_ratio": 1.4186046511627908, "no_speech_prob": 0.02611391805112362}, {"id": 23, "seek": 12900, "start": 134.0, "end": 134.5, "text": " Jak\u0105?", "tokens": [50614, 15029, 1611, 30, 50639], "temperature": 0.0, "avg_logprob": -0.09659563240252043, "compression_ratio": 1.4186046511627908, "no_speech_prob": 0.02611391805112362}, {"id": 24, "seek": 12900, "start": 135.0, "end": 139.5, "text": " Niewa\u017cne, czy zadajesz mu pytanie o sens \u017cycia, czy prosisz o dodanie dwa do dw\u00f3ch.", "tokens": [50664, 426, 27806, 716, 11, 6430, 710, 1538, 73, 10430, 2992, 36610, 277, 2923, 44343, 11, 6430, 6267, 23848, 277, 13886, 7155, 35045, 360, 27379, 812, 339, 13, 50889], "temperature": 0.0, "avg_logprob": -0.09659563240252043, "compression_ratio": 1.4186046511627908, "no_speech_prob": 0.02611391805112362}, {"id": 25, "seek": 12900, "start": 140.0, "end": 143.5, "text": " Pan g\u0119sty musi zaanga\u017cowa\u0107 ka\u017cd\u0105 kom\u00f3rk\u0119 swojego m\u00f3zgu.", "tokens": [50914, 7557, 290, 1274, 25134, 37587, 7949, 656, 18264, 11445, 21912, 67, 1611, 5207, 15614, 15724, 13291, 39738, 32515, 89, 2794, 13, 51089], "temperature": 0.0, "avg_logprob": -0.09659563240252043, "compression_ratio": 1.4186046511627908, "no_speech_prob": 0.02611391805112362}, {"id": 26, "seek": 12900, "start": 144.0, "end": 147.5, "text": " Ca\u0142a jego wiedza jest aktywowana do najprostszego zadania.", "tokens": [51114, 7544, 5024, 26542, 46894, 2394, 3492, 9308, 874, 86, 40458, 360, 11212, 1424, 555, 15453, 6308, 42788, 5609, 13, 51289], "temperature": 0.0, "avg_logprob": -0.09659563240252043, "compression_ratio": 1.4186046511627908, "no_speech_prob": 0.02611391805112362}, {"id": 27, "seek": 12900, "start": 148.0, "end": 152.0, "text": " Aha, czyli to jest pot\u0119\u017cne, ale absurdalnie nieefektywne.", "tokens": [51314, 27448, 11, 16591, 281, 3492, 1847, 1274, 1427, 716, 11, 6775, 19774, 304, 2766, 2838, 5666, 916, 874, 86, 716, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09659563240252043, "compression_ratio": 1.4186046511627908, "no_speech_prob": 0.02611391805112362}, {"id": 28, "seek": 12900, "start": 152.5, "end": 158.0, "text": " Dok\u0142adnie. I tu wchodzi GLAM, kt\u00f3ry jest przedstawicielem architektury Sparsely Activated,", "tokens": [51539, 29768, 10358, 2766, 13, 286, 2604, 261, 34616, 16225, 2865, 11, 9913, 3492, 45616, 28434, 10386, 3912, 642, 2320, 2598, 1738, 685, 736, 28550, 770, 11, 51814], "temperature": 0.0, "avg_logprob": -0.09659563240252043, "compression_ratio": 1.4186046511627908, "no_speech_prob": 0.02611391805112362}, {"id": 29, "seek": 15800, "start": 158.0, "end": 160.0, "text": " czyli rzadko aktywowanej.", "tokens": [50364, 16591, 367, 89, 345, 4093, 9308, 874, 86, 23066, 73, 13, 50464], "temperature": 0.0, "avg_logprob": -0.11855172457760327, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.020983707159757614}, {"id": 30, "seek": 15800, "start": 160.5, "end": 163.0, "text": " Czyli koniec z jednym przepracowanym geniuszem.", "tokens": [50489, 37099, 5897, 35733, 710, 5232, 12996, 30829, 12080, 23341, 76, 14017, 24313, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11855172457760327, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.020983707159757614}, {"id": 31, "seek": 15800, "start": 163.5, "end": 168.0, "text": " W\u0142a\u015bnie, to jest podej\u015bcie oparty na koncepcji Mixture of Experts w skr\u00f3cie MOE.", "tokens": [50639, 343, 5024, 12221, 11, 281, 3492, 7468, 73, 9815, 999, 446, 88, 1667, 5897, 27493, 19649, 10204, 8890, 295, 12522, 1373, 261, 1110, 11721, 4260, 19290, 36, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11855172457760327, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.020983707159757614}, {"id": 32, "seek": 15800, "start": 168.5, "end": 174.0, "text": " Zostaj\u0105c przy naszej analogii, GLAM to nie jeden ekspert, to ogromna korporacja ekspert\u00f3w.", "tokens": [50889, 1176, 555, 38757, 6501, 42946, 16660, 5597, 11, 16225, 2865, 281, 2838, 12906, 30724, 15346, 11, 281, 34416, 298, 629, 14784, 2816, 23395, 30724, 15346, 3901, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11855172457760327, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.020983707159757614}, {"id": 33, "seek": 15800, "start": 174.5, "end": 175.0, "text": " Okej.", "tokens": [51189, 29094, 73, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11855172457760327, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.020983707159757614}, {"id": 34, "seek": 15800, "start": 175.5, "end": 180.0, "text": " Masz tam setki specjalist\u00f3w od poezji, od paitona, od biologii molekularnej.", "tokens": [51239, 5224, 89, 7677, 992, 2984, 46433, 468, 3901, 3611, 714, 4371, 4013, 11, 3611, 280, 1001, 4037, 11, 3611, 3228, 1132, 5597, 6353, 74, 1040, 11794, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11855172457760327, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.020983707159757614}, {"id": 35, "seek": 15800, "start": 180.5, "end": 184.5, "text": " Ale kluczowy jest tu nowy pracownik. Taka inteligentna recepcja.", "tokens": [51489, 9366, 9671, 1311, 89, 10089, 3492, 2604, 586, 88, 22404, 44895, 13, 314, 7849, 24777, 25002, 629, 2268, 79, 34056, 13, 51689], "temperature": 0.0, "avg_logprob": -0.11855172457760327, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.020983707159757614}, {"id": 36, "seek": 18450, "start": 184.5, "end": 189.0, "text": " W technicznej terminologii to jest ta gating function. Tak. Dok\u0142adnie.", "tokens": [50364, 343, 1537, 17946, 11794, 10761, 1132, 5597, 281, 3492, 1846, 290, 990, 2445, 13, 9118, 13, 29768, 10358, 2766, 13, 50589], "temperature": 0.0, "avg_logprob": -0.10284231893671383, "compression_ratio": 1.489296636085627, "no_speech_prob": 0.023972200229763985}, {"id": 37, "seek": 18450, "start": 189.5, "end": 193.5, "text": " Kiedy do modelu trafia jakie\u015b zadanie, powiedzmy jedno s\u0142owo, czyli token,", "tokens": [50614, 591, 16446, 360, 2316, 84, 944, 22054, 31163, 42788, 7155, 11, 27617, 2226, 5232, 1771, 15116, 19941, 11, 16591, 14862, 11, 50814], "temperature": 0.0, "avg_logprob": -0.10284231893671383, "compression_ratio": 1.489296636085627, "no_speech_prob": 0.023972200229763985}, {"id": 38, "seek": 18450, "start": 194.0, "end": 196.0, "text": " ta recepcja w u\u0142amku sekundy decyduje.", "tokens": [50839, 1846, 2268, 79, 34056, 261, 344, 20177, 5279, 17215, 49996, 979, 88, 769, 2884, 13, 50939], "temperature": 0.0, "avg_logprob": -0.10284231893671383, "compression_ratio": 1.489296636085627, "no_speech_prob": 0.023972200229763985}, {"id": 39, "seek": 18450, "start": 196.5, "end": 201.0, "text": " Dobra, do tego s\u0142owa, w tym kontek\u015bcie, potrzebuje naszego link wisty i speca od historii.", "tokens": [50964, 413, 24393, 11, 360, 8627, 15116, 5528, 11, 261, 8107, 14373, 916, 9815, 11, 28577, 6021, 2884, 44517, 2113, 261, 38618, 741, 768, 496, 3611, 4058, 5597, 13, 51189], "temperature": 0.0, "avg_logprob": -0.10284231893671383, "compression_ratio": 1.489296636085627, "no_speech_prob": 0.023972200229763985}, {"id": 40, "seek": 18450, "start": 201.5, "end": 203.0, "text": " Reszta zespo\u0142u mo\u017ce pi\u0107 kaw\u0119.", "tokens": [51214, 5015, 89, 1328, 710, 279, 2259, 24066, 12034, 3895, 2162, 350, 1607, 1274, 13, 51289], "temperature": 0.0, "avg_logprob": -0.10284231893671383, "compression_ratio": 1.489296636085627, "no_speech_prob": 0.023972200229763985}, {"id": 41, "seek": 18450, "start": 203.5, "end": 207.5, "text": " Czyli sednem jest to, \u017ce chocia\u017c ca\u0142a ta korporacja wiedzy jest gigantyczna?", "tokens": [51314, 37099, 9643, 25989, 3492, 281, 11, 3561, 48929, 1335, 5024, 1846, 14784, 2816, 23395, 46894, 1229, 3492, 8741, 394, 17466, 629, 30, 51514], "temperature": 0.0, "avg_logprob": -0.10284231893671383, "compression_ratio": 1.489296636085627, "no_speech_prob": 0.023972200229763985}, {"id": 42, "seek": 18450, "start": 208.0, "end": 213.5, "text": " To do wykonania konkretnej pracy anga\u017cowana jest tylko male\u0144ka wyspecjalizowana cz\u0119\u015b\u0107.", "tokens": [51539, 1407, 360, 46702, 5609, 36500, 11794, 35591, 2562, 18264, 40458, 3492, 13219, 7133, 5248, 2330, 27062, 494, 66, 22600, 590, 40458, 47149, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10284231893671383, "compression_ratio": 1.489296636085627, "no_speech_prob": 0.023972200229763985}, {"id": 43, "seek": 21350, "start": 213.5, "end": 219.0, "text": " To brzmi rewolucyjnie. Jak to wygl\u0105da w liczbach? Bo s\u0142ysza\u0142em, \u017ce one robi\u0105 najwi\u0119ksze wra\u017cenie.", "tokens": [50364, 1407, 738, 89, 3057, 319, 48481, 1311, 88, 73, 2766, 13, 15029, 281, 32015, 261, 6169, 89, 32096, 30, 3286, 15116, 749, 2394, 11126, 11, 3561, 472, 3870, 11404, 48636, 1694, 1381, 7843, 41118, 13, 50639], "temperature": 0.0, "avg_logprob": -0.09013129535474275, "compression_ratio": 1.4078947368421053, "no_speech_prob": 0.007826131768524647}, {"id": 44, "seek": 21350, "start": 219.5, "end": 224.5, "text": " I s\u0142usznie. Najwi\u0119kszy model GLAM ma 1,2 biliona parametr\u00f3w.", "tokens": [50664, 286, 15116, 22378, 2766, 13, 31576, 22423, 1694, 1229, 2316, 16225, 2865, 463, 502, 11, 17, 8588, 21758, 6220, 27965, 3901, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09013129535474275, "compression_ratio": 1.4078947368421053, "no_speech_prob": 0.007826131768524647}, {"id": 45, "seek": 21350, "start": 225.0, "end": 229.5, "text": " Parametry to wiesz, w takim du\u017cym uproszczeniu pokr\u0119t\u0142a i suwaki,", "tokens": [50939, 34882, 9889, 281, 261, 15347, 11, 261, 31732, 21783, 4199, 493, 2635, 89, 66, 39651, 13010, 81, 46788, 5024, 741, 459, 86, 7421, 11, 51164], "temperature": 0.0, "avg_logprob": -0.09013129535474275, "compression_ratio": 1.4078947368421053, "no_speech_prob": 0.007826131768524647}, {"id": 46, "seek": 21350, "start": 230.0, "end": 232.0, "text": " kt\u00f3re model ustawia podczas treningu, \u017ceby si\u0119 uczy\u0107.", "tokens": [51189, 8864, 2316, 26189, 34953, 2497, 30989, 2192, 773, 84, 11, 11316, 3244, 344, 33967, 13, 51289], "temperature": 0.0, "avg_logprob": -0.09013129535474275, "compression_ratio": 1.4078947368421053, "no_speech_prob": 0.007826131768524647}, {"id": 47, "seek": 21350, "start": 232.5, "end": 234.0, "text": " To w nich jest ca\u0142a jego wiedza?", "tokens": [51314, 1407, 261, 25570, 3492, 1335, 5024, 26542, 46894, 2394, 30, 51389], "temperature": 0.0, "avg_logprob": -0.09013129535474275, "compression_ratio": 1.4078947368421053, "no_speech_prob": 0.007826131768524647}, {"id": 48, "seek": 21350, "start": 234.5, "end": 242.0, "text": " Tak. I teraz por\u00f3wnajmy to z GPT-3, kt\u00f3ry wtedy by\u0142 gigantem, mia\u0142 175 miliard\u00f3w parametr\u00f3w.", "tokens": [51414, 9118, 13, 286, 16854, 1515, 3901, 20981, 2226, 281, 710, 26039, 51, 12, 18, 11, 9913, 26959, 16673, 8741, 394, 443, 11, 27989, 41165, 1962, 72, 515, 3901, 6220, 27965, 3901, 13, 51789], "temperature": 0.0, "avg_logprob": -0.09013129535474275, "compression_ratio": 1.4078947368421053, "no_speech_prob": 0.007826131768524647}, {"id": 49, "seek": 24200, "start": 242.0, "end": 245.5, "text": " GLAM jest wi\u0119c prawie siedem razy wi\u0119kszy.", "tokens": [50364, 16225, 2865, 3492, 16677, 3206, 8699, 262, 1091, 443, 9639, 88, 29968, 1229, 13, 50539], "temperature": 0.0, "avg_logprob": -0.1046598025730678, "compression_ratio": 1.4, "no_speech_prob": 0.010223382152616978}, {"id": 50, "seek": 24200, "start": 246.0, "end": 250.5, "text": " Chwila, chwila. Czyli jest siedem razy wi\u0119kszy, a ma by\u0107 ta\u0144szy?", "tokens": [50564, 761, 86, 7371, 11, 26237, 7371, 13, 37099, 3492, 262, 1091, 443, 9639, 88, 29968, 1229, 11, 257, 463, 15069, 1846, 5248, 7706, 30, 50789], "temperature": 0.0, "avg_logprob": -0.1046598025730678, "compression_ratio": 1.4, "no_speech_prob": 0.010223382152616978}, {"id": 51, "seek": 24200, "start": 251.0, "end": 254.0, "text": " To brzmi w bref intuicji. Gdzie jest haczyk?", "tokens": [50814, 1407, 738, 89, 3057, 261, 1403, 69, 560, 84, 299, 4013, 13, 460, 13096, 3492, 324, 6522, 74, 30, 50964], "temperature": 0.0, "avg_logprob": -0.1046598025730678, "compression_ratio": 1.4, "no_speech_prob": 0.010223382152616978}, {"id": 52, "seek": 24200, "start": 254.5, "end": 257.5, "text": " Haczek jest w\u0142a\u015bnie w tej leniwej aktywacji.", "tokens": [50989, 389, 14875, 916, 3492, 14234, 261, 12573, 287, 15711, 826, 73, 9308, 874, 86, 13152, 13, 51139], "temperature": 0.0, "avg_logprob": -0.1046598025730678, "compression_ratio": 1.4, "no_speech_prob": 0.010223382152616978}, {"id": 53, "seek": 24200, "start": 258.0, "end": 264.0, "text": " Bo chocia\u017c GLAM ma ten 1,2 biliona parametr\u00f3w, to podczas przetwarzania jednego tokenu,", "tokens": [51164, 3286, 48929, 16225, 2865, 463, 2064, 502, 11, 17, 8588, 21758, 6220, 27965, 3901, 11, 281, 2497, 30989, 6541, 302, 31991, 5609, 5232, 11858, 14862, 84, 11, 51464], "temperature": 0.0, "avg_logprob": -0.1046598025730678, "compression_ratio": 1.4, "no_speech_prob": 0.010223382152616978}, {"id": 54, "seek": 24200, "start": 264.5, "end": 268.5, "text": " aktywuje zaledwie 96,6 miliarda z nich.", "tokens": [51489, 9308, 874, 86, 13008, 710, 5573, 8699, 24124, 11, 21, 1962, 72, 19218, 710, 25570, 13, 51689], "temperature": 0.0, "avg_logprob": -0.1046598025730678, "compression_ratio": 1.4, "no_speech_prob": 0.010223382152616978}, {"id": 55, "seek": 24200, "start": 269.0, "end": 270.5, "text": " To jest jakie\u015b 8% ca\u0142o\u015bci?", "tokens": [51714, 1407, 3492, 31163, 1649, 4, 1335, 35059, 30, 51789], "temperature": 0.0, "avg_logprob": -0.1046598025730678, "compression_ratio": 1.4, "no_speech_prob": 0.010223382152616978}, {"id": 56, "seek": 27050, "start": 270.5, "end": 275.0, "text": " Dok\u0142adnie. Czyli masz do dyspozycji wiedz\u0119 z siedmym encyklopedii,", "tokens": [50364, 29768, 10358, 2766, 13, 37099, 2300, 89, 360, 15243, 2259, 1229, 19649, 46894, 11052, 710, 262, 1091, 2226, 76, 465, 1344, 7837, 27277, 5597, 11, 50589], "temperature": 0.0, "avg_logprob": -0.08290019916121368, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.018747352063655853}, {"id": 57, "seek": 27050, "start": 275.5, "end": 279.0, "text": " ale w danym momencie czytasz tylko dwa najbardziej trafne akapity.", "tokens": [50614, 6775, 261, 274, 1325, 76, 40883, 6430, 83, 19601, 13219, 35045, 41857, 944, 69, 716, 9308, 569, 507, 13, 50789], "temperature": 0.0, "avg_logprob": -0.08290019916121368, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.018747352063655853}, {"id": 58, "seek": 27050, "start": 279.5, "end": 280.5, "text": " St\u0105d to wydajno\u015b\u0107.", "tokens": [50814, 745, 18962, 281, 25984, 1805, 23293, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08290019916121368, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.018747352063655853}, {"id": 59, "seek": 27050, "start": 281.0, "end": 286.5, "text": " Ok. Czyli oszcz\u0119dno\u015b\u0107 jest gigantyczna, ale to nic nie znaczy, je\u015bli wyniki s\u0105 gorsze.", "tokens": [50889, 3477, 13, 37099, 3003, 43771, 6298, 23293, 3492, 8741, 394, 17466, 629, 11, 6775, 281, 6201, 2838, 36584, 11, 25630, 31936, 9850, 9015, 290, 830, 1381, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08290019916121368, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.018747352063655853}, {"id": 60, "seek": 27050, "start": 287.0, "end": 291.0, "text": " Jak GLAM wypada w realnych testach w por\u00f3wnaniu z GPT-3?", "tokens": [51189, 15029, 16225, 2865, 46392, 1538, 261, 957, 9399, 1500, 608, 261, 1515, 812, 895, 25849, 710, 26039, 51, 12, 18, 30, 51389], "temperature": 0.0, "avg_logprob": -0.08290019916121368, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.018747352063655853}, {"id": 61, "seek": 27050, "start": 291.5, "end": 294.0, "text": " Czy ta oszcz\u0119dno\u015b\u0107 nie odbywa si\u0119 kosztem jako\u015bci?", "tokens": [51414, 19832, 1846, 3003, 43771, 6298, 23293, 2838, 3611, 2322, 4151, 3244, 19532, 2682, 443, 17123, 6199, 30, 51539], "temperature": 0.0, "avg_logprob": -0.08290019916121368, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.018747352063655853}, {"id": 62, "seek": 27050, "start": 294.5, "end": 298.5, "text": " To jest pytanie za milion dolar\u00f3w i autorze artyku\u0142u doskonale o tym wiedzieli.", "tokens": [51564, 1407, 3492, 36610, 7949, 1962, 313, 360, 2200, 3901, 741, 19510, 1381, 594, 874, 5279, 24066, 4491, 18295, 1220, 277, 8107, 261, 15338, 23099, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08290019916121368, "compression_ratio": 1.435483870967742, "no_speech_prob": 0.018747352063655853}, {"id": 63, "seek": 29850, "start": 298.5, "end": 303.0, "text": " Dlatego zrobili bezpo\u015brednie por\u00f3wnanie, a wyniki s\u0105 no mia\u017cd\u017c\u0105ce.", "tokens": [50364, 47184, 44399, 2312, 10782, 2259, 1788, 986, 2766, 1515, 812, 895, 7155, 11, 257, 31936, 9850, 9015, 572, 21290, 1427, 67, 1427, 1611, 384, 13, 50589], "temperature": 0.0, "avg_logprob": -0.10049555711685472, "compression_ratio": 1.3711340206185567, "no_speech_prob": 0.030738553032279015}, {"id": 64, "seek": 29850, "start": 303.5, "end": 304.5, "text": " Zacznijmy od koszt\u00f3w.", "tokens": [50614, 1176, 14875, 77, 1718, 2226, 3611, 19532, 2682, 3901, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10049555711685472, "compression_ratio": 1.3711340206185567, "no_speech_prob": 0.030738553032279015}, {"id": 65, "seek": 29850, "start": 305.0, "end": 310.5, "text": " Dobra. Training GLAM-u zu\u017cy\u0142 oko\u0142o 1,3 energii potrzebnej do wytrenowania GPT-3.", "tokens": [50689, 413, 24393, 13, 20620, 16225, 2865, 12, 84, 710, 84, 7735, 1221, 45730, 5249, 502, 11, 18, 10575, 5597, 37595, 11794, 360, 261, 4328, 1095, 21308, 26039, 51, 12, 18, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10049555711685472, "compression_ratio": 1.3711340206185567, "no_speech_prob": 0.030738553032279015}, {"id": 66, "seek": 29850, "start": 311.0, "end": 312.0, "text": " 1,3?", "tokens": [50989, 502, 11, 18, 30, 51039], "temperature": 0.0, "avg_logprob": -0.10049555711685472, "compression_ratio": 1.3711340206185567, "no_speech_prob": 0.030738553032279015}, {"id": 67, "seek": 29850, "start": 312.5, "end": 315.5, "text": " Tak. M\u00f3wimy o setkach megawatogodzin r\u00f3\u017cnicy.", "tokens": [51064, 9118, 13, 376, 3901, 13189, 277, 992, 41326, 10816, 1607, 267, 664, 378, 23584, 19637, 77, 2632, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10049555711685472, "compression_ratio": 1.3711340206185567, "no_speech_prob": 0.030738553032279015}, {"id": 68, "seek": 29850, "start": 316.0, "end": 319.0, "text": " A co z dzia\u0142aniem, czyli tak zwanym wnioskowaniem?", "tokens": [51239, 316, 598, 710, 27121, 282, 4907, 11, 16591, 991, 11873, 1325, 76, 45368, 2717, 74, 37345, 4907, 30, 51389], "temperature": 0.0, "avg_logprob": -0.10049555711685472, "compression_ratio": 1.3711340206185567, "no_speech_prob": 0.030738553032279015}, {"id": 69, "seek": 29850, "start": 319.5, "end": 325.0, "text": " Tutaj dzilem potrzebuje o prawie 50% mniej mocy obliczeniowej na ka\u017cdy token.", "tokens": [51414, 41819, 9758, 794, 76, 28577, 6021, 2884, 277, 3206, 8699, 2625, 4, 39513, 705, 1344, 1111, 1050, 42124, 21091, 1667, 31615, 14862, 13, 51689], "temperature": 0.0, "avg_logprob": -0.10049555711685472, "compression_ratio": 1.3711340206185567, "no_speech_prob": 0.030738553032279015}, {"id": 70, "seek": 29850, "start": 325.5, "end": 327.5, "text": " A co to jest tam moc obliczeniowa?", "tokens": [51714, 316, 598, 281, 3492, 7677, 34962, 1111, 1050, 42124, 5528, 30, 51814], "temperature": 0.0, "avg_logprob": -0.10049555711685472, "compression_ratio": 1.3711340206185567, "no_speech_prob": 0.030738553032279015}, {"id": 71, "seek": 32850, "start": 329.5, "end": 332.5, "text": " W artykule pojawia si\u0119 termin flops.", "tokens": [50414, 343, 594, 874, 74, 2271, 30655, 654, 3244, 10761, 932, 3370, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09740273495937915, "compression_ratio": 1.4424460431654675, "no_speech_prob": 0.002000025473535061}, {"id": 72, "seek": 32850, "start": 333.0, "end": 336.5, "text": " Flops, czyli floating point operations per second?", "tokens": [50589, 3235, 3370, 11, 16591, 12607, 935, 7705, 680, 1150, 30, 50764], "temperature": 0.0, "avg_logprob": -0.09740273495937915, "compression_ratio": 1.4424460431654675, "no_speech_prob": 0.002000025473535061}, {"id": 73, "seek": 32850, "start": 337.0, "end": 341.0, "text": " M\u00f3wi\u0105c po ludzku, to liczba operacji matematycznych, kt\u00f3re komputer musi wykona\u0107.", "tokens": [50789, 376, 3901, 11404, 66, 714, 15946, 89, 5279, 11, 281, 6169, 89, 4231, 2208, 13152, 3803, 8615, 17466, 9399, 11, 8864, 5207, 13849, 37587, 39287, 4037, 2162, 13, 50989], "temperature": 0.0, "avg_logprob": -0.09740273495937915, "compression_ratio": 1.4424460431654675, "no_speech_prob": 0.002000025473535061}, {"id": 74, "seek": 32850, "start": 341.5, "end": 346.0, "text": " Mniej flops oznacza, \u017ce model jest po prostu rzejszy do uruchomienia.", "tokens": [51014, 376, 10402, 932, 3370, 277, 22672, 326, 2394, 11, 3561, 2316, 3492, 714, 19518, 367, 16920, 7706, 360, 4038, 625, 298, 18811, 13, 51239], "temperature": 0.0, "avg_logprob": -0.09740273495937915, "compression_ratio": 1.4424460431654675, "no_speech_prob": 0.002000025473535061}, {"id": 75, "seek": 32850, "start": 346.5, "end": 352.0, "text": " Jasne. Dobrze, jest taniej, jest wydajniej, ale wracam do pytania o jako\u015b\u0107.", "tokens": [51264, 34023, 716, 13, 29679, 13503, 11, 3492, 256, 7155, 73, 11, 3492, 25984, 1805, 10402, 11, 6775, 928, 47190, 360, 25878, 5609, 277, 17123, 7753, 13, 51539], "temperature": 0.0, "avg_logprob": -0.09740273495937915, "compression_ratio": 1.4424460431654675, "no_speech_prob": 0.002000025473535061}, {"id": 76, "seek": 32850, "start": 352.5, "end": 354.5, "text": " Czy dzilem jest m\u0105drzejszy?", "tokens": [51564, 19832, 9758, 794, 76, 3492, 275, 18962, 13503, 73, 7706, 30, 51664], "temperature": 0.0, "avg_logprob": -0.09740273495937915, "compression_ratio": 1.4424460431654675, "no_speech_prob": 0.002000025473535061}, {"id": 77, "seek": 32850, "start": 355.0, "end": 358.0, "text": " Okazuje si\u0119, \u017ce tak. I to w spos\u00f3b znacz\u0105cy.", "tokens": [51689, 3477, 43317, 3244, 11, 3561, 991, 13, 286, 281, 261, 22904, 15397, 326, 8925, 1344, 13, 51839], "temperature": 0.0, "avg_logprob": -0.09740273495937915, "compression_ratio": 1.4424460431654675, "no_speech_prob": 0.002000025473535061}, {"id": 78, "seek": 35850, "start": 358.5, "end": 363.5, "text": " Przetestowano go na 29 r\u00f3\u017cnych benchmarkach i \u015brednio uzyska\u0142 lepsze wyniki.", "tokens": [50364, 2114, 40399, 377, 305, 3730, 352, 1667, 9413, 42602, 18927, 608, 741, 8299, 986, 41084, 16851, 749, 2330, 1221, 476, 1878, 1381, 31936, 9850, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10672698830658535, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.0009022597805596888}, {"id": 79, "seek": 35850, "start": 364.0, "end": 366.5, "text": " W tych r\u00f3\u017cnych trybach, typu zero shot, one shot.", "tokens": [50639, 343, 15180, 42602, 853, 32096, 11, 2125, 84, 4018, 3347, 11, 472, 3347, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10672698830658535, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.0009022597805596888}, {"id": 80, "seek": 35850, "start": 367.0, "end": 367.5, "text": " Dok\u0142adnie.", "tokens": [50789, 29768, 10358, 2766, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10672698830658535, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.0009022597805596888}, {"id": 81, "seek": 35850, "start": 368.0, "end": 371.5, "text": " We wszystkich tych kategoriach dzilem okaza\u0142 si\u0119 lepsze od GPT-3.", "tokens": [50839, 492, 34234, 15180, 350, 2968, 7386, 608, 9758, 794, 76, 3133, 12257, 1221, 3244, 476, 1878, 1381, 3611, 26039, 51, 12, 18, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10672698830658535, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.0009022597805596888}, {"id": 82, "seek": 35850, "start": 372.0, "end": 375.5, "text": " Jest jaki\u015b konkretny przyk\u0142ad, kt\u00f3ry szczeg\u00f3lnie pokazuje t\u0119 przewag\u0119?", "tokens": [51039, 24918, 34721, 36500, 1634, 23144, 11, 9913, 49624, 2766, 13010, 43317, 32489, 39758, 40748, 30, 51214], "temperature": 0.0, "avg_logprob": -0.10672698830658535, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.0009022597805596888}, {"id": 83, "seek": 35850, "start": 376.0, "end": 378.0, "text": " Co\u015b, co wiesz, naprawd\u0119 wbija w fotel?", "tokens": [51239, 3066, 1788, 11, 598, 261, 15347, 11, 20970, 261, 65, 20642, 261, 15418, 338, 30, 51339], "temperature": 0.0, "avg_logprob": -0.10672698830658535, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.0009022597805596888}, {"id": 84, "seek": 35850, "start": 378.5, "end": 379.5, "text": " Zdecydowanie.", "tokens": [51364, 1176, 1479, 1344, 67, 22028, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10672698830658535, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.0009022597805596888}, {"id": 85, "seek": 35850, "start": 380.0, "end": 384.5, "text": " Benchmark o nazwie trivia QA to jest taki test wiedzy og\u00f3lnej,", "tokens": [51439, 3964, 339, 5638, 277, 20151, 8699, 48770, 1249, 32, 281, 3492, 20065, 1500, 46894, 1229, 5360, 15741, 11794, 11, 51664], "temperature": 0.0, "avg_logprob": -0.10672698830658535, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.0009022597805596888}, {"id": 86, "seek": 35850, "start": 385.0, "end": 387.5, "text": " pytania typu, kto by\u0142 drugim cz\u0142owiekiem na ksi\u0119\u017cycu.", "tokens": [51689, 25878, 5609, 2125, 84, 11, 23780, 16673, 4110, 332, 36282, 26116, 1667, 350, 82, 5034, 7735, 12032, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10672698830658535, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.0009022597805596888}, {"id": 87, "seek": 38750, "start": 387.5, "end": 390.0, "text": " I tu wydarzy\u0142o si\u0119 co\u015b spektakularnego.", "tokens": [50364, 286, 2604, 4628, 20327, 1229, 5249, 3244, 19241, 768, 2320, 514, 1040, 11858, 13, 50489], "temperature": 0.0, "avg_logprob": -0.13721815255972056, "compression_ratio": 1.308880308880309, "no_speech_prob": 0.017943259328603745}, {"id": 88, "seek": 38750, "start": 390.5, "end": 395.0, "text": " Glam w trybie one shot, czyli po zobaczeniu tylko jednego przyk\u0142adu,", "tokens": [50514, 460, 4326, 261, 853, 7392, 472, 3347, 11, 16591, 714, 25100, 326, 39651, 13219, 5232, 11858, 23144, 84, 11, 50739], "temperature": 0.0, "avg_logprob": -0.13721815255972056, "compression_ratio": 1.308880308880309, "no_speech_prob": 0.017943259328603745}, {"id": 89, "seek": 38750, "start": 395.5, "end": 398.5, "text": " osi\u0105gn\u0105\u0142 75-8% dok\u0142adno\u015bci.", "tokens": [50764, 3003, 11404, 4568, 1611, 1221, 9562, 12, 23, 4, 45864, 16438, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13721815255972056, "compression_ratio": 1.308880308880309, "no_speech_prob": 0.017943259328603745}, {"id": 90, "seek": 38750, "start": 399.0, "end": 401.0, "text": " Wow. A jak wypad\u0142 GPT-3?", "tokens": [50939, 3153, 13, 316, 4207, 4628, 13647, 1221, 26039, 51, 12, 18, 30, 51039], "temperature": 0.0, "avg_logprob": -0.13721815255972056, "compression_ratio": 1.308880308880309, "no_speech_prob": 0.017943259328603745}, {"id": 91, "seek": 38750, "start": 401.5, "end": 406.5, "text": " I tu jest ca\u0142a magia. GPT-3, \u017ceby w og\u00f3le zbli\u017cy\u0107 si\u0119 do tego wyniku,", "tokens": [51064, 286, 2604, 3492, 1335, 5024, 2258, 654, 13, 26039, 51, 12, 18, 11, 11316, 261, 29229, 710, 32117, 39687, 3244, 360, 8627, 31936, 24320, 11, 51314], "temperature": 0.0, "avg_logprob": -0.13721815255972056, "compression_ratio": 1.308880308880309, "no_speech_prob": 0.017943259328603745}, {"id": 92, "seek": 38750, "start": 407.0, "end": 410.5, "text": " potrzebowa\u0142 a\u017c 64 przyk\u0142ad\u00f3w, czyli fused.", "tokens": [51339, 37595, 30105, 48134, 12145, 23144, 3901, 11, 16591, 283, 4717, 13, 51514], "temperature": 0.0, "avg_logprob": -0.13721815255972056, "compression_ratio": 1.308880308880309, "no_speech_prob": 0.017943259328603745}, {"id": 93, "seek": 38750, "start": 411.0, "end": 411.5, "text": " A\u017c tylu?", "tokens": [51539, 316, 1427, 1104, 2781, 30, 51564], "temperature": 0.0, "avg_logprob": -0.13721815255972056, "compression_ratio": 1.308880308880309, "no_speech_prob": 0.017943259328603745}, {"id": 94, "seek": 38750, "start": 412.0, "end": 415.0, "text": " Tak. A i tak uzyska\u0142 tylko 71-2%.", "tokens": [51589, 9118, 13, 316, 741, 991, 16851, 749, 2330, 1221, 13219, 30942, 12, 17, 6856, 51739], "temperature": 0.0, "avg_logprob": -0.13721815255972056, "compression_ratio": 1.308880308880309, "no_speech_prob": 0.017943259328603745}, {"id": 95, "seek": 41500, "start": 415.0, "end": 417.0, "text": " Ale to nie wszystko.", "tokens": [50364, 9366, 281, 2838, 22607, 13, 50464], "temperature": 0.0, "avg_logprob": -0.12823445622513935, "compression_ratio": 1.4738461538461538, "no_speech_prob": 0.04206681624054909}, {"id": 96, "seek": 41500, "start": 417.5, "end": 421.0, "text": " Wynik Glam pobi\u0142 nawet poprzedni najlepszy model w tym benchmarku,", "tokens": [50489, 343, 2534, 1035, 460, 4326, 714, 5614, 1221, 22696, 1665, 81, 11312, 3722, 41903, 1878, 1229, 2316, 261, 8107, 18927, 84, 11, 50664], "temperature": 0.0, "avg_logprob": -0.12823445622513935, "compression_ratio": 1.4738461538461538, "no_speech_prob": 0.04206681624054909}, {"id": 97, "seek": 41500, "start": 421.5, "end": 422.5, "text": " kt\u00f3ry by\u0142 specjalnie feintuned.", "tokens": [50689, 9913, 16673, 46433, 2766, 579, 686, 43703, 13, 50739], "temperature": 0.0, "avg_logprob": -0.12823445622513935, "compression_ratio": 1.4738461538461538, "no_speech_prob": 0.04206681624054909}, {"id": 98, "seek": 41500, "start": 423.0, "end": 426.5, "text": " Czyli dostrajany, przygotowywany wy\u0142\u0105cznie do tego jednego zadania.", "tokens": [50764, 37099, 20568, 48690, 1325, 11, 35914, 10089, 86, 1325, 4628, 15926, 19923, 360, 8627, 5232, 11858, 42788, 5609, 13, 50939], "temperature": 0.0, "avg_logprob": -0.12823445622513935, "compression_ratio": 1.4738461538461538, "no_speech_prob": 0.04206681624054909}, {"id": 99, "seek": 41500, "start": 427.0, "end": 430.0, "text": " Dok\u0142adnie. By\u0142 dostrajany przez wiele godzin, tylko do trivia QA.", "tokens": [50964, 29768, 10358, 2766, 13, 3146, 1221, 20568, 48690, 1325, 14064, 33137, 3044, 23584, 11, 13219, 360, 48770, 1249, 32, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12823445622513935, "compression_ratio": 1.4738461538461538, "no_speech_prob": 0.04206681624054909}, {"id": 100, "seek": 41500, "start": 430.5, "end": 435.5, "text": " Niesamowite. To tak jakby amator przyszed\u0142 i wygra\u0142 z zawodowcem na jego w\u0142asnym boisku.", "tokens": [51139, 426, 530, 335, 305, 642, 13, 1407, 991, 28976, 669, 1639, 6541, 749, 11312, 1221, 741, 4628, 20735, 1221, 710, 28165, 378, 305, 26422, 1667, 26542, 43572, 12996, 748, 271, 5279, 13, 51389], "temperature": 0.0, "avg_logprob": -0.12823445622513935, "compression_ratio": 1.4738461538461538, "no_speech_prob": 0.04206681624054909}, {"id": 101, "seek": 41500, "start": 436.0, "end": 442.0, "text": " \u015apiona pojemno\u015b\u0107 Glamu, te wszystkie nieaktywne parametry, to nie jest martwy balast.", "tokens": [51414, 27933, 79, 21758, 714, 30833, 23293, 460, 4326, 84, 11, 535, 31723, 2838, 514, 874, 86, 716, 6220, 9889, 11, 281, 2838, 3492, 12396, 9726, 3119, 525, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12823445622513935, "compression_ratio": 1.4738461538461538, "no_speech_prob": 0.04206681624054909}, {"id": 102, "seek": 41500, "start": 442.5, "end": 444.5, "text": " Tylko gigantyczne rezerwo war wiedzy.", "tokens": [51739, 49286, 4093, 8741, 394, 17466, 716, 319, 4527, 6120, 1516, 46894, 1229, 13, 51839], "temperature": 0.0, "avg_logprob": -0.12823445622513935, "compression_ratio": 1.4738461538461538, "no_speech_prob": 0.04206681624054909}, {"id": 103, "seek": 44500, "start": 445.0, "end": 448.0, "text": " Do kt\u00f3rego model potrafi si\u0119gn\u0105\u0107, kiedy trzeba.", "tokens": [50364, 1144, 46951, 2316, 1847, 10437, 72, 3244, 4568, 36374, 11, 18777, 25860, 13, 50514], "temperature": 0.0, "avg_logprob": -0.0764406176580899, "compression_ratio": 1.4203389830508475, "no_speech_prob": 0.002551677403971553}, {"id": 104, "seek": 44500, "start": 448.5, "end": 454.5, "text": " Dzi\u0119ki specjalizacji znajduje w\u0142a\u015bciw\u0105 informacj\u0119 znacznie precyzyjnie ni\u017c g\u0119sty model.", "tokens": [50539, 413, 34546, 46433, 590, 13152, 47570, 2884, 40112, 86, 1611, 1356, 29924, 15397, 14875, 2766, 659, 1344, 1229, 73, 2766, 28502, 290, 1274, 25134, 2316, 13, 50839], "temperature": 0.0, "avg_logprob": -0.0764406176580899, "compression_ratio": 1.4203389830508475, "no_speech_prob": 0.002551677403971553}, {"id": 105, "seek": 44500, "start": 455.0, "end": 457.5, "text": " To prowadzi mnie do naturalnego pytania.", "tokens": [50864, 1407, 36590, 3992, 17661, 360, 3303, 11858, 25878, 5609, 13, 50989], "temperature": 0.0, "avg_logprob": -0.0764406176580899, "compression_ratio": 1.4203389830508475, "no_speech_prob": 0.002551677403971553}, {"id": 106, "seek": 44500, "start": 458.0, "end": 464.5, "text": " Czy za tym sukcesem stoi wy\u0142\u0105cznie tak genialna architektura, a co z paliwem, czyli danymi?", "tokens": [51014, 19832, 7949, 8107, 46432, 887, 443, 342, 4869, 4628, 15926, 19923, 991, 48228, 629, 3912, 642, 2320, 2991, 11, 257, 598, 710, 3984, 72, 86, 443, 11, 16591, 274, 1325, 3057, 30, 51339], "temperature": 0.0, "avg_logprob": -0.0764406176580899, "compression_ratio": 1.4203389830508475, "no_speech_prob": 0.002551677403971553}, {"id": 107, "seek": 44500, "start": 465.0, "end": 468.5, "text": " Mo\u017ce po prostu zalali go lepszymi danymi i st\u0105d te wyniki?", "tokens": [51364, 43774, 714, 19518, 29599, 5103, 352, 476, 1878, 1229, 3057, 274, 1325, 3057, 741, 342, 18962, 535, 31936, 9850, 30, 51539], "temperature": 0.0, "avg_logprob": -0.0764406176580899, "compression_ratio": 1.4203389830508475, "no_speech_prob": 0.002551677403971553}, {"id": 108, "seek": 44500, "start": 469.0, "end": 473.0, "text": " To jest kluczowa kwestia i na szcz\u0119\u015bcie autorze postanowili to sprawdzi\u0107.", "tokens": [51564, 1407, 3492, 9671, 1311, 89, 5528, 42035, 654, 741, 1667, 22090, 1274, 9815, 19510, 1381, 2183, 282, 305, 2312, 281, 46192, 28496, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0764406176580899, "compression_ratio": 1.4203389830508475, "no_speech_prob": 0.002551677403971553}, {"id": 109, "seek": 47300, "start": 473.0, "end": 479.0, "text": " Przeprowadzili eksperyment, kt\u00f3ry moim zdaniem jest jedn\u0105 z najwa\u017cniejszych lekcji p\u0142yn\u0105cych z ca\u0142ej tej pracy.", "tokens": [50364, 2114, 46342, 1892, 345, 89, 2312, 30724, 610, 88, 518, 11, 9913, 48569, 710, 10312, 4907, 3492, 5232, 13113, 710, 11212, 27111, 10402, 45021, 30863, 19649, 28695, 2534, 1611, 31306, 710, 47631, 73, 12573, 35591, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09578858589639469, "compression_ratio": 1.4480286738351253, "no_speech_prob": 0.06801825016736984}, {"id": 110, "seek": 47300, "start": 479.5, "end": 480.0, "text": " Czyli?", "tokens": [50689, 37099, 30, 50714], "temperature": 0.0, "avg_logprob": -0.09578858589639469, "compression_ratio": 1.4480286738351253, "no_speech_prob": 0.06801825016736984}, {"id": 111, "seek": 47300, "start": 480.5, "end": 483.5, "text": " Wzi\u0119li dwa identyczne, mniejsze modele Glamu.", "tokens": [50739, 343, 16706, 2081, 35045, 2473, 17466, 716, 11, 275, 44258, 4391, 306, 460, 4326, 84, 13, 50889], "temperature": 0.0, "avg_logprob": -0.09578858589639469, "compression_ratio": 1.4480286738351253, "no_speech_prob": 0.06801825016736984}, {"id": 112, "seek": 47300, "start": 484.0, "end": 490.5, "text": " Jeden wytrenowali na gigantycznym, ale niefiltrowanym zbiorze danych z internetu oko\u0142o 7 bilion\u00f3w token\u00f3w.", "tokens": [50914, 508, 6876, 261, 4328, 1095, 305, 5103, 1667, 8741, 394, 17466, 12996, 11, 6775, 2838, 69, 2352, 1892, 1325, 76, 710, 33362, 1381, 274, 34644, 710, 4705, 84, 45730, 5249, 1614, 8588, 313, 3901, 14862, 3901, 13, 51239], "temperature": 0.0, "avg_logprob": -0.09578858589639469, "compression_ratio": 1.4480286738351253, "no_speech_prob": 0.06801825016736984}, {"id": 113, "seek": 47300, "start": 491.0, "end": 491.5, "text": " Potw\u00f3r.", "tokens": [51264, 9145, 86, 15614, 13, 51289], "temperature": 0.0, "avg_logprob": -0.09578858589639469, "compression_ratio": 1.4480286738351253, "no_speech_prob": 0.06801825016736984}, {"id": 114, "seek": 47300, "start": 492.0, "end": 500.0, "text": " A drugi model dosta\u0142 znacznie mniejszy, ale starannie wyselekcjonowany, oczyszczony, w wysokiej jako\u015bci zbi\u00f3r.", "tokens": [51314, 316, 4110, 72, 2316, 274, 8638, 1221, 15397, 14875, 2766, 39513, 7706, 11, 6775, 3543, 43433, 4628, 405, 29205, 45677, 23341, 11, 277, 3689, 20589, 3689, 2526, 11, 261, 27062, 453, 7764, 17123, 6199, 710, 5614, 15614, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09578858589639469, "compression_ratio": 1.4480286738351253, "no_speech_prob": 0.06801825016736984}, {"id": 115, "seek": 50000, "start": 500.0, "end": 503.5, "text": " Mia\u0142 zaledwie 143 miliardy token\u00f3w.", "tokens": [50364, 376, 8908, 710, 5573, 8699, 3499, 18, 1962, 72, 515, 88, 14862, 3901, 13, 50539], "temperature": 0.0, "avg_logprob": -0.0874361251962596, "compression_ratio": 1.2820512820512822, "no_speech_prob": 0.0896613672375679}, {"id": 116, "seek": 50000, "start": 504.0, "end": 511.5, "text": " Chwila, chwila. To brzmi kompletnie wbrew intuicji. M\u00f3wisz, \u017ce jeden model dosta\u0142 prawie 50 razy mniej danych ni\u017c drugi.", "tokens": [50564, 761, 86, 7371, 11, 26237, 7371, 13, 1407, 738, 89, 3057, 5207, 14657, 2766, 261, 65, 2236, 560, 84, 299, 4013, 13, 376, 3901, 23848, 11, 3561, 12906, 2316, 274, 8638, 1221, 3206, 8699, 2625, 9639, 88, 39513, 274, 34644, 28502, 4110, 72, 13, 50939], "temperature": 0.0, "avg_logprob": -0.0874361251962596, "compression_ratio": 1.2820512820512822, "no_speech_prob": 0.0896613672375679}, {"id": 117, "seek": 50000, "start": 512.0, "end": 512.5, "text": " Dok\u0142adnie tak.", "tokens": [50964, 29768, 10358, 2766, 991, 13, 50989], "temperature": 0.0, "avg_logprob": -0.0874361251962596, "compression_ratio": 1.2820512820512822, "no_speech_prob": 0.0896613672375679}, {"id": 118, "seek": 50000, "start": 513.0, "end": 520.0, "text": " No to z g\u00f3ry wiadomo kto powinien wygra\u0107. Ilo\u015b\u0107 kontrajako\u015b\u0107 te\u017c bym tak pomy\u015bla\u0142a, ale wynik by\u0142 jednoznaczny.", "tokens": [51014, 883, 281, 710, 290, 812, 627, 26393, 40633, 23780, 27310, 1053, 4628, 20735, 2162, 13, 286, 752, 7753, 14373, 424, 73, 18501, 7753, 9516, 538, 76, 991, 280, 8488, 1788, 875, 5024, 11, 6775, 31936, 1035, 16673, 5232, 1771, 22672, 14875, 1634, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0874361251962596, "compression_ratio": 1.2820512820512822, "no_speech_prob": 0.0896613672375679}, {"id": 119, "seek": 52000, "start": 521.0, "end": 523.5, "text": " Jako\u015b\u0107 wygra\u0142a przez knockout serio?", "tokens": [50414, 15029, 78, 7753, 4628, 20735, 5024, 14064, 6728, 346, 49531, 30, 50539], "temperature": 0.0, "avg_logprob": -0.06850893545470782, "compression_ratio": 1.47098976109215, "no_speech_prob": 0.7780017852783203}, {"id": 120, "seek": 52000, "start": 524.0, "end": 530.5, "text": " Model trenowany na mniejszym, ale czystszym zbiorze osi\u0105gn\u0105\u0142 znacznie lepsze wyniki we wszystkich testowanych zadaniach.", "tokens": [50564, 17105, 23136, 23341, 1667, 39513, 7706, 76, 11, 6775, 6430, 372, 7706, 76, 710, 33362, 1381, 3003, 11404, 4568, 1611, 1221, 15397, 14875, 2766, 476, 1878, 1381, 31936, 9850, 321, 34234, 1500, 23341, 339, 42788, 3782, 608, 13, 50889], "temperature": 0.0, "avg_logprob": -0.06850893545470782, "compression_ratio": 1.47098976109215, "no_speech_prob": 0.7780017852783203}, {"id": 121, "seek": 52000, "start": 531.0, "end": 535.0, "text": " Wniosek jest pot\u0119\u017cny. Jako\u015b\u0107 danych jest wa\u017cniejsza ni\u017c ich ilo\u015b\u0107.", "tokens": [50914, 343, 3722, 541, 74, 3492, 1847, 1274, 1427, 1634, 13, 15029, 78, 7753, 274, 34644, 3492, 27777, 30295, 2394, 28502, 1893, 1930, 78, 7753, 13, 51114], "temperature": 0.0, "avg_logprob": -0.06850893545470782, "compression_ratio": 1.47098976109215, "no_speech_prob": 0.7780017852783203}, {"id": 122, "seek": 52000, "start": 535.5, "end": 538.5, "text": " Karmienie modelu \u015bmieciami prowadzi do \u015bmieciowych rezultat\u00f3w.", "tokens": [51139, 591, 4452, 27385, 2316, 84, 8299, 25210, 537, 4526, 36590, 3992, 360, 8299, 25210, 537, 19605, 48060, 723, 267, 3901, 13, 51289], "temperature": 0.0, "avg_logprob": -0.06850893545470782, "compression_ratio": 1.47098976109215, "no_speech_prob": 0.7780017852783203}, {"id": 123, "seek": 52000, "start": 539.0, "end": 546.5, "text": " To jest fascynuj\u0105ce, bo intuicja podpowiada\u0142aby, \u017ce przy tak gigantycznej skali model sam powinien odfiltrowa\u0107 te \u015bmieci.", "tokens": [51314, 1407, 3492, 30632, 1344, 77, 13263, 384, 11, 748, 560, 84, 299, 2938, 2497, 14701, 39018, 1221, 2509, 11, 3561, 6501, 991, 8741, 394, 17466, 11794, 1110, 5103, 2316, 3247, 27310, 1053, 3611, 69, 2352, 1892, 43379, 535, 8299, 25210, 537, 13, 51689], "temperature": 0.0, "avg_logprob": -0.06850893545470782, "compression_ratio": 1.47098976109215, "no_speech_prob": 0.7780017852783203}, {"id": 124, "seek": 54650, "start": 546.5, "end": 551.0, "text": " Dlaczego tak si\u0119 nie dzieje? Czy te \u015bmieciowe dane aktywnie go og\u0142upiaj\u0105?", "tokens": [50364, 413, 75, 39329, 991, 3244, 2838, 17953, 2884, 30, 19832, 535, 8299, 25210, 537, 6880, 49206, 9308, 874, 14215, 352, 5360, 1221, 1010, 48125, 30, 50589], "temperature": 0.0, "avg_logprob": -0.05981670970648107, "compression_ratio": 1.417857142857143, "no_speech_prob": 0.09422899782657623}, {"id": 125, "seek": 54650, "start": 551.5, "end": 555.0, "text": " Dok\u0142adnie tak. Model uczy si\u0119 na wzorcach statystycznych.", "tokens": [50614, 29768, 10358, 2766, 991, 13, 17105, 344, 6522, 3244, 1667, 24809, 284, 66, 608, 2219, 38593, 17466, 9399, 13, 50789], "temperature": 0.0, "avg_logprob": -0.05981670970648107, "compression_ratio": 1.417857142857143, "no_speech_prob": 0.09422899782657623}, {"id": 126, "seek": 54650, "start": 555.5, "end": 563.0, "text": " Je\u015bli w danych jest mn\u00f3stwo szumu, b\u0142\u0119d\u00f3w, teorii spiskowych, on to traktuje jako prawomocny sygna\u0142.", "tokens": [50814, 37086, 261, 274, 34644, 3492, 275, 77, 45052, 6120, 7870, 30034, 11, 272, 1221, 6298, 3901, 11, 40238, 5597, 637, 7797, 19605, 11, 322, 281, 944, 2320, 13008, 17123, 22508, 298, 905, 1634, 943, 70, 629, 1221, 13, 51189], "temperature": 0.0, "avg_logprob": -0.05981670970648107, "compression_ratio": 1.417857142857143, "no_speech_prob": 0.09422899782657623}, {"id": 127, "seek": 54650, "start": 563.5, "end": 566.0, "text": " Uczy si\u0119 tych nieprawid\u0142owych korelacji.", "tokens": [51214, 624, 6522, 3244, 15180, 2838, 79, 5131, 327, 1221, 19605, 350, 418, 75, 13152, 13, 51339], "temperature": 0.0, "avg_logprob": -0.05981670970648107, "compression_ratio": 1.417857142857143, "no_speech_prob": 0.09422899782657623}, {"id": 128, "seek": 54650, "start": 566.5, "end": 567.0, "text": " Aha.", "tokens": [51364, 27448, 13, 51389], "temperature": 0.0, "avg_logprob": -0.05981670970648107, "compression_ratio": 1.417857142857143, "no_speech_prob": 0.09422899782657623}, {"id": 129, "seek": 54650, "start": 567.5, "end": 573.0, "text": " To tak jakby\u015b pr\u00f3bowa\u0142a nauczy\u0107 si\u0119 j\u0119zyka obcego tylko z for\u00f3w internetowych pe\u0142nych b\u0142\u0119d\u00f3w.", "tokens": [51414, 1407, 991, 28976, 1788, 8565, 65, 5528, 5024, 49103, 27150, 3244, 42309, 40940, 1111, 384, 1571, 13219, 710, 337, 3901, 4705, 19605, 43205, 9399, 272, 1221, 6298, 3901, 13, 51689], "temperature": 0.0, "avg_logprob": -0.05981670970648107, "compression_ratio": 1.417857142857143, "no_speech_prob": 0.09422899782657623}, {"id": 130, "seek": 57300, "start": 573.0, "end": 577.5, "text": " Nauczysz si\u0119 mn\u00f3stwa s\u0142\u00f3w, ale twoja gramatyka b\u0119dzie fatalna.", "tokens": [50364, 6056, 1311, 89, 20589, 3244, 275, 77, 45052, 4151, 15116, 3901, 11, 6775, 732, 2938, 21353, 21398, 2330, 10562, 24069, 629, 13, 50589], "temperature": 0.0, "avg_logprob": -0.09725173090545225, "compression_ratio": 1.4357142857142857, "no_speech_prob": 0.03385496512055397}, {"id": 131, "seek": 57300, "start": 578.0, "end": 581.5, "text": " Rozumiem. Czyste dane ucz\u0105 go o solidniejszych podstaw.", "tokens": [50614, 43313, 449, 4907, 13, 19832, 2941, 49206, 35403, 1611, 352, 277, 5100, 10402, 45021, 43443, 13, 50789], "temperature": 0.0, "avg_logprob": -0.09725173090545225, "compression_ratio": 1.4357142857142857, "no_speech_prob": 0.03385496512055397}, {"id": 132, "seek": 57300, "start": 582.0, "end": 583.5, "text": " W\u0142a\u015bnie, wi\u0119c co to wszystko oznacza?", "tokens": [50814, 343, 5024, 12221, 11, 16677, 598, 281, 22607, 277, 22672, 326, 2394, 30, 50889], "temperature": 0.0, "avg_logprob": -0.09725173090545225, "compression_ratio": 1.4357142857142857, "no_speech_prob": 0.03385496512055397}, {"id": 133, "seek": 57300, "start": 584.0, "end": 592.5, "text": " Mamy model ta\u0144szy w treningu, wyda\u0144niejszy w dzia\u0142aniu, z lekszymi wynikami, a do tego udowadnia, \u017ce kluczem jest kura tela danych.", "tokens": [50914, 376, 7804, 2316, 1846, 5248, 7706, 261, 2192, 773, 84, 11, 4628, 2675, 5248, 10402, 7706, 261, 27121, 25849, 11, 710, 476, 1694, 1229, 3057, 31936, 1035, 4526, 11, 257, 360, 8627, 11727, 22647, 12679, 11, 3561, 9671, 1311, 24313, 3492, 350, 2991, 29203, 274, 34644, 13, 51339], "temperature": 0.0, "avg_logprob": -0.09725173090545225, "compression_ratio": 1.4357142857142857, "no_speech_prob": 0.03385496512055397}, {"id": 134, "seek": 57300, "start": 593.0, "end": 599.5, "text": " Brzmi jak rewolucja, ale zawsze gdy co\u015b brzmi zbyt dobrze, pojawia si\u0119 pytanie, gdzie jest haczyk?", "tokens": [51364, 1603, 89, 3057, 4207, 319, 48481, 1311, 2938, 11, 6775, 30964, 28405, 19241, 738, 89, 3057, 710, 2322, 83, 28335, 11, 30655, 654, 3244, 36610, 11, 18922, 3492, 324, 6522, 74, 30, 51689], "temperature": 0.0, "avg_logprob": -0.09725173090545225, "compression_ratio": 1.4357142857142857, "no_speech_prob": 0.03385496512055397}, {"id": 135, "seek": 59950, "start": 599.5, "end": 606.0, "text": " I s\u0142usznie, bo kompromis istnieje i jest bardzo istotny. Nie ma darmowych lunczy, nawet w AI.", "tokens": [50364, 286, 15116, 22378, 2766, 11, 748, 5207, 28722, 271, 1418, 2766, 2884, 741, 3492, 9034, 1418, 310, 1634, 13, 12016, 463, 4072, 76, 19605, 19039, 6522, 11, 22696, 261, 7318, 13, 50689], "temperature": 0.0, "avg_logprob": -0.06732438843825768, "compression_ratio": 1.4055944055944056, "no_speech_prob": 0.040153615176677704}, {"id": 136, "seek": 59950, "start": 606.5, "end": 614.0, "text": " Mimo, \u017ce GLAM jest wyda\u0144niejszy podczas wnioskowania, jego ca\u0142kowita liczba parametr\u00f3w jest, jak ustalili\u015bmy, przeogromna.", "tokens": [50714, 376, 6934, 11, 3561, 16225, 2865, 3492, 4628, 2675, 5248, 10402, 7706, 2497, 30989, 45368, 2717, 74, 21308, 11, 26542, 35224, 74, 305, 2786, 6169, 89, 4231, 6220, 27965, 3901, 3492, 11, 4207, 26189, 304, 43912, 11, 8325, 664, 4397, 629, 13, 51089], "temperature": 0.0, "avg_logprob": -0.06732438843825768, "compression_ratio": 1.4055944055944056, "no_speech_prob": 0.040153615176677704}, {"id": 137, "seek": 59950, "start": 614.5, "end": 615.5, "text": " Co to oznacza w praktyce?", "tokens": [51114, 3066, 281, 277, 22672, 326, 2394, 261, 3206, 74, 874, 384, 30, 51164], "temperature": 0.0, "avg_logprob": -0.06732438843825768, "compression_ratio": 1.4055944055944056, "no_speech_prob": 0.040153615176677704}, {"id": 138, "seek": 59950, "start": 616.0, "end": 627.0, "text": " To oznacza, \u017ce sam fakt za\u0142adowania tego 1,2 bilionowego potwora do pami\u0119ci i utrzymywanie go w gotowo\u015bci wymaga gigantycznych zasob\u00f3w sprz\u0119towych.", "tokens": [51189, 1407, 277, 22672, 326, 2394, 11, 3561, 3247, 21310, 7949, 10358, 21308, 8627, 502, 11, 17, 8588, 313, 26576, 1847, 86, 3252, 360, 31088, 537, 741, 2839, 13047, 2226, 86, 7155, 352, 261, 658, 19941, 6199, 29764, 9286, 8741, 394, 17466, 9399, 26530, 996, 3901, 6103, 11052, 83, 19605, 13, 51739], "temperature": 0.0, "avg_logprob": -0.06732438843825768, "compression_ratio": 1.4055944055944056, "no_speech_prob": 0.040153615176677704}, {"id": 139, "seek": 62700, "start": 627.5, "end": 629.5, "text": " Mo\u017cesz to wyja\u015bni\u0107 na jakim\u015b przyk\u0142adzie?", "tokens": [50389, 44736, 10430, 281, 4628, 2938, 1788, 3722, 2162, 1667, 49410, 1788, 23144, 3283, 30, 50489], "temperature": 0.0, "avg_logprob": -0.10175508771623884, "compression_ratio": 1.352112676056338, "no_speech_prob": 0.023026548326015472}, {"id": 140, "seek": 62700, "start": 630.0, "end": 639.5, "text": " Jasne. Pomy\u015bl o tym tak. Model g\u0119sty, jak GPT-3, to podr\u0119czna, ale bardzo gruba encyklopedia. Zmie\u015bcisz j\u0105 na jednym solidnym regale.", "tokens": [50514, 34023, 716, 13, 430, 8488, 19212, 277, 8107, 991, 13, 17105, 290, 1274, 25134, 11, 4207, 26039, 51, 12, 18, 11, 281, 15305, 1274, 3689, 629, 11, 6775, 9034, 677, 12584, 465, 1344, 7837, 47795, 13, 1176, 25210, 1788, 66, 23848, 35692, 1667, 5232, 12996, 5100, 12996, 1121, 1220, 13, 50989], "temperature": 0.0, "avg_logprob": -0.10175508771623884, "compression_ratio": 1.352112676056338, "no_speech_prob": 0.023026548326015472}, {"id": 141, "seek": 62700, "start": 640.0, "end": 642.0, "text": " GLAM to ca\u0142a biblioteka narodowa.", "tokens": [51014, 16225, 2865, 281, 1335, 5024, 34344, 310, 36361, 6714, 378, 5528, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10175508771623884, "compression_ratio": 1.352112676056338, "no_speech_prob": 0.023026548326015472}, {"id": 142, "seek": 62700, "start": 642.5, "end": 644.0, "text": " Ok, to dobra analogia.", "tokens": [51139, 3477, 11, 281, 360, 6198, 16660, 654, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10175508771623884, "compression_ratio": 1.352112676056338, "no_speech_prob": 0.023026548326015472}, {"id": 143, "seek": 62700, "start": 644.5, "end": 652.5, "text": " Odpowied\u017a na ka\u017cde pytanie jest gdzie\u015b w \u015brodku, a wyspecjalizowany bibliotekarz, nasza gating function, znajdzie Ci j\u0105 b\u0142yskawicznie.", "tokens": [51239, 12210, 14701, 1091, 10659, 1667, 21912, 1479, 36610, 3492, 41359, 261, 28580, 5279, 11, 257, 27062, 494, 66, 22600, 590, 23341, 34344, 310, 916, 49763, 11, 5382, 2394, 290, 990, 2445, 11, 27318, 13096, 20188, 35692, 272, 1221, 749, 74, 1607, 17946, 2766, 13, 51639], "temperature": 0.0, "avg_logprob": -0.10175508771623884, "compression_ratio": 1.352112676056338, "no_speech_prob": 0.023026548326015472}, {"id": 144, "seek": 65250, "start": 653.5, "end": 658.0, "text": " Problem w tym, \u017ce musisz mie\u0107 budynek wielko\u015bci pa\u0142acu, \u017ceby w og\u00f3le t\u0119 bibliotek\u0119 zmie\u015bci\u0107.", "tokens": [50414, 11676, 261, 8107, 11, 3561, 1038, 23848, 35612, 3265, 88, 23255, 20570, 4093, 6199, 2502, 1221, 326, 84, 11, 11316, 261, 29229, 32489, 34344, 310, 916, 1274, 17020, 414, 6199, 2162, 13, 50639], "temperature": 0.0, "avg_logprob": -0.07188670620596482, "compression_ratio": 1.4712990936555892, "no_speech_prob": 0.12963935732841492}, {"id": 145, "seek": 65250, "start": 658.5, "end": 660.5, "text": " Czyli to rozwi\u0105zanie nie jest dla ka\u017cdego.", "tokens": [50664, 37099, 281, 9544, 22620, 7155, 2838, 3492, 12285, 21912, 67, 6308, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07188670620596482, "compression_ratio": 1.4712990936555892, "no_speech_prob": 0.12963935732841492}, {"id": 146, "seek": 65250, "start": 661.0, "end": 664.5, "text": " Komu w takim razie najbardziej op\u0142aca si\u0119 budowa takiego pa\u0142acu?", "tokens": [50789, 14286, 84, 261, 31732, 9639, 414, 41857, 999, 1221, 6628, 3244, 3265, 5528, 32296, 2502, 1221, 326, 84, 30, 50964], "temperature": 0.0, "avg_logprob": -0.07188670620596482, "compression_ratio": 1.4712990936555892, "no_speech_prob": 0.12963935732841492}, {"id": 147, "seek": 65250, "start": 665.0, "end": 667.5, "text": " To jest architektura stworzona dla najwi\u0119kszych graczy.", "tokens": [50989, 1407, 3492, 3912, 642, 2320, 2991, 342, 28321, 13383, 12285, 48636, 1694, 28051, 11625, 1229, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07188670620596482, "compression_ratio": 1.4712990936555892, "no_speech_prob": 0.12963935732841492}, {"id": 148, "seek": 65250, "start": 668.0, "end": 671.0, "text": " Dla firm, kt\u00f3re obs\u0142uguj\u0105 miliardy zapyta\u0144 dziennie.", "tokens": [51139, 413, 875, 6174, 11, 8864, 3181, 34077, 13263, 1962, 72, 515, 88, 14223, 88, 1328, 5248, 9758, 1053, 2766, 13, 51289], "temperature": 0.0, "avg_logprob": -0.07188670620596482, "compression_ratio": 1.4712990936555892, "no_speech_prob": 0.12963935732841492}, {"id": 149, "seek": 65250, "start": 671.5, "end": 679.0, "text": " W takiej skali oszcz\u0119dno\u015b\u0107 na ka\u017cdym zapytaniu jest tak ogromna, \u017ce uzasadnia gigantyczny sta\u0142y koszt utrzymania infrastruktury.", "tokens": [51314, 343, 38941, 1110, 5103, 3003, 43771, 6298, 23293, 1667, 31615, 76, 14223, 4328, 25849, 3492, 991, 34416, 298, 629, 11, 3561, 16851, 296, 345, 12679, 8741, 394, 17466, 1634, 11135, 6825, 19532, 2682, 2839, 13047, 37268, 6534, 19977, 2598, 13, 51689], "temperature": 0.0, "avg_logprob": -0.07188670620596482, "compression_ratio": 1.4712990936555892, "no_speech_prob": 0.12963935732841492}, {"id": 150, "seek": 65250, "start": 679.5, "end": 681.0, "text": " A dla mniejszej firmy?", "tokens": [51714, 316, 12285, 39513, 82, 16920, 12159, 2226, 30, 51789], "temperature": 0.0, "avg_logprob": -0.07188670620596482, "compression_ratio": 1.4712990936555892, "no_speech_prob": 0.12963935732841492}, {"id": 151, "seek": 68100, "start": 681.0, "end": 687.5, "text": " Dla startupu, kt\u00f3ry ma sporadyczny ruch, utrzymywanie takiego giganta w gotowo\u015bci by\u0142oby skralnie nieop\u0142acalne.", "tokens": [50364, 413, 875, 18578, 84, 11, 9913, 463, 43729, 880, 3689, 1634, 367, 625, 11, 2839, 13047, 2226, 86, 7155, 32296, 8741, 5983, 261, 658, 19941, 6199, 16673, 13944, 1110, 2155, 2766, 2838, 404, 1221, 326, 304, 716, 13, 50689], "temperature": 0.0, "avg_logprob": -0.042491861490102915, "compression_ratio": 1.4419475655430711, "no_speech_prob": 0.0010635851649567485}, {"id": 152, "seek": 68100, "start": 688.0, "end": 695.5, "text": " Lepiej mie\u0107 mniejszy, g\u0119sty model, kt\u00f3ry mo\u017ce i jest wolniejszy, ale nie wymaga tak pot\u0119\u017cnej infrastruktury.", "tokens": [50714, 441, 595, 7764, 35612, 39513, 7706, 11, 290, 1274, 25134, 2316, 11, 9913, 12034, 741, 3492, 20960, 10402, 7706, 11, 6775, 2838, 29764, 9286, 991, 1847, 1274, 1427, 11794, 6534, 19977, 2598, 13, 51089], "temperature": 0.0, "avg_logprob": -0.042491861490102915, "compression_ratio": 1.4419475655430711, "no_speech_prob": 0.0010635851649567485}, {"id": 153, "seek": 68100, "start": 696.0, "end": 703.5, "text": " Rozumiem, czyli to nie uniwersalny zamiennik, ale nowe, pot\u0119\u017cne narz\u0119dzie do zastosowa\u0144 na masow\u0105 skal\u0119.", "tokens": [51114, 43313, 449, 4907, 11, 16591, 281, 2838, 36435, 5364, 304, 1634, 19876, 1053, 13123, 11, 6775, 586, 68, 11, 1847, 1274, 1427, 716, 6714, 89, 42643, 360, 36746, 329, 5528, 5248, 1667, 2300, 30297, 16890, 1274, 13, 51489], "temperature": 0.0, "avg_logprob": -0.042491861490102915, "compression_ratio": 1.4419475655430711, "no_speech_prob": 0.0010635851649567485}, {"id": 154, "seek": 68100, "start": 704.0, "end": 706.5, "text": " Zmienia zasady gry, ale nie dla wszystkich.", "tokens": [51514, 1176, 76, 18811, 26530, 880, 41974, 11, 6775, 2838, 12285, 34234, 13, 51639], "temperature": 0.0, "avg_logprob": -0.042491861490102915, "compression_ratio": 1.4419475655430711, "no_speech_prob": 0.0010635851649567485}, {"id": 155, "seek": 70650, "start": 707.0, "end": 711.0, "text": " Dok\u0142adnie. Podsumowuj\u0105c g\u0142\u00f3wna lekcja z Glam wydaje si\u0119 prosta.", "tokens": [50389, 29768, 10358, 2766, 13, 12646, 82, 449, 305, 44733, 18117, 3901, 629, 30863, 34056, 710, 460, 4326, 49165, 3244, 582, 8638, 13, 50589], "temperature": 0.0, "avg_logprob": -0.13775421821907774, "compression_ratio": 1.4161490683229814, "no_speech_prob": 0.11338257789611816}, {"id": 156, "seek": 70650, "start": 711.5, "end": 716.0, "text": " Wi\u0119cej nie zawsze znaczy lepiej, a przynajmniej nie wi\u0119cej wszystkiego naraz.", "tokens": [50614, 30127, 20811, 2838, 30964, 36584, 476, 39699, 11, 257, 6501, 20981, 47658, 2838, 26004, 14615, 12200, 6714, 921, 13, 50839], "temperature": 0.0, "avg_logprob": -0.13775421821907774, "compression_ratio": 1.4161490683229814, "no_speech_prob": 0.11338257789611816}, {"id": 157, "seek": 70650, "start": 716.5, "end": 726.0, "text": " Tak, zamiast budowa\u0107 coraz wi\u0119ksze monolityczne m\u00f3zgi, kt\u00f3re zu\u017cywaj\u0105 ca\u0142\u0105 energi\u0119 na ka\u017cde zadanie, przysz\u0142o\u015b\u0107 mo\u017ce le\u017cy\u0107 w inteligentnej specjalizacji.", "tokens": [50864, 9118, 11, 710, 4526, 525, 3265, 11445, 25899, 29968, 1381, 1108, 401, 507, 38491, 32515, 89, 7834, 11, 8864, 2164, 7735, 86, 11133, 1335, 15926, 10575, 5034, 1667, 21912, 1479, 42788, 7155, 11, 44018, 44742, 12034, 476, 39687, 261, 24777, 25002, 11794, 46433, 590, 13152, 13, 51339], "temperature": 0.0, "avg_logprob": -0.13775421821907774, "compression_ratio": 1.4161490683229814, "no_speech_prob": 0.11338257789611816}, {"id": 158, "seek": 70650, "start": 726.5, "end": 730.0, "text": " Leniwe obliczania, jak w tej architekturze MiX czy RoF Experts.", "tokens": [51364, 441, 15711, 826, 1111, 1050, 89, 5609, 11, 4207, 261, 12573, 3912, 642, 2320, 374, 1381, 10204, 55, 6430, 3101, 37, 12522, 1373, 13, 51539], "temperature": 0.0, "avg_logprob": -0.13775421821907774, "compression_ratio": 1.4161490683229814, "no_speech_prob": 0.11338257789611816}, {"id": 159, "seek": 70650, "start": 730.5, "end": 734.0, "text": " Dok\u0142adnie. Gdzie do pracy anga\u017cowani s\u0105 tylko niezb\u0119dni specjali\u015bci.", "tokens": [51564, 29768, 10358, 2766, 13, 460, 13096, 360, 35591, 2562, 18264, 305, 3782, 9015, 13219, 33511, 65, 6298, 3722, 1608, 73, 5103, 6199, 13, 51739], "temperature": 0.0, "avg_logprob": -0.13775421821907774, "compression_ratio": 1.4161490683229814, "no_speech_prob": 0.11338257789611816}, {"id": 160, "seek": 73400, "start": 734.0, "end": 739.5, "text": " To mo\u017ce by\u0107 klucz do bardziej wydajnej i, co wa\u017cne, bardziej zr\u00f3wnowa\u017conej przysz\u0142o\u015bci AI.", "tokens": [50364, 1407, 12034, 15069, 9671, 1311, 89, 360, 27209, 25984, 1805, 11794, 741, 11, 598, 46110, 11, 27209, 710, 11721, 895, 5528, 1427, 546, 73, 44018, 35059, 7318, 13, 50639], "temperature": 0.0, "avg_logprob": -0.06291209882305514, "compression_ratio": 1.393822393822394, "no_speech_prob": 0.11688025295734406}, {"id": 161, "seek": 73400, "start": 740.0, "end": 743.5, "text": " I to sk\u0142ania do pewnej intryguj\u0105cej my\u015bli na koniec?", "tokens": [50664, 286, 281, 1110, 1221, 5609, 360, 25889, 11794, 560, 627, 2794, 8555, 20811, 452, 15350, 1667, 5897, 35733, 30, 50839], "temperature": 0.0, "avg_logprob": -0.06291209882305514, "compression_ratio": 1.393822393822394, "no_speech_prob": 0.11688025295734406}, {"id": 162, "seek": 73400, "start": 744.0, "end": 758.5, "text": " Tak. Skoro specjalizacja na tak niskim poziomie pojedynczych s\u0142\u00f3w przynosi tak spektakularne korzy\u015bci, to co by si\u0119 sta\u0142o, gdyby\u015bmy zastosowali t\u0105 sam\u0105 logik\u0119 na wy\u017cszym poziomie abstrakcji?", "tokens": [50864, 9118, 13, 7324, 10780, 46433, 590, 23395, 1667, 991, 297, 7797, 332, 38503, 40120, 714, 40543, 2534, 6522, 339, 15116, 3901, 6501, 16751, 72, 991, 768, 2320, 514, 1040, 716, 14784, 1229, 6199, 11, 281, 598, 538, 3244, 11135, 5249, 11, 28405, 2322, 10513, 36746, 329, 305, 5103, 32294, 3247, 1611, 3565, 1035, 1274, 1667, 4628, 1427, 7706, 76, 38503, 40120, 10823, 11272, 19649, 30, 51589], "temperature": 0.0, "avg_logprob": -0.06291209882305514, "compression_ratio": 1.393822393822394, "no_speech_prob": 0.11688025295734406}, {"id": 163, "seek": 73400, "start": 759.0, "end": 759.5, "text": " Czyli?", "tokens": [51614, 37099, 30, 51639], "temperature": 0.0, "avg_logprob": -0.06291209882305514, "compression_ratio": 1.393822393822394, "no_speech_prob": 0.11688025295734406}, {"id": 164, "seek": 75950, "start": 760.5, "end": 771.0, "text": " Co je\u015bli przysz\u0142o\u015b\u0107 AI to nie jeden wszechwiedz\u0105cy model, ale raczej spo\u0142eczno\u015b\u0107 mniejszych wyspecjalizowanych modeli, kt\u00f3re dynamicznie ze sob\u0105 wsp\u00f3\u0142pracuj\u0105?", "tokens": [50414, 3066, 25630, 44018, 44742, 7318, 281, 2838, 12906, 37647, 19439, 86, 1091, 8925, 1344, 2316, 11, 6775, 4129, 16920, 36851, 89, 23293, 39513, 45021, 27062, 494, 66, 22600, 590, 23341, 339, 2316, 72, 11, 8864, 8546, 89, 2766, 5277, 18253, 1611, 39069, 1424, 326, 13263, 30, 50939], "temperature": 0.0, "avg_logprob": -0.07477330613410337, "compression_ratio": 1.3013698630136987, "no_speech_prob": 0.08101034164428711}, {"id": 165, "seek": 75950, "start": 771.5, "end": 774.0, "text": " Model lekarz konsultuj\u0105cy si\u0119 z modelem prawnikiem.", "tokens": [50964, 17105, 476, 12303, 89, 27896, 723, 13263, 1344, 3244, 710, 4391, 10386, 37047, 1035, 4907, 13, 51089], "temperature": 0.0, "avg_logprob": -0.07477330613410337, "compression_ratio": 1.3013698630136987, "no_speech_prob": 0.08101034164428711}, {"id": 166, "seek": 75950, "start": 774.5, "end": 778.0, "text": " Dok\u0142adnie. \u017beby rozwi\u0105za\u0107 jaki\u015b skomplikowany problem.", "tokens": [51114, 29768, 10358, 2766, 13, 46864, 2322, 9544, 18234, 35873, 34721, 1110, 298, 564, 1035, 23341, 1154, 13, 51289], "temperature": 0.0, "avg_logprob": -0.07477330613410337, "compression_ratio": 1.3013698630136987, "no_speech_prob": 0.08101034164428711}, {"id": 167, "seek": 77800, "start": 778.5, "end": 784.0, "text": " GIM pokazuje, \u017ce idea zespo\u0142u ekspert\u00f3w nawet na poziomie mikro dzia\u0142a.", "tokens": [50389, 460, 6324, 13010, 43317, 11, 3561, 1558, 710, 279, 2259, 24066, 30724, 15346, 3901, 22696, 1667, 38503, 40120, 23959, 340, 37903, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12081031288419451, "compression_ratio": 1.175, "no_speech_prob": 0.32355108857154846}, {"id": 168, "seek": 77800, "start": 784.5, "end": 791.0, "text": " By\u0107 mo\u017ce to jest w\u0142a\u015bnie kierunek. Nie jeden super m\u00f3zg, ale ca\u0142e spo\u0142ecze\u0144stwo inteligentnych agent\u00f3w.", "tokens": [50689, 3146, 2162, 12034, 281, 3492, 14234, 38767, 409, 916, 13, 12016, 12906, 1687, 32515, 89, 70, 11, 6775, 47631, 36851, 1381, 12229, 6120, 24777, 25002, 9399, 9461, 3901, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12081031288419451, "compression_ratio": 1.175, "no_speech_prob": 0.32355108857154846}], "language": "pl"}