{"text": " Wielkie modele j\u0119zykowe s\u0105 ju\u017c, no c\u00f3\u017c, praktycznie wsz\u0119dzie. U\u017cywamy ich w pracy, w wyszukiwarkach, w telefonach. Ale jest z nimi wydaje mi si\u0119 taki jeden fundamentalny problem. Ten najpot\u0119\u017cniejsze, te kt\u00f3re naprawd\u0119 wiesz kszta\u0142tuj\u0105 nasz\u0105 cyfrow\u0105 rzeczywisto\u015b\u0107, s\u0105 jak czarne skrzynki. Nie mamy poj\u0119cia na jakich dok\u0142adnie danych by\u0142y trenowane, jak wygl\u0105da\u0142 sam proces uczenia, jakie b\u0142\u0119dy pope\u0142niono po drodze. Dzia\u0142aj\u0105, ale tak do ko\u0144ca nie wiemy dlaczego. I to jest problem, kt\u00f3ry sp\u0119dza sens powiek badaczom, bo je\u015bli nie znasz, powiedzmy, sk\u0142adnik\u00f3w i przepisu, to jak masz zbada\u0107, czy danie jest zdrowe. Bez dost\u0119pu do danych treningowych i co wa\u017cne ca\u0142ego procesu rzetelne badanie ich stronniczo\u015bci, potencjalnych zagro\u017ce\u0144, czy nawet weryfikacja ich faktycznych mo\u017cliwo\u015bci jest praktycznie niemo\u017cliwa. To troch\u0119 tak jakby in\u017cynier lotniczy mia\u0142 oceni\u0107 bezpiecze\u0144stwo nowego silnika odrzutowego, maj\u0105c do dyspozycji tylko jego l\u015bni\u0105c\u0105 obudow\u0119 i filmik promocyjny. I to jest idealne wprowadzenie do naszej dzisiejszej analizy. Bierzemy na warsztat prace naukow\u0105 z Allen Institute for Artificial Intelligence. Prezentuje ona model o nazwie O.L. Mo. I na pierwszy rzut oka, no to kolejny gracz w lidze modeli j\u0119zykowych. Ale jego tw\u00f3rcy twierdz\u0105, \u017ce stworzyli co\u015b, co nazywaj\u0105 prawdziwie otwartym modelem. Co to w og\u00f3le znaczy w \u015bwiecie, gdzie s\u0142owo otwarty jest odmieniane przez chyba wszystkie przypadki? I tu jest, wiesz, sedno sprawy. Ta prawdziwa otwarto\u015b\u0107, to ca\u0142a filozofia. Ona wykracza daleko poza to, co widzieli\u015bmy do Tide w tej pory. Wiele modeli, jak Mixtral czy cz\u0119\u015bciowo Lama, okre\u015blano jako otwarte, bo udost\u0119pniono ich wagi. Czyli w takim du\u017cym uproszczeniu gotowy, wytremowany m\u00f3zg. Ale Olmo to jest co\u015b zupe\u0142nie innego. To jest udost\u0119pnienie ca\u0142ego laboratorium. Dostajemy nie tylko wagi, ale pe\u0142ny kod treningowy, kompletny zbi\u00f3r danych treningowych, m\u00f3wimy o trzech bilionach token\u00f3w, do tego szczeg\u00f3\u0142owe logi z ka\u017cdej minuty procesu uczenia, ponad 500 po\u015brednich wersji modelu. 500? Tak, z r\u00f3\u017cnych etap\u00f3w treningu. I jeszcze wszystkie narz\u0119dzia do ewaluacji. To jest no, orz\u0105d wielko\u015bci wi\u0119cej, ni\u017c w projektach typu Pitya czy Bloom. Okej, spr\u00f3bujmy wi\u0119c dzisiaj odpowiedzie\u0107 na kluczowe pytanie. Co tak naprawd\u0119 daje ta, nazwijmy to, radykalna otwarto\u015b\u0107? Czy Olmo to tylko ciekawostka dla naukowc\u00f3w, a mo\u017ce co\u015b, co realnie zmienia zasady gry i czy w tym konkretnym przypadku otwarty faktycznie oznacza lepszy? Dobra, to roz\u0142\u00f3\u017cmy ten projekt na czynniki pierwsze. Skoro dostajemy dost\u0119p do ca\u0142ego warsztatu, no to zacznijmy od tego, co jest w \u015brodku. Jaka jest architektura samego modelu OLM-u? Jest tu jaka\u015b rewolucja? Architektonicznie to jest, powiedzia\u0142abym, solidna, nowoczesna in\u017cynieria, ale bez fajerwerk\u00f3w. Mamy tu do czynienia z architektur\u0105 typu Decoder Only Transformer, kt\u00f3ra bazuje oczywi\u015bcie na s\u0142ynnej pracy Attention Is All You Need. W uproszczeniu to maszyna zaprojektowana od podstaw do jednego, ale za to kluczowego zadania. Ma perfekcyjnie przewidywa\u0107 nast\u0119pne s\u0142owo w sekwencji. Ca\u0142a jej inteligencja bierze si\u0119 z mistrzowskiego opanowania tej jednej, jedynej umiej\u0119tno\u015bci. Czyli pod mask\u0105 nie ma jakiej\u015b kosmicznej technologii. To raczej ewolucja i inteligentne z\u0142o\u017cenie najlepszych dost\u0119pnych klock\u00f3w. Jakie to klocki? Dok\u0142adnie tak. To jest zbi\u00f3r sprawdzonych w boju ulepsze\u0144. Po pierwsze, podobnie jak w LAM-a, usuni\u0119to wszystkie cz\u0142ony BAYAS w architekturze, co po prostu poprawia stabilno\u015b\u0107 treningu. Po drugie zastosowano kilka modyfikacji, kt\u00f3re sta\u0142y si\u0119 ju\u017c, wiesz, standardem w bran\u017cy. Czekaj, bo wymieni\u0142a\u015b tu kilka rzeczy na jednym wdechu. Zatrzymajmy si\u0119 na chwil\u0119. M\u00f3wisz o non parametrik layer norm, swiglu, rotary positional embeddings. Dla kogo\u015b, kto nie jest in\u017cynierem machine learning, to brzmi troch\u0119 jak zakl\u0119cia. Co te elementy tak naprawd\u0119 robi\u0105? S\u0142uszna uwaga, rozbijmy to. Swiglu to po prostu nowsza i wydajniejsza funkcja aktywacji ni\u017c standardowe relu. Testy pokaza\u0142y, \u017ce modele ucz\u0105 si\u0119 dzi\u0119ki niej troszeczk\u0119 lepiej. Non parametrik layer norm to taki techniczny detal, kt\u00f3ry sprawia, \u017ce obliczenia s\u0105 szybsze i bardziej stabilne. A rotary positional embeddings, czyli rope, to ju\u017c w zasadzie standard. To standardowy spos\u00f3b, by model rozumia\u0142 kolejno\u015b\u0107 s\u0142\u00f3w w zdaniu. Jest znacznie skuteczniejszy ni\u017c starsze metody. Wi\u0119c tak jak powiedzia\u0142e\u015b, to nie rewolucja, a bardzo solidne rzemios\u0142o. W tej pracy skupili si\u0119 na dw\u00f3ch wariantach. Mamy wersj\u0119 7 miliard\u00f3w parametr\u00f3w, czyli Olmo 7B i mniejsz\u0105 miliardow\u0105. Rozumiem, czyli solidne, sprawdzone fundamenty. To prowadzi mnie do wniosku, \u017ce skoro architektura nie jest prze\u0142omowa, to prawdziw\u0105 gwiazd\u0105 tego show musz\u0105 by\u0107 dane. Co wiemy o zbiorze Dolma. I to jest strza\u0142 w dziesi\u0105tk\u0119. Dolma to jest serce tego projektu. M\u00f3wimy o zbiorze 3 bilion\u00f3w token\u00f3w. \u017beby da\u0107 skal\u0119, to odpowiednik przeczytania kilkudziesi\u0119ciu milion\u00f3w ksi\u0105\u017cek. Ale kluczowe jest nie tylko to, jak du\u017cy jest ten zbi\u00f3r, ale to, \u017ce jest w pe\u0142ni przezroczysty. Znamy ka\u017cdy jego sk\u0142adnik, co jest w menu. G\u0142\u00f3wnym daniem, stanowi\u0105cym prawie 75% ca\u0142o\u015bci, jest Common Crawl. To w zasadzie zaskrobana i przefiltrowana ogromna cz\u0119\u015b\u0107 publicznego internetu. Czyli taki internet w pigu\u0142ce. Ze wszystkimi jego zaletami i, co pewnie wa\u017cniejsze, wadami. Dok\u0142adnie i w\u0142a\u015bnie dlatego takie istotne s\u0105 te pozosta\u0142e sk\u0142adniki, kt\u00f3re maj\u0105, no wiesz, zbalansowa\u0107 t\u0105 diet\u0119. Mamy ponad 340 miliard\u00f3w token\u00f3w z GitHub'a, \u017ceby nauczy\u0107 model rozumienia i pisania kodu. Mamy 80 miliard\u00f3w token\u00f3w z Reddit'a, co uczy go bardziej nieformalnego, konwersacyjnego stylu. A reszt\u0119 uzupe\u0142niaj\u0105 sk\u0142adniki wysokiej jako\u015bci. Artyku\u0142y naukowe, ksi\u0105\u017cki z projektu Gutenberg czy ca\u0142a angielska Wikipedia. I po raz pierwszy badacze mog\u0105 dok\u0142adnie prze\u015bledzi\u0107, jak konkretne \u017ar\u00f3d\u0142a danych wp\u0142ywaj\u0105 na zdolno\u015bci modelu. Domy\u015blam si\u0119, \u017ce to otwiera zupe\u0142nie nowe mo\u017cliwo\u015bci badawcze. Mo\u017cna na przyk\u0142ad sprawdzi\u0107, co si\u0119 stanie, je\u015bli, nie wiem, usuniemy Reddit'a ze zbioru danych. Czy model stanie si\u0119 mniej kreatywny, ale za to bardziej formalny? W\u0142a\u015bnie o to chodzi. Mo\u017cna zadawa\u0107 takie pytania. I co wa\u017cniejsze, mo\u017cna na nie odpowiada\u0107 w spos\u00f3b naukowy. Bo mamy pe\u0142en wgl\u0105d w dane i nawet narz\u0119dzia do ich odtworzenia. To jest bezprecedensowe. To naprawd\u0119 zmienia zasady gry w badaniach nad AI. Okej, czyli mamy architektur\u0119 oparto na solidnych fundamentach i ten gigantyczny przezroczysty zbi\u00f3r danych. Ale to wszystko teoria. Najciekawsze jest to, jak z tych klock\u00f3w zbudowano co\u015b dzia\u0142aj\u0105cego. Sam proces treningu musia\u0142 by\u0107 pot\u0119\u017cnym wyzwaniem logistycznym. Ogromnym. Do zarz\u0105dzania tym procesem na setkach procesor\u00f3w graficznych u\u017cyli w frameworku PyTorch FSDP ze strategi\u0105 optymalizatora zero. To kolejne techniczne terminy, jak\u0105 one pe\u0142ni\u0105 rol\u0119. Co one rozwi\u0105zuj\u0105? M\u00f3wi\u0105c najpro\u015bciej rozwi\u0105zuj\u0105 problem pami\u0119ci. Wsp\u00f3\u0142czesne modele s\u0105 tak gigantyczne, \u017ce nie mieszcz\u0105 si\u0119 w pami\u0119ci nawet najpot\u0119\u017cniejszego pojedynczego GPU. FSDP i zero to takie sprytne techniki, kt\u00f3re pozwalaj\u0105 pokroi\u0107 model i jego stany obliczeniowe na kawa\u0142ki i rozdzieli\u0107 je pomi\u0119dzy setki kart graficznych, kt\u00f3re pracuj\u0105 razem. To kluczowa technologia, \u017ceby w og\u00f3le trenowa\u0107 takie bestie. A jest w tym procesie treningu co\u015b, co szczeg\u00f3lnie przykr\u00f3\u0142o twoj\u0105 uwag\u0119? Zdecydowanie. Jeden detal, kt\u00f3ry pokazuje, jak bardzo powa\u017cnie podeszli do idei otwarto\u015bci i uniwersalno\u015bci. Trening prowadzono r\u00f3wnolegle na dw\u00f3ch zupe\u0142nie r\u00f3\u017cnych klastrach sprz\u0119towych. Jeden by\u0142 oparty na GPU o ten VD, model A100, a drugi na konkurencyjnych kartach od AMD, MI250X. Tego cel. Czy to nie komplikuje sprawy? To jak? Ale cel by\u0142 w\u0142a\u015bnie taki. Udowodni\u0107, \u017ce ich kod jest przeno\u015bny. \u017be nie jest przyspawany do jednego dostawcy sprz\u0119tu. To jest pot\u0119\u017cny sygna\u0142 dla ca\u0142ej spo\u0142eczno\u015bci, \u017ce mo\u017cna budowa\u0107 wielkie modele bez zamykania si\u0119 w jednym ekosystemie. A co najwa\u017cniejsze, pokazali, \u017ce wyniki na obu platformach by\u0142y praktycznie identyczne. To \u015bwiadczy o niesamowitej solidno\u015bci ich kodu. To robi wra\u017cenie. A jak monitorowali post\u0119py? Czekali miesi\u0105cami na wynik ko\u0144cowy z za\u0142o\u017conymi r\u0119kami? Wr\u0119cz przeciwnie. Zastosowali co\u015b, co nazywaj\u0105 ewaluacj\u0105 w p\u0119tli, czyli in the loop evaluation. Co 1000 krok\u00f3w treningowych, czyli co oko\u0142o 4 miliardy przetworzonych token\u00f3w, automatycznie uruchamiali ca\u0142\u0105 bateri\u0119 test\u00f3w. To dawa\u0142o im niemal na bie\u017c\u0105co obraz tego, jak model si\u0119 uczy i czy wszystko idzie w dobr\u0105 kierunku. Zamiast czeka\u0107 miesi\u0105c, \u017ceby odkry\u0107, \u017ce jaki\u015b hiperparametr by\u0142 \u017ale ustawiony, mieli sygna\u0142 zwrotny niemal od razu. Przejd\u017amy wi\u0119c do wynik\u00f3w tej ewaluacji. Jak OLMO 7B wypada w bezpo\u015brednim starciu z innymi modelami tej samej wielko\u015bci, takimi jak LAMMA 2 7B czy Falcon 7B? Jest jaki\u015b knockout? Wyniki s\u0105 bardzo, bardzo solidne. W serii o\u015bmupopularnych test\u00f3w, sprawdzaj\u0105cych rozumowanie i wiedz\u0119, takich jak ARK, Hela Swagg czy Winogrande, OLMO 7B jest w \u015bcis\u0142ej czo\u0142\u00f3wce. Jego \u015bredni wynik jest w pe\u0142ni por\u00f3wnywalny z LAMMA 2 7B. Nie dominuje we wszystkich kategoriach, ale te\u017c w \u017cadnej znacz\u0105co nie odstaje. Jest po prostu bardzo konkurencyjny. Czyli jest po prostu tak samo dobry. Nie brzmi to jak rewolucja, kt\u00f3rej mo\u017cna by si\u0119 spodziewa\u0107. Czy w tych danych jest co\u015b, co nas zaskoczy\u0142o? Jaki\u015b nie wiem, nieintuicyjny wzorzec, kt\u00f3ry rzuci\u0142 si\u0119 w oczy. I to jest \u015bwietne pytanie, bo najciekawszy wniosek nie kryje si\u0119 w samej \u015bredniej, ale w szczeg\u00f3\u0142ach procesu. W pracy jest wykres pokazuj\u0105cy wydajno\u015b\u0107 modelu w trakcie ca\u0142ego treningu. I wida\u0107 na nim co\u015b fascynuj\u0105cego. Nag\u0142y, bardzo wyra\u017any skok wydajno\u015bci na samym samiuteinkim ko\u0144cu. Okaza\u0142o si\u0119, \u017ce ten wzrost to efekt banalnie prostego zabiegu. Liniowego zmniejszenia learning rate do zera w ci\u0105gu ostatnich tysi\u0105ca krok\u00f3w. To jest niezwykle praktyczna, cenna wskaz\u00f3wka dla ka\u017cdego, kto trenuje modele. I to jest ten rodzaj wiedzy, kt\u00f3ry zyskujemy tylko i wy\u0142\u0105cznie dzi\u0119ki otwartemu procesowi i szczeg\u00f3\u0142owym logom. W przypadku zamkni\u0119tego modelu nigdy by\u015bmy si\u0119 o tym nie dowiedzieli. W prac\u0119 cz\u0119sto pojawia si\u0119 te\u017c miara zwana Perplexity. Wyja\u015bnili\u015bmy j\u0105 kiedy\u015b, ale warto przypomnie\u0107, co ona nam m\u00f3wi o modelu. Perplexity to miara tego, jak bardzo model jest zaskoczony nowym tekstem. I mniejsza warto\u015b\u0107, tym lepiej. Oznacza to, \u017ce model lepiej przewiduje kolejne s\u0142owa, co \u015bwiadczy o tym, \u017ce lepiej rozumie struktur\u0119 j\u0119zyka. Do pomiar\u00f3w u\u017cyli benchmarku Paloma. I tu wniosek jest ciekawy. OLMO ma najni\u017csze Perplexity, czyli jest najlepszy na dany w pochodz\u0105cych z Common Crawl. Co jest logiczne, bo to by\u0142a lwia cz\u0119\u015bci jego diety. Jest jednak troch\u0119 s\u0142abszy na tekstach z Wikipedia czy ksi\u0105\u017cek. Ale jest tu chyba jaki\u015b haczyk, prawda? W artykule mocno podkre\u015blaj\u0105 co\u015b, co nazywaj\u0105 Decontamination. Decontamination, czyli odkarzanie, to proces, w kt\u00f3rym autorzy bardzo skrupulatnie usun\u0119li ze swojego zbioru treningowego OLMO wszystkie fragmenty, kt\u00f3re pokrywa\u0142y si\u0119 z danymi testowymi z benchmarku Paloma. Innymi s\u0142owy upewnili si\u0119, \u017ce model na pewno nie widzia\u0142 wcze\u015bniej pyta\u0144 z egzaminu. A w przypadku innych modeli nie mamy tej pewno\u015bci? Dok\u0142adnie. W przypadku modeli zamkni\u0119tych, kt\u00f3rych dane treningowe s\u0105 tajemnic\u0105, istnieje ogromne ryzyko, \u017ce by\u0142y one nie\u015bwiadomie trenowane na danych testowych. To troch\u0119 jakby ucze\u0144 przed matur\u0105 dosta\u0142 arkusz z odpowiedziami. Jego wysoki wynik niczego nie dowodzi. OLMO, nawet je\u015bli jego wynik perplexity nie jest w ka\u017cdej kategorii najni\u017cszy, daje nam gwarancj\u0119 uczciwego pomiaru. Z naukowego punktu widzenia to jest o wiele, wiele cenniejsze. Surowy, wytrenowany model to jedno. Ale wi\u0119kszo\u015b\u0107 z nas ma doczynienia z modelami, kt\u00f3re s\u0105 dostosowane do rozmowy z asystentem. Czy OLMO nadaje si\u0119 na tak\u0105 baz\u0119? Jak najbardziej i autorzy pokazali, jak to zrobi\u0107 przeprowadzili dwuetapowy proces adaptacji. Najpierw u\u017cyli metody Instruction Fine Tuning, znane jako SFT, a potem Direct Preference Optimization, czyli DPO. Wspomnia\u0142a\u015b o dw\u00f3ch metodach. SFT i DPO. Czym one si\u0119 r\u00f3\u017cni\u0105 w praktycy? SFT to jak uczenie dziecka przez dawanie mu konkretnych przyk\u0142ad\u00f3w. Na pytanie o stolice Francji odpowied\u017a Pary\u017c. Model uczy si\u0119 na tysi\u0105cach takich pary. Pytanie, odpowied\u017a, \u017ceby pod\u0105\u017ca\u0107 za instrukcjami. DPO jest bardziej subtelne. Tu pokazujemy modelowi dwie mo\u017cliwe odpowiedzi na to samo pytanie i m\u00f3wimy, ta odpowied\u017a jest lepsza od tej. Model nie dostaje gotowca, musi sam domy\u015bli\u0107 si\u0119, dlaczego jedna odpowied\u017a jest preferowana. Mo\u017ce jest bardziej pomocna, mniej toksyczna, bardziej szczeg\u00f3\u0142owa w ten spos\u00f3b uczy si\u0119 naszych preferencji. Jakie by\u0142y efekty tej dwuetapowej operacji? Spektakularne. Sp\u00f3jrzmy na wyniki. W te\u015bcie MMLU, kt\u00f3re mierzy szerok\u0105 wiedz\u0119 og\u00f3ln\u0105, surowy Olmo osi\u0105ga\u0142 wynik 28.3. Po adaptacji SFT i DPO ten wynik skacze do 46.2. To jest przepa\u015b\u0107. Ale jeszcze bardziej dramatyczna zmiana zasz\u0142a gdzie indziej. Gdzie? Poziomie toksyczno\u015bci. W te\u015bcie TOXYGEN surowy model generowa\u0142 tre\u015bci uznane za toksyczne w ponad 81% przypadk\u00f3w. To pokazuje, jak surowy jest internet, na kt\u00f3rym si\u0119 uczy\u0142. Po adaptacji ten wska\u017anik spada do zalednie 1 w sprzecinek 7%. Wow, to jest gigantyczna zmiana. Zatem z takiego surowego, nieco dzikiego lingwisty staje si\u0119 on znacznie bezpieczniejszym i bardziej u\u017cytecznym asystentem. Dok\u0142adnie. W tej nowej roli jest bardzo konkurencyjny. Jest jednak pewien niedosyt, bo w niekt\u00f3rych testach wci\u0105\u017c ust\u0119puje innemu otwartemu modelowi CULU 2, kt\u00f3ry bazuje na Lama 2. Autorki maj\u0105 na to dwie hipotezy. Po pierwsze, dane u\u017cyte do adaptacji by\u0142y oryginalnie projektowane z my\u015bl\u0105 o modelach z rodziny Lama. Ale druga hipoteza wraca do naszego wcze\u015bniejszego punktu. Mhm, czyli do potencjalnego przecieku danych testowych do zbioru treningowego Lama 2. Co mog\u0142o da\u0107 mu nieuczciw\u0105 przewag\u0119? W\u0142a\u015bnie. To kolejny mocny argument za transparentno\u015bci\u0105. Bez niej nigdy nie wiemy, czy wy\u017cszy wynik jest efektem lepszej architektury i treningu, czy po prostu tego, \u017ce model zna\u0142 odpowiedzi na egzaminie. O LMO, nawet je\u015bli w tym jednym por\u00f3wnaniu wypada minimalnie gorzej, daje nam pewno\u015b\u0107 co do uczciwo\u015bci tej oceny. Podsumowuj\u0105c to wszystko, mam takie wra\u017cenie, \u017ce najwi\u0119kszym produktem tego projektu nie jest sam model OLMO. Zgadzam si\u0119 w stu procentach. Najwi\u0119ksz\u0105 warto\u015bci\u0105 nie jest pobicie rekordu wydajno\u015bci w jakim\u015b bend marku. Jest ni\u0105 stworzenie pierwszego, prawdziwie, kompletnego, otwartego laboratorium do badania modeli j\u0119zykowych. To jest ten prze\u0142om. Te wszystkie udost\u0119pnione artefakty, ponad 500 punkt\u00f3w kontrolnych modelu, dane dolma, kod, logi, to pozwala ca\u0142ej spo\u0142eczno\u015bci naukowej na zadawanie pyta\u0144, kt\u00f3re do tej pory by\u0142y czysto teoretyczne. Jakich na przyk\u0142ad? Na przyk\u0142ad. Co dok\u0142adnie dzieje si\u0119 z wywn\u0119trzn\u0105 reprezentacj\u0105 wiedzy w modelu po przetworzeniu biliona token\u00f3w w por\u00f3wnaniu do p\u00f3\u0142tora biliona? Czy model najpierw uczy si\u0119 sk\u0142adnie, a dopiero potem fakty? Jaki jest faktyczny, mierzalny wp\u0142yw usuni\u0119cia danych z GitHub'a na zdolno\u015b\u0107 modelu do rozumowania logicznego? Dzi\u0119ki Olmo mo\u017cna wzi\u0105\u0107 konkretny checkpoint z dowolnego momentu treningu i to po prostu zbada\u0107. To otwiera dwi\u017cej do prawdziwej nauki o EI, a nie tylko in\u017cynierii. Oczywi\u015bcie \u017caden projekt nie jest idealny. O jakich ograniczeniach wspominaj\u0105 sami autorzy? S\u0105 w tym wzgl\u0119dzie r\u00f3wnie transparentni? S\u0105 i to si\u0119 chwali. Po pierwsze dane. Mimo ich ogromu skupiaj\u0105 si\u0119 g\u0142\u00f3wnie na j\u0119zyku angielskim, co jest standardem, ale i oczywistym ograniczeniem. Po drugie sam proces treningu. Pisz\u0105 wprost, \u017ce jest extremalnie trudny i kosztowny, a ich praca nie dokumentuje wszystkich \u015blepych za\u0142uk\u00f3w, nieudanych eksperyment\u00f3w i problem\u00f3w, kt\u00f3re napotkali. To wa\u017cne uwaga, \u017ce za tym sukcesem stoj\u0105 liczne pora\u017cki, o kt\u00f3rych nie pisze si\u0119 w artyku\u0142ach. A co z samym modelem? Nawet po tej ca\u0142ej adaptacji? Nawet po dostosowaniu wci\u0105\u017c mo\u017ce on generowa\u0107 tre\u015bci, stronnicze, nieprawdziwe lub szkodliwe. To nie jest magiczne rozwi\u0105zanie wszystkich problem\u00f3w. I wreszcie sami przyznaj\u0105, \u017ce standardowe benchmarki, cho\u0107 u\u017cyteczne, nie zawsze odzwierciedlaj\u0105 to, jak ludzie faktycznie u\u017cywaj\u0105 modeli j\u0119zykowych. To szerszy problem ca\u0142ej dziedziny. Wygl\u0105da wi\u0119c na to, \u017ce OLMO to nie tyle linia mety w wy\u015bcigu o najlepszy model, co raczej linia startowa. Linia startowa dla ca\u0142ej spo\u0142eczno\u015bci badawczej. To zaproszenie do wsp\u00f3lnego zagl\u0105dania podmask\u0119 i budowania wiedzy o tym, jak naprawd\u0119 dzia\u0142aj\u0105 te technologie. Dok\u0142adnie. Celem jest przyspieszenie nauki o modelach j\u0119zykowych, a nie tylko wypuszczenie kolejnego produktu. To fundamentalna zmiana filozofii, kt\u00f3ra w d\u0142ugiej perspektywie mo\u017ce przynie\u015b\u0107 znacznie wi\u0119cej korzy\u015bci, ni\u017c kolejny, nieznacznie lepszy, ale wci\u0105\u017c zamkni\u0119ty model. I to pozostawia nas z niezwykle wa\u017cn\u0105 my\u015bl\u0105 na koniec. W \u015bwiecie, gdzie najpot\u0119czniejsze modele AI s\u0105 zamkni\u0119te i kontrolowane przez zaledwie kilka korporacji, pojawia si\u0119 fundamentalne pytanie, kt\u00f3re ten projekt stawia w centrum debaty. To pytanie brzmi, czy ryzyko zwi\u0105zane z potencjalnym, niew\u0142a\u015bciwym wykorzystaniem w pe\u0142ni otwartych i replikowalnych modeli, takich jak Olmo, jest wi\u0119ksze, ni\u017c ryzyko zwi\u0105zane z ca\u0142kowitym brakiem zrozumienia i jakiegokolwiek spo\u0142ecznego nadzoru nad zamkni\u0119tymi modelami, kt\u00f3re ju\u017c teraz, w tej chwili, kszta\u0142tuj\u0105 nasz\u0105 rzeczywisto\u015b\u0107. Projekt Olmo stawia odwa\u017cn\u0105 tez\u0119, \u017ce w tej grze naszym najwi\u0119kszym wrogiem i najwi\u0119kszym zagro\u017ceniem jest ignorancja.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.0, "text": " Wielkie modele j\u0119zykowe s\u0105 ju\u017c, no c\u00f3\u017c, praktycznie wsz\u0119dzie.", "tokens": [50364, 343, 1187, 22872, 4391, 306, 49055, 74, 6880, 9015, 10678, 11, 572, 6333, 1427, 11, 3206, 74, 45586, 38322, 42643, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10203944512133328, "compression_ratio": 1.4398826979472141, "no_speech_prob": 0.012546355836093426}, {"id": 1, "seek": 0, "start": 4.0, "end": 8.0, "text": " U\u017cywamy ich w pracy, w wyszukiwarkach, w telefonach.", "tokens": [50564, 624, 7735, 86, 7804, 1893, 261, 35591, 11, 261, 261, 20589, 11788, 86, 809, 608, 11, 261, 26812, 608, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10203944512133328, "compression_ratio": 1.4398826979472141, "no_speech_prob": 0.012546355836093426}, {"id": 2, "seek": 0, "start": 8.0, "end": 12.0, "text": " Ale jest z nimi wydaje mi si\u0119 taki jeden fundamentalny problem.", "tokens": [50764, 9366, 3492, 710, 297, 10121, 49165, 2752, 3244, 20065, 12906, 8088, 1634, 1154, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10203944512133328, "compression_ratio": 1.4398826979472141, "no_speech_prob": 0.012546355836093426}, {"id": 3, "seek": 0, "start": 12.0, "end": 19.0, "text": " Ten najpot\u0119\u017cniejsze, te kt\u00f3re naprawd\u0119 wiesz kszta\u0142tuj\u0105 nasz\u0105 cyfrow\u0105 rzeczywisto\u015b\u0107, s\u0105 jak czarne skrzynki.", "tokens": [50964, 9380, 11212, 17698, 1274, 1427, 44258, 11, 535, 8864, 20970, 261, 15347, 350, 15453, 46426, 83, 13263, 5382, 8925, 3185, 69, 1892, 1611, 26297, 86, 9334, 7753, 11, 9015, 4207, 6472, 289, 716, 1110, 13047, 77, 2984, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10203944512133328, "compression_ratio": 1.4398826979472141, "no_speech_prob": 0.012546355836093426}, {"id": 4, "seek": 0, "start": 19.0, "end": 27.0, "text": " Nie mamy poj\u0119cia na jakich dok\u0142adnie danych by\u0142y trenowane, jak wygl\u0105da\u0142 sam proces uczenia, jakie b\u0142\u0119dy pope\u0142niono po drodze.", "tokens": [51314, 12016, 17335, 714, 11115, 2755, 1667, 4207, 480, 45864, 2766, 274, 34644, 26366, 23136, 23066, 11, 4207, 32015, 1221, 3247, 17565, 344, 38517, 11, 22124, 272, 46564, 3173, 42248, 1221, 77, 49020, 714, 3789, 67, 1381, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10203944512133328, "compression_ratio": 1.4398826979472141, "no_speech_prob": 0.012546355836093426}, {"id": 5, "seek": 0, "start": 27.0, "end": 29.0, "text": " Dzia\u0142aj\u0105, ale tak do ko\u0144ca nie wiemy dlaczego.", "tokens": [51714, 39448, 8908, 11133, 11, 6775, 991, 360, 26470, 496, 2838, 3355, 2226, 37873, 39329, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10203944512133328, "compression_ratio": 1.4398826979472141, "no_speech_prob": 0.012546355836093426}, {"id": 6, "seek": 2900, "start": 29.0, "end": 39.0, "text": " I to jest problem, kt\u00f3ry sp\u0119dza sens powiek badaczom, bo je\u015bli nie znasz, powiedzmy, sk\u0142adnik\u00f3w i przepisu, to jak masz zbada\u0107, czy danie jest zdrowe.", "tokens": [50364, 286, 281, 3492, 1154, 11, 9913, 637, 6298, 2394, 2923, 3388, 19487, 1578, 14875, 298, 11, 748, 25630, 2838, 15397, 19601, 11, 27617, 2226, 11, 1110, 10358, 47447, 741, 30829, 25871, 11, 281, 4207, 2300, 89, 710, 65, 1538, 2162, 11, 6430, 3277, 414, 3492, 49745, 68, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11819235483805339, "compression_ratio": 1.461847389558233, "no_speech_prob": 0.03568043187260628}, {"id": 7, "seek": 2900, "start": 39.0, "end": 48.0, "text": " Bez dost\u0119pu do danych treningowych i co wa\u017cne ca\u0142ego procesu rzetelne badanie ich stronniczo\u015bci, potencjalnych zagro\u017ce\u0144,", "tokens": [50864, 879, 89, 48209, 84, 360, 274, 34644, 2192, 773, 19605, 741, 598, 46110, 35224, 6308, 17565, 84, 367, 40399, 338, 716, 1578, 7155, 1893, 45766, 7692, 4765, 6199, 11, 1847, 22660, 22600, 9399, 27001, 340, 2875, 5248, 11, 51314], "temperature": 0.0, "avg_logprob": -0.11819235483805339, "compression_ratio": 1.461847389558233, "no_speech_prob": 0.03568043187260628}, {"id": 8, "seek": 2900, "start": 48.0, "end": 53.0, "text": " czy nawet weryfikacja ich faktycznych mo\u017cliwo\u015bci jest praktycznie niemo\u017cliwa.", "tokens": [51314, 6430, 22696, 261, 2109, 31230, 23395, 1893, 33647, 874, 3689, 9399, 30854, 36476, 3492, 3206, 74, 45586, 2838, 3280, 1427, 2081, 4151, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11819235483805339, "compression_ratio": 1.461847389558233, "no_speech_prob": 0.03568043187260628}, {"id": 9, "seek": 5300, "start": 53.0, "end": 66.0, "text": " To troch\u0119 tak jakby in\u017cynier lotniczy mia\u0142 oceni\u0107 bezpiecze\u0144stwo nowego silnika odrzutowego, maj\u0105c do dyspozycji tylko jego l\u015bni\u0105c\u0105 obudow\u0119 i filmik promocyjny.", "tokens": [50364, 1407, 24926, 991, 28976, 294, 1427, 2534, 811, 688, 7692, 1229, 27989, 10409, 268, 12757, 47153, 9680, 12229, 6120, 586, 6308, 3425, 77, 5439, 3611, 19390, 325, 26576, 11, 26064, 66, 360, 15243, 2259, 1229, 19649, 13219, 26542, 287, 1788, 3722, 1611, 32557, 1111, 532, 305, 1274, 741, 2007, 1035, 2234, 31078, 73, 1634, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09693103790283203, "compression_ratio": 1.3577586206896552, "no_speech_prob": 0.10509347915649414}, {"id": 10, "seek": 5300, "start": 66.0, "end": 70.0, "text": " I to jest idealne wprowadzenie do naszej dzisiejszej analizy.", "tokens": [51014, 286, 281, 3492, 7157, 716, 46733, 16778, 360, 42946, 9758, 50117, 82, 16920, 2624, 590, 88, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09693103790283203, "compression_ratio": 1.3577586206896552, "no_speech_prob": 0.10509347915649414}, {"id": 11, "seek": 5300, "start": 70.0, "end": 75.0, "text": " Bierzemy na warsztat prace naukow\u0105 z Allen Institute for Artificial Intelligence.", "tokens": [51214, 363, 34602, 3633, 1667, 13718, 2682, 267, 582, 617, 35616, 74, 30297, 710, 17160, 9446, 337, 5735, 10371, 27274, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09693103790283203, "compression_ratio": 1.3577586206896552, "no_speech_prob": 0.10509347915649414}, {"id": 12, "seek": 7500, "start": 76.0, "end": 78.0, "text": " Prezentuje ona model o nazwie O.L. Mo.", "tokens": [50414, 6001, 14185, 13008, 20325, 2316, 277, 20151, 8699, 422, 13, 43, 13, 3335, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10028289831601657, "compression_ratio": 1.3275109170305677, "no_speech_prob": 0.6553035974502563}, {"id": 13, "seek": 7500, "start": 78.0, "end": 83.0, "text": " I na pierwszy rzut oka, no to kolejny gracz w lidze modeli j\u0119zykowych.", "tokens": [50514, 286, 1667, 34016, 367, 89, 325, 277, 2330, 11, 572, 281, 23749, 1634, 11625, 89, 261, 10252, 1381, 2316, 72, 49055, 74, 19605, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10028289831601657, "compression_ratio": 1.3275109170305677, "no_speech_prob": 0.6553035974502563}, {"id": 14, "seek": 7500, "start": 83.0, "end": 89.0, "text": " Ale jego tw\u00f3rcy twierdz\u0105, \u017ce stworzyli co\u015b, co nazywaj\u0105 prawdziwie otwartym modelem.", "tokens": [50764, 9366, 26542, 683, 15614, 1344, 683, 811, 67, 8925, 11, 3561, 342, 28321, 1229, 2081, 19241, 11, 598, 20151, 27112, 11133, 41175, 3992, 8699, 4337, 29587, 4199, 4391, 10386, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10028289831601657, "compression_ratio": 1.3275109170305677, "no_speech_prob": 0.6553035974502563}, {"id": 15, "seek": 7500, "start": 89.0, "end": 95.0, "text": " Co to w og\u00f3le znaczy w \u015bwiecie, gdzie s\u0142owo otwarty jest odmieniane przez chyba wszystkie przypadki?", "tokens": [51064, 3066, 281, 261, 29229, 36584, 261, 40078, 4260, 11, 18922, 15116, 19941, 4337, 29587, 88, 3492, 3611, 76, 1053, 21133, 14064, 31532, 31723, 33100, 2984, 30, 51364], "temperature": 0.0, "avg_logprob": -0.10028289831601657, "compression_ratio": 1.3275109170305677, "no_speech_prob": 0.6553035974502563}, {"id": 16, "seek": 9500, "start": 95.0, "end": 102.0, "text": " I tu jest, wiesz, sedno sprawy. Ta prawdziwa otwarto\u015b\u0107, to ca\u0142a filozofia.", "tokens": [50364, 286, 2604, 3492, 11, 261, 15347, 11, 9643, 1771, 22734, 88, 13, 6551, 41175, 3992, 4151, 4337, 86, 15864, 7753, 11, 281, 1335, 5024, 1387, 15151, 2670, 654, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11722171306610107, "compression_ratio": 1.3294573643410852, "no_speech_prob": 0.38295289874076843}, {"id": 17, "seek": 9500, "start": 102.0, "end": 106.0, "text": " Ona wykracza daleko poza to, co widzieli\u015bmy do Tide w tej pory.", "tokens": [50714, 49793, 39287, 12080, 2394, 11702, 34241, 714, 2394, 281, 11, 598, 27486, 23099, 10513, 360, 314, 482, 261, 12573, 280, 827, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11722171306610107, "compression_ratio": 1.3294573643410852, "no_speech_prob": 0.38295289874076843}, {"id": 18, "seek": 9500, "start": 106.0, "end": 113.0, "text": " Wiele modeli, jak Mixtral czy cz\u0119\u015bciowo Lama, okre\u015blano jako otwarte, bo udost\u0119pniono ich wagi.", "tokens": [50914, 9233, 306, 2316, 72, 11, 4207, 10204, 734, 2155, 6430, 41314, 19941, 441, 2404, 11, 3133, 265, 19212, 3730, 17123, 4337, 86, 11026, 11, 748, 11727, 555, 18085, 77, 49020, 1893, 261, 20291, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11722171306610107, "compression_ratio": 1.3294573643410852, "no_speech_prob": 0.38295289874076843}, {"id": 19, "seek": 9500, "start": 113.0, "end": 118.0, "text": " Czyli w takim du\u017cym uproszczeniu gotowy, wytremowany m\u00f3zg.", "tokens": [51264, 37099, 261, 31732, 21783, 4199, 493, 2635, 89, 66, 39651, 658, 10089, 11, 261, 4328, 2579, 23341, 32515, 89, 70, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11722171306610107, "compression_ratio": 1.3294573643410852, "no_speech_prob": 0.38295289874076843}, {"id": 20, "seek": 9500, "start": 118.0, "end": 121.0, "text": " Ale Olmo to jest co\u015b zupe\u0142nie innego.", "tokens": [51514, 9366, 6141, 3280, 281, 3492, 19241, 49922, 294, 11858, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11722171306610107, "compression_ratio": 1.3294573643410852, "no_speech_prob": 0.38295289874076843}, {"id": 21, "seek": 12100, "start": 122.0, "end": 125.0, "text": " To jest udost\u0119pnienie ca\u0142ego laboratorium.", "tokens": [50414, 1407, 3492, 11727, 555, 18085, 77, 27385, 35224, 6308, 5938, 41679, 13, 50564], "temperature": 0.0, "avg_logprob": -0.0996953696012497, "compression_ratio": 1.3813229571984436, "no_speech_prob": 0.6506820917129517}, {"id": 22, "seek": 12100, "start": 125.0, "end": 128.0, "text": " Dostajemy nie tylko wagi, ale pe\u0142ny kod treningowy,", "tokens": [50564, 413, 555, 1805, 3633, 2838, 13219, 261, 20291, 11, 6775, 43205, 1634, 350, 378, 2192, 773, 10089, 11, 50714], "temperature": 0.0, "avg_logprob": -0.0996953696012497, "compression_ratio": 1.3813229571984436, "no_speech_prob": 0.6506820917129517}, {"id": 23, "seek": 12100, "start": 128.0, "end": 132.0, "text": " kompletny zbi\u00f3r danych treningowych, m\u00f3wimy o trzech bilionach token\u00f3w,", "tokens": [50714, 5207, 14657, 1634, 710, 5614, 15614, 274, 34644, 2192, 773, 19605, 11, 13489, 13189, 277, 504, 19439, 8588, 313, 608, 14862, 3901, 11, 50914], "temperature": 0.0, "avg_logprob": -0.0996953696012497, "compression_ratio": 1.3813229571984436, "no_speech_prob": 0.6506820917129517}, {"id": 24, "seek": 12100, "start": 132.0, "end": 136.0, "text": " do tego szczeg\u00f3\u0142owe logi z ka\u017cdej minuty procesu uczenia,", "tokens": [50914, 360, 8627, 22090, 1146, 16181, 6880, 3565, 72, 710, 21912, 1479, 73, 923, 6432, 17565, 84, 344, 38517, 11, 51114], "temperature": 0.0, "avg_logprob": -0.0996953696012497, "compression_ratio": 1.3813229571984436, "no_speech_prob": 0.6506820917129517}, {"id": 25, "seek": 12100, "start": 136.0, "end": 139.0, "text": " ponad 500 po\u015brednich wersji modelu.", "tokens": [51114, 9224, 345, 5923, 714, 1788, 986, 77, 480, 261, 433, 4013, 2316, 84, 13, 51264], "temperature": 0.0, "avg_logprob": -0.0996953696012497, "compression_ratio": 1.3813229571984436, "no_speech_prob": 0.6506820917129517}, {"id": 26, "seek": 12100, "start": 139.0, "end": 140.0, "text": " 500?", "tokens": [51264, 5923, 30, 51314], "temperature": 0.0, "avg_logprob": -0.0996953696012497, "compression_ratio": 1.3813229571984436, "no_speech_prob": 0.6506820917129517}, {"id": 27, "seek": 12100, "start": 140.0, "end": 142.0, "text": " Tak, z r\u00f3\u017cnych etap\u00f3w treningu.", "tokens": [51314, 9118, 11, 710, 42602, 47634, 3901, 2192, 773, 84, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0996953696012497, "compression_ratio": 1.3813229571984436, "no_speech_prob": 0.6506820917129517}, {"id": 28, "seek": 12100, "start": 142.0, "end": 145.0, "text": " I jeszcze wszystkie narz\u0119dzia do ewaluacji.", "tokens": [51414, 286, 14168, 31723, 6714, 89, 6298, 40395, 360, 43364, 4929, 13152, 13, 51564], "temperature": 0.0, "avg_logprob": -0.0996953696012497, "compression_ratio": 1.3813229571984436, "no_speech_prob": 0.6506820917129517}, {"id": 29, "seek": 14500, "start": 145.0, "end": 151.0, "text": " To jest no, orz\u0105d wielko\u015bci wi\u0119cej, ni\u017c w projektach typu Pitya czy Bloom.", "tokens": [50364, 1407, 3492, 572, 11, 420, 23876, 20570, 4093, 6199, 26004, 11, 28502, 261, 26261, 608, 2125, 84, 430, 507, 64, 6430, 25927, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09838491145188247, "compression_ratio": 1.378787878787879, "no_speech_prob": 0.2972894012928009}, {"id": 30, "seek": 14500, "start": 151.0, "end": 156.0, "text": " Okej, spr\u00f3bujmy wi\u0119c dzisiaj odpowiedzie\u0107 na kluczowe pytanie.", "tokens": [50664, 29094, 73, 11, 6103, 14216, 4579, 2226, 16677, 25772, 24314, 22078, 1667, 9671, 1311, 89, 6880, 36610, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09838491145188247, "compression_ratio": 1.378787878787879, "no_speech_prob": 0.2972894012928009}, {"id": 31, "seek": 14500, "start": 156.0, "end": 162.0, "text": " Co tak naprawd\u0119 daje ta, nazwijmy to, radykalna otwarto\u015b\u0107?", "tokens": [50914, 3066, 991, 20970, 1120, 2884, 1846, 11, 20151, 36652, 2226, 281, 11, 367, 880, 19990, 629, 4337, 86, 15864, 7753, 30, 51214], "temperature": 0.0, "avg_logprob": -0.09838491145188247, "compression_ratio": 1.378787878787879, "no_speech_prob": 0.2972894012928009}, {"id": 32, "seek": 14500, "start": 162.0, "end": 165.0, "text": " Czy Olmo to tylko ciekawostka dla naukowc\u00f3w,", "tokens": [51214, 19832, 6141, 3280, 281, 13219, 46419, 1607, 555, 2330, 12285, 35616, 74, 305, 29268, 11, 51364], "temperature": 0.0, "avg_logprob": -0.09838491145188247, "compression_ratio": 1.378787878787879, "no_speech_prob": 0.2972894012928009}, {"id": 33, "seek": 14500, "start": 165.0, "end": 168.0, "text": " a mo\u017ce co\u015b, co realnie zmienia zasady gry", "tokens": [51364, 257, 12034, 19241, 11, 598, 957, 2766, 17020, 18811, 26530, 880, 41974, 51514], "temperature": 0.0, "avg_logprob": -0.09838491145188247, "compression_ratio": 1.378787878787879, "no_speech_prob": 0.2972894012928009}, {"id": 34, "seek": 14500, "start": 168.0, "end": 173.0, "text": " i czy w tym konkretnym przypadku otwarty faktycznie oznacza lepszy?", "tokens": [51514, 741, 6430, 261, 8107, 36500, 12996, 41955, 4337, 29587, 88, 33647, 45586, 277, 22672, 326, 2394, 476, 1878, 1229, 30, 51764], "temperature": 0.0, "avg_logprob": -0.09838491145188247, "compression_ratio": 1.378787878787879, "no_speech_prob": 0.2972894012928009}, {"id": 35, "seek": 17300, "start": 173.0, "end": 177.0, "text": " Dobra, to roz\u0142\u00f3\u017cmy ten projekt na czynniki pierwsze.", "tokens": [50364, 413, 24393, 11, 281, 9544, 1221, 812, 1427, 2226, 2064, 26261, 1667, 6430, 26384, 9850, 45994, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07495439714855617, "compression_ratio": 1.3890909090909092, "no_speech_prob": 0.018205903470516205}, {"id": 36, "seek": 17300, "start": 177.0, "end": 180.0, "text": " Skoro dostajemy dost\u0119p do ca\u0142ego warsztatu,", "tokens": [50564, 7324, 10780, 20568, 1805, 3633, 48209, 360, 35224, 6308, 13718, 2682, 20546, 11, 50714], "temperature": 0.0, "avg_logprob": -0.07495439714855617, "compression_ratio": 1.3890909090909092, "no_speech_prob": 0.018205903470516205}, {"id": 37, "seek": 17300, "start": 180.0, "end": 183.0, "text": " no to zacznijmy od tego, co jest w \u015brodku.", "tokens": [50714, 572, 281, 710, 14875, 77, 1718, 2226, 3611, 8627, 11, 598, 3492, 261, 28580, 5279, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07495439714855617, "compression_ratio": 1.3890909090909092, "no_speech_prob": 0.018205903470516205}, {"id": 38, "seek": 17300, "start": 183.0, "end": 187.0, "text": " Jaka jest architektura samego modelu OLM-u?", "tokens": [50864, 508, 7849, 3492, 3912, 642, 2320, 2991, 912, 1571, 2316, 84, 39191, 44, 12, 84, 30, 51064], "temperature": 0.0, "avg_logprob": -0.07495439714855617, "compression_ratio": 1.3890909090909092, "no_speech_prob": 0.018205903470516205}, {"id": 39, "seek": 17300, "start": 187.0, "end": 189.0, "text": " Jest tu jaka\u015b rewolucja?", "tokens": [51064, 24918, 2604, 4207, 64, 1788, 319, 48481, 1311, 2938, 30, 51164], "temperature": 0.0, "avg_logprob": -0.07495439714855617, "compression_ratio": 1.3890909090909092, "no_speech_prob": 0.018205903470516205}, {"id": 40, "seek": 17300, "start": 189.0, "end": 193.0, "text": " Architektonicznie to jest, powiedzia\u0142abym, solidna, nowoczesna in\u017cynieria,", "tokens": [51164, 10984, 642, 74, 1756, 17946, 2766, 281, 3492, 11, 48539, 2509, 76, 11, 5100, 629, 11, 586, 905, 12214, 629, 294, 1427, 2534, 811, 654, 11, 51364], "temperature": 0.0, "avg_logprob": -0.07495439714855617, "compression_ratio": 1.3890909090909092, "no_speech_prob": 0.018205903470516205}, {"id": 41, "seek": 17300, "start": 193.0, "end": 195.0, "text": " ale bez fajerwerk\u00f3w.", "tokens": [51364, 6775, 10782, 34001, 260, 1554, 23849, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07495439714855617, "compression_ratio": 1.3890909090909092, "no_speech_prob": 0.018205903470516205}, {"id": 42, "seek": 17300, "start": 195.0, "end": 200.0, "text": " Mamy tu do czynienia z architektur\u0105 typu Decoder Only Transformer,", "tokens": [51464, 376, 7804, 2604, 360, 6430, 77, 18811, 710, 3912, 642, 2320, 374, 1611, 2125, 84, 12427, 19866, 5686, 27938, 260, 11, 51714], "temperature": 0.0, "avg_logprob": -0.07495439714855617, "compression_ratio": 1.3890909090909092, "no_speech_prob": 0.018205903470516205}, {"id": 43, "seek": 20000, "start": 200.0, "end": 205.0, "text": " kt\u00f3ra bazuje oczywi\u015bcie na s\u0142ynnej pracy Attention Is All You Need.", "tokens": [50364, 19456, 27147, 13008, 23862, 1667, 15116, 2534, 11794, 35591, 31858, 1119, 1057, 509, 16984, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08690512277246491, "compression_ratio": 1.3882783882783882, "no_speech_prob": 0.04207246005535126}, {"id": 44, "seek": 20000, "start": 205.0, "end": 209.0, "text": " W uproszczeniu to maszyna zaprojektowana od podstaw do jednego,", "tokens": [50614, 343, 493, 2635, 89, 66, 39651, 281, 2300, 1229, 629, 14223, 340, 14930, 40458, 3611, 43443, 360, 5232, 11858, 11, 50814], "temperature": 0.0, "avg_logprob": -0.08690512277246491, "compression_ratio": 1.3882783882783882, "no_speech_prob": 0.04207246005535126}, {"id": 45, "seek": 20000, "start": 209.0, "end": 211.0, "text": " ale za to kluczowego zadania.", "tokens": [50814, 6775, 7949, 281, 9671, 1311, 89, 26576, 42788, 5609, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08690512277246491, "compression_ratio": 1.3882783882783882, "no_speech_prob": 0.04207246005535126}, {"id": 46, "seek": 20000, "start": 211.0, "end": 215.0, "text": " Ma perfekcyjnie przewidywa\u0107 nast\u0119pne s\u0142owo w sekwencji.", "tokens": [50914, 4042, 13826, 916, 42949, 2766, 39758, 38836, 25234, 39662, 716, 15116, 19941, 261, 17215, 15615, 19649, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08690512277246491, "compression_ratio": 1.3882783882783882, "no_speech_prob": 0.04207246005535126}, {"id": 47, "seek": 20000, "start": 215.0, "end": 222.0, "text": " Ca\u0142a jej inteligencja bierze si\u0119 z mistrzowskiego opanowania tej jednej, jedynej umiej\u0119tno\u015bci.", "tokens": [51114, 7544, 5024, 28924, 24777, 3213, 34056, 272, 811, 1381, 3244, 710, 3544, 19390, 1509, 42349, 999, 282, 21308, 12573, 5232, 11794, 11, 5232, 88, 11794, 1105, 7764, 46788, 16438, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08690512277246491, "compression_ratio": 1.3882783882783882, "no_speech_prob": 0.04207246005535126}, {"id": 48, "seek": 20000, "start": 222.0, "end": 226.0, "text": " Czyli pod mask\u0105 nie ma jakiej\u015b kosmicznej technologii.", "tokens": [51464, 37099, 2497, 6094, 1611, 2838, 463, 4207, 7764, 1788, 19532, 13195, 89, 11794, 1537, 1132, 5597, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08690512277246491, "compression_ratio": 1.3882783882783882, "no_speech_prob": 0.04207246005535126}, {"id": 49, "seek": 22600, "start": 226.0, "end": 231.0, "text": " To raczej ewolucja i inteligentne z\u0142o\u017cenie najlepszych dost\u0119pnych klock\u00f3w.", "tokens": [50364, 1407, 4129, 16920, 43364, 401, 1311, 2938, 741, 24777, 25002, 716, 710, 5249, 41118, 41903, 1878, 28051, 48209, 9399, 350, 4102, 3901, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07531706946236746, "compression_ratio": 1.343859649122807, "no_speech_prob": 0.00599288335070014}, {"id": 50, "seek": 22600, "start": 231.0, "end": 233.0, "text": " Jakie to klocki?", "tokens": [50614, 15029, 414, 281, 350, 4102, 72, 30, 50714], "temperature": 0.0, "avg_logprob": -0.07531706946236746, "compression_ratio": 1.343859649122807, "no_speech_prob": 0.00599288335070014}, {"id": 51, "seek": 22600, "start": 233.0, "end": 234.0, "text": " Dok\u0142adnie tak.", "tokens": [50714, 29768, 10358, 2766, 991, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07531706946236746, "compression_ratio": 1.343859649122807, "no_speech_prob": 0.00599288335070014}, {"id": 52, "seek": 22600, "start": 234.0, "end": 238.0, "text": " To jest zbi\u00f3r sprawdzonych w boju ulepsze\u0144.", "tokens": [50764, 1407, 3492, 710, 5614, 15614, 46192, 44479, 339, 261, 748, 8954, 344, 306, 1878, 49689, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07531706946236746, "compression_ratio": 1.343859649122807, "no_speech_prob": 0.00599288335070014}, {"id": 53, "seek": 22600, "start": 238.0, "end": 241.0, "text": " Po pierwsze, podobnie jak w LAM-a,", "tokens": [50964, 6165, 45994, 11, 43024, 2766, 4207, 261, 441, 2865, 12, 64, 11, 51114], "temperature": 0.0, "avg_logprob": -0.07531706946236746, "compression_ratio": 1.343859649122807, "no_speech_prob": 0.00599288335070014}, {"id": 54, "seek": 22600, "start": 241.0, "end": 245.0, "text": " usuni\u0119to wszystkie cz\u0142ony BAYAS w architekturze,", "tokens": [51114, 505, 409, 5034, 1353, 31723, 31083, 2526, 363, 4299, 3160, 261, 3912, 642, 2320, 374, 1381, 11, 51314], "temperature": 0.0, "avg_logprob": -0.07531706946236746, "compression_ratio": 1.343859649122807, "no_speech_prob": 0.00599288335070014}, {"id": 55, "seek": 22600, "start": 245.0, "end": 248.0, "text": " co po prostu poprawia stabilno\u015b\u0107 treningu.", "tokens": [51314, 598, 714, 19518, 1665, 5131, 654, 11652, 23293, 2192, 773, 84, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07531706946236746, "compression_ratio": 1.343859649122807, "no_speech_prob": 0.00599288335070014}, {"id": 56, "seek": 22600, "start": 248.0, "end": 251.0, "text": " Po drugie zastosowano kilka modyfikacji,", "tokens": [51464, 6165, 4110, 414, 36746, 329, 305, 3730, 36466, 275, 843, 31230, 13152, 11, 51614], "temperature": 0.0, "avg_logprob": -0.07531706946236746, "compression_ratio": 1.343859649122807, "no_speech_prob": 0.00599288335070014}, {"id": 57, "seek": 22600, "start": 251.0, "end": 254.0, "text": " kt\u00f3re sta\u0142y si\u0119 ju\u017c, wiesz, standardem w bran\u017cy.", "tokens": [51614, 8864, 11135, 6825, 3244, 10678, 11, 261, 15347, 11, 3832, 443, 261, 12029, 7735, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07531706946236746, "compression_ratio": 1.343859649122807, "no_speech_prob": 0.00599288335070014}, {"id": 58, "seek": 25400, "start": 254.0, "end": 258.0, "text": " Czekaj, bo wymieni\u0142a\u015b tu kilka rzeczy na jednym wdechu.", "tokens": [50364, 383, 19878, 1805, 11, 748, 29764, 35462, 5024, 1788, 2604, 36466, 26297, 1667, 5232, 12996, 261, 1479, 29013, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08952993070575553, "compression_ratio": 1.3737024221453287, "no_speech_prob": 0.08411514014005661}, {"id": 59, "seek": 25400, "start": 258.0, "end": 260.0, "text": " Zatrzymajmy si\u0119 na chwil\u0119.", "tokens": [50564, 1176, 267, 13047, 1696, 73, 2226, 3244, 1667, 41941, 1274, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08952993070575553, "compression_ratio": 1.3737024221453287, "no_speech_prob": 0.08411514014005661}, {"id": 60, "seek": 25400, "start": 260.0, "end": 266.0, "text": " M\u00f3wisz o non parametrik layer norm, swiglu, rotary positional embeddings.", "tokens": [50664, 376, 3901, 23848, 277, 2107, 6220, 302, 14456, 4583, 2026, 11, 1693, 328, 2781, 11, 45811, 2535, 304, 12240, 29432, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08952993070575553, "compression_ratio": 1.3737024221453287, "no_speech_prob": 0.08411514014005661}, {"id": 61, "seek": 25400, "start": 266.0, "end": 271.0, "text": " Dla kogo\u015b, kto nie jest in\u017cynierem machine learning, to brzmi troch\u0119 jak zakl\u0119cia.", "tokens": [50964, 413, 875, 350, 23515, 1788, 11, 23780, 2838, 3492, 294, 1427, 2534, 72, 7333, 3479, 2539, 11, 281, 738, 89, 3057, 24926, 4207, 23810, 75, 1274, 2755, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08952993070575553, "compression_ratio": 1.3737024221453287, "no_speech_prob": 0.08411514014005661}, {"id": 62, "seek": 25400, "start": 271.0, "end": 273.0, "text": " Co te elementy tak naprawd\u0119 robi\u0105?", "tokens": [51214, 3066, 535, 4478, 88, 991, 20970, 3870, 11404, 30, 51314], "temperature": 0.0, "avg_logprob": -0.08952993070575553, "compression_ratio": 1.3737024221453287, "no_speech_prob": 0.08411514014005661}, {"id": 63, "seek": 25400, "start": 273.0, "end": 276.0, "text": " S\u0142uszna uwaga, rozbijmy to.", "tokens": [51314, 318, 1221, 22378, 629, 23147, 9286, 11, 9544, 30418, 2226, 281, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08952993070575553, "compression_ratio": 1.3737024221453287, "no_speech_prob": 0.08411514014005661}, {"id": 64, "seek": 25400, "start": 276.0, "end": 281.0, "text": " Swiglu to po prostu nowsza i wydajniejsza funkcja aktywacji ni\u017c standardowe relu.", "tokens": [51464, 3926, 328, 2781, 281, 714, 19518, 586, 82, 2394, 741, 25984, 1805, 30295, 2394, 26476, 34056, 9308, 874, 86, 13152, 28502, 3832, 6880, 1039, 84, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08952993070575553, "compression_ratio": 1.3737024221453287, "no_speech_prob": 0.08411514014005661}, {"id": 65, "seek": 28100, "start": 281.0, "end": 286.0, "text": " Testy pokaza\u0142y, \u017ce modele ucz\u0105 si\u0119 dzi\u0119ki niej troszeczk\u0119 lepiej.", "tokens": [50364, 314, 7819, 13010, 12257, 6825, 11, 3561, 4391, 306, 35403, 1611, 3244, 45003, 2838, 73, 45692, 1381, 3689, 15724, 476, 39699, 13, 50614], "temperature": 0.0, "avg_logprob": -0.0633507183619908, "compression_ratio": 1.4617834394904459, "no_speech_prob": 0.009422363713383675}, {"id": 66, "seek": 28100, "start": 286.0, "end": 290.0, "text": " Non parametrik layer norm to taki techniczny detal,", "tokens": [50614, 8774, 6220, 302, 14456, 4583, 2026, 281, 20065, 1537, 17946, 1634, 33185, 11, 50814], "temperature": 0.0, "avg_logprob": -0.0633507183619908, "compression_ratio": 1.4617834394904459, "no_speech_prob": 0.009422363713383675}, {"id": 67, "seek": 28100, "start": 290.0, "end": 293.0, "text": " kt\u00f3ry sprawia, \u017ce obliczenia s\u0105 szybsze i bardziej stabilne.", "tokens": [50814, 9913, 22734, 654, 11, 3561, 1111, 1050, 14320, 9015, 30526, 929, 1381, 741, 27209, 11652, 716, 13, 50964], "temperature": 0.0, "avg_logprob": -0.0633507183619908, "compression_ratio": 1.4617834394904459, "no_speech_prob": 0.009422363713383675}, {"id": 68, "seek": 28100, "start": 293.0, "end": 298.0, "text": " A rotary positional embeddings, czyli rope, to ju\u017c w zasadzie standard.", "tokens": [50964, 316, 45811, 2535, 304, 12240, 29432, 11, 16591, 13540, 11, 281, 10678, 261, 44585, 3283, 3832, 13, 51214], "temperature": 0.0, "avg_logprob": -0.0633507183619908, "compression_ratio": 1.4617834394904459, "no_speech_prob": 0.009422363713383675}, {"id": 69, "seek": 28100, "start": 298.0, "end": 302.0, "text": " To standardowy spos\u00f3b, by model rozumia\u0142 kolejno\u015b\u0107 s\u0142\u00f3w w zdaniu.", "tokens": [51214, 1407, 3832, 10089, 22904, 11, 538, 2316, 48797, 8908, 23749, 23293, 15116, 3901, 261, 16221, 25849, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0633507183619908, "compression_ratio": 1.4617834394904459, "no_speech_prob": 0.009422363713383675}, {"id": 70, "seek": 28100, "start": 302.0, "end": 306.0, "text": " Jest znacznie skuteczniejszy ni\u017c starsze metody.", "tokens": [51414, 24918, 15397, 14875, 2766, 1110, 1169, 3689, 10402, 7706, 28502, 6105, 1381, 1131, 843, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0633507183619908, "compression_ratio": 1.4617834394904459, "no_speech_prob": 0.009422363713383675}, {"id": 71, "seek": 28100, "start": 306.0, "end": 310.0, "text": " Wi\u0119c tak jak powiedzia\u0142e\u015b, to nie rewolucja, a bardzo solidne rzemios\u0142o.", "tokens": [51614, 32508, 991, 4207, 48539, 68, 1788, 11, 281, 2838, 319, 48481, 1311, 2938, 11, 257, 9034, 5100, 716, 367, 24313, 2717, 5249, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0633507183619908, "compression_ratio": 1.4617834394904459, "no_speech_prob": 0.009422363713383675}, {"id": 72, "seek": 31000, "start": 310.0, "end": 313.0, "text": " W tej pracy skupili si\u0119 na dw\u00f3ch wariantach.", "tokens": [50364, 343, 12573, 35591, 1110, 1010, 2312, 3244, 1667, 27379, 812, 339, 1516, 5798, 608, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07794283514153467, "compression_ratio": 1.4306569343065694, "no_speech_prob": 0.0569593720138073}, {"id": 73, "seek": 31000, "start": 313.0, "end": 319.0, "text": " Mamy wersj\u0119 7 miliard\u00f3w parametr\u00f3w, czyli Olmo 7B i mniejsz\u0105 miliardow\u0105.", "tokens": [50514, 376, 7804, 261, 433, 11115, 1614, 1962, 72, 515, 3901, 6220, 27965, 3901, 11, 16591, 6141, 3280, 1614, 33, 741, 275, 30295, 8925, 1962, 72, 515, 30297, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07794283514153467, "compression_ratio": 1.4306569343065694, "no_speech_prob": 0.0569593720138073}, {"id": 74, "seek": 31000, "start": 319.0, "end": 323.0, "text": " Rozumiem, czyli solidne, sprawdzone fundamenty.", "tokens": [50814, 43313, 449, 4907, 11, 16591, 5100, 716, 11, 46192, 16896, 6073, 88, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07794283514153467, "compression_ratio": 1.4306569343065694, "no_speech_prob": 0.0569593720138073}, {"id": 75, "seek": 31000, "start": 323.0, "end": 327.0, "text": " To prowadzi mnie do wniosku, \u017ce skoro architektura nie jest prze\u0142omowa,", "tokens": [51014, 1407, 36590, 3992, 17661, 360, 45368, 2717, 5279, 11, 3561, 1110, 10780, 3912, 642, 2320, 2991, 2838, 3492, 8325, 1221, 298, 5528, 11, 51214], "temperature": 0.0, "avg_logprob": -0.07794283514153467, "compression_ratio": 1.4306569343065694, "no_speech_prob": 0.0569593720138073}, {"id": 76, "seek": 31000, "start": 327.0, "end": 331.0, "text": " to prawdziw\u0105 gwiazd\u0105 tego show musz\u0105 by\u0107 dane.", "tokens": [51214, 281, 41175, 3992, 86, 1611, 29255, 654, 31278, 1611, 8627, 855, 1038, 8925, 15069, 49206, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07794283514153467, "compression_ratio": 1.4306569343065694, "no_speech_prob": 0.0569593720138073}, {"id": 77, "seek": 31000, "start": 331.0, "end": 333.0, "text": " Co wiemy o zbiorze Dolma.", "tokens": [51414, 3066, 3355, 2226, 277, 710, 33362, 1381, 18786, 1696, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07794283514153467, "compression_ratio": 1.4306569343065694, "no_speech_prob": 0.0569593720138073}, {"id": 78, "seek": 31000, "start": 333.0, "end": 335.0, "text": " I to jest strza\u0142 w dziesi\u0105tk\u0119.", "tokens": [51514, 286, 281, 3492, 1056, 2394, 1221, 261, 9758, 530, 11404, 83, 15724, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07794283514153467, "compression_ratio": 1.4306569343065694, "no_speech_prob": 0.0569593720138073}, {"id": 79, "seek": 31000, "start": 335.0, "end": 337.0, "text": " Dolma to jest serce tego projektu.", "tokens": [51614, 18786, 1696, 281, 3492, 816, 384, 8627, 26261, 84, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07794283514153467, "compression_ratio": 1.4306569343065694, "no_speech_prob": 0.0569593720138073}, {"id": 80, "seek": 33700, "start": 337.0, "end": 340.0, "text": " M\u00f3wimy o zbiorze 3 bilion\u00f3w token\u00f3w.", "tokens": [50364, 376, 3901, 13189, 277, 710, 33362, 1381, 805, 8588, 313, 3901, 281, 2653, 3901, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07054517143651058, "compression_ratio": 1.414715719063545, "no_speech_prob": 0.04818154498934746}, {"id": 81, "seek": 33700, "start": 340.0, "end": 346.0, "text": " \u017beby da\u0107 skal\u0119, to odpowiednik przeczytania kilkudziesi\u0119ciu milion\u00f3w ksi\u0105\u017cek.", "tokens": [50514, 46864, 2322, 1120, 2162, 16890, 1274, 11, 281, 36574, 13123, 8325, 6522, 83, 5609, 5128, 74, 532, 89, 530, 5034, 30795, 1962, 313, 3901, 39311, 916, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07054517143651058, "compression_ratio": 1.414715719063545, "no_speech_prob": 0.04818154498934746}, {"id": 82, "seek": 33700, "start": 346.0, "end": 349.0, "text": " Ale kluczowe jest nie tylko to, jak du\u017cy jest ten zbi\u00f3r,", "tokens": [50814, 9366, 9671, 1311, 89, 6880, 3492, 2838, 13219, 281, 11, 4207, 1581, 7735, 3492, 2064, 710, 5614, 15614, 11, 50964], "temperature": 0.0, "avg_logprob": -0.07054517143651058, "compression_ratio": 1.414715719063545, "no_speech_prob": 0.04818154498934746}, {"id": 83, "seek": 33700, "start": 349.0, "end": 352.0, "text": " ale to, \u017ce jest w pe\u0142ni przezroczysty.", "tokens": [50964, 6775, 281, 11, 3561, 3492, 261, 43205, 3722, 14064, 340, 6522, 25134, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07054517143651058, "compression_ratio": 1.414715719063545, "no_speech_prob": 0.04818154498934746}, {"id": 84, "seek": 33700, "start": 352.0, "end": 356.0, "text": " Znamy ka\u017cdy jego sk\u0142adnik, co jest w menu.", "tokens": [51114, 1176, 5378, 88, 31615, 26542, 1110, 10358, 13123, 11, 598, 3492, 261, 6510, 13, 51314], "temperature": 0.0, "avg_logprob": -0.07054517143651058, "compression_ratio": 1.414715719063545, "no_speech_prob": 0.04818154498934746}, {"id": 85, "seek": 33700, "start": 356.0, "end": 361.0, "text": " G\u0142\u00f3wnym daniem, stanowi\u0105cym prawie 75% ca\u0142o\u015bci, jest Common Crawl.", "tokens": [51314, 460, 1221, 812, 895, 4199, 3277, 4907, 11, 27984, 47886, 1344, 76, 3206, 8699, 9562, 4, 1335, 35059, 11, 3492, 18235, 37877, 75, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07054517143651058, "compression_ratio": 1.414715719063545, "no_speech_prob": 0.04818154498934746}, {"id": 86, "seek": 33700, "start": 361.0, "end": 366.0, "text": " To w zasadzie zaskrobana i przefiltrowana ogromna cz\u0119\u015b\u0107 publicznego internetu.", "tokens": [51564, 1407, 261, 44585, 3283, 710, 3863, 16614, 2095, 741, 8325, 69, 2352, 1892, 2095, 34416, 298, 629, 47149, 1908, 89, 11858, 4705, 84, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07054517143651058, "compression_ratio": 1.414715719063545, "no_speech_prob": 0.04818154498934746}, {"id": 87, "seek": 36600, "start": 366.0, "end": 369.0, "text": " Czyli taki internet w pigu\u0142ce.", "tokens": [50364, 37099, 20065, 4705, 261, 8120, 84, 1221, 384, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09740359966571514, "compression_ratio": 1.4385964912280702, "no_speech_prob": 0.017648112028837204}, {"id": 88, "seek": 36600, "start": 369.0, "end": 373.0, "text": " Ze wszystkimi jego zaletami i, co pewnie wa\u017cniejsze, wadami.", "tokens": [50514, 4853, 14615, 10121, 26542, 29599, 302, 4526, 741, 11, 598, 520, 14215, 27777, 44258, 11, 261, 345, 4526, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09740359966571514, "compression_ratio": 1.4385964912280702, "no_speech_prob": 0.017648112028837204}, {"id": 89, "seek": 36600, "start": 373.0, "end": 378.0, "text": " Dok\u0142adnie i w\u0142a\u015bnie dlatego takie istotne s\u0105 te pozosta\u0142e sk\u0142adniki,", "tokens": [50714, 29768, 10358, 2766, 741, 14234, 32205, 15963, 1418, 310, 716, 9015, 535, 21281, 8638, 19827, 1110, 10358, 77, 9850, 11, 50964], "temperature": 0.0, "avg_logprob": -0.09740359966571514, "compression_ratio": 1.4385964912280702, "no_speech_prob": 0.017648112028837204}, {"id": 90, "seek": 36600, "start": 378.0, "end": 381.0, "text": " kt\u00f3re maj\u0105, no wiesz, zbalansowa\u0107 t\u0105 diet\u0119.", "tokens": [50964, 8864, 26064, 11, 572, 261, 15347, 11, 710, 2645, 599, 11445, 32294, 6339, 1274, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09740359966571514, "compression_ratio": 1.4385964912280702, "no_speech_prob": 0.017648112028837204}, {"id": 91, "seek": 36600, "start": 381.0, "end": 385.0, "text": " Mamy ponad 340 miliard\u00f3w token\u00f3w z GitHub'a,", "tokens": [51114, 376, 7804, 9224, 345, 805, 5254, 1962, 72, 515, 3901, 281, 2653, 3901, 710, 23331, 6, 64, 11, 51314], "temperature": 0.0, "avg_logprob": -0.09740359966571514, "compression_ratio": 1.4385964912280702, "no_speech_prob": 0.017648112028837204}, {"id": 92, "seek": 36600, "start": 385.0, "end": 388.0, "text": " \u017ceby nauczy\u0107 model rozumienia i pisania kodu.", "tokens": [51314, 11316, 49103, 27150, 2316, 48797, 18811, 741, 26584, 5609, 350, 34873, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09740359966571514, "compression_ratio": 1.4385964912280702, "no_speech_prob": 0.017648112028837204}, {"id": 93, "seek": 36600, "start": 388.0, "end": 391.0, "text": " Mamy 80 miliard\u00f3w token\u00f3w z Reddit'a,", "tokens": [51464, 376, 7804, 4688, 1962, 72, 515, 3901, 281, 2653, 3901, 710, 32210, 6, 64, 11, 51614], "temperature": 0.0, "avg_logprob": -0.09740359966571514, "compression_ratio": 1.4385964912280702, "no_speech_prob": 0.017648112028837204}, {"id": 94, "seek": 36600, "start": 391.0, "end": 394.0, "text": " co uczy go bardziej nieformalnego, konwersacyjnego stylu.", "tokens": [51614, 598, 344, 6522, 352, 27209, 2838, 837, 304, 11858, 11, 5897, 5364, 31285, 11858, 7952, 2781, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09740359966571514, "compression_ratio": 1.4385964912280702, "no_speech_prob": 0.017648112028837204}, {"id": 95, "seek": 39400, "start": 394.0, "end": 397.0, "text": " A reszt\u0119 uzupe\u0142niaj\u0105 sk\u0142adniki wysokiej jako\u015bci.", "tokens": [50364, 316, 725, 2682, 1274, 344, 11728, 31457, 12679, 8555, 1110, 10358, 77, 9850, 27062, 453, 7764, 17123, 6199, 13, 50514], "temperature": 0.0, "avg_logprob": -0.06546465555826823, "compression_ratio": 1.4820359281437125, "no_speech_prob": 0.0046516237780451775}, {"id": 96, "seek": 39400, "start": 397.0, "end": 402.0, "text": " Artyku\u0142y naukowe, ksi\u0105\u017cki z projektu Gutenberg czy ca\u0142a angielska Wikipedia.", "tokens": [50514, 5735, 88, 5279, 6825, 35616, 74, 6880, 11, 39311, 2984, 710, 26261, 84, 42833, 6873, 6430, 1335, 5024, 2562, 1187, 20771, 28999, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06546465555826823, "compression_ratio": 1.4820359281437125, "no_speech_prob": 0.0046516237780451775}, {"id": 97, "seek": 39400, "start": 402.0, "end": 406.0, "text": " I po raz pierwszy badacze mog\u0105 dok\u0142adnie prze\u015bledzi\u0107,", "tokens": [50764, 286, 714, 9639, 34016, 1578, 326, 1381, 34123, 45864, 2766, 8325, 1788, 1493, 28496, 11, 50964], "temperature": 0.0, "avg_logprob": -0.06546465555826823, "compression_ratio": 1.4820359281437125, "no_speech_prob": 0.0046516237780451775}, {"id": 98, "seek": 39400, "start": 406.0, "end": 410.0, "text": " jak konkretne \u017ar\u00f3d\u0142a danych wp\u0142ywaj\u0105 na zdolno\u015bci modelu.", "tokens": [50964, 4207, 36500, 716, 50212, 43678, 5024, 274, 34644, 32444, 6825, 86, 11133, 1667, 16221, 401, 16438, 2316, 84, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06546465555826823, "compression_ratio": 1.4820359281437125, "no_speech_prob": 0.0046516237780451775}, {"id": 99, "seek": 39400, "start": 410.0, "end": 413.0, "text": " Domy\u015blam si\u0119, \u017ce to otwiera zupe\u0142nie nowe mo\u017cliwo\u015bci badawcze.", "tokens": [51164, 413, 8488, 1788, 4326, 3244, 11, 3561, 281, 4337, 86, 10609, 49922, 586, 68, 30854, 36476, 272, 1538, 86, 9680, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06546465555826823, "compression_ratio": 1.4820359281437125, "no_speech_prob": 0.0046516237780451775}, {"id": 100, "seek": 39400, "start": 413.0, "end": 416.0, "text": " Mo\u017cna na przyk\u0142ad sprawdzi\u0107, co si\u0119 stanie,", "tokens": [51314, 44736, 629, 1667, 23144, 46192, 28496, 11, 598, 3244, 40013, 11, 51464], "temperature": 0.0, "avg_logprob": -0.06546465555826823, "compression_ratio": 1.4820359281437125, "no_speech_prob": 0.0046516237780451775}, {"id": 101, "seek": 39400, "start": 416.0, "end": 419.0, "text": " je\u015bli, nie wiem, usuniemy Reddit'a ze zbioru danych.", "tokens": [51464, 25630, 11, 2838, 26522, 11, 505, 409, 414, 2226, 32210, 6, 64, 5277, 710, 33362, 84, 274, 34644, 13, 51614], "temperature": 0.0, "avg_logprob": -0.06546465555826823, "compression_ratio": 1.4820359281437125, "no_speech_prob": 0.0046516237780451775}, {"id": 102, "seek": 39400, "start": 419.0, "end": 422.0, "text": " Czy model stanie si\u0119 mniej kreatywny, ale za to bardziej formalny?", "tokens": [51614, 19832, 2316, 40013, 3244, 39513, 350, 620, 88, 43682, 11, 6775, 7949, 281, 27209, 9860, 1634, 30, 51764], "temperature": 0.0, "avg_logprob": -0.06546465555826823, "compression_ratio": 1.4820359281437125, "no_speech_prob": 0.0046516237780451775}, {"id": 103, "seek": 42200, "start": 422.0, "end": 426.0, "text": " W\u0142a\u015bnie o to chodzi. Mo\u017cna zadawa\u0107 takie pytania.", "tokens": [50364, 343, 5024, 12221, 277, 281, 23998, 13, 44736, 629, 710, 1538, 25234, 15963, 25878, 5609, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08833867784530397, "compression_ratio": 1.3828996282527881, "no_speech_prob": 0.04865996167063713}, {"id": 104, "seek": 42200, "start": 426.0, "end": 431.0, "text": " I co wa\u017cniejsze, mo\u017cna na nie odpowiada\u0107 w spos\u00f3b naukowy.", "tokens": [50564, 286, 598, 27777, 44258, 11, 17790, 1667, 2838, 24314, 39018, 2162, 261, 22904, 35616, 74, 10089, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08833867784530397, "compression_ratio": 1.3828996282527881, "no_speech_prob": 0.04865996167063713}, {"id": 105, "seek": 42200, "start": 431.0, "end": 436.0, "text": " Bo mamy pe\u0142en wgl\u0105d w dane i nawet narz\u0119dzia do ich odtworzenia.", "tokens": [50814, 3286, 17335, 43205, 268, 261, 7191, 18962, 261, 49206, 741, 22696, 6714, 89, 6298, 40395, 360, 1893, 3611, 20270, 284, 14320, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08833867784530397, "compression_ratio": 1.3828996282527881, "no_speech_prob": 0.04865996167063713}, {"id": 106, "seek": 42200, "start": 436.0, "end": 438.0, "text": " To jest bezprecedensowe.", "tokens": [51064, 1407, 3492, 10782, 3712, 1232, 694, 6880, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08833867784530397, "compression_ratio": 1.3828996282527881, "no_speech_prob": 0.04865996167063713}, {"id": 107, "seek": 42200, "start": 438.0, "end": 441.0, "text": " To naprawd\u0119 zmienia zasady gry w badaniach nad AI.", "tokens": [51164, 1407, 20970, 17020, 18811, 26530, 880, 41974, 261, 1578, 3782, 608, 12617, 7318, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08833867784530397, "compression_ratio": 1.3828996282527881, "no_speech_prob": 0.04865996167063713}, {"id": 108, "seek": 42200, "start": 441.0, "end": 445.0, "text": " Okej, czyli mamy architektur\u0119 oparto na solidnych fundamentach", "tokens": [51314, 29094, 73, 11, 16591, 17335, 3912, 642, 2320, 374, 1274, 999, 15864, 1667, 5100, 9399, 6073, 608, 51514], "temperature": 0.0, "avg_logprob": -0.08833867784530397, "compression_ratio": 1.3828996282527881, "no_speech_prob": 0.04865996167063713}, {"id": 109, "seek": 42200, "start": 445.0, "end": 448.0, "text": " i ten gigantyczny przezroczysty zbi\u00f3r danych.", "tokens": [51514, 741, 2064, 8741, 394, 17466, 1634, 14064, 340, 6522, 25134, 710, 5614, 15614, 274, 34644, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08833867784530397, "compression_ratio": 1.3828996282527881, "no_speech_prob": 0.04865996167063713}, {"id": 110, "seek": 44800, "start": 448.0, "end": 450.0, "text": " Ale to wszystko teoria.", "tokens": [50364, 9366, 281, 22607, 535, 8172, 13, 50464], "temperature": 0.0, "avg_logprob": -0.08640024677807132, "compression_ratio": 1.4234527687296417, "no_speech_prob": 0.32730230689048767}, {"id": 111, "seek": 44800, "start": 450.0, "end": 454.0, "text": " Najciekawsze jest to, jak z tych klock\u00f3w zbudowano co\u015b dzia\u0142aj\u0105cego.", "tokens": [50464, 31576, 4260, 74, 28354, 3492, 281, 11, 4207, 710, 15180, 350, 4102, 3901, 710, 18281, 305, 3730, 19241, 27121, 11133, 384, 1571, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08640024677807132, "compression_ratio": 1.4234527687296417, "no_speech_prob": 0.32730230689048767}, {"id": 112, "seek": 44800, "start": 454.0, "end": 457.0, "text": " Sam proces treningu musia\u0142 by\u0107 pot\u0119\u017cnym wyzwaniem logistycznym.", "tokens": [50664, 4832, 17565, 2192, 773, 84, 1038, 8908, 15069, 1847, 1274, 1427, 12996, 4628, 89, 7916, 4907, 3565, 468, 17466, 12996, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08640024677807132, "compression_ratio": 1.4234527687296417, "no_speech_prob": 0.32730230689048767}, {"id": 113, "seek": 44800, "start": 457.0, "end": 462.0, "text": " Ogromnym. Do zarz\u0105dzania tym procesem na setkach procesor\u00f3w graficznych", "tokens": [50814, 422, 861, 298, 12996, 13, 1144, 22675, 23876, 89, 5609, 8107, 17565, 443, 1667, 992, 41326, 17565, 284, 3901, 1295, 1786, 89, 9399, 51064], "temperature": 0.0, "avg_logprob": -0.08640024677807132, "compression_ratio": 1.4234527687296417, "no_speech_prob": 0.32730230689048767}, {"id": 114, "seek": 44800, "start": 462.0, "end": 468.0, "text": " u\u017cyli w frameworku PyTorch FSDP ze strategi\u0105 optymalizatora zero.", "tokens": [51064, 34097, 2081, 261, 8388, 84, 9953, 51, 284, 339, 41138, 11373, 5277, 5464, 11404, 2427, 4199, 304, 590, 1639, 64, 4018, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08640024677807132, "compression_ratio": 1.4234527687296417, "no_speech_prob": 0.32730230689048767}, {"id": 115, "seek": 44800, "start": 468.0, "end": 471.0, "text": " To kolejne techniczne terminy, jak\u0105 one pe\u0142ni\u0105 rol\u0119.", "tokens": [51364, 1407, 23749, 716, 1537, 17946, 716, 1433, 3519, 11, 46719, 472, 43205, 3722, 1611, 34109, 1274, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08640024677807132, "compression_ratio": 1.4234527687296417, "no_speech_prob": 0.32730230689048767}, {"id": 116, "seek": 44800, "start": 471.0, "end": 473.0, "text": " Co one rozwi\u0105zuj\u0105?", "tokens": [51514, 3066, 472, 9544, 22620, 13263, 30, 51614], "temperature": 0.0, "avg_logprob": -0.08640024677807132, "compression_ratio": 1.4234527687296417, "no_speech_prob": 0.32730230689048767}, {"id": 117, "seek": 44800, "start": 473.0, "end": 476.0, "text": " M\u00f3wi\u0105c najpro\u015bciej rozwi\u0105zuj\u0105 problem pami\u0119ci.", "tokens": [51614, 376, 3901, 11404, 66, 11212, 4318, 9815, 73, 9544, 22620, 13263, 1154, 31088, 537, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08640024677807132, "compression_ratio": 1.4234527687296417, "no_speech_prob": 0.32730230689048767}, {"id": 118, "seek": 47600, "start": 476.0, "end": 479.0, "text": " Wsp\u00f3\u0142czesne modele s\u0105 tak gigantyczne,", "tokens": [50364, 343, 4952, 16181, 3689, 279, 716, 4391, 306, 9015, 991, 8741, 394, 17466, 716, 11, 50514], "temperature": 0.0, "avg_logprob": -0.06151818466186523, "compression_ratio": 1.3901515151515151, "no_speech_prob": 0.05370517075061798}, {"id": 119, "seek": 47600, "start": 479.0, "end": 484.0, "text": " \u017ce nie mieszcz\u0105 si\u0119 w pami\u0119ci nawet najpot\u0119\u017cniejszego pojedynczego GPU.", "tokens": [50514, 3561, 2838, 33039, 3689, 1611, 3244, 261, 31088, 537, 22696, 11212, 17698, 1274, 1427, 10402, 15453, 6308, 714, 40543, 2534, 3689, 6308, 18407, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06151818466186523, "compression_ratio": 1.3901515151515151, "no_speech_prob": 0.05370517075061798}, {"id": 120, "seek": 47600, "start": 484.0, "end": 487.0, "text": " FSDP i zero to takie sprytne techniki,", "tokens": [50764, 41138, 11373, 741, 4018, 281, 15963, 637, 627, 83, 716, 1537, 9850, 11, 50914], "temperature": 0.0, "avg_logprob": -0.06151818466186523, "compression_ratio": 1.3901515151515151, "no_speech_prob": 0.05370517075061798}, {"id": 121, "seek": 47600, "start": 487.0, "end": 492.0, "text": " kt\u00f3re pozwalaj\u0105 pokroi\u0107 model i jego stany obliczeniowe na kawa\u0142ki", "tokens": [50914, 8864, 40557, 304, 11133, 13010, 340, 12757, 2316, 741, 26542, 342, 1325, 1111, 1050, 42124, 6880, 1667, 350, 10449, 1221, 2984, 51164], "temperature": 0.0, "avg_logprob": -0.06151818466186523, "compression_ratio": 1.3901515151515151, "no_speech_prob": 0.05370517075061798}, {"id": 122, "seek": 47600, "start": 492.0, "end": 497.0, "text": " i rozdzieli\u0107 je pomi\u0119dzy setki kart graficznych, kt\u00f3re pracuj\u0105 razem.", "tokens": [51164, 741, 9544, 28168, 1187, 12757, 1506, 12991, 49485, 992, 2984, 29120, 1295, 1786, 89, 9399, 11, 8864, 22404, 13263, 40225, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06151818466186523, "compression_ratio": 1.3901515151515151, "no_speech_prob": 0.05370517075061798}, {"id": 123, "seek": 47600, "start": 497.0, "end": 501.0, "text": " To kluczowa technologia, \u017ceby w og\u00f3le trenowa\u0107 takie bestie.", "tokens": [51414, 1407, 9671, 1311, 89, 5528, 1537, 24103, 11, 11316, 261, 29229, 23136, 11445, 15963, 1151, 414, 13, 51614], "temperature": 0.0, "avg_logprob": -0.06151818466186523, "compression_ratio": 1.3901515151515151, "no_speech_prob": 0.05370517075061798}, {"id": 124, "seek": 50100, "start": 501.0, "end": 506.0, "text": " A jest w tym procesie treningu co\u015b, co szczeg\u00f3lnie przykr\u00f3\u0142o twoj\u0105 uwag\u0119?", "tokens": [50364, 316, 3492, 261, 8107, 17565, 414, 2192, 773, 84, 19241, 11, 598, 49624, 2766, 6501, 74, 11721, 5249, 732, 8555, 43696, 30, 50614], "temperature": 0.0, "avg_logprob": -0.10381229718526204, "compression_ratio": 1.3562091503267975, "no_speech_prob": 0.08525265753269196}, {"id": 125, "seek": 50100, "start": 506.0, "end": 509.0, "text": " Zdecydowanie. Jeden detal, kt\u00f3ry pokazuje,", "tokens": [50614, 1176, 1479, 1344, 67, 22028, 13, 508, 6876, 33185, 11, 9913, 13010, 43317, 11, 50764], "temperature": 0.0, "avg_logprob": -0.10381229718526204, "compression_ratio": 1.3562091503267975, "no_speech_prob": 0.08525265753269196}, {"id": 126, "seek": 50100, "start": 509.0, "end": 514.0, "text": " jak bardzo powa\u017cnie podeszli do idei otwarto\u015bci i uniwersalno\u015bci.", "tokens": [50764, 4207, 9034, 3388, 18264, 2766, 2497, 10430, 2081, 360, 1153, 72, 4337, 86, 15864, 6199, 741, 36435, 5364, 304, 16438, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10381229718526204, "compression_ratio": 1.3562091503267975, "no_speech_prob": 0.08525265753269196}, {"id": 127, "seek": 50100, "start": 514.0, "end": 519.0, "text": " Trening prowadzono r\u00f3wnolegle na dw\u00f3ch zupe\u0142nie r\u00f3\u017cnych klastrach sprz\u0119towych.", "tokens": [51014, 8648, 773, 36590, 89, 8957, 11416, 895, 4812, 22631, 1667, 27379, 812, 339, 49922, 42602, 9671, 525, 81, 608, 6103, 11052, 83, 19605, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10381229718526204, "compression_ratio": 1.3562091503267975, "no_speech_prob": 0.08525265753269196}, {"id": 128, "seek": 50100, "start": 519.0, "end": 522.0, "text": " Jeden by\u0142 oparty na GPU o ten VD, model A100,", "tokens": [51264, 508, 6876, 16673, 999, 446, 88, 1667, 18407, 277, 2064, 691, 35, 11, 2316, 316, 6879, 11, 51414], "temperature": 0.0, "avg_logprob": -0.10381229718526204, "compression_ratio": 1.3562091503267975, "no_speech_prob": 0.08525265753269196}, {"id": 129, "seek": 50100, "start": 522.0, "end": 528.0, "text": " a drugi na konkurencyjnych kartach od AMD, MI250X. Tego cel.", "tokens": [51414, 257, 4110, 72, 1667, 21428, 9873, 42949, 9399, 29120, 608, 3611, 34808, 11, 13696, 23538, 55, 13, 314, 6308, 9277, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10381229718526204, "compression_ratio": 1.3562091503267975, "no_speech_prob": 0.08525265753269196}, {"id": 130, "seek": 50100, "start": 528.0, "end": 530.0, "text": " Czy to nie komplikuje sprawy?", "tokens": [51714, 19832, 281, 2838, 24526, 1035, 13008, 22734, 88, 30, 51814], "temperature": 0.0, "avg_logprob": -0.10381229718526204, "compression_ratio": 1.3562091503267975, "no_speech_prob": 0.08525265753269196}, {"id": 131, "seek": 53000, "start": 530.0, "end": 533.0, "text": " To jak? Ale cel by\u0142 w\u0142a\u015bnie taki.", "tokens": [50364, 1407, 4207, 30, 9366, 9277, 16673, 14234, 20065, 13, 50514], "temperature": 0.0, "avg_logprob": -0.06675839424133301, "compression_ratio": 1.4833948339483394, "no_speech_prob": 0.048331622034311295}, {"id": 132, "seek": 53000, "start": 533.0, "end": 536.0, "text": " Udowodni\u0107, \u017ce ich kod jest przeno\u015bny.", "tokens": [50514, 624, 67, 305, 378, 3722, 2162, 11, 3561, 1893, 350, 378, 3492, 582, 2904, 78, 1788, 1634, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06675839424133301, "compression_ratio": 1.4833948339483394, "no_speech_prob": 0.048331622034311295}, {"id": 133, "seek": 53000, "start": 536.0, "end": 540.0, "text": " \u017be nie jest przyspawany do jednego dostawcy sprz\u0119tu.", "tokens": [50664, 46864, 2838, 3492, 6541, 749, 79, 1607, 1325, 360, 5232, 11858, 20568, 1607, 1344, 6103, 11052, 9179, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06675839424133301, "compression_ratio": 1.4833948339483394, "no_speech_prob": 0.048331622034311295}, {"id": 134, "seek": 53000, "start": 540.0, "end": 543.0, "text": " To jest pot\u0119\u017cny sygna\u0142 dla ca\u0142ej spo\u0142eczno\u015bci,", "tokens": [50864, 1407, 3492, 1847, 1274, 1427, 1634, 943, 70, 629, 1221, 12285, 47631, 73, 36851, 89, 16438, 11, 51014], "temperature": 0.0, "avg_logprob": -0.06675839424133301, "compression_ratio": 1.4833948339483394, "no_speech_prob": 0.048331622034311295}, {"id": 135, "seek": 53000, "start": 543.0, "end": 548.0, "text": " \u017ce mo\u017cna budowa\u0107 wielkie modele bez zamykania si\u0119 w jednym ekosystemie.", "tokens": [51014, 3561, 17790, 3265, 11445, 20570, 22872, 4391, 306, 10782, 710, 7804, 5225, 654, 3244, 261, 5232, 12996, 13359, 329, 9321, 414, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06675839424133301, "compression_ratio": 1.4833948339483394, "no_speech_prob": 0.048331622034311295}, {"id": 136, "seek": 53000, "start": 548.0, "end": 550.0, "text": " A co najwa\u017cniejsze, pokazali,", "tokens": [51264, 316, 598, 11212, 27111, 44258, 11, 13010, 921, 5103, 11, 51364], "temperature": 0.0, "avg_logprob": -0.06675839424133301, "compression_ratio": 1.4833948339483394, "no_speech_prob": 0.048331622034311295}, {"id": 137, "seek": 53000, "start": 550.0, "end": 554.0, "text": " \u017ce wyniki na obu platformach by\u0142y praktycznie identyczne.", "tokens": [51364, 3561, 31936, 9850, 1667, 1111, 84, 3663, 608, 26366, 3206, 74, 45586, 2473, 17466, 716, 13, 51564], "temperature": 0.0, "avg_logprob": -0.06675839424133301, "compression_ratio": 1.4833948339483394, "no_speech_prob": 0.048331622034311295}, {"id": 138, "seek": 53000, "start": 554.0, "end": 557.0, "text": " To \u015bwiadczy o niesamowitej solidno\u015bci ich kodu.", "tokens": [51564, 1407, 21485, 345, 6522, 277, 48100, 335, 305, 642, 73, 5100, 16438, 1893, 350, 34873, 13, 51714], "temperature": 0.0, "avg_logprob": -0.06675839424133301, "compression_ratio": 1.4833948339483394, "no_speech_prob": 0.048331622034311295}, {"id": 139, "seek": 55700, "start": 557.0, "end": 560.0, "text": " To robi wra\u017cenie. A jak monitorowali post\u0119py?", "tokens": [50364, 1407, 47380, 7843, 41118, 13, 316, 4207, 6002, 305, 5103, 2183, 1274, 8200, 30, 50514], "temperature": 0.0, "avg_logprob": -0.06401375822118811, "compression_ratio": 1.4006849315068493, "no_speech_prob": 0.06908753514289856}, {"id": 140, "seek": 55700, "start": 560.0, "end": 564.0, "text": " Czekali miesi\u0105cami na wynik ko\u0144cowy z za\u0142o\u017conymi r\u0119kami?", "tokens": [50514, 383, 19878, 5103, 41543, 11404, 66, 4526, 1667, 31936, 1035, 26470, 66, 10089, 710, 7949, 5249, 1427, 2526, 3057, 41197, 48737, 30, 50714], "temperature": 0.0, "avg_logprob": -0.06401375822118811, "compression_ratio": 1.4006849315068493, "no_speech_prob": 0.06908753514289856}, {"id": 141, "seek": 55700, "start": 564.0, "end": 566.0, "text": " Wr\u0119cz przeciwnie.", "tokens": [50714, 10159, 1274, 3689, 39622, 14215, 13, 50814], "temperature": 0.0, "avg_logprob": -0.06401375822118811, "compression_ratio": 1.4006849315068493, "no_speech_prob": 0.06908753514289856}, {"id": 142, "seek": 55700, "start": 566.0, "end": 572.0, "text": " Zastosowali co\u015b, co nazywaj\u0105 ewaluacj\u0105 w p\u0119tli, czyli in the loop evaluation.", "tokens": [50814, 1176, 525, 329, 305, 5103, 19241, 11, 598, 20151, 27112, 11133, 43364, 4929, 326, 8555, 261, 280, 46788, 2081, 11, 16591, 294, 264, 6367, 13344, 13, 51114], "temperature": 0.0, "avg_logprob": -0.06401375822118811, "compression_ratio": 1.4006849315068493, "no_speech_prob": 0.06908753514289856}, {"id": 143, "seek": 55700, "start": 572.0, "end": 574.0, "text": " Co 1000 krok\u00f3w treningowych,", "tokens": [51114, 3066, 9714, 45909, 23849, 2192, 773, 19605, 11, 51214], "temperature": 0.0, "avg_logprob": -0.06401375822118811, "compression_ratio": 1.4006849315068493, "no_speech_prob": 0.06908753514289856}, {"id": 144, "seek": 55700, "start": 574.0, "end": 577.0, "text": " czyli co oko\u0142o 4 miliardy przetworzonych token\u00f3w,", "tokens": [51214, 16591, 598, 45730, 5249, 1017, 1962, 72, 515, 88, 6541, 302, 28321, 44479, 339, 14862, 3901, 11, 51364], "temperature": 0.0, "avg_logprob": -0.06401375822118811, "compression_ratio": 1.4006849315068493, "no_speech_prob": 0.06908753514289856}, {"id": 145, "seek": 55700, "start": 577.0, "end": 580.0, "text": " automatycznie uruchamiali ca\u0142\u0105 bateri\u0119 test\u00f3w.", "tokens": [51364, 28034, 17466, 2766, 4038, 625, 335, 831, 72, 1335, 15926, 25735, 5034, 1500, 3901, 13, 51514], "temperature": 0.0, "avg_logprob": -0.06401375822118811, "compression_ratio": 1.4006849315068493, "no_speech_prob": 0.06908753514289856}, {"id": 146, "seek": 55700, "start": 580.0, "end": 584.0, "text": " To dawa\u0142o im niemal na bie\u017c\u0105co obraz tego, jak model si\u0119 uczy", "tokens": [51514, 1407, 1120, 4151, 5249, 566, 2838, 5579, 1667, 272, 414, 1427, 1611, 1291, 22798, 89, 8627, 11, 4207, 2316, 3244, 344, 6522, 51714], "temperature": 0.0, "avg_logprob": -0.06401375822118811, "compression_ratio": 1.4006849315068493, "no_speech_prob": 0.06908753514289856}, {"id": 147, "seek": 58400, "start": 584.0, "end": 587.0, "text": " i czy wszystko idzie w dobr\u0105 kierunku.", "tokens": [50364, 741, 6430, 22607, 4496, 3283, 261, 23067, 1611, 38767, 49910, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10246900009782348, "compression_ratio": 1.3275862068965518, "no_speech_prob": 0.07992219924926758}, {"id": 148, "seek": 58400, "start": 587.0, "end": 589.0, "text": " Zamiast czeka\u0107 miesi\u0105c, \u017ceby odkry\u0107,", "tokens": [50514, 1176, 4526, 525, 6472, 36361, 2162, 41543, 11404, 66, 11, 11316, 3611, 43298, 2162, 11, 50614], "temperature": 0.0, "avg_logprob": -0.10246900009782348, "compression_ratio": 1.3275862068965518, "no_speech_prob": 0.07992219924926758}, {"id": 149, "seek": 58400, "start": 589.0, "end": 592.0, "text": " \u017ce jaki\u015b hiperparametr by\u0142 \u017ale ustawiony,", "tokens": [50614, 3561, 34721, 4879, 610, 2181, 335, 27965, 16673, 50212, 306, 26189, 1607, 46184, 11, 50764], "temperature": 0.0, "avg_logprob": -0.10246900009782348, "compression_ratio": 1.3275862068965518, "no_speech_prob": 0.07992219924926758}, {"id": 150, "seek": 58400, "start": 592.0, "end": 594.0, "text": " mieli sygna\u0142 zwrotny niemal od razu.", "tokens": [50764, 41214, 943, 70, 629, 1221, 49111, 310, 1634, 2838, 5579, 3611, 367, 8813, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10246900009782348, "compression_ratio": 1.3275862068965518, "no_speech_prob": 0.07992219924926758}, {"id": 151, "seek": 58400, "start": 594.0, "end": 597.0, "text": " Przejd\u017amy wi\u0119c do wynik\u00f3w tej ewaluacji.", "tokens": [50864, 2114, 16920, 67, 10659, 2226, 16677, 360, 31936, 1035, 3901, 12573, 43364, 4929, 13152, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10246900009782348, "compression_ratio": 1.3275862068965518, "no_speech_prob": 0.07992219924926758}, {"id": 152, "seek": 58400, "start": 597.0, "end": 603.0, "text": " Jak OLMO 7B wypada w bezpo\u015brednim starciu z innymi modelami tej samej wielko\u015bci,", "tokens": [51014, 15029, 422, 43, 18976, 1614, 33, 46392, 1538, 261, 10782, 2259, 1788, 986, 39223, 3543, 30795, 710, 294, 31813, 2316, 4526, 12573, 912, 73, 20570, 4093, 6199, 11, 51314], "temperature": 0.0, "avg_logprob": -0.10246900009782348, "compression_ratio": 1.3275862068965518, "no_speech_prob": 0.07992219924926758}, {"id": 153, "seek": 58400, "start": 603.0, "end": 608.0, "text": " takimi jak LAMMA 2 7B czy Falcon 7B?", "tokens": [51314, 991, 10121, 4207, 441, 2865, 9998, 568, 1614, 33, 6430, 31801, 1614, 33, 30, 51564], "temperature": 0.0, "avg_logprob": -0.10246900009782348, "compression_ratio": 1.3275862068965518, "no_speech_prob": 0.07992219924926758}, {"id": 154, "seek": 58400, "start": 608.0, "end": 610.0, "text": " Jest jaki\u015b knockout?", "tokens": [51564, 24918, 34721, 6728, 346, 30, 51664], "temperature": 0.0, "avg_logprob": -0.10246900009782348, "compression_ratio": 1.3275862068965518, "no_speech_prob": 0.07992219924926758}, {"id": 155, "seek": 58400, "start": 610.0, "end": 612.0, "text": " Wyniki s\u0105 bardzo, bardzo solidne.", "tokens": [51664, 343, 2534, 9850, 9015, 9034, 11, 9034, 5100, 716, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10246900009782348, "compression_ratio": 1.3275862068965518, "no_speech_prob": 0.07992219924926758}, {"id": 156, "seek": 61200, "start": 612.0, "end": 614.0, "text": " W serii o\u015bmupopularnych test\u00f3w,", "tokens": [50364, 343, 816, 5597, 277, 1788, 76, 1010, 404, 1040, 9399, 1500, 3901, 11, 50464], "temperature": 0.0, "avg_logprob": -0.0789272250551166, "compression_ratio": 1.370253164556962, "no_speech_prob": 0.011492383666336536}, {"id": 157, "seek": 61200, "start": 614.0, "end": 616.0, "text": " sprawdzaj\u0105cych rozumowanie i wiedz\u0119,", "tokens": [50464, 46192, 89, 11133, 31306, 48797, 22028, 741, 46894, 11052, 11, 50564], "temperature": 0.0, "avg_logprob": -0.0789272250551166, "compression_ratio": 1.370253164556962, "no_speech_prob": 0.011492383666336536}, {"id": 158, "seek": 61200, "start": 616.0, "end": 619.0, "text": " takich jak ARK, Hela Swagg czy Winogrande,", "tokens": [50564, 29607, 4207, 8943, 42, 11, 389, 4053, 3926, 559, 70, 6430, 10427, 664, 3699, 68, 11, 50714], "temperature": 0.0, "avg_logprob": -0.0789272250551166, "compression_ratio": 1.370253164556962, "no_speech_prob": 0.011492383666336536}, {"id": 159, "seek": 61200, "start": 619.0, "end": 622.0, "text": " OLMO 7B jest w \u015bcis\u0142ej czo\u0142\u00f3wce.", "tokens": [50714, 422, 43, 18976, 1614, 33, 3492, 261, 8299, 26720, 19827, 73, 269, 4765, 1221, 3901, 384, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0789272250551166, "compression_ratio": 1.370253164556962, "no_speech_prob": 0.011492383666336536}, {"id": 160, "seek": 61200, "start": 622.0, "end": 627.0, "text": " Jego \u015bredni wynik jest w pe\u0142ni por\u00f3wnywalny z LAMMA 2 7B.", "tokens": [50864, 508, 6308, 8299, 986, 3722, 31936, 1035, 3492, 261, 43205, 3722, 1515, 812, 895, 27112, 304, 1634, 710, 441, 2865, 9998, 568, 1614, 33, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0789272250551166, "compression_ratio": 1.370253164556962, "no_speech_prob": 0.011492383666336536}, {"id": 161, "seek": 61200, "start": 627.0, "end": 629.0, "text": " Nie dominuje we wszystkich kategoriach,", "tokens": [51114, 12016, 8859, 13008, 321, 34234, 350, 2968, 7386, 608, 11, 51214], "temperature": 0.0, "avg_logprob": -0.0789272250551166, "compression_ratio": 1.370253164556962, "no_speech_prob": 0.011492383666336536}, {"id": 162, "seek": 61200, "start": 629.0, "end": 631.0, "text": " ale te\u017c w \u017cadnej znacz\u0105co nie odstaje.", "tokens": [51214, 6775, 9516, 261, 39628, 11794, 15397, 326, 8925, 1291, 2838, 3611, 372, 11153, 13, 51314], "temperature": 0.0, "avg_logprob": -0.0789272250551166, "compression_ratio": 1.370253164556962, "no_speech_prob": 0.011492383666336536}, {"id": 163, "seek": 61200, "start": 631.0, "end": 633.0, "text": " Jest po prostu bardzo konkurencyjny.", "tokens": [51314, 24918, 714, 19518, 9034, 21428, 9873, 42949, 1634, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0789272250551166, "compression_ratio": 1.370253164556962, "no_speech_prob": 0.011492383666336536}, {"id": 164, "seek": 61200, "start": 633.0, "end": 636.0, "text": " Czyli jest po prostu tak samo dobry.", "tokens": [51414, 37099, 3492, 714, 19518, 991, 36422, 35884, 13, 51564], "temperature": 0.0, "avg_logprob": -0.0789272250551166, "compression_ratio": 1.370253164556962, "no_speech_prob": 0.011492383666336536}, {"id": 165, "seek": 61200, "start": 636.0, "end": 639.0, "text": " Nie brzmi to jak rewolucja, kt\u00f3rej mo\u017cna by si\u0119 spodziewa\u0107.", "tokens": [51564, 12016, 738, 89, 3057, 281, 4207, 319, 48481, 1311, 2938, 11, 36023, 17790, 538, 3244, 637, 378, 89, 1093, 43379, 13, 51714], "temperature": 0.0, "avg_logprob": -0.0789272250551166, "compression_ratio": 1.370253164556962, "no_speech_prob": 0.011492383666336536}, {"id": 166, "seek": 63900, "start": 639.0, "end": 642.0, "text": " Czy w tych danych jest co\u015b, co nas zaskoczy\u0142o?", "tokens": [50364, 19832, 261, 15180, 274, 34644, 3492, 19241, 11, 598, 5382, 710, 3863, 905, 1229, 5249, 30, 50514], "temperature": 0.0, "avg_logprob": -0.07809261322021484, "compression_ratio": 1.4640522875816993, "no_speech_prob": 0.04913051053881645}, {"id": 167, "seek": 63900, "start": 642.0, "end": 646.0, "text": " Jaki\u015b nie wiem, nieintuicyjny wzorzec, kt\u00f3ry rzuci\u0142 si\u0119 w oczy.", "tokens": [50514, 508, 7421, 1788, 2838, 26522, 11, 2838, 686, 84, 2632, 73, 1634, 24809, 284, 1381, 66, 11, 9913, 367, 11728, 537, 1221, 3244, 261, 277, 6522, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07809261322021484, "compression_ratio": 1.4640522875816993, "no_speech_prob": 0.04913051053881645}, {"id": 168, "seek": 63900, "start": 646.0, "end": 648.0, "text": " I to jest \u015bwietne pytanie,", "tokens": [50714, 286, 281, 3492, 8299, 39083, 716, 36610, 11, 50814], "temperature": 0.0, "avg_logprob": -0.07809261322021484, "compression_ratio": 1.4640522875816993, "no_speech_prob": 0.04913051053881645}, {"id": 169, "seek": 63900, "start": 648.0, "end": 651.0, "text": " bo najciekawszy wniosek nie kryje si\u0119 w samej \u015bredniej,", "tokens": [50814, 748, 11212, 4260, 74, 12282, 1229, 261, 3722, 541, 74, 2838, 34847, 2884, 3244, 261, 912, 73, 8299, 986, 10402, 11, 50964], "temperature": 0.0, "avg_logprob": -0.07809261322021484, "compression_ratio": 1.4640522875816993, "no_speech_prob": 0.04913051053881645}, {"id": 170, "seek": 63900, "start": 651.0, "end": 653.0, "text": " ale w szczeg\u00f3\u0142ach procesu.", "tokens": [50964, 6775, 261, 22090, 1146, 16181, 608, 17565, 84, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07809261322021484, "compression_ratio": 1.4640522875816993, "no_speech_prob": 0.04913051053881645}, {"id": 171, "seek": 63900, "start": 653.0, "end": 656.0, "text": " W pracy jest wykres pokazuj\u0105cy wydajno\u015b\u0107 modelu", "tokens": [51064, 343, 35591, 3492, 39287, 495, 13010, 921, 13263, 1344, 25984, 1805, 23293, 2316, 84, 51214], "temperature": 0.0, "avg_logprob": -0.07809261322021484, "compression_ratio": 1.4640522875816993, "no_speech_prob": 0.04913051053881645}, {"id": 172, "seek": 63900, "start": 656.0, "end": 658.0, "text": " w trakcie ca\u0142ego treningu.", "tokens": [51214, 261, 944, 74, 4260, 35224, 6308, 2192, 773, 84, 13, 51314], "temperature": 0.0, "avg_logprob": -0.07809261322021484, "compression_ratio": 1.4640522875816993, "no_speech_prob": 0.04913051053881645}, {"id": 173, "seek": 63900, "start": 658.0, "end": 660.0, "text": " I wida\u0107 na nim co\u015b fascynuj\u0105cego.", "tokens": [51314, 286, 261, 46898, 1667, 24887, 19241, 30632, 1344, 77, 13263, 384, 1571, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07809261322021484, "compression_ratio": 1.4640522875816993, "no_speech_prob": 0.04913051053881645}, {"id": 174, "seek": 63900, "start": 660.0, "end": 663.0, "text": " Nag\u0142y, bardzo wyra\u017any skok wydajno\u015bci", "tokens": [51414, 18913, 6825, 11, 9034, 4628, 424, 10659, 1634, 1110, 453, 25984, 1805, 16438, 51564], "temperature": 0.0, "avg_logprob": -0.07809261322021484, "compression_ratio": 1.4640522875816993, "no_speech_prob": 0.04913051053881645}, {"id": 175, "seek": 63900, "start": 663.0, "end": 665.0, "text": " na samym samiuteinkim ko\u0144cu.", "tokens": [51564, 1667, 3247, 4199, 3247, 72, 1169, 475, 332, 26470, 12032, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07809261322021484, "compression_ratio": 1.4640522875816993, "no_speech_prob": 0.04913051053881645}, {"id": 176, "seek": 63900, "start": 665.0, "end": 667.0, "text": " Okaza\u0142o si\u0119, \u017ce ten wzrost", "tokens": [51664, 3477, 12257, 5249, 3244, 11, 3561, 2064, 24809, 27494, 51764], "temperature": 0.0, "avg_logprob": -0.07809261322021484, "compression_ratio": 1.4640522875816993, "no_speech_prob": 0.04913051053881645}, {"id": 177, "seek": 66700, "start": 667.0, "end": 670.0, "text": " to efekt banalnie prostego zabiegu.", "tokens": [50364, 281, 31482, 8192, 5643, 304, 2766, 10293, 6308, 24838, 414, 2794, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07004390185392356, "compression_ratio": 1.4451612903225806, "no_speech_prob": 0.03298657014966011}, {"id": 178, "seek": 66700, "start": 670.0, "end": 673.0, "text": " Liniowego zmniejszenia learning rate do zera", "tokens": [50514, 441, 3812, 26576, 17020, 30295, 14320, 2539, 3314, 360, 710, 1663, 50664], "temperature": 0.0, "avg_logprob": -0.07004390185392356, "compression_ratio": 1.4451612903225806, "no_speech_prob": 0.03298657014966011}, {"id": 179, "seek": 66700, "start": 673.0, "end": 676.0, "text": " w ci\u0105gu ostatnich tysi\u0105ca krok\u00f3w.", "tokens": [50664, 261, 42398, 2794, 32686, 77, 480, 38156, 11404, 496, 45909, 23849, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07004390185392356, "compression_ratio": 1.4451612903225806, "no_speech_prob": 0.03298657014966011}, {"id": 180, "seek": 66700, "start": 676.0, "end": 679.0, "text": " To jest niezwykle praktyczna, cenna wskaz\u00f3wka dla ka\u017cdego,", "tokens": [50814, 1407, 3492, 33511, 9726, 14677, 3206, 74, 874, 3689, 629, 11, 27900, 629, 261, 5161, 921, 3901, 2330, 12285, 21912, 67, 6308, 11, 50964], "temperature": 0.0, "avg_logprob": -0.07004390185392356, "compression_ratio": 1.4451612903225806, "no_speech_prob": 0.03298657014966011}, {"id": 181, "seek": 66700, "start": 679.0, "end": 681.0, "text": " kto trenuje modele.", "tokens": [50964, 23780, 23136, 13008, 4391, 306, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07004390185392356, "compression_ratio": 1.4451612903225806, "no_speech_prob": 0.03298657014966011}, {"id": 182, "seek": 66700, "start": 681.0, "end": 683.0, "text": " I to jest ten rodzaj wiedzy,", "tokens": [51064, 286, 281, 3492, 2064, 28607, 1805, 46894, 1229, 11, 51164], "temperature": 0.0, "avg_logprob": -0.07004390185392356, "compression_ratio": 1.4451612903225806, "no_speech_prob": 0.03298657014966011}, {"id": 183, "seek": 66700, "start": 683.0, "end": 685.0, "text": " kt\u00f3ry zyskujemy tylko i wy\u0142\u0105cznie", "tokens": [51164, 9913, 710, 749, 74, 21767, 13219, 741, 4628, 15926, 19923, 51264], "temperature": 0.0, "avg_logprob": -0.07004390185392356, "compression_ratio": 1.4451612903225806, "no_speech_prob": 0.03298657014966011}, {"id": 184, "seek": 66700, "start": 685.0, "end": 688.0, "text": " dzi\u0119ki otwartemu procesowi i szczeg\u00f3\u0142owym logom.", "tokens": [51264, 45003, 4337, 29587, 37552, 17565, 24503, 741, 22090, 1146, 16181, 31691, 3565, 298, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07004390185392356, "compression_ratio": 1.4451612903225806, "no_speech_prob": 0.03298657014966011}, {"id": 185, "seek": 66700, "start": 688.0, "end": 690.0, "text": " W przypadku zamkni\u0119tego modelu", "tokens": [51414, 343, 41955, 19876, 74, 35938, 975, 1571, 2316, 84, 51514], "temperature": 0.0, "avg_logprob": -0.07004390185392356, "compression_ratio": 1.4451612903225806, "no_speech_prob": 0.03298657014966011}, {"id": 186, "seek": 66700, "start": 690.0, "end": 693.0, "text": " nigdy by\u015bmy si\u0119 o tym nie dowiedzieli.", "tokens": [51514, 26996, 3173, 538, 10513, 3244, 277, 8107, 2838, 9459, 15338, 23099, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07004390185392356, "compression_ratio": 1.4451612903225806, "no_speech_prob": 0.03298657014966011}, {"id": 187, "seek": 66700, "start": 693.0, "end": 696.0, "text": " W prac\u0119 cz\u0119sto pojawia si\u0119 te\u017c miara zwana Perplexity.", "tokens": [51664, 343, 22404, 1274, 34369, 30655, 654, 3244, 9516, 2752, 2419, 11873, 2095, 3026, 18945, 507, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07004390185392356, "compression_ratio": 1.4451612903225806, "no_speech_prob": 0.03298657014966011}, {"id": 188, "seek": 69600, "start": 696.0, "end": 699.0, "text": " Wyja\u015bnili\u015bmy j\u0105 kiedy\u015b, ale warto przypomnie\u0107,", "tokens": [50364, 14458, 2938, 1788, 77, 43912, 35692, 18777, 1788, 11, 6775, 31830, 41780, 298, 2766, 2162, 11, 50514], "temperature": 0.0, "avg_logprob": -0.06552866987279944, "compression_ratio": 1.4472727272727273, "no_speech_prob": 0.008111980743706226}, {"id": 189, "seek": 69600, "start": 699.0, "end": 701.0, "text": " co ona nam m\u00f3wi o modelu.", "tokens": [50514, 598, 20325, 8835, 24592, 277, 2316, 84, 13, 50614], "temperature": 0.0, "avg_logprob": -0.06552866987279944, "compression_ratio": 1.4472727272727273, "no_speech_prob": 0.008111980743706226}, {"id": 190, "seek": 69600, "start": 701.0, "end": 703.0, "text": " Perplexity to miara tego,", "tokens": [50614, 3026, 18945, 507, 281, 2752, 2419, 8627, 11, 50714], "temperature": 0.0, "avg_logprob": -0.06552866987279944, "compression_ratio": 1.4472727272727273, "no_speech_prob": 0.008111980743706226}, {"id": 191, "seek": 69600, "start": 703.0, "end": 707.0, "text": " jak bardzo model jest zaskoczony nowym tekstem.", "tokens": [50714, 4207, 9034, 2316, 3492, 710, 3863, 905, 44479, 586, 4199, 16624, 1099, 13, 50914], "temperature": 0.0, "avg_logprob": -0.06552866987279944, "compression_ratio": 1.4472727272727273, "no_speech_prob": 0.008111980743706226}, {"id": 192, "seek": 69600, "start": 707.0, "end": 710.0, "text": " I mniejsza warto\u015b\u0107, tym lepiej.", "tokens": [50914, 286, 275, 30295, 2394, 31830, 7753, 11, 8107, 476, 39699, 13, 51064], "temperature": 0.0, "avg_logprob": -0.06552866987279944, "compression_ratio": 1.4472727272727273, "no_speech_prob": 0.008111980743706226}, {"id": 193, "seek": 69600, "start": 710.0, "end": 713.0, "text": " Oznacza to, \u017ce model lepiej przewiduje kolejne s\u0142owa,", "tokens": [51064, 422, 22672, 326, 2394, 281, 11, 3561, 2316, 476, 39699, 39758, 327, 13008, 23749, 716, 15116, 5528, 11, 51214], "temperature": 0.0, "avg_logprob": -0.06552866987279944, "compression_ratio": 1.4472727272727273, "no_speech_prob": 0.008111980743706226}, {"id": 194, "seek": 69600, "start": 713.0, "end": 716.0, "text": " co \u015bwiadczy o tym, \u017ce lepiej rozumie struktur\u0119 j\u0119zyka.", "tokens": [51214, 598, 21485, 345, 6522, 277, 8107, 11, 3561, 476, 39699, 48797, 414, 342, 31543, 1274, 42309, 40940, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06552866987279944, "compression_ratio": 1.4472727272727273, "no_speech_prob": 0.008111980743706226}, {"id": 195, "seek": 69600, "start": 716.0, "end": 719.0, "text": " Do pomiar\u00f3w u\u017cyli benchmarku Paloma.", "tokens": [51364, 1144, 280, 9220, 289, 3901, 34097, 2081, 18927, 84, 6116, 6440, 13, 51514], "temperature": 0.0, "avg_logprob": -0.06552866987279944, "compression_ratio": 1.4472727272727273, "no_speech_prob": 0.008111980743706226}, {"id": 196, "seek": 69600, "start": 719.0, "end": 721.0, "text": " I tu wniosek jest ciekawy.", "tokens": [51514, 286, 2604, 261, 3722, 541, 74, 3492, 46419, 41961, 13, 51614], "temperature": 0.0, "avg_logprob": -0.06552866987279944, "compression_ratio": 1.4472727272727273, "no_speech_prob": 0.008111980743706226}, {"id": 197, "seek": 69600, "start": 721.0, "end": 724.0, "text": " OLMO ma najni\u017csze Perplexity,", "tokens": [51614, 39191, 18976, 463, 11212, 3722, 1427, 82, 1381, 3026, 18945, 507, 11, 51764], "temperature": 0.0, "avg_logprob": -0.06552866987279944, "compression_ratio": 1.4472727272727273, "no_speech_prob": 0.008111980743706226}, {"id": 198, "seek": 72400, "start": 724.0, "end": 729.0, "text": " czyli jest najlepszy na dany w pochodz\u0105cych z Common Crawl.", "tokens": [50364, 16591, 3492, 41903, 1878, 1229, 1667, 274, 1325, 261, 714, 29914, 8925, 31306, 710, 18235, 37877, 75, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10848897568722989, "compression_ratio": 1.4271186440677965, "no_speech_prob": 0.06157609447836876}, {"id": 199, "seek": 72400, "start": 729.0, "end": 732.0, "text": " Co jest logiczne, bo to by\u0142a lwia cz\u0119\u015bci jego diety.", "tokens": [50614, 3066, 3492, 9952, 43077, 11, 748, 281, 23936, 287, 86, 654, 41314, 26542, 1026, 2210, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10848897568722989, "compression_ratio": 1.4271186440677965, "no_speech_prob": 0.06157609447836876}, {"id": 200, "seek": 72400, "start": 732.0, "end": 736.0, "text": " Jest jednak troch\u0119 s\u0142abszy na tekstach z Wikipedia czy ksi\u0105\u017cek.", "tokens": [50764, 24918, 25897, 24926, 15116, 455, 7706, 1667, 16624, 372, 608, 710, 28999, 6430, 39311, 916, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10848897568722989, "compression_ratio": 1.4271186440677965, "no_speech_prob": 0.06157609447836876}, {"id": 201, "seek": 72400, "start": 736.0, "end": 738.0, "text": " Ale jest tu chyba jaki\u015b haczyk, prawda?", "tokens": [50964, 9366, 3492, 2604, 31532, 34721, 324, 6522, 74, 11, 43607, 30, 51064], "temperature": 0.0, "avg_logprob": -0.10848897568722989, "compression_ratio": 1.4271186440677965, "no_speech_prob": 0.06157609447836876}, {"id": 202, "seek": 72400, "start": 738.0, "end": 742.0, "text": " W artykule mocno podkre\u015blaj\u0105 co\u015b, co nazywaj\u0105 Decontamination.", "tokens": [51064, 343, 594, 874, 74, 2271, 34962, 1771, 2497, 27885, 1788, 875, 8555, 19241, 11, 598, 20151, 27112, 11133, 1346, 9000, 335, 2486, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10848897568722989, "compression_ratio": 1.4271186440677965, "no_speech_prob": 0.06157609447836876}, {"id": 203, "seek": 72400, "start": 742.0, "end": 745.0, "text": " Decontamination, czyli odkarzanie,", "tokens": [51264, 1346, 9000, 335, 2486, 11, 16591, 3611, 12303, 89, 7155, 11, 51414], "temperature": 0.0, "avg_logprob": -0.10848897568722989, "compression_ratio": 1.4271186440677965, "no_speech_prob": 0.06157609447836876}, {"id": 204, "seek": 72400, "start": 745.0, "end": 748.0, "text": " to proces, w kt\u00f3rym autorzy bardzo skrupulatnie", "tokens": [51414, 281, 17565, 11, 261, 30120, 19510, 1229, 9034, 1110, 11976, 425, 267, 2766, 51564], "temperature": 0.0, "avg_logprob": -0.10848897568722989, "compression_ratio": 1.4271186440677965, "no_speech_prob": 0.06157609447836876}, {"id": 205, "seek": 72400, "start": 748.0, "end": 751.0, "text": " usun\u0119li ze swojego zbioru treningowego OLMO", "tokens": [51564, 505, 409, 1274, 2081, 5277, 13291, 39738, 710, 33362, 84, 2192, 773, 26576, 39191, 18976, 51714], "temperature": 0.0, "avg_logprob": -0.10848897568722989, "compression_ratio": 1.4271186440677965, "no_speech_prob": 0.06157609447836876}, {"id": 206, "seek": 75100, "start": 751.0, "end": 754.0, "text": " wszystkie fragmenty, kt\u00f3re pokrywa\u0142y si\u0119 z danymi testowymi", "tokens": [50364, 31723, 26424, 88, 11, 8864, 13010, 627, 4151, 6825, 3244, 710, 274, 1325, 3057, 1500, 10089, 3057, 50514], "temperature": 0.0, "avg_logprob": -0.07511974309946036, "compression_ratio": 1.5032258064516129, "no_speech_prob": 0.04753531515598297}, {"id": 207, "seek": 75100, "start": 754.0, "end": 756.0, "text": " z benchmarku Paloma.", "tokens": [50514, 710, 18927, 84, 6116, 6440, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07511974309946036, "compression_ratio": 1.5032258064516129, "no_speech_prob": 0.04753531515598297}, {"id": 208, "seek": 75100, "start": 756.0, "end": 759.0, "text": " Innymi s\u0142owy upewnili si\u0119, \u017ce model na pewno nie widzia\u0142", "tokens": [50614, 682, 31813, 15116, 10089, 493, 68, 895, 2312, 3244, 11, 3561, 2316, 1667, 33002, 2838, 27486, 8908, 50764], "temperature": 0.0, "avg_logprob": -0.07511974309946036, "compression_ratio": 1.5032258064516129, "no_speech_prob": 0.04753531515598297}, {"id": 209, "seek": 75100, "start": 759.0, "end": 761.0, "text": " wcze\u015bniej pyta\u0144 z egzaminu.", "tokens": [50764, 40785, 10664, 1328, 5248, 710, 24263, 89, 7428, 84, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07511974309946036, "compression_ratio": 1.5032258064516129, "no_speech_prob": 0.04753531515598297}, {"id": 210, "seek": 75100, "start": 761.0, "end": 764.0, "text": " A w przypadku innych modeli nie mamy tej pewno\u015bci?", "tokens": [50864, 316, 261, 41955, 36286, 2316, 72, 2838, 17335, 12573, 33002, 6199, 30, 51014], "temperature": 0.0, "avg_logprob": -0.07511974309946036, "compression_ratio": 1.5032258064516129, "no_speech_prob": 0.04753531515598297}, {"id": 211, "seek": 75100, "start": 764.0, "end": 766.0, "text": " Dok\u0142adnie.", "tokens": [51014, 29768, 10358, 2766, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07511974309946036, "compression_ratio": 1.5032258064516129, "no_speech_prob": 0.04753531515598297}, {"id": 212, "seek": 75100, "start": 766.0, "end": 770.0, "text": " W przypadku modeli zamkni\u0119tych, kt\u00f3rych dane treningowe s\u0105 tajemnic\u0105,", "tokens": [51114, 343, 41955, 2316, 72, 19876, 74, 35938, 874, 339, 11, 30382, 49206, 2192, 773, 6880, 9015, 256, 1805, 443, 7692, 1611, 11, 51314], "temperature": 0.0, "avg_logprob": -0.07511974309946036, "compression_ratio": 1.5032258064516129, "no_speech_prob": 0.04753531515598297}, {"id": 213, "seek": 75100, "start": 770.0, "end": 772.0, "text": " istnieje ogromne ryzyko,", "tokens": [51314, 1418, 2766, 2884, 34416, 298, 716, 20791, 1229, 4093, 11, 51414], "temperature": 0.0, "avg_logprob": -0.07511974309946036, "compression_ratio": 1.5032258064516129, "no_speech_prob": 0.04753531515598297}, {"id": 214, "seek": 75100, "start": 772.0, "end": 775.0, "text": " \u017ce by\u0142y one nie\u015bwiadomie trenowane na danych testowych.", "tokens": [51414, 3561, 26366, 472, 2838, 37750, 40120, 23136, 23066, 1667, 274, 34644, 1500, 19605, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07511974309946036, "compression_ratio": 1.5032258064516129, "no_speech_prob": 0.04753531515598297}, {"id": 215, "seek": 75100, "start": 775.0, "end": 778.0, "text": " To troch\u0119 jakby ucze\u0144 przed matur\u0105 dosta\u0142 arkusz z odpowiedziami.", "tokens": [51564, 1407, 24926, 28976, 344, 9680, 5248, 18334, 3803, 374, 1611, 274, 8638, 1221, 14408, 22378, 710, 36574, 3992, 4526, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07511974309946036, "compression_ratio": 1.5032258064516129, "no_speech_prob": 0.04753531515598297}, {"id": 216, "seek": 77800, "start": 778.0, "end": 781.0, "text": " Jego wysoki wynik niczego nie dowodzi.", "tokens": [50364, 508, 6308, 27062, 17056, 31936, 1035, 6201, 27725, 2838, 9459, 14543, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07851753772144586, "compression_ratio": 1.4395604395604396, "no_speech_prob": 0.13117718696594238}, {"id": 217, "seek": 77800, "start": 781.0, "end": 784.0, "text": " OLMO, nawet je\u015bli jego wynik perplexity", "tokens": [50514, 39191, 18976, 11, 22696, 25630, 26542, 31936, 1035, 680, 18945, 507, 50664], "temperature": 0.0, "avg_logprob": -0.07851753772144586, "compression_ratio": 1.4395604395604396, "no_speech_prob": 0.13117718696594238}, {"id": 218, "seek": 77800, "start": 784.0, "end": 787.0, "text": " nie jest w ka\u017cdej kategorii najni\u017cszy,", "tokens": [50664, 2838, 3492, 261, 21912, 1479, 73, 350, 2968, 284, 5597, 11212, 3722, 1427, 7706, 11, 50814], "temperature": 0.0, "avg_logprob": -0.07851753772144586, "compression_ratio": 1.4395604395604396, "no_speech_prob": 0.13117718696594238}, {"id": 219, "seek": 77800, "start": 787.0, "end": 790.0, "text": " daje nam gwarancj\u0119 uczciwego pomiaru.", "tokens": [50814, 1120, 2884, 8835, 290, 6925, 4463, 11115, 35403, 537, 826, 1571, 280, 9220, 16870, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07851753772144586, "compression_ratio": 1.4395604395604396, "no_speech_prob": 0.13117718696594238}, {"id": 220, "seek": 77800, "start": 790.0, "end": 793.0, "text": " Z naukowego punktu widzenia to jest o wiele, wiele cenniejsze.", "tokens": [50964, 1176, 35616, 74, 26576, 39561, 84, 5274, 14320, 281, 3492, 277, 33137, 11, 33137, 27900, 44258, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07851753772144586, "compression_ratio": 1.4395604395604396, "no_speech_prob": 0.13117718696594238}, {"id": 221, "seek": 77800, "start": 793.0, "end": 796.0, "text": " Surowy, wytrenowany model to jedno.", "tokens": [51114, 6732, 10089, 11, 261, 4328, 1095, 23341, 2316, 281, 5232, 1771, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07851753772144586, "compression_ratio": 1.4395604395604396, "no_speech_prob": 0.13117718696594238}, {"id": 222, "seek": 77800, "start": 796.0, "end": 799.0, "text": " Ale wi\u0119kszo\u015b\u0107 z nas ma doczynienia z modelami,", "tokens": [51264, 9366, 29968, 4765, 7753, 710, 5382, 463, 360, 6522, 77, 18811, 710, 2316, 4526, 11, 51414], "temperature": 0.0, "avg_logprob": -0.07851753772144586, "compression_ratio": 1.4395604395604396, "no_speech_prob": 0.13117718696594238}, {"id": 223, "seek": 77800, "start": 799.0, "end": 802.0, "text": " kt\u00f3re s\u0105 dostosowane do rozmowy z asystentem.", "tokens": [51414, 8864, 9015, 20568, 329, 23066, 360, 35234, 10089, 710, 382, 38593, 317, 443, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07851753772144586, "compression_ratio": 1.4395604395604396, "no_speech_prob": 0.13117718696594238}, {"id": 224, "seek": 77800, "start": 802.0, "end": 806.0, "text": " Czy OLMO nadaje si\u0119 na tak\u0105 baz\u0119?", "tokens": [51564, 19832, 39191, 18976, 8096, 2884, 3244, 1667, 31069, 27147, 1274, 30, 51764], "temperature": 0.0, "avg_logprob": -0.07851753772144586, "compression_ratio": 1.4395604395604396, "no_speech_prob": 0.13117718696594238}, {"id": 225, "seek": 80600, "start": 806.0, "end": 809.0, "text": " Jak najbardziej i autorzy pokazali, jak to zrobi\u0107", "tokens": [50364, 15029, 41857, 741, 19510, 1229, 13010, 921, 5103, 11, 4207, 281, 31785, 50514], "temperature": 0.0, "avg_logprob": -0.11133157185145787, "compression_ratio": 1.3612040133779264, "no_speech_prob": 0.0351041778922081}, {"id": 226, "seek": 80600, "start": 809.0, "end": 812.0, "text": " przeprowadzili dwuetapowy proces adaptacji.", "tokens": [50514, 30829, 1892, 345, 89, 2312, 27379, 15382, 569, 10089, 17565, 6231, 13152, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11133157185145787, "compression_ratio": 1.3612040133779264, "no_speech_prob": 0.0351041778922081}, {"id": 227, "seek": 80600, "start": 812.0, "end": 816.0, "text": " Najpierw u\u017cyli metody Instruction Fine Tuning, znane jako SFT,", "tokens": [50664, 31576, 45119, 86, 34097, 2081, 1131, 843, 2730, 3826, 12024, 21363, 278, 11, 15397, 1929, 17123, 318, 25469, 11, 50864], "temperature": 0.0, "avg_logprob": -0.11133157185145787, "compression_ratio": 1.3612040133779264, "no_speech_prob": 0.0351041778922081}, {"id": 228, "seek": 80600, "start": 816.0, "end": 820.0, "text": " a potem Direct Preference Optimization, czyli DPO.", "tokens": [50864, 257, 36513, 18308, 6001, 5158, 35013, 2144, 11, 16591, 413, 34885, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11133157185145787, "compression_ratio": 1.3612040133779264, "no_speech_prob": 0.0351041778922081}, {"id": 229, "seek": 80600, "start": 820.0, "end": 822.0, "text": " Wspomnia\u0142a\u015b o dw\u00f3ch metodach.", "tokens": [51064, 343, 4952, 45438, 5024, 1788, 277, 27379, 812, 339, 1131, 378, 608, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11133157185145787, "compression_ratio": 1.3612040133779264, "no_speech_prob": 0.0351041778922081}, {"id": 230, "seek": 80600, "start": 822.0, "end": 824.0, "text": " SFT i DPO.", "tokens": [51164, 318, 25469, 741, 413, 34885, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11133157185145787, "compression_ratio": 1.3612040133779264, "no_speech_prob": 0.0351041778922081}, {"id": 231, "seek": 80600, "start": 824.0, "end": 827.0, "text": " Czym one si\u0119 r\u00f3\u017cni\u0105 w praktycy?", "tokens": [51264, 19832, 76, 472, 3244, 19637, 3722, 1611, 261, 3206, 74, 874, 1344, 30, 51414], "temperature": 0.0, "avg_logprob": -0.11133157185145787, "compression_ratio": 1.3612040133779264, "no_speech_prob": 0.0351041778922081}, {"id": 232, "seek": 80600, "start": 827.0, "end": 831.0, "text": " SFT to jak uczenie dziecka przez dawanie mu konkretnych przyk\u0142ad\u00f3w.", "tokens": [51414, 318, 25469, 281, 4207, 344, 39043, 17953, 39342, 14064, 43438, 7155, 2992, 36500, 9399, 23144, 3901, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11133157185145787, "compression_ratio": 1.3612040133779264, "no_speech_prob": 0.0351041778922081}, {"id": 233, "seek": 80600, "start": 831.0, "end": 834.0, "text": " Na pytanie o stolice Francji odpowied\u017a Pary\u017c.", "tokens": [51614, 6056, 36610, 277, 43553, 573, 8686, 4013, 3611, 14701, 1091, 10659, 430, 822, 1427, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11133157185145787, "compression_ratio": 1.3612040133779264, "no_speech_prob": 0.0351041778922081}, {"id": 234, "seek": 83400, "start": 835.0, "end": 838.0, "text": " Model uczy si\u0119 na tysi\u0105cach takich pary.", "tokens": [50414, 17105, 344, 6522, 3244, 1667, 38156, 11404, 66, 608, 29607, 280, 822, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08855204010009765, "compression_ratio": 1.4960629921259843, "no_speech_prob": 0.0749368742108345}, {"id": 235, "seek": 83400, "start": 838.0, "end": 842.0, "text": " Pytanie, odpowied\u017a, \u017ceby pod\u0105\u017ca\u0107 za instrukcjami.", "tokens": [50564, 430, 4328, 7155, 11, 36574, 10659, 11, 11316, 2497, 27242, 43379, 7949, 1058, 25126, 66, 73, 4526, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08855204010009765, "compression_ratio": 1.4960629921259843, "no_speech_prob": 0.0749368742108345}, {"id": 236, "seek": 83400, "start": 842.0, "end": 845.0, "text": " DPO jest bardziej subtelne.", "tokens": [50764, 413, 34885, 3492, 27209, 7257, 338, 716, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08855204010009765, "compression_ratio": 1.4960629921259843, "no_speech_prob": 0.0749368742108345}, {"id": 237, "seek": 83400, "start": 845.0, "end": 848.0, "text": " Tu pokazujemy modelowi dwie mo\u017cliwe odpowiedzi na to samo pytanie", "tokens": [50914, 7836, 13010, 921, 21767, 2316, 24503, 274, 8699, 30854, 826, 36574, 3992, 1667, 281, 36422, 36610, 51064], "temperature": 0.0, "avg_logprob": -0.08855204010009765, "compression_ratio": 1.4960629921259843, "no_speech_prob": 0.0749368742108345}, {"id": 238, "seek": 83400, "start": 848.0, "end": 852.0, "text": " i m\u00f3wimy, ta odpowied\u017a jest lepsza od tej.", "tokens": [51064, 741, 13489, 13189, 11, 1846, 36574, 10659, 3492, 476, 1878, 2394, 3611, 12573, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08855204010009765, "compression_ratio": 1.4960629921259843, "no_speech_prob": 0.0749368742108345}, {"id": 239, "seek": 83400, "start": 852.0, "end": 855.0, "text": " Model nie dostaje gotowca, musi sam domy\u015bli\u0107 si\u0119,", "tokens": [51264, 17105, 2838, 20568, 11153, 658, 305, 496, 11, 37587, 3247, 3285, 88, 15350, 2162, 3244, 11, 51414], "temperature": 0.0, "avg_logprob": -0.08855204010009765, "compression_ratio": 1.4960629921259843, "no_speech_prob": 0.0749368742108345}, {"id": 240, "seek": 83400, "start": 855.0, "end": 858.0, "text": " dlaczego jedna odpowied\u017a jest preferowana.", "tokens": [51414, 37873, 39329, 5232, 629, 36574, 10659, 3492, 4382, 40458, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08855204010009765, "compression_ratio": 1.4960629921259843, "no_speech_prob": 0.0749368742108345}, {"id": 241, "seek": 83400, "start": 858.0, "end": 860.0, "text": " Mo\u017ce jest bardziej pomocna, mniej toksyczna,", "tokens": [51564, 43774, 3492, 27209, 48962, 629, 11, 39513, 281, 1694, 17466, 629, 11, 51664], "temperature": 0.0, "avg_logprob": -0.08855204010009765, "compression_ratio": 1.4960629921259843, "no_speech_prob": 0.0749368742108345}, {"id": 242, "seek": 86000, "start": 860.0, "end": 864.0, "text": " bardziej szczeg\u00f3\u0142owa w ten spos\u00f3b uczy si\u0119 naszych preferencji.", "tokens": [50364, 27209, 22090, 1146, 16181, 5528, 261, 2064, 22904, 344, 6522, 3244, 45002, 4382, 268, 19649, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08511114120483398, "compression_ratio": 1.3170731707317074, "no_speech_prob": 0.09256650507450104}, {"id": 243, "seek": 86000, "start": 864.0, "end": 867.0, "text": " Jakie by\u0142y efekty tej dwuetapowej operacji?", "tokens": [50564, 15029, 414, 26366, 31482, 916, 874, 12573, 27379, 15382, 569, 21091, 2208, 13152, 30, 50714], "temperature": 0.0, "avg_logprob": -0.08511114120483398, "compression_ratio": 1.3170731707317074, "no_speech_prob": 0.09256650507450104}, {"id": 244, "seek": 86000, "start": 867.0, "end": 870.0, "text": " Spektakularne. Sp\u00f3jrzmy na wyniki.", "tokens": [50714, 3550, 2320, 514, 1040, 716, 13, 1738, 18999, 19390, 2226, 1667, 31936, 9850, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08511114120483398, "compression_ratio": 1.3170731707317074, "no_speech_prob": 0.09256650507450104}, {"id": 245, "seek": 86000, "start": 870.0, "end": 874.0, "text": " W te\u015bcie MMLU, kt\u00f3re mierzy szerok\u0105 wiedz\u0119 og\u00f3ln\u0105,", "tokens": [50864, 343, 535, 9815, 376, 12683, 52, 11, 8864, 47448, 1229, 36160, 453, 1611, 46894, 11052, 5360, 15741, 13113, 11, 51064], "temperature": 0.0, "avg_logprob": -0.08511114120483398, "compression_ratio": 1.3170731707317074, "no_speech_prob": 0.09256650507450104}, {"id": 246, "seek": 86000, "start": 874.0, "end": 877.0, "text": " surowy Olmo osi\u0105ga\u0142 wynik 28.3.", "tokens": [51064, 1022, 10089, 6141, 3280, 3003, 11404, 3680, 1221, 31936, 1035, 7562, 13, 18, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08511114120483398, "compression_ratio": 1.3170731707317074, "no_speech_prob": 0.09256650507450104}, {"id": 247, "seek": 86000, "start": 877.0, "end": 882.0, "text": " Po adaptacji SFT i DPO ten wynik skacze do 46.2.", "tokens": [51214, 6165, 6231, 13152, 318, 25469, 741, 413, 34885, 2064, 31936, 1035, 1110, 326, 1381, 360, 17835, 13, 17, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08511114120483398, "compression_ratio": 1.3170731707317074, "no_speech_prob": 0.09256650507450104}, {"id": 248, "seek": 86000, "start": 882.0, "end": 884.0, "text": " To jest przepa\u015b\u0107.", "tokens": [51464, 1407, 3492, 30829, 64, 7753, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08511114120483398, "compression_ratio": 1.3170731707317074, "no_speech_prob": 0.09256650507450104}, {"id": 249, "seek": 86000, "start": 884.0, "end": 887.0, "text": " Ale jeszcze bardziej dramatyczna zmiana zasz\u0142a gdzie indziej.", "tokens": [51564, 9366, 14168, 27209, 42749, 17466, 629, 17020, 8497, 710, 19601, 5024, 18922, 1016, 19554, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08511114120483398, "compression_ratio": 1.3170731707317074, "no_speech_prob": 0.09256650507450104}, {"id": 250, "seek": 86000, "start": 887.0, "end": 888.0, "text": " Gdzie?", "tokens": [51714, 460, 13096, 30, 51764], "temperature": 0.0, "avg_logprob": -0.08511114120483398, "compression_ratio": 1.3170731707317074, "no_speech_prob": 0.09256650507450104}, {"id": 251, "seek": 88800, "start": 888.0, "end": 890.0, "text": " Poziomie toksyczno\u015bci.", "tokens": [50364, 6165, 3992, 298, 414, 281, 1694, 17466, 16438, 13, 50464], "temperature": 0.0, "avg_logprob": -0.08165472500944791, "compression_ratio": 1.4175438596491228, "no_speech_prob": 0.0023939376696944237}, {"id": 252, "seek": 88800, "start": 890.0, "end": 894.0, "text": " W te\u015bcie TOXYGEN surowy model generowa\u0142 tre\u015bci uznane za toksyczne", "tokens": [50464, 343, 535, 9815, 8232, 55, 56, 38, 2195, 1022, 10089, 2316, 1337, 30105, 2192, 6199, 16851, 77, 1929, 7949, 281, 1694, 17466, 716, 50664], "temperature": 0.0, "avg_logprob": -0.08165472500944791, "compression_ratio": 1.4175438596491228, "no_speech_prob": 0.0023939376696944237}, {"id": 253, "seek": 88800, "start": 894.0, "end": 897.0, "text": " w ponad 81% przypadk\u00f3w.", "tokens": [50664, 261, 9224, 345, 30827, 4, 33100, 23849, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08165472500944791, "compression_ratio": 1.4175438596491228, "no_speech_prob": 0.0023939376696944237}, {"id": 254, "seek": 88800, "start": 897.0, "end": 901.0, "text": " To pokazuje, jak surowy jest internet, na kt\u00f3rym si\u0119 uczy\u0142.", "tokens": [50814, 1407, 13010, 43317, 11, 4207, 1022, 10089, 3492, 4705, 11, 1667, 30120, 3244, 344, 6522, 1221, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08165472500944791, "compression_ratio": 1.4175438596491228, "no_speech_prob": 0.0023939376696944237}, {"id": 255, "seek": 88800, "start": 901.0, "end": 906.0, "text": " Po adaptacji ten wska\u017anik spada do zalednie 1 w sprzecinek 7%.", "tokens": [51014, 6165, 6231, 13152, 2064, 261, 20771, 10659, 13123, 637, 1538, 360, 710, 5573, 2766, 502, 261, 6103, 1381, 66, 48421, 1614, 6856, 51264], "temperature": 0.0, "avg_logprob": -0.08165472500944791, "compression_ratio": 1.4175438596491228, "no_speech_prob": 0.0023939376696944237}, {"id": 256, "seek": 88800, "start": 906.0, "end": 909.0, "text": " Wow, to jest gigantyczna zmiana.", "tokens": [51264, 3153, 11, 281, 3492, 8741, 394, 17466, 629, 17020, 8497, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08165472500944791, "compression_ratio": 1.4175438596491228, "no_speech_prob": 0.0023939376696944237}, {"id": 257, "seek": 88800, "start": 909.0, "end": 912.0, "text": " Zatem z takiego surowego, nieco dzikiego lingwisty", "tokens": [51414, 1176, 26851, 710, 32296, 1022, 26576, 11, 2838, 1291, 9758, 1035, 12200, 22949, 86, 38618, 51564], "temperature": 0.0, "avg_logprob": -0.08165472500944791, "compression_ratio": 1.4175438596491228, "no_speech_prob": 0.0023939376696944237}, {"id": 258, "seek": 88800, "start": 912.0, "end": 916.0, "text": " staje si\u0119 on znacznie bezpieczniejszym i bardziej u\u017cytecznym asystentem.", "tokens": [51564, 342, 11153, 3244, 322, 15397, 14875, 2766, 47153, 3689, 10402, 7706, 76, 741, 27209, 34097, 975, 3689, 12996, 382, 38593, 317, 443, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08165472500944791, "compression_ratio": 1.4175438596491228, "no_speech_prob": 0.0023939376696944237}, {"id": 259, "seek": 91600, "start": 917.0, "end": 918.0, "text": " Dok\u0142adnie.", "tokens": [50414, 29768, 10358, 2766, 13, 50464], "temperature": 0.0, "avg_logprob": -0.119018081330905, "compression_ratio": 1.3920863309352518, "no_speech_prob": 0.03791127726435661}, {"id": 260, "seek": 91600, "start": 918.0, "end": 920.0, "text": " W tej nowej roli jest bardzo konkurencyjny.", "tokens": [50464, 343, 12573, 586, 40779, 744, 2081, 3492, 9034, 21428, 9873, 42949, 1634, 13, 50564], "temperature": 0.0, "avg_logprob": -0.119018081330905, "compression_ratio": 1.3920863309352518, "no_speech_prob": 0.03791127726435661}, {"id": 261, "seek": 91600, "start": 920.0, "end": 922.0, "text": " Jest jednak pewien niedosyt,", "tokens": [50564, 24918, 25897, 25889, 1053, 32488, 329, 4328, 11, 50664], "temperature": 0.0, "avg_logprob": -0.119018081330905, "compression_ratio": 1.3920863309352518, "no_speech_prob": 0.03791127726435661}, {"id": 262, "seek": 91600, "start": 922.0, "end": 927.0, "text": " bo w niekt\u00f3rych testach wci\u0105\u017c ust\u0119puje innemu otwartemu modelowi CULU 2,", "tokens": [50664, 748, 261, 2838, 43073, 627, 339, 1500, 608, 261, 537, 27242, 26189, 18085, 13008, 7714, 37552, 4337, 29587, 37552, 2316, 24503, 383, 10253, 52, 568, 11, 50914], "temperature": 0.0, "avg_logprob": -0.119018081330905, "compression_ratio": 1.3920863309352518, "no_speech_prob": 0.03791127726435661}, {"id": 263, "seek": 91600, "start": 927.0, "end": 929.0, "text": " kt\u00f3ry bazuje na Lama 2.", "tokens": [50914, 9913, 27147, 13008, 1667, 441, 2404, 568, 13, 51014], "temperature": 0.0, "avg_logprob": -0.119018081330905, "compression_ratio": 1.3920863309352518, "no_speech_prob": 0.03791127726435661}, {"id": 264, "seek": 91600, "start": 929.0, "end": 932.0, "text": " Autorki maj\u0105 na to dwie hipotezy.", "tokens": [51014, 6049, 1284, 72, 26064, 1667, 281, 274, 8699, 8103, 1370, 1229, 13, 51164], "temperature": 0.0, "avg_logprob": -0.119018081330905, "compression_ratio": 1.3920863309352518, "no_speech_prob": 0.03791127726435661}, {"id": 265, "seek": 91600, "start": 932.0, "end": 937.0, "text": " Po pierwsze, dane u\u017cyte do adaptacji by\u0142y oryginalnie projektowane z my\u015bl\u0105", "tokens": [51164, 6165, 45994, 11, 49206, 34097, 975, 360, 6231, 13152, 26366, 420, 88, 1494, 304, 2766, 26261, 23066, 710, 452, 19212, 1611, 51414], "temperature": 0.0, "avg_logprob": -0.119018081330905, "compression_ratio": 1.3920863309352518, "no_speech_prob": 0.03791127726435661}, {"id": 266, "seek": 91600, "start": 937.0, "end": 939.0, "text": " o modelach z rodziny Lama.", "tokens": [51414, 277, 2316, 608, 710, 28607, 3519, 441, 2404, 13, 51514], "temperature": 0.0, "avg_logprob": -0.119018081330905, "compression_ratio": 1.3920863309352518, "no_speech_prob": 0.03791127726435661}, {"id": 267, "seek": 91600, "start": 939.0, "end": 943.0, "text": " Ale druga hipoteza wraca do naszego wcze\u015bniejszego punktu.", "tokens": [51514, 9366, 4110, 64, 8103, 1370, 2394, 928, 6628, 360, 44517, 40785, 15453, 6308, 39561, 84, 13, 51714], "temperature": 0.0, "avg_logprob": -0.119018081330905, "compression_ratio": 1.3920863309352518, "no_speech_prob": 0.03791127726435661}, {"id": 268, "seek": 94300, "start": 943.0, "end": 946.0, "text": " Mhm, czyli do potencjalnego przecieku danych testowych", "tokens": [50364, 26272, 11, 16591, 360, 1847, 22660, 22600, 11858, 8325, 4260, 5279, 274, 34644, 1500, 19605, 50514], "temperature": 0.0, "avg_logprob": -0.07153851645333427, "compression_ratio": 1.3959044368600682, "no_speech_prob": 0.002784580457955599}, {"id": 269, "seek": 94300, "start": 946.0, "end": 949.0, "text": " do zbioru treningowego Lama 2.", "tokens": [50514, 360, 710, 33362, 84, 2192, 773, 26576, 441, 2404, 568, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07153851645333427, "compression_ratio": 1.3959044368600682, "no_speech_prob": 0.002784580457955599}, {"id": 270, "seek": 94300, "start": 949.0, "end": 951.0, "text": " Co mog\u0142o da\u0107 mu nieuczciw\u0105 przewag\u0119?", "tokens": [50664, 3066, 13172, 5249, 1120, 2162, 2992, 2838, 1311, 89, 537, 86, 1611, 39758, 40748, 30, 50764], "temperature": 0.0, "avg_logprob": -0.07153851645333427, "compression_ratio": 1.3959044368600682, "no_speech_prob": 0.002784580457955599}, {"id": 271, "seek": 94300, "start": 951.0, "end": 952.0, "text": " W\u0142a\u015bnie.", "tokens": [50764, 343, 5024, 12221, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07153851645333427, "compression_ratio": 1.3959044368600682, "no_speech_prob": 0.002784580457955599}, {"id": 272, "seek": 94300, "start": 952.0, "end": 955.0, "text": " To kolejny mocny argument za transparentno\u015bci\u0105.", "tokens": [50814, 1407, 23749, 1634, 34962, 1634, 6770, 7949, 12737, 16438, 1611, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07153851645333427, "compression_ratio": 1.3959044368600682, "no_speech_prob": 0.002784580457955599}, {"id": 273, "seek": 94300, "start": 955.0, "end": 957.0, "text": " Bez niej nigdy nie wiemy,", "tokens": [50964, 879, 89, 2838, 73, 26996, 3173, 2838, 3355, 2226, 11, 51064], "temperature": 0.0, "avg_logprob": -0.07153851645333427, "compression_ratio": 1.3959044368600682, "no_speech_prob": 0.002784580457955599}, {"id": 274, "seek": 94300, "start": 957.0, "end": 961.0, "text": " czy wy\u017cszy wynik jest efektem lepszej architektury i treningu,", "tokens": [51064, 6430, 4628, 1427, 7706, 31936, 1035, 3492, 31482, 8192, 443, 476, 1878, 16920, 3912, 642, 2320, 2598, 741, 2192, 773, 84, 11, 51264], "temperature": 0.0, "avg_logprob": -0.07153851645333427, "compression_ratio": 1.3959044368600682, "no_speech_prob": 0.002784580457955599}, {"id": 275, "seek": 94300, "start": 961.0, "end": 965.0, "text": " czy po prostu tego, \u017ce model zna\u0142 odpowiedzi na egzaminie.", "tokens": [51264, 6430, 714, 19518, 8627, 11, 3561, 2316, 710, 629, 1221, 36574, 3992, 1667, 24263, 89, 7428, 414, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07153851645333427, "compression_ratio": 1.3959044368600682, "no_speech_prob": 0.002784580457955599}, {"id": 276, "seek": 94300, "start": 965.0, "end": 969.0, "text": " O LMO, nawet je\u015bli w tym jednym por\u00f3wnaniu wypada minimalnie gorzej,", "tokens": [51464, 422, 441, 18976, 11, 22696, 25630, 261, 8107, 5232, 12996, 1515, 812, 895, 25849, 46392, 1538, 13206, 2766, 24012, 16920, 11, 51664], "temperature": 0.0, "avg_logprob": -0.07153851645333427, "compression_ratio": 1.3959044368600682, "no_speech_prob": 0.002784580457955599}, {"id": 277, "seek": 96900, "start": 969.0, "end": 973.0, "text": " daje nam pewno\u015b\u0107 co do uczciwo\u015bci tej oceny.", "tokens": [50364, 1120, 2884, 8835, 33002, 7753, 598, 360, 35403, 537, 36476, 12573, 10409, 43100, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07110140130326555, "compression_ratio": 1.4933333333333334, "no_speech_prob": 0.06923714280128479}, {"id": 278, "seek": 96900, "start": 973.0, "end": 975.0, "text": " Podsumowuj\u0105c to wszystko, mam takie wra\u017cenie,", "tokens": [50564, 12646, 82, 449, 305, 44733, 281, 22607, 11, 13524, 15963, 7843, 41118, 11, 50664], "temperature": 0.0, "avg_logprob": -0.07110140130326555, "compression_ratio": 1.4933333333333334, "no_speech_prob": 0.06923714280128479}, {"id": 279, "seek": 96900, "start": 975.0, "end": 979.0, "text": " \u017ce najwi\u0119kszym produktem tego projektu nie jest sam model OLMO.", "tokens": [50664, 3561, 48636, 1694, 26681, 42816, 443, 8627, 26261, 84, 2838, 3492, 3247, 2316, 422, 43, 18976, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07110140130326555, "compression_ratio": 1.4933333333333334, "no_speech_prob": 0.06923714280128479}, {"id": 280, "seek": 96900, "start": 979.0, "end": 981.0, "text": " Zgadzam si\u0119 w stu procentach.", "tokens": [50864, 1176, 70, 345, 28915, 3244, 261, 342, 84, 38826, 608, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07110140130326555, "compression_ratio": 1.4933333333333334, "no_speech_prob": 0.06923714280128479}, {"id": 281, "seek": 96900, "start": 981.0, "end": 985.0, "text": " Najwi\u0119ksz\u0105 warto\u015bci\u0105 nie jest pobicie rekordu wydajno\u015bci w jakim\u015b bend marku.", "tokens": [50964, 31576, 22423, 1694, 8925, 31830, 50227, 2838, 3492, 714, 65, 28434, 33881, 28655, 25984, 1805, 16438, 261, 49410, 1788, 11229, 1491, 84, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07110140130326555, "compression_ratio": 1.4933333333333334, "no_speech_prob": 0.06923714280128479}, {"id": 282, "seek": 96900, "start": 985.0, "end": 988.0, "text": " Jest ni\u0105 stworzenie pierwszego, prawdziwie,", "tokens": [51164, 24918, 3867, 1611, 342, 28321, 16778, 27623, 27725, 11, 41175, 3992, 8699, 11, 51314], "temperature": 0.0, "avg_logprob": -0.07110140130326555, "compression_ratio": 1.4933333333333334, "no_speech_prob": 0.06923714280128479}, {"id": 283, "seek": 96900, "start": 988.0, "end": 992.0, "text": " kompletnego, otwartego laboratorium do badania modeli j\u0119zykowych.", "tokens": [51314, 5207, 14657, 11858, 11, 4337, 29587, 6308, 5938, 41679, 360, 1578, 5609, 2316, 72, 49055, 74, 19605, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07110140130326555, "compression_ratio": 1.4933333333333334, "no_speech_prob": 0.06923714280128479}, {"id": 284, "seek": 96900, "start": 992.0, "end": 994.0, "text": " To jest ten prze\u0142om.", "tokens": [51514, 1407, 3492, 2064, 8325, 1221, 298, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07110140130326555, "compression_ratio": 1.4933333333333334, "no_speech_prob": 0.06923714280128479}, {"id": 285, "seek": 96900, "start": 994.0, "end": 996.0, "text": " Te wszystkie udost\u0119pnione artefakty,", "tokens": [51614, 1989, 31723, 11727, 555, 18085, 77, 5328, 29159, 69, 514, 874, 11, 51714], "temperature": 0.0, "avg_logprob": -0.07110140130326555, "compression_ratio": 1.4933333333333334, "no_speech_prob": 0.06923714280128479}, {"id": 286, "seek": 99600, "start": 996.0, "end": 1000.0, "text": " ponad 500 punkt\u00f3w kontrolnych modelu, dane dolma,", "tokens": [50364, 9224, 345, 5923, 39561, 3901, 14373, 6623, 9399, 2316, 84, 11, 49206, 17858, 1696, 11, 50564], "temperature": 0.0, "avg_logprob": -0.08490987975021888, "compression_ratio": 1.4607142857142856, "no_speech_prob": 0.07102970033884048}, {"id": 287, "seek": 99600, "start": 1000.0, "end": 1003.0, "text": " kod, logi, to pozwala ca\u0142ej spo\u0142eczno\u015bci naukowej", "tokens": [50564, 350, 378, 11, 3565, 72, 11, 281, 40557, 5159, 47631, 73, 36851, 89, 16438, 35616, 74, 21091, 50714], "temperature": 0.0, "avg_logprob": -0.08490987975021888, "compression_ratio": 1.4607142857142856, "no_speech_prob": 0.07102970033884048}, {"id": 288, "seek": 99600, "start": 1003.0, "end": 1007.0, "text": " na zadawanie pyta\u0144, kt\u00f3re do tej pory by\u0142y czysto teoretyczne.", "tokens": [50714, 1667, 710, 1538, 86, 7155, 10664, 1328, 5248, 11, 8864, 360, 12573, 280, 827, 26366, 6430, 20875, 535, 418, 874, 38491, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08490987975021888, "compression_ratio": 1.4607142857142856, "no_speech_prob": 0.07102970033884048}, {"id": 289, "seek": 99600, "start": 1007.0, "end": 1009.0, "text": " Jakich na przyk\u0142ad?", "tokens": [50914, 15029, 480, 1667, 23144, 30, 51014], "temperature": 0.0, "avg_logprob": -0.08490987975021888, "compression_ratio": 1.4607142857142856, "no_speech_prob": 0.07102970033884048}, {"id": 290, "seek": 99600, "start": 1009.0, "end": 1011.0, "text": " Na przyk\u0142ad. Co dok\u0142adnie dzieje si\u0119", "tokens": [51014, 6056, 23144, 13, 3066, 45864, 2766, 17953, 2884, 3244, 51114], "temperature": 0.0, "avg_logprob": -0.08490987975021888, "compression_ratio": 1.4607142857142856, "no_speech_prob": 0.07102970033884048}, {"id": 291, "seek": 99600, "start": 1011.0, "end": 1014.0, "text": " z wywn\u0119trzn\u0105 reprezentacj\u0105 wiedzy w modelu", "tokens": [51114, 710, 4628, 895, 1274, 6903, 89, 13113, 1085, 265, 14185, 326, 8555, 46894, 1229, 261, 2316, 84, 51264], "temperature": 0.0, "avg_logprob": -0.08490987975021888, "compression_ratio": 1.4607142857142856, "no_speech_prob": 0.07102970033884048}, {"id": 292, "seek": 99600, "start": 1014.0, "end": 1016.0, "text": " po przetworzeniu biliona token\u00f3w", "tokens": [51264, 714, 6541, 302, 28321, 39651, 8588, 21758, 14862, 3901, 51364], "temperature": 0.0, "avg_logprob": -0.08490987975021888, "compression_ratio": 1.4607142857142856, "no_speech_prob": 0.07102970033884048}, {"id": 293, "seek": 99600, "start": 1016.0, "end": 1018.0, "text": " w por\u00f3wnaniu do p\u00f3\u0142tora biliona?", "tokens": [51364, 261, 1515, 812, 895, 25849, 360, 47907, 83, 3252, 8588, 21758, 30, 51464], "temperature": 0.0, "avg_logprob": -0.08490987975021888, "compression_ratio": 1.4607142857142856, "no_speech_prob": 0.07102970033884048}, {"id": 294, "seek": 99600, "start": 1018.0, "end": 1022.0, "text": " Czy model najpierw uczy si\u0119 sk\u0142adnie, a dopiero potem fakty?", "tokens": [51464, 19832, 2316, 11212, 45119, 86, 344, 6522, 3244, 1110, 10358, 2766, 11, 257, 21900, 12030, 36513, 33647, 874, 30, 51664], "temperature": 0.0, "avg_logprob": -0.08490987975021888, "compression_ratio": 1.4607142857142856, "no_speech_prob": 0.07102970033884048}, {"id": 295, "seek": 102200, "start": 1022.0, "end": 1027.0, "text": " Jaki jest faktyczny, mierzalny wp\u0142yw usuni\u0119cia danych z GitHub'a", "tokens": [50364, 508, 7421, 3492, 33647, 874, 3689, 1634, 11, 47448, 89, 304, 1634, 32444, 6825, 86, 505, 409, 5034, 2755, 274, 34644, 710, 23331, 6, 64, 50614], "temperature": 0.0, "avg_logprob": -0.089301801790857, "compression_ratio": 1.4100946372239747, "no_speech_prob": 0.005388067103922367}, {"id": 296, "seek": 102200, "start": 1027.0, "end": 1030.0, "text": " na zdolno\u015b\u0107 modelu do rozumowania logicznego?", "tokens": [50614, 1667, 16221, 401, 23293, 2316, 84, 360, 48797, 21308, 9952, 89, 11858, 30, 50764], "temperature": 0.0, "avg_logprob": -0.089301801790857, "compression_ratio": 1.4100946372239747, "no_speech_prob": 0.005388067103922367}, {"id": 297, "seek": 102200, "start": 1030.0, "end": 1033.0, "text": " Dzi\u0119ki Olmo mo\u017cna wzi\u0105\u0107 konkretny checkpoint", "tokens": [50764, 413, 34546, 6141, 3280, 17790, 261, 3992, 36374, 36500, 1634, 42269, 50914], "temperature": 0.0, "avg_logprob": -0.089301801790857, "compression_ratio": 1.4100946372239747, "no_speech_prob": 0.005388067103922367}, {"id": 298, "seek": 102200, "start": 1033.0, "end": 1036.0, "text": " z dowolnego momentu treningu i to po prostu zbada\u0107.", "tokens": [50914, 710, 9459, 401, 11858, 1623, 84, 2192, 773, 84, 741, 281, 714, 19518, 710, 65, 1538, 2162, 13, 51064], "temperature": 0.0, "avg_logprob": -0.089301801790857, "compression_ratio": 1.4100946372239747, "no_speech_prob": 0.005388067103922367}, {"id": 299, "seek": 102200, "start": 1036.0, "end": 1040.0, "text": " To otwiera dwi\u017cej do prawdziwej nauki o EI, a nie tylko in\u017cynierii.", "tokens": [51064, 1407, 4337, 86, 10609, 274, 6253, 38493, 360, 41175, 3992, 826, 73, 35616, 2984, 277, 462, 40, 11, 257, 2838, 13219, 294, 1427, 2534, 811, 5597, 13, 51264], "temperature": 0.0, "avg_logprob": -0.089301801790857, "compression_ratio": 1.4100946372239747, "no_speech_prob": 0.005388067103922367}, {"id": 300, "seek": 102200, "start": 1040.0, "end": 1044.0, "text": " Oczywi\u015bcie \u017caden projekt nie jest idealny.", "tokens": [51264, 42980, 19625, 14771, 26261, 2838, 3492, 7157, 1634, 13, 51464], "temperature": 0.0, "avg_logprob": -0.089301801790857, "compression_ratio": 1.4100946372239747, "no_speech_prob": 0.005388067103922367}, {"id": 301, "seek": 102200, "start": 1044.0, "end": 1047.0, "text": " O jakich ograniczeniach wspominaj\u0105 sami autorzy?", "tokens": [51464, 422, 4207, 480, 34416, 30732, 42124, 608, 17757, 49217, 8555, 3247, 72, 19510, 1229, 30, 51614], "temperature": 0.0, "avg_logprob": -0.089301801790857, "compression_ratio": 1.4100946372239747, "no_speech_prob": 0.005388067103922367}, {"id": 302, "seek": 102200, "start": 1047.0, "end": 1049.0, "text": " S\u0105 w tym wzgl\u0119dzie r\u00f3wnie transparentni?", "tokens": [51614, 318, 1611, 261, 8107, 48538, 42643, 11416, 14215, 12737, 3722, 30, 51714], "temperature": 0.0, "avg_logprob": -0.089301801790857, "compression_ratio": 1.4100946372239747, "no_speech_prob": 0.005388067103922367}, {"id": 303, "seek": 102200, "start": 1049.0, "end": 1051.0, "text": " S\u0105 i to si\u0119 chwali.", "tokens": [51714, 318, 1611, 741, 281, 3244, 26237, 5103, 13, 51814], "temperature": 0.0, "avg_logprob": -0.089301801790857, "compression_ratio": 1.4100946372239747, "no_speech_prob": 0.005388067103922367}, {"id": 304, "seek": 105100, "start": 1051.0, "end": 1053.0, "text": " Po pierwsze dane.", "tokens": [50364, 6165, 45994, 49206, 13, 50464], "temperature": 0.0, "avg_logprob": -0.05621500082418952, "compression_ratio": 1.4525547445255473, "no_speech_prob": 0.028043581172823906}, {"id": 305, "seek": 105100, "start": 1053.0, "end": 1057.0, "text": " Mimo ich ogromu skupiaj\u0105 si\u0119 g\u0142\u00f3wnie na j\u0119zyku angielskim,", "tokens": [50464, 376, 6934, 1893, 34416, 298, 84, 1110, 1010, 48125, 3244, 18117, 812, 14215, 1667, 49055, 5279, 2562, 1187, 5161, 332, 11, 50664], "temperature": 0.0, "avg_logprob": -0.05621500082418952, "compression_ratio": 1.4525547445255473, "no_speech_prob": 0.028043581172823906}, {"id": 306, "seek": 105100, "start": 1057.0, "end": 1061.0, "text": " co jest standardem, ale i oczywistym ograniczeniem.", "tokens": [50664, 598, 3492, 3832, 443, 11, 6775, 741, 277, 6522, 86, 468, 4199, 34416, 30732, 2904, 4907, 13, 50864], "temperature": 0.0, "avg_logprob": -0.05621500082418952, "compression_ratio": 1.4525547445255473, "no_speech_prob": 0.028043581172823906}, {"id": 307, "seek": 105100, "start": 1061.0, "end": 1064.0, "text": " Po drugie sam proces treningu.", "tokens": [50864, 6165, 4110, 414, 3247, 17565, 2192, 773, 84, 13, 51014], "temperature": 0.0, "avg_logprob": -0.05621500082418952, "compression_ratio": 1.4525547445255473, "no_speech_prob": 0.028043581172823906}, {"id": 308, "seek": 105100, "start": 1064.0, "end": 1068.0, "text": " Pisz\u0105 wprost, \u017ce jest extremalnie trudny i kosztowny,", "tokens": [51014, 43263, 8925, 261, 1424, 555, 11, 3561, 3492, 4040, 304, 2766, 32007, 1634, 741, 19532, 2682, 648, 88, 11, 51214], "temperature": 0.0, "avg_logprob": -0.05621500082418952, "compression_ratio": 1.4525547445255473, "no_speech_prob": 0.028043581172823906}, {"id": 309, "seek": 105100, "start": 1068.0, "end": 1071.0, "text": " a ich praca nie dokumentuje wszystkich \u015blepych za\u0142uk\u00f3w,", "tokens": [51214, 257, 1893, 582, 6628, 2838, 40858, 13008, 34234, 8299, 306, 8200, 339, 7949, 1221, 2034, 3901, 11, 51364], "temperature": 0.0, "avg_logprob": -0.05621500082418952, "compression_ratio": 1.4525547445255473, "no_speech_prob": 0.028043581172823906}, {"id": 310, "seek": 105100, "start": 1071.0, "end": 1075.0, "text": " nieudanych eksperyment\u00f3w i problem\u00f3w, kt\u00f3re napotkali.", "tokens": [51364, 2838, 532, 34644, 30724, 610, 88, 518, 3901, 741, 1154, 3901, 11, 8864, 9296, 310, 74, 5103, 13, 51564], "temperature": 0.0, "avg_logprob": -0.05621500082418952, "compression_ratio": 1.4525547445255473, "no_speech_prob": 0.028043581172823906}, {"id": 311, "seek": 105100, "start": 1075.0, "end": 1079.0, "text": " To wa\u017cne uwaga, \u017ce za tym sukcesem stoj\u0105 liczne pora\u017cki,", "tokens": [51564, 1407, 46110, 23147, 9286, 11, 3561, 7949, 8107, 46432, 887, 443, 22784, 8555, 6169, 43077, 1515, 18264, 2984, 11, 51764], "temperature": 0.0, "avg_logprob": -0.05621500082418952, "compression_ratio": 1.4525547445255473, "no_speech_prob": 0.028043581172823906}, {"id": 312, "seek": 107900, "start": 1079.0, "end": 1081.0, "text": " o kt\u00f3rych nie pisze si\u0119 w artyku\u0142ach.", "tokens": [50364, 277, 30382, 2838, 26584, 1381, 3244, 261, 594, 874, 5279, 1221, 608, 13, 50464], "temperature": 0.0, "avg_logprob": -0.05277237860985052, "compression_ratio": 1.4605263157894737, "no_speech_prob": 0.04562990367412567}, {"id": 313, "seek": 107900, "start": 1081.0, "end": 1085.0, "text": " A co z samym modelem? Nawet po tej ca\u0142ej adaptacji?", "tokens": [50464, 316, 598, 710, 3247, 4199, 4391, 10386, 30, 40315, 302, 714, 12573, 47631, 73, 6231, 13152, 30, 50664], "temperature": 0.0, "avg_logprob": -0.05277237860985052, "compression_ratio": 1.4605263157894737, "no_speech_prob": 0.04562990367412567}, {"id": 314, "seek": 107900, "start": 1085.0, "end": 1089.0, "text": " Nawet po dostosowaniu wci\u0105\u017c mo\u017ce on generowa\u0107 tre\u015bci,", "tokens": [50664, 40315, 302, 714, 20568, 329, 305, 25849, 261, 537, 27242, 12034, 322, 1337, 11445, 2192, 6199, 11, 50864], "temperature": 0.0, "avg_logprob": -0.05277237860985052, "compression_ratio": 1.4605263157894737, "no_speech_prob": 0.04562990367412567}, {"id": 315, "seek": 107900, "start": 1089.0, "end": 1092.0, "text": " stronnicze, nieprawdziwe lub szkodliwe.", "tokens": [50864, 45766, 7692, 1381, 11, 2838, 79, 15889, 3992, 826, 15980, 7870, 74, 378, 2081, 826, 13, 51014], "temperature": 0.0, "avg_logprob": -0.05277237860985052, "compression_ratio": 1.4605263157894737, "no_speech_prob": 0.04562990367412567}, {"id": 316, "seek": 107900, "start": 1092.0, "end": 1095.0, "text": " To nie jest magiczne rozwi\u0105zanie wszystkich problem\u00f3w.", "tokens": [51014, 1407, 2838, 3492, 5585, 43077, 9544, 22620, 7155, 34234, 1154, 3901, 13, 51164], "temperature": 0.0, "avg_logprob": -0.05277237860985052, "compression_ratio": 1.4605263157894737, "no_speech_prob": 0.04562990367412567}, {"id": 317, "seek": 107900, "start": 1095.0, "end": 1099.0, "text": " I wreszcie sami przyznaj\u0105, \u017ce standardowe benchmarki,", "tokens": [51164, 286, 261, 495, 89, 4260, 3247, 72, 6501, 35458, 8555, 11, 3561, 3832, 6880, 18927, 72, 11, 51364], "temperature": 0.0, "avg_logprob": -0.05277237860985052, "compression_ratio": 1.4605263157894737, "no_speech_prob": 0.04562990367412567}, {"id": 318, "seek": 107900, "start": 1099.0, "end": 1102.0, "text": " cho\u0107 u\u017cyteczne, nie zawsze odzwierciedlaj\u0105 to,", "tokens": [51364, 1586, 2162, 34097, 975, 38491, 11, 2838, 30964, 3611, 14406, 811, 537, 292, 875, 8555, 281, 11, 51514], "temperature": 0.0, "avg_logprob": -0.05277237860985052, "compression_ratio": 1.4605263157894737, "no_speech_prob": 0.04562990367412567}, {"id": 319, "seek": 107900, "start": 1102.0, "end": 1105.0, "text": " jak ludzie faktycznie u\u017cywaj\u0105 modeli j\u0119zykowych.", "tokens": [51514, 4207, 37025, 33647, 45586, 34097, 86, 11133, 2316, 72, 49055, 74, 19605, 13, 51664], "temperature": 0.0, "avg_logprob": -0.05277237860985052, "compression_ratio": 1.4605263157894737, "no_speech_prob": 0.04562990367412567}, {"id": 320, "seek": 107900, "start": 1105.0, "end": 1107.0, "text": " To szerszy problem ca\u0142ej dziedziny.", "tokens": [51664, 1407, 7870, 433, 1229, 1154, 47631, 73, 9758, 15338, 3519, 13, 51764], "temperature": 0.0, "avg_logprob": -0.05277237860985052, "compression_ratio": 1.4605263157894737, "no_speech_prob": 0.04562990367412567}, {"id": 321, "seek": 110700, "start": 1108.0, "end": 1112.0, "text": " Wygl\u0105da wi\u0119c na to, \u017ce OLMO to nie tyle linia mety", "tokens": [50414, 14458, 7191, 26398, 16677, 1667, 281, 11, 3561, 422, 43, 18976, 281, 2838, 39293, 22896, 654, 1131, 88, 50614], "temperature": 0.0, "avg_logprob": -0.05977033658792044, "compression_ratio": 1.4464944649446494, "no_speech_prob": 0.04878094792366028}, {"id": 322, "seek": 110700, "start": 1112.0, "end": 1116.0, "text": " w wy\u015bcigu o najlepszy model, co raczej linia startowa.", "tokens": [50614, 261, 4628, 1788, 66, 16397, 277, 41903, 1878, 1229, 2316, 11, 598, 4129, 16920, 22896, 654, 722, 5528, 13, 50814], "temperature": 0.0, "avg_logprob": -0.05977033658792044, "compression_ratio": 1.4464944649446494, "no_speech_prob": 0.04878094792366028}, {"id": 323, "seek": 110700, "start": 1116.0, "end": 1119.0, "text": " Linia startowa dla ca\u0142ej spo\u0142eczno\u015bci badawczej.", "tokens": [50814, 9355, 654, 722, 5528, 12285, 47631, 73, 36851, 89, 16438, 272, 1538, 86, 9680, 73, 13, 50964], "temperature": 0.0, "avg_logprob": -0.05977033658792044, "compression_ratio": 1.4464944649446494, "no_speech_prob": 0.04878094792366028}, {"id": 324, "seek": 110700, "start": 1119.0, "end": 1123.0, "text": " To zaproszenie do wsp\u00f3lnego zagl\u0105dania podmask\u0119", "tokens": [50964, 1407, 14223, 2635, 16778, 360, 47148, 11858, 27001, 75, 18962, 5609, 2497, 3799, 15724, 51164], "temperature": 0.0, "avg_logprob": -0.05977033658792044, "compression_ratio": 1.4464944649446494, "no_speech_prob": 0.04878094792366028}, {"id": 325, "seek": 110700, "start": 1123.0, "end": 1127.0, "text": " i budowania wiedzy o tym, jak naprawd\u0119 dzia\u0142aj\u0105 te technologie.", "tokens": [51164, 741, 3265, 21308, 46894, 1229, 277, 8107, 11, 4207, 20970, 27121, 11133, 535, 1537, 20121, 13, 51364], "temperature": 0.0, "avg_logprob": -0.05977033658792044, "compression_ratio": 1.4464944649446494, "no_speech_prob": 0.04878094792366028}, {"id": 326, "seek": 110700, "start": 1127.0, "end": 1131.0, "text": " Dok\u0142adnie. Celem jest przyspieszenie nauki o modelach j\u0119zykowych,", "tokens": [51364, 29768, 10358, 2766, 13, 8257, 10386, 3492, 6541, 749, 79, 530, 16778, 35616, 2984, 277, 2316, 608, 49055, 74, 19605, 11, 51564], "temperature": 0.0, "avg_logprob": -0.05977033658792044, "compression_ratio": 1.4464944649446494, "no_speech_prob": 0.04878094792366028}, {"id": 327, "seek": 110700, "start": 1131.0, "end": 1134.0, "text": " a nie tylko wypuszczenie kolejnego produktu.", "tokens": [51564, 257, 2838, 13219, 46392, 22378, 39043, 23749, 11858, 42816, 84, 13, 51714], "temperature": 0.0, "avg_logprob": -0.05977033658792044, "compression_ratio": 1.4464944649446494, "no_speech_prob": 0.04878094792366028}, {"id": 328, "seek": 113400, "start": 1134.0, "end": 1138.0, "text": " To fundamentalna zmiana filozofii, kt\u00f3ra w d\u0142ugiej perspektywie", "tokens": [50364, 1407, 8088, 629, 17020, 8497, 1387, 15151, 2670, 5597, 11, 19456, 261, 274, 34077, 7764, 868, 32659, 874, 8699, 50564], "temperature": 0.0, "avg_logprob": -0.052262285329999714, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.030285675078630447}, {"id": 329, "seek": 113400, "start": 1138.0, "end": 1140.0, "text": " mo\u017ce przynie\u015b\u0107 znacznie wi\u0119cej korzy\u015bci,", "tokens": [50564, 12034, 6501, 2766, 7753, 15397, 14875, 2766, 26004, 14784, 1229, 6199, 11, 50664], "temperature": 0.0, "avg_logprob": -0.052262285329999714, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.030285675078630447}, {"id": 330, "seek": 113400, "start": 1140.0, "end": 1143.0, "text": " ni\u017c kolejny, nieznacznie lepszy, ale wci\u0105\u017c zamkni\u0119ty model.", "tokens": [50664, 28502, 23749, 1634, 11, 2838, 22672, 14875, 2766, 476, 1878, 1229, 11, 6775, 261, 537, 27242, 19876, 74, 35938, 874, 2316, 13, 50814], "temperature": 0.0, "avg_logprob": -0.052262285329999714, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.030285675078630447}, {"id": 331, "seek": 113400, "start": 1143.0, "end": 1147.0, "text": " I to pozostawia nas z niezwykle wa\u017cn\u0105 my\u015bl\u0105 na koniec.", "tokens": [50814, 286, 281, 21281, 555, 34953, 5382, 710, 33511, 9726, 14677, 27777, 13113, 452, 19212, 1611, 1667, 5897, 35733, 13, 51014], "temperature": 0.0, "avg_logprob": -0.052262285329999714, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.030285675078630447}, {"id": 332, "seek": 113400, "start": 1147.0, "end": 1150.0, "text": " W \u015bwiecie, gdzie najpot\u0119czniejsze modele AI", "tokens": [51014, 343, 40078, 4260, 11, 18922, 11212, 17698, 1274, 3689, 44258, 4391, 306, 7318, 51164], "temperature": 0.0, "avg_logprob": -0.052262285329999714, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.030285675078630447}, {"id": 333, "seek": 113400, "start": 1150.0, "end": 1154.0, "text": " s\u0105 zamkni\u0119te i kontrolowane przez zaledwie kilka korporacji,", "tokens": [51164, 9015, 19876, 74, 35938, 975, 741, 14373, 6623, 23066, 14064, 710, 5573, 8699, 36466, 14784, 2816, 13152, 11, 51364], "temperature": 0.0, "avg_logprob": -0.052262285329999714, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.030285675078630447}, {"id": 334, "seek": 113400, "start": 1154.0, "end": 1159.0, "text": " pojawia si\u0119 fundamentalne pytanie, kt\u00f3re ten projekt stawia w centrum debaty.", "tokens": [51364, 30655, 654, 3244, 8088, 716, 36610, 11, 8864, 2064, 26261, 342, 34953, 261, 1489, 6247, 3001, 21398, 13, 51614], "temperature": 0.0, "avg_logprob": -0.052262285329999714, "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.030285675078630447}, {"id": 335, "seek": 115900, "start": 1159.0, "end": 1163.0, "text": " To pytanie brzmi, czy ryzyko zwi\u0105zane z potencjalnym,", "tokens": [50364, 1407, 36610, 738, 89, 3057, 11, 6430, 20791, 1229, 4093, 27741, 1929, 710, 1847, 22660, 22600, 12996, 11, 50564], "temperature": 0.0, "avg_logprob": -0.07682891483724552, "compression_ratio": 1.4641509433962263, "no_speech_prob": 0.03373418003320694}, {"id": 336, "seek": 115900, "start": 1163.0, "end": 1168.0, "text": " niew\u0142a\u015bciwym wykorzystaniem w pe\u0142ni otwartych i replikowalnych modeli,", "tokens": [50564, 43622, 5024, 6199, 86, 4199, 43606, 1229, 18758, 4907, 261, 43205, 3722, 4337, 29587, 16384, 741, 3248, 1035, 305, 304, 9399, 2316, 72, 11, 50814], "temperature": 0.0, "avg_logprob": -0.07682891483724552, "compression_ratio": 1.4641509433962263, "no_speech_prob": 0.03373418003320694}, {"id": 337, "seek": 115900, "start": 1168.0, "end": 1172.0, "text": " takich jak Olmo, jest wi\u0119ksze, ni\u017c ryzyko zwi\u0105zane", "tokens": [50814, 29607, 4207, 6141, 3280, 11, 3492, 29968, 1381, 11, 28502, 20791, 1229, 4093, 27741, 1929, 51014], "temperature": 0.0, "avg_logprob": -0.07682891483724552, "compression_ratio": 1.4641509433962263, "no_speech_prob": 0.03373418003320694}, {"id": 338, "seek": 115900, "start": 1172.0, "end": 1176.0, "text": " z ca\u0142kowitym brakiem zrozumienia i jakiegokolwiek spo\u0142ecznego nadzoru", "tokens": [51014, 710, 35224, 74, 305, 507, 76, 1548, 26116, 710, 27857, 449, 18811, 741, 4207, 20408, 49207, 44674, 36851, 89, 11858, 12617, 89, 32963, 51214], "temperature": 0.0, "avg_logprob": -0.07682891483724552, "compression_ratio": 1.4641509433962263, "no_speech_prob": 0.03373418003320694}, {"id": 339, "seek": 115900, "start": 1176.0, "end": 1180.0, "text": " nad zamkni\u0119tymi modelami, kt\u00f3re ju\u017c teraz, w tej chwili,", "tokens": [51214, 12617, 19876, 74, 35938, 874, 3057, 2316, 4526, 11, 8864, 10678, 16854, 11, 261, 12573, 26237, 2312, 11, 51414], "temperature": 0.0, "avg_logprob": -0.07682891483724552, "compression_ratio": 1.4641509433962263, "no_speech_prob": 0.03373418003320694}, {"id": 340, "seek": 115900, "start": 1180.0, "end": 1183.0, "text": " kszta\u0142tuj\u0105 nasz\u0105 rzeczywisto\u015b\u0107.", "tokens": [51414, 350, 15453, 46426, 83, 13263, 5382, 8925, 26297, 86, 9334, 7753, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07682891483724552, "compression_ratio": 1.4641509433962263, "no_speech_prob": 0.03373418003320694}, {"id": 341, "seek": 115900, "start": 1183.0, "end": 1186.0, "text": " Projekt Olmo stawia odwa\u017cn\u0105 tez\u0119,", "tokens": [51564, 34804, 6141, 3280, 342, 34953, 3611, 27111, 13113, 535, 11052, 11, 51714], "temperature": 0.0, "avg_logprob": -0.07682891483724552, "compression_ratio": 1.4641509433962263, "no_speech_prob": 0.03373418003320694}, {"id": 342, "seek": 118600, "start": 1186.0, "end": 1189.0, "text": " \u017ce w tej grze naszym najwi\u0119kszym wrogiem", "tokens": [50364, 3561, 261, 12573, 677, 1381, 48094, 48636, 1694, 26681, 261, 6675, 4907, 50514], "temperature": 0.0, "avg_logprob": -0.0702169418334961, "compression_ratio": 1.12987012987013, "no_speech_prob": 0.05584641546010971}, {"id": 343, "seek": 118600, "start": 1189.0, "end": 1192.0, "text": " i najwi\u0119kszym zagro\u017ceniem jest ignorancja.", "tokens": [50514, 741, 48636, 1694, 26681, 27001, 340, 24930, 4907, 3492, 14698, 4463, 2938, 13, 50664], "temperature": 0.0, "avg_logprob": -0.0702169418334961, "compression_ratio": 1.12987012987013, "no_speech_prob": 0.05584641546010971}], "language": "pl"}