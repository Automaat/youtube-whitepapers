{"text": " Witajcie w naszej kolejnej analizie. Mamy dzisiaj na stole fascynuj\u0105cy paradoks, kt\u00f3ry w zasadzie zdefiniowa\u0142 ca\u0142\u0105 er\u0119 w rozwoju AI. Z jednej strony jest ta \u017celazna zasada. Modele j\u0119zykowe staj\u0105 si\u0119 pot\u0119\u017cniejsze, m\u0105drzejsze, im po prostu s\u0105 wi\u0119ksze. Tak to ju\u017c by\u0142o udowodnione? A z drugiej, no c\u00f3\u017c, twarde prawa fizyki. Jak zmie\u015bci\u0107 model z miliardami parametr\u00f3w? Co\u015b co jest obj\u0119to\u015bciowo no znacznie wi\u0119ksze ni\u017c pami\u0119\u0107 jakiegokolwiek procesora na jednej karcie graficznej. To jak pr\u00f3ba wlania ca\u0142ego oceanu do jednej szklanki. Dok\u0142adnie. Idealne por\u00f3wnanie. Dzi\u015b przyjrzymy si\u0119 materia\u0142owi, kt\u00f3ry nie tylko podj\u0105\u0142 o to wyzwanie, ale i zaproponowa\u0142 rozwi\u0105zanie, z kt\u00f3rego korzystamy w zasadzie do dzi\u015b. Artyku\u0142 NVD? Tak. Megatron LM. Training multi-billion parameter language models. Using model parallelism. Nasza misja na dzisiaj. Zrozumie\u0107 jak oni obe szli te fundamentaln\u0105 barier\u0119 pami\u0119ci. Zobaczymy jak ich w sumie prosta metoda zadzia\u0142a\u0142a w praktyce i dlaczego otworzy\u0142a drzwi do no zupe\u0142nie nowej ery w trenowaniu AI. Dok\u0142adnie. A ten paradoksz, o kt\u00f3rym m\u00f3wisz, by\u0142 wtedy absolutnie no pal\u0105cem problemem. Mieli\u015bmy ju\u017c twarde dowody, \u017ce te wi\u0119ksze modele jak GPT-2 czy BERT radz\u0105 sobie niepor\u00f3wniwalnie lepiej z realnymi zadaniami. Jasne. Problem by\u0142 czysto in\u017cynteryjny. Model nie mie\u015bci\u0142 si\u0119 w pami\u0119ci pojedniczego GPU. I nie chodzi\u0142o tylko o same wagi. Trzeba pami\u0119ta\u0107, \u017ce proces trenowania wymaga przechowywania dodatkowych danych. Na przyk\u0142ad stan\u00f3w optymalizatora Adam, kt\u00f3re potrafi\u0105 zaj\u0105\u0107 2 nawet 3 razy wi\u0119cej miejsca ni\u017c sam model. Ale pr\u00f3bowano to ju\u017c jako\u015b rozwi\u0105za\u0107, prawda? To nie by\u0142 pierwszy raz, kiedy kto\u015b pr\u00f3bowa\u0142 po\u0142\u0105czy\u0107 si\u0142y kilku GPU. Oczywi\u015bcie, \u017ce nie. Istnia\u0142y ju\u017c techniki np. Pipeline Parallelizm, kt\u00f3r\u0105 spopularyzowa\u0142 Google w swoim GPipe. Aha. Polega\u0142o to wiesz na krojeniu modelu na takie pionowe bloki warstw i ka\u017cdy blok l\u0105dowa\u0142 na innym GPU. Dane przep\u0142ywa\u0142y przez niej jak na ta\u015bm\u0119 produkcyjn\u0105. Brzmi logicznie. Tak, ale problem w tym, \u017ce takie podej\u015bcie generowa\u0142o co\u015b, co nazywamy Pipeline Bobbles. Czyli b\u0105belki. Dok\u0142adnie. Momenty, w kt\u00f3rych cz\u0119\u015b\u0107 procesor\u00f3w po prostu czeka bezczynnie, a\u017c dane do nich dotr\u0105. To by\u0142a ogromna strata mocy. Inne rozwi\u0105zania, jak mesh tensor flow wymaga\u0142y z kolei pisania kodu pod specjalne kompilatory, co by\u0142o bardzo, bardzo nieelastyczne. Dobrze. To oroz\u0142\u00f3\u017cmy naczynniki pierwsze propozycji z tego artyku\u0142u. M\u00f3wi\u0105 o prostym i wydajnym podej\u015bciu, kt\u00f3re nazywaj\u0105 Intralayer Model Parallelizm. Co to w\u0142a\u015bciwie znaczy? I czym si\u0119 r\u00f3\u017cni od tej ta\u015bmy produkcyjnej, kt\u00f3ra mia\u0142a swoje, no, przestoje? Co jest tutaj fascynuj\u0105ce, to kompletna zmiana perspektywy. Zamiast dzieli\u0107 ca\u0142y model na du\u017ce bloki warstw i rozrzuca\u0107 si\u0119 po procesorach, autorzy postanowili zej\u015b\u0107 znacznie g\u0142\u0119biej, zdecydowali si\u0119 podzieli\u0107 same obliczenia, ale wewn\u0105trz ka\u017cdej pojedynczej warstwy typu transformer. Czyli zamiast dawa\u0107 ka\u017cdemu kucharzowi osobn\u0105 cz\u0119\u015b\u0107 przepisu do wykonania po kolei, posadzili wszystkich przy jednym wielkim stole i kazali im pracowa\u0107 nad tym samym daniem jednocze\u015bnie. To jest idealna analogia. W podej\u015bciu Pipeline jeden kucharz najpierw kroi warzywa, a potem przekazuje je drugiemu, kt\u00f3ry miesza sos. Jest sekwencja, jest czekanie? W\u0142a\u015bnie. A tutaj w podej\u015bciu Intralayer obaj kucharze pracuj\u0105 nad sosem w tym samym czasie. Jeden sieka czosnek, drugi miesza pomidory, a na ko\u0144cu \u0142\u0105cz\u0105 to w jednym garnku. Dzia\u0142aj\u0105 r\u00f3wnolegle. To eliminuje momenty bezczynno\u015bci. Dok\u0142adnie. Bo tak sprytnie podzieli\u0107 prac\u0119. Warstwa Transformer ma takie dwa g\u0142\u00f3wne klocki obliczeniowe. Block MLP, czyli Multilayer Perceptron i Block Self-Attention. W bloku MLP, kt\u00f3ry w uproszczeniu sk\u0142ada si\u0119 z dw\u00f3ch du\u017cych operacji mno\u017cenia macierzy, czyli GEM, zrobili co\u015b bardzo sprytnego. To znaczy? Pierwsz\u0105 macie\u017c WAK podzielili kolumnowo. Ka\u017cde GPU dosta\u0142o sw\u00f3j powiedzmy pionowy pasek. A drug\u0105 macie\u017c WAK z kolejnej operacji podzielili wierszowo. Czekaj, to brzmi troch\u0119 kontry intuicyjnie. Dlaczego Rastaka raz inaczej? Jaki jest w tym cel? I tu jest ca\u0142a magia. Bo wyj\u015bcie z operacji mno\u017cenia przez macie\u017c podzielon\u0105 kolumnowo staje si\u0119 idealnym wej\u015bciem dla operacji mno\u017cenia przez macie\u017c podzielon\u0105 wierszowo. Aha. Dzi\u0119ki temu obliczenia mog\u0105 p\u0142yn\u0105\u0107 dalej na tym samym GPU bez potrzeby wielkiej operacji tasowania danych pomi\u0119dzy wszystkimi procesorami. To w\u0142a\u015bnie ta sztuczka sprawi\u0142a, \u017ce komunikacja mi\u0119dzy GPU zosta\u0142a zredukowana do absolutnego minimum. Czyli ten sprytny zabieg geometryczny sprawia, \u017ce wyniki z pierwszego etapu idealnie pasuj\u0105 jako dane wej\u015bciowe do drugiego bez dodatkowego chaosu. Dok\u0142adnie. A kiedy ta komunikacja w og\u00f3le jest potrzebna? W\u0142a\u015bnie. Komunikacja, a konkretnie operacja AllReduce jest potrzebna dopiero na samym ko\u0144cu. Po tym drugim mno\u017ceniu, \u017ceby zebra\u0107 cz\u0119\u015bciowy wyniki w jedn\u0105 ca\u0142o\u015b\u0107. Wspomnia\u0142e\u015b o operacji AllReduce. Dla tych z nas, kt\u00f3rzy nie buduj\u0105 superkomputer\u00f3w na co dzie\u0144, czy mo\u017cemy to szybko zdefiniowa\u0107, co to w\u0142a\u015bciwie jest? Jasne. W najprostszych s\u0142owach to jak spotkanie zespo\u0142u. Ka\u017cdy pracowa\u0142 nad swoj\u0105 cz\u0119\u015bci\u0105 i przynosi sw\u00f3j wynik. W operacji AllReduce wszyscy dziel\u0105 si\u0119 swoimi wynikami i wsp\u00f3lnie obliczaj\u0105 jeden globalny rezultat, np. sum\u0119, kt\u00f3ry potem ka\u017cdy dostaje z powrotem. Ok. To jest kluczowy, ale te\u017c bardzo kosztowny moment synchronizacji. Ca\u0142a sztuka polega na tym, \u017ceby takich spotka\u0144 by\u0142o jak najmniej. Rozumiem. Mamy wi\u0119c za\u0142atwion\u0105 cz\u0119\u015b\u0107 MLP, ale to tylko po\u0142owa sukces\u00f3w w warstwie transformer, co z tym s\u0142ynnym mechanizmem uwagi. Sercem tych modeli. On przecie\u017c te\u017c generuje ogromne macierze. Jak poradzili sobie tam, \u017ceby nie zepsu\u0107 tej pi\u0119knej wydajno\u015bci? Tam wykorzystali naturaln\u0105 struktur\u0119 multi-head attention. Mechanizm uwagi w modelach transformer nie bez powodu nazywa si\u0119 wielog\u0142owym. Sk\u0142ada si\u0119 z wielu tzw. g\u0142\u00f3w uwagi, czyli attention heads, kt\u00f3re z natury dzia\u0142aj\u0105 niezale\u017cnie od siebie. No tak. Autorzy po prostu podzielili te g\u0142owy mi\u0119dzy dost\u0119pne GPU. Ka\u017cdy procesor dosta\u0142 do obliczenia swoj\u0105, sprawiedliw\u0105 cz\u0119\u015b\u0107. Macierze Quiry, Key i Value s\u0105 dzielone, a praca odbywa si\u0119 r\u00f3wnolegle. I znowu minimalna komunikacja. Znowu, podobnie jak w bloku MLP, komunikacja jest potrzebna dopiero na samym ko\u0144cu, \u017ceby po\u0142\u0105czy\u0107 wyniki ze wszystkich g\u0142\u00f3w. A wi\u0119c to jest ich rozwi\u0105zanie paradoksu oceanu w szklance. Zamiast pr\u00f3bowa\u0107 wla\u0107 go ca\u0142ego, znale\u017ali spos\u00f3b, by ka\u017cdy procesor wypi\u0142 swoj\u0105, idealnie odmierzon\u0105 porcj\u0119. I to w tym samym czasie. Podsumowuj\u0105c, jak rzadko te procesory musz\u0105 si\u0119 ze sob\u0105 komunikowa\u0107? I tu dochodzimy do sedna ich sukcesu. W ca\u0142ej pojedynczej skomplikowanej warstwie transformer potrzebne s\u0105 tylko dwie operacje all reduce w przej\u015bciu do przodu. W forward pass. Tak, forward pass i dwie w przej\u015bciu wstecz w backward pass. To jest niewiarygodnie ma\u0142o. Niewiarygodne. Co wi\u0119cej, i to te\u017c jest kluczowe, zainplementowali to wszystko w standardowym frameworku PyTorch. Nie musieli tworzy\u0107 nowego kompilatora. Czyli to by\u0142o praktyczne rozwi\u0105zanie. Bardzo. Wystarczy\u0142o doda\u0107 kilka prostych operacji komunikacyjnych. Pokazuj\u0105 w artykule, \u017ce to dos\u0142ownie kilka liniki kod\u00f3w Python'a, kt\u00f3re mo\u017cna wstawi\u0107 do istniej\u0105cego modelu. Ok, teoria jest elegancka, ale \u015bwiat AI jest pe\u0142en pi\u0119knych teorii, kt\u00f3re niestety, no, nie dzia\u0142aj\u0105 na du\u017c\u0105 skal\u0119. Mamy tu do czynienia z setkami procesor\u00f3w, czy oni faktycznie to zbudowali? I udowodnili, \u017ce ten system nie rozpada si\u0119 pod w\u0142asnym ci\u0119\u017carem. Jak du\u017cy model uda\u0142o im si\u0119 tym uci\u0105gn\u0105\u0107? I to jest moment, w kt\u00f3rym ten artyku\u0142 naprawd\u0119 b\u0142yszczy. Przeszli od teorii do praktyki na skal\u0119, kt\u00f3ra wtedy by\u0142a, no, niewyobra\u017calna. Wytrenowali model w architekturze GPT-2, kt\u00f3ry mia\u0142 8,3 miliarda parametr\u00f3w. 8,3 miliarda. U\u017cywaj\u0105c do tego klastra 512 procesor\u00f3w graficznych NVIDIA V100. 512 GPU. To ju\u017c ma\u0142a elektrownia. A jak z wydajno\u015bci\u0105? Czy te wszystkie procesory faktycznie ze sob\u0105 wsp\u00f3\u0142pracowa\u0142y, czy g\u0142\u00f3wnie czeka\u0142y na siebie nawzajem? Wyniki skalowania s\u0105 imponuj\u0105ce. Je\u015bli spojrzymy na rysunek jeden w artykule, zobaczymy wykres, kt\u00f3ry pokazuje niemal idealnie liniow\u0105 skalowalno\u015b\u0107. Osi\u0105gn\u0119li moc obliczeniow\u0105 na poziomie 15-5 lops\u00f3w. 15-5 lops\u00f3w? Tak. Co stanowi\u0142o a\u017c 76% wydajno\u015bci w por\u00f3wnaniu do teoretycznego, idealnego liniowego wzrostu. Czekaj, 76% to rzeczywi\u015bcie \u015bwietny wynik in\u017cynieryjny, ale to wci\u0105\u017c oznacza, \u017ce tracimy prawie 1,4 mocy obliczeniowej na sam\u0105 komunikacj\u0119 i synchronizacj\u0119. Czy to nie jest argument za tym, \u017ce takie brutalne skalowanie zawsze b\u0119dzie nieefektywne i mo\u017ce powinni\u015bmy szuka\u0107 zupe\u0142nie innych, bardziej kompaktowych architektur? To jest doskona\u0142e pytanie. I tak i nie. W \u015bwiecie oblicze\u0144 rozproszonych osi\u0105gni\u0119cie 100% efektywno\u015bci jest praktycznie niemo\u017cliwe. Zawsze jest jaki\u015b narzut. Wynik powy\u017cej 70% na tak ogromnej skali przy tak z\u0142o\u017conych obliczeniach by\u0142 absolutnym prze\u0142omem. Pokaza\u0142, \u017ce ta droga ma sens, \u017ce skalowanie horyzontalne jest mo\u017cliwe bez wpadania w pu\u0142apk\u0119 malej\u0105cych zysk\u00f3w. Czyli to by\u0142 dow\u00f3d, \u017ce warto budowa\u0107 jeszcze wi\u0119ksze klastry? Dok\u0142adnie tak. Dobrze. Czyli metoda jest wydajna, pozwala u\u017cywa\u0107 setek GPU niemal bez strat. Ale pozostaje kluczowe pytanie dla kogo\u015b, kto chce z tych modeli korzysta\u0107. Czy one s\u0105 faktycznie lepsze? Czy ta ca\u0142a dodatkowa moc obliczeniowa prze\u0142o\u017cy\u0142a si\u0119 na wy\u017csz\u0105 jako\u015b\u0107 i lepsze wyniki? Zdecydowanie tak. I to na kilku frontach. Zacznijmy od tego gigantycznego modelu GPT2 z 8-3 miliardami parametr\u00f3w. Ustanowi\u0142 on nowe rekordy, czyli State of the Art, na dw\u00f3ch kluczowych benchmarkach j\u0119zykowych. Na wiki tekst 103 osi\u0105gn\u0105\u0142 Perplexity na poziomie 10-8. Zatrzymajmy si\u0119 na chwil\u0119. Perplexity to metryka, kt\u00f3r\u0105 cz\u0119sto widzimy w artyku\u0142ach o modelach j\u0119zykowych. Niszcza warto\u015b\u0107 jest lepsza, ale co ona tak naprawd\u0119 nam m\u00f3wi o modelu? To jest miara tego, jak bardzo model jest zdziwiony tekstem, kt\u00f3ry widzi. I mi ni\u017csza Perplexity, tym model jest pewniejszy swoich przewidywa\u0144 kolejnych s\u0142\u00f3w. Jakby jego niepewno\u015b\u0107. W\u0142a\u015bnie. Mo\u017cna o tym my\u015ble\u0107 jako niepewno\u015b\u0107 modelu, a skok z poprzedniego rekordu, kt\u00f3ry wynosi\u0142 15-8 do 10-8 to jest ogromna redukcja tej niepewno\u015bci. Model po prostu znacznie lepiej rozumia\u0142 struktur\u0119 j\u0119zyka. A drugi benchmark? To lambada, kt\u00f3ry testuje rozumienie d\u0142ugiego kontekstu, ka\u017c\u0105c modelowi przewidzie\u0107 ostatnie s\u0142owo w d\u0142ugim akapicie. Tam osi\u0105gn\u0105\u0142 celno\u015b\u0107 66,5% r\u00f3wnie\u017c pobijaj\u0105c poprzedni rekord. Co wi\u0119cej, rysunek sze\u015b\u0107 w artykule pokazuje co\u015b fascynuj\u0105cego. Wi\u0119ksze modele nie tylko osi\u0105gaj\u0105 lepsze wyniki ko\u0144cowe, ale te\u017c ucz\u0105 si\u0119 znacznie szybciej. Aha. Krzywa b\u0142\u0119du dla modelu 8-3B opada o wiele gwa\u0142towniej ni\u017c dla mniejszych modeli. Czyli dla GPT-2 wszystko posz\u0142o g\u0142adko. Wi\u0119kszy model lepszy wyniki. Ale badali te\u017c drug\u0105, r\u00f3wnie wa\u017cn\u0105 architektur\u0119, czyli BERT, jak posz\u0142o tam. I tu dochodzimy do chyba najciekawszego, niemal przypadkowego odkrycia w ca\u0142ym tym artykule. Wcze\u015bniejsze pr\u00f3by powi\u0119kszania modelu BERT, podejmowane przez innych badaczy, ko\u0144czy\u0142y si\u0119 katastrof\u0105. Chwila, jak to katastrof\u0105. M\u00f3wisz, \u017ce wi\u0119kszy i teoretycznie pot\u0119\u017cniejszy model by\u0142 gorszy od swojego mniejszego poprzednika, to przeczy ca\u0142ej idei skalowania. Dok\u0142adnie. Okazywa\u0142o si\u0119, \u017ce po przekroczeniu pewnego rozmiaru model stawa\u0142 si\u0119 niestabilny w trakcie treningu. Jego b\u0142\u0105d, zamiast male\u0107, nagle eksplodowa\u0142. Wow. Wyniki zamiast si\u0119 poprawia\u0107, ulega\u0142y po gorszeniu. A zesp\u00f3\u0142 Megatron LM nie tylko umo\u017cliwi\u0142 fizyczne skalowanie BERT-a, ale te\u017c przy okazji zdiagnozowa\u0142, dlaczego wcze\u015bniej si\u0119 to nie udawa\u0142o. To rodzi bardzo wa\u017cne pytanie. Co by\u0142o przyczyn\u0105? Jaka\u015b fundamentalna wada w architekturze BERT, kt\u00f3ra ujawnia\u0142a si\u0119 dopiero w du\u017cej skali? Odpowied\u017a jest zaskakuj\u0105ca. W swojej prostocie. Problemem nie by\u0142 sam rozmiar, a drobny wydawa\u0142oby si\u0119 detal w architekturze. Jaki? Kolejno\u015b\u0107 operacji Layer Normalization i tak zwanych Residual Connections, czyli tych po\u0142\u0105cze\u0144, kt\u00f3re przepuszczaj\u0105 informacje z wcze\u015bniejszych etap\u00f3w. \u017bartujesz? Czyli najwi\u0119kszym problemem w skalowaniu BERT-a nie by\u0142a jaka\u015b fundamentalna granica mo\u017cliwo\u015bci AI, tylko z\u0142a kolejno\u015b\u0107 klock\u00f3w? Dok\u0142adnie tak. To brzmi jak odkrycie, na kt\u00f3re kto\u015b m\u00f3g\u0142 wpa\u015b\u0107 w przez przypadek, a kt\u00f3re zmieni\u0142o wszystko. To pokazuje, jak diabe\u0142 tkwi w szczeg\u00f3\u0142ach. Dok\u0142adnie tak. Na rysunku 7 wida\u0107 to jak na d\u0142oni. Pokazuje on schematycznie star\u0105 architektur\u0119 i now\u0105 poprawion\u0105. R\u00f3\u017cnica jest subtelna. To tylko przesuni\u0119cie bloku Layer Norm w inne miejsce. Ale z ogromnymi konsekwencjami. Ogromnymi. Wykres obok dowodzi, jak kluczowa jest ta zmiana. Czerwona linia pokazuje, co dzia\u0142o si\u0119 podczas treningu wi\u0119kszego modelu na starej architekturze. Po pewnym czasie trening si\u0119 za\u0142amywa\u0142, b\u0142\u0105d gwa\u0142townier\u00f3z. A niebieska? A niebieska linia dla tej samej wielko\u015bci modelu, ale z now\u0105 poprawion\u0105 architektur\u0105 pokazuje stabilny spadek b\u0142\u0119du przez ca\u0142y czas. To by\u0142o fundamentalne odkrycie, kt\u00f3re pozwoli\u0142o na reszcie stabilnie trenowa\u0107 gigantyczne modele typu BERT. Niesamowite. Czyli przyszli rozwi\u0105za\u0107 problem in\u017cynteryjny, a przy okazji dokonali odkrycia na poziomie samej architektury AI, kt\u00f3re odblokowa\u0142o ca\u0142\u0105 ga\u0142\u0105\u017a bada\u0144. W\u0142a\u015bnie. I to odkrycie natychmiast przynios\u0142o owoce. Po wprowadzeniu tej poprawki model BERT o wielko\u015bci 3,9 miliarda parametr\u00f3w r\u00f3wnie\u017c osi\u0105gn\u0105 wyniki State of the Art. Na przyk\u0142ad na zadaniu czytania ze zrozumieniem o nazwie RISE uzyska\u0142 99% celno\u015bci, co by\u0142o nowym rekordem. Okej. Wniosek jest wi\u0119c taki, \u017ce sukces Megatron LM to nie tylko genialna in\u017cynieria system\u00f3w, ale tak\u017ce fundamentalny wgl\u0105d w to, jak budowa\u0107 stabilne, bardzo g\u0142\u0119bokie sieci Transformer. A wi\u0119c co to wszystko oznacza podsumowuj\u0105c? Megatron LM to nie jest tylko chwytliwa nazwa pot\u0119\u017cnego modelu. Zdecydowanie nie. To przede wszystkim prosta i niezwykle skuteczna technika model paralelizm, kt\u00f3ra pozwoli\u0142a prze\u0142ama\u0107 barier\u0119 pami\u0119ci GPU. Rozwi\u0105za\u0142a ten problem oceanu w szklance. Umo\u017cliwi\u0142a trenowanie modeli oskali, kt\u00f3ra wcze\u015bniej by\u0142a kompletnie nieosi\u0105galna i co r\u00f3wnie wa\u017cne. Przy okazji odkryto kluczowy element, kt\u00f3ry stabilizuje architektur\u0119 BERT przy du\u017cych rozmiarach. Je\u015bli po\u0142\u0105czymy to z szerszym obrazem, ta praca po prostu otworzy\u0142a drog\u0119 dla ca\u0142ej bran\u017cy. To dzi\u0119ki niej i podobnym technikom, kt\u00f3re na nie bazowa\u0142y nied\u0142ugo p\u00f3\u017aniej powsta\u0142y jeszcze wi\u0119ksze modele. Czyli to by\u0142 ten kamie\u0144 milowy? Tak. Zreszt\u0105 sam artyku\u0142 wspomina, \u017ce w wsp\u00f3\u0142pracy z Microsoftem, u\u017cywaj\u0105c Megatrona, wytremowano model Turing NLG maj\u0105cy 17 miliard\u00f3w parametr\u00f3w. To by\u0142 bezpo\u015bredni nast\u0119pca. Megatron LM dostarci\u0142 plan i narz\u0119dzia, kt\u00f3re nap\u0119dzi\u0142y ten wy\u015bcig w skalowaniu AI, prowadz\u0105c nas prosto do modeli, z kt\u00f3rymi pracujemy dzisiaj. Czyli to jeden z tych fundamentalnych artyku\u0142\u00f3w, do kt\u00f3rego wci\u0105\u017c wracamy, nawet je\u015bli technologia posz\u0142a ju\u017c naprz\u00f3d? Zdecydowanie. Wyznaczy\u0142 standardy i pokaza\u0142, \u017ce to jest mo\u017cliwe. Artyku\u0142 wspomina te\u017c o przysz\u0142ych kierunkach bada\u0144, takich jak dalsze skalowanie czy knowledge distillation. Destylacja wiedzy? Tak. I to prowadzi do ciekawej my\u015bli na koniec. Zaintrykowa\u0142a\u015b mnie. Przez lata, nap\u0119dzani takimi pracami jak Megatron LM, skupiali\u015bmy si\u0119 na budowaniu coraz wi\u0119kszych, cyfrowych m\u00f3zg\u00f3w, kt\u00f3re wymagaj\u0105 setek GPU do dzia\u0142ania. Osi\u0105gn\u0119li\u015bmy w tym mistrzostwo. To prawda. Ale mo\u017ce teraz, gdy ju\u017c wiemy, jak je budowa\u0107, nast\u0119pnym wielkim wyzwaniem jest co\u015b zupe\u0142nie odwrotnego. Czy nie powinni\u015bmy si\u0119 teraz skupi\u0107 na tym, jak destylowa\u0107 ich ogromn\u0105 wiedz\u0119 do znacznie mniejszych, bardziej wydajnych modeli? Modeli, z kt\u00f3rych ka\u017cdy m\u00f3g\u0142by korzysta\u0107 na co dzie\u0144. W\u0142a\u015bnie. Na przyk\u0142ad na swoim telefonie, bez dost\u0119pu do superkomputera, mo\u017ce era budowania katedr dobiega ko\u0144ca, a zaczyna si\u0119 era budowania z nich u\u017cytecznych, podr\u0119cznych narz\u0119dzi.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.68, "text": " Witajcie w naszej kolejnej analizie. Mamy dzisiaj na stole fascynuj\u0105cy paradoks, kt\u00f3ry w zasadzie zdefiniowa\u0142 ca\u0142\u0105 er\u0119 w rozwoju AI.", "tokens": [50364, 42299, 47276, 261, 42946, 23749, 11794, 2624, 590, 414, 13, 376, 7804, 25772, 1667, 16326, 30632, 1344, 77, 13263, 1344, 13480, 25500, 11, 9913, 261, 44585, 3283, 710, 20595, 3812, 30105, 1335, 15926, 1189, 1274, 261, 9544, 6120, 8954, 7318, 13, 50848], "temperature": 0.0, "avg_logprob": -0.19199855960145287, "compression_ratio": 1.3288288288288288, "no_speech_prob": 0.05914975702762604}, {"id": 1, "seek": 0, "start": 9.68, "end": 19.28, "text": " Z jednej strony jest ta \u017celazna zasada. Modele j\u0119zykowe staj\u0105 si\u0119 pot\u0119\u017cniejsze, m\u0105drzejsze, im po prostu s\u0105 wi\u0119ksze.", "tokens": [50848, 1176, 5232, 11794, 32406, 3492, 1846, 19625, 338, 921, 629, 26530, 1538, 13, 20500, 306, 49055, 74, 6880, 342, 11133, 3244, 1847, 1274, 1427, 44258, 11, 275, 18962, 13503, 25530, 1381, 11, 566, 714, 19518, 9015, 29968, 1381, 13, 51328], "temperature": 0.0, "avg_logprob": -0.19199855960145287, "compression_ratio": 1.3288288288288288, "no_speech_prob": 0.05914975702762604}, {"id": 2, "seek": 0, "start": 19.28, "end": 21.080000000000002, "text": " Tak to ju\u017c by\u0142o udowodnione?", "tokens": [51328, 9118, 281, 10678, 14811, 11727, 305, 378, 77, 5328, 30, 51418], "temperature": 0.0, "avg_logprob": -0.19199855960145287, "compression_ratio": 1.3288288288288288, "no_speech_prob": 0.05914975702762604}, {"id": 3, "seek": 2108, "start": 21.08, "end": 34.28, "text": " A z drugiej, no c\u00f3\u017c, twarde prawa fizyki. Jak zmie\u015bci\u0107 model z miliardami parametr\u00f3w? Co\u015b co jest obj\u0119to\u015bciowo no znacznie wi\u0119ksze ni\u017c pami\u0119\u0107 jakiegokolwiek procesora na jednej karcie graficznej.", "tokens": [50364, 316, 710, 47373, 11, 572, 6333, 1427, 11, 683, 10866, 3206, 4151, 21000, 88, 2984, 13, 15029, 17020, 414, 6199, 2162, 2316, 710, 1962, 72, 515, 4526, 6220, 27965, 3901, 30, 3066, 1788, 598, 3492, 1111, 11115, 1353, 6199, 19941, 572, 15397, 14875, 2766, 29968, 1381, 28502, 31088, 2162, 4207, 20408, 49207, 44674, 17565, 3252, 1667, 5232, 11794, 7917, 4260, 1295, 1786, 89, 11794, 13, 51024], "temperature": 0.0, "avg_logprob": -0.13949796088836477, "compression_ratio": 1.436532507739938, "no_speech_prob": 0.5771161913871765}, {"id": 4, "seek": 2108, "start": 34.28, "end": 37.76, "text": " To jak pr\u00f3ba wlania ca\u0142ego oceanu do jednej szklanki.", "tokens": [51024, 1407, 4207, 8565, 4231, 261, 75, 5609, 35224, 6308, 7810, 84, 360, 5232, 11794, 7870, 7837, 27203, 13, 51198], "temperature": 0.0, "avg_logprob": -0.13949796088836477, "compression_ratio": 1.436532507739938, "no_speech_prob": 0.5771161913871765}, {"id": 5, "seek": 2108, "start": 37.76, "end": 47.56, "text": " Dok\u0142adnie. Idealne por\u00f3wnanie. Dzi\u015b przyjrzymy si\u0119 materia\u0142owi, kt\u00f3ry nie tylko podj\u0105\u0142 o to wyzwanie, ale i zaproponowa\u0142 rozwi\u0105zanie, z kt\u00f3rego korzystamy w zasadzie do dzi\u015b.", "tokens": [51198, 29768, 10358, 2766, 13, 13090, 304, 716, 1515, 812, 895, 7155, 13, 413, 3992, 1788, 6501, 73, 13047, 2226, 3244, 2389, 8908, 24503, 11, 9913, 2838, 13219, 2497, 8555, 1221, 277, 281, 4628, 14406, 7155, 11, 6775, 741, 14223, 1513, 266, 30105, 9544, 22620, 7155, 11, 710, 46951, 14784, 36049, 7804, 261, 44585, 3283, 360, 31981, 1788, 13, 51688], "temperature": 0.0, "avg_logprob": -0.13949796088836477, "compression_ratio": 1.436532507739938, "no_speech_prob": 0.5771161913871765}, {"id": 6, "seek": 2108, "start": 47.56, "end": 48.68, "text": " Artyku\u0142 NVD?", "tokens": [51688, 5735, 88, 5279, 1221, 426, 53, 35, 30, 51744], "temperature": 0.0, "avg_logprob": -0.13949796088836477, "compression_ratio": 1.436532507739938, "no_speech_prob": 0.5771161913871765}, {"id": 7, "seek": 4868, "start": 49.08, "end": 58.4, "text": " Tak. Megatron LM. Training multi-billion parameter language models. Using model parallelism.", "tokens": [50384, 9118, 13, 9986, 267, 2044, 46529, 13, 20620, 4825, 12, 65, 11836, 13075, 2856, 5245, 13, 11142, 2316, 8952, 1434, 13, 50850], "temperature": 0.0, "avg_logprob": -0.1577152427361936, "compression_ratio": 1.367965367965368, "no_speech_prob": 0.004287577234208584}, {"id": 8, "seek": 4868, "start": 58.4, "end": 74.2, "text": " Nasza misja na dzisiaj. Zrozumie\u0107 jak oni obe szli te fundamentaln\u0105 barier\u0119 pami\u0119ci. Zobaczymy jak ich w sumie prosta metoda zadzia\u0142a\u0142a w praktyce i dlaczego otworzy\u0142a drzwi do no zupe\u0142nie nowej ery w trenowaniu AI.", "tokens": [50850, 16151, 2394, 3346, 2938, 1667, 25772, 13, 1176, 27857, 449, 414, 2162, 4207, 36317, 36346, 7870, 2081, 535, 8088, 13113, 2159, 811, 1274, 31088, 537, 13, 1176, 996, 14691, 2226, 4207, 1893, 261, 2408, 414, 582, 8638, 1131, 13449, 42788, 89, 25605, 5024, 261, 3206, 74, 874, 384, 741, 37873, 39329, 4337, 28321, 1229, 5024, 1224, 89, 6253, 360, 572, 49922, 586, 40779, 1189, 88, 261, 23136, 305, 25849, 7318, 13, 51640], "temperature": 0.0, "avg_logprob": -0.1577152427361936, "compression_ratio": 1.367965367965368, "no_speech_prob": 0.004287577234208584}, {"id": 9, "seek": 7420, "start": 74.2, "end": 89.52000000000001, "text": " Dok\u0142adnie. A ten paradoksz, o kt\u00f3rym m\u00f3wisz, by\u0142 wtedy absolutnie no pal\u0105cem problemem. Mieli\u015bmy ju\u017c twarde dowody, \u017ce te wi\u0119ksze modele jak GPT-2 czy BERT radz\u0105 sobie niepor\u00f3wniwalnie lepiej z realnymi zadaniami.", "tokens": [50364, 29768, 10358, 2766, 13, 316, 2064, 13480, 453, 15453, 11, 277, 30120, 13489, 23848, 11, 16673, 26959, 18757, 2766, 572, 3984, 1611, 26422, 1154, 443, 13, 376, 23099, 10513, 10678, 683, 10866, 9459, 843, 11, 3561, 535, 29968, 1381, 4391, 306, 4207, 26039, 51, 12, 17, 6430, 363, 31479, 2843, 8925, 13652, 2838, 2816, 812, 895, 72, 29530, 2766, 476, 39699, 710, 957, 31813, 710, 11338, 15568, 13, 51130], "temperature": 0.0, "avg_logprob": -0.1604718542718268, "compression_ratio": 1.1968911917098446, "no_speech_prob": 0.07779465615749359}, {"id": 10, "seek": 7420, "start": 89.52000000000001, "end": 90.12, "text": " Jasne.", "tokens": [51130, 34023, 716, 13, 51160], "temperature": 0.0, "avg_logprob": -0.1604718542718268, "compression_ratio": 1.1968911917098446, "no_speech_prob": 0.07779465615749359}, {"id": 11, "seek": 9012, "start": 90.2, "end": 102.32000000000001, "text": " Problem by\u0142 czysto in\u017cynteryjny. Model nie mie\u015bci\u0142 si\u0119 w pami\u0119ci pojedniczego GPU. I nie chodzi\u0142o tylko o same wagi. Trzeba pami\u0119ta\u0107, \u017ce proces trenowania wymaga przechowywania dodatkowych danych.", "tokens": [50368, 11676, 16673, 6430, 20875, 294, 1427, 2534, 12733, 73, 1634, 13, 17105, 2838, 12597, 6199, 1221, 3244, 261, 31088, 537, 714, 40543, 7692, 27725, 18407, 13, 286, 2838, 23998, 5249, 13219, 277, 912, 261, 20291, 13, 1765, 1381, 4231, 31088, 42931, 11, 3561, 17565, 23136, 21308, 29764, 9286, 8325, 339, 10089, 86, 5609, 13886, 33525, 19605, 274, 34644, 13, 50974], "temperature": 0.0, "avg_logprob": -0.11858256657918294, "compression_ratio": 1.4119496855345912, "no_speech_prob": 0.13969042897224426}, {"id": 12, "seek": 9012, "start": 102.32000000000001, "end": 109.04, "text": " Na przyk\u0142ad stan\u00f3w optymalizatora Adam, kt\u00f3re potrafi\u0105 zaj\u0105\u0107 2 nawet 3 razy wi\u0119cej miejsca ni\u017c sam model.", "tokens": [50974, 6056, 23144, 27984, 3901, 2427, 4199, 304, 590, 1639, 64, 7938, 11, 8864, 1847, 10437, 11404, 710, 11133, 2162, 568, 22696, 805, 9639, 88, 26004, 18522, 44239, 28502, 3247, 2316, 13, 51310], "temperature": 0.0, "avg_logprob": -0.11858256657918294, "compression_ratio": 1.4119496855345912, "no_speech_prob": 0.13969042897224426}, {"id": 13, "seek": 9012, "start": 109.04, "end": 115.24000000000001, "text": " Ale pr\u00f3bowano to ju\u017c jako\u015b rozwi\u0105za\u0107, prawda? To nie by\u0142 pierwszy raz, kiedy kto\u015b pr\u00f3bowa\u0142 po\u0142\u0105czy\u0107 si\u0142y kilku GPU.", "tokens": [51310, 9366, 8565, 8202, 3730, 281, 10678, 17123, 1788, 9544, 18234, 35873, 11, 43607, 30, 1407, 2838, 16673, 34016, 9639, 11, 18777, 32982, 8565, 65, 30105, 714, 15926, 33967, 1511, 6825, 5128, 5279, 18407, 13, 51620], "temperature": 0.0, "avg_logprob": -0.11858256657918294, "compression_ratio": 1.4119496855345912, "no_speech_prob": 0.13969042897224426}, {"id": 14, "seek": 11524, "start": 115.32, "end": 123.11999999999999, "text": " Oczywi\u015bcie, \u017ce nie. Istnia\u0142y ju\u017c techniki np. Pipeline Parallelizm, kt\u00f3r\u0105 spopularyzowa\u0142 Google w swoim GPipe.", "tokens": [50368, 42980, 11, 3561, 2838, 13, 12810, 12679, 6825, 10678, 1537, 9850, 33808, 13, 35396, 5440, 3457, 336, 338, 590, 76, 11, 37415, 637, 404, 425, 822, 89, 30105, 3329, 261, 13291, 332, 26039, 6527, 13, 50758], "temperature": 0.0, "avg_logprob": -0.20056805910764042, "compression_ratio": 1.4241379310344828, "no_speech_prob": 0.02474980801343918}, {"id": 15, "seek": 11524, "start": 123.11999999999999, "end": 124.03999999999999, "text": " Aha.", "tokens": [50758, 27448, 13, 50804], "temperature": 0.0, "avg_logprob": -0.20056805910764042, "compression_ratio": 1.4241379310344828, "no_speech_prob": 0.02474980801343918}, {"id": 16, "seek": 11524, "start": 124.03999999999999, "end": 134.07999999999998, "text": " Polega\u0142o to wiesz na krojeniu modelu na takie pionowe bloki warstw i ka\u017cdy blok l\u0105dowa\u0142 na innym GPU. Dane przep\u0142ywa\u0142y przez niej jak na ta\u015bm\u0119 produkcyjn\u0105.", "tokens": [50804, 34212, 3680, 5249, 281, 261, 15347, 1667, 45909, 15378, 5951, 2316, 84, 1667, 15963, 280, 313, 6880, 888, 17056, 1516, 372, 86, 741, 31615, 888, 453, 287, 18962, 30105, 1667, 294, 12996, 18407, 13, 413, 1929, 30829, 6825, 4151, 6825, 14064, 2838, 73, 4207, 1667, 1846, 1788, 76, 1274, 33699, 42949, 13113, 13, 51306], "temperature": 0.0, "avg_logprob": -0.20056805910764042, "compression_ratio": 1.4241379310344828, "no_speech_prob": 0.02474980801343918}, {"id": 17, "seek": 11524, "start": 134.07999999999998, "end": 135.24, "text": " Brzmi logicznie.", "tokens": [51306, 1603, 89, 3057, 9952, 89, 2766, 13, 51364], "temperature": 0.0, "avg_logprob": -0.20056805910764042, "compression_ratio": 1.4241379310344828, "no_speech_prob": 0.02474980801343918}, {"id": 18, "seek": 11524, "start": 135.24, "end": 140.76, "text": " Tak, ale problem w tym, \u017ce takie podej\u015bcie generowa\u0142o co\u015b, co nazywamy Pipeline Bobbles.", "tokens": [51364, 9118, 11, 6775, 1154, 261, 8107, 11, 3561, 15963, 7468, 73, 9815, 1337, 5528, 5249, 19241, 11, 598, 20151, 27112, 7804, 35396, 5440, 6085, 8806, 13, 51640], "temperature": 0.0, "avg_logprob": -0.20056805910764042, "compression_ratio": 1.4241379310344828, "no_speech_prob": 0.02474980801343918}, {"id": 19, "seek": 11524, "start": 140.76, "end": 142.16, "text": " Czyli b\u0105belki.", "tokens": [51640, 37099, 272, 1611, 5390, 2984, 13, 51710], "temperature": 0.0, "avg_logprob": -0.20056805910764042, "compression_ratio": 1.4241379310344828, "no_speech_prob": 0.02474980801343918}, {"id": 20, "seek": 14216, "start": 142.28, "end": 147.72, "text": " Dok\u0142adnie. Momenty, w kt\u00f3rych cz\u0119\u015b\u0107 procesor\u00f3w po prostu czeka bezczynnie, a\u017c dane do nich dotr\u0105.", "tokens": [50370, 29768, 10358, 2766, 13, 5576, 4179, 11, 261, 30382, 47149, 17565, 284, 3901, 714, 19518, 6472, 36361, 10782, 6522, 77, 2766, 11, 48134, 49206, 360, 25570, 5893, 32881, 13, 50642], "temperature": 0.0, "avg_logprob": -0.14357512527041966, "compression_ratio": 1.4150326797385622, "no_speech_prob": 0.003974521532654762}, {"id": 21, "seek": 14216, "start": 147.72, "end": 149.6, "text": " To by\u0142a ogromna strata mocy.", "tokens": [50642, 1407, 23936, 34416, 298, 629, 1056, 3274, 705, 1344, 13, 50736], "temperature": 0.0, "avg_logprob": -0.14357512527041966, "compression_ratio": 1.4150326797385622, "no_speech_prob": 0.003974521532654762}, {"id": 22, "seek": 14216, "start": 149.6, "end": 157.64, "text": " Inne rozwi\u0105zania, jak mesh tensor flow wymaga\u0142y z kolei pisania kodu pod specjalne kompilatory, co by\u0142o bardzo, bardzo nieelastyczne.", "tokens": [50736, 682, 716, 9544, 22620, 5609, 11, 4207, 17407, 40863, 3095, 29764, 9286, 6825, 710, 18303, 72, 26584, 5609, 350, 34873, 2497, 46433, 716, 5207, 79, 388, 4745, 11, 598, 14811, 9034, 11, 9034, 2838, 338, 9820, 38491, 13, 51138], "temperature": 0.0, "avg_logprob": -0.14357512527041966, "compression_ratio": 1.4150326797385622, "no_speech_prob": 0.003974521532654762}, {"id": 23, "seek": 14216, "start": 157.64, "end": 161.6, "text": " Dobrze. To oroz\u0142\u00f3\u017cmy naczynniki pierwsze propozycji z tego artyku\u0142u.", "tokens": [51138, 29679, 13503, 13, 1407, 277, 27857, 1221, 812, 1427, 2226, 297, 14691, 26384, 9850, 45994, 447, 2259, 1229, 19649, 710, 8627, 594, 874, 5279, 24066, 13, 51336], "temperature": 0.0, "avg_logprob": -0.14357512527041966, "compression_ratio": 1.4150326797385622, "no_speech_prob": 0.003974521532654762}, {"id": 24, "seek": 14216, "start": 161.6, "end": 167.04, "text": " M\u00f3wi\u0105 o prostym i wydajnym podej\u015bciu, kt\u00f3re nazywaj\u0105 Intralayer Model Parallelizm.", "tokens": [51336, 376, 3901, 11404, 277, 10293, 4199, 741, 25984, 1805, 12996, 7468, 73, 6199, 84, 11, 8864, 20151, 27112, 11133, 5681, 2155, 11167, 17105, 3457, 336, 338, 590, 76, 13, 51608], "temperature": 0.0, "avg_logprob": -0.14357512527041966, "compression_ratio": 1.4150326797385622, "no_speech_prob": 0.003974521532654762}, {"id": 25, "seek": 16704, "start": 167.12, "end": 173.04, "text": " Co to w\u0142a\u015bciwie znaczy? I czym si\u0119 r\u00f3\u017cni od tej ta\u015bmy produkcyjnej, kt\u00f3ra mia\u0142a swoje, no, przestoje?", "tokens": [50368, 3066, 281, 50108, 36584, 30, 286, 31466, 3244, 19637, 3722, 3611, 12573, 1846, 10513, 33699, 42949, 11794, 11, 19456, 21290, 5024, 29489, 11, 572, 11, 6541, 18465, 2884, 30, 50664], "temperature": 0.0, "avg_logprob": -0.1040763094805289, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.02495398186147213}, {"id": 26, "seek": 16704, "start": 173.04, "end": 177.44, "text": " Co jest tutaj fascynuj\u0105ce, to kompletna zmiana perspektywy.", "tokens": [50664, 3066, 3492, 12749, 30632, 1344, 77, 13263, 384, 11, 281, 5207, 14657, 629, 17020, 8497, 868, 32659, 874, 9726, 13, 50884], "temperature": 0.0, "avg_logprob": -0.1040763094805289, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.02495398186147213}, {"id": 27, "seek": 16704, "start": 177.44, "end": 183.48, "text": " Zamiast dzieli\u0107 ca\u0142y model na du\u017ce bloki warstw i rozrzuca\u0107 si\u0119 po procesorach,", "tokens": [50884, 1176, 4526, 525, 9758, 1187, 12757, 35226, 2316, 1667, 1581, 2875, 888, 17056, 1516, 372, 86, 741, 9544, 81, 11728, 496, 2162, 3244, 714, 17565, 284, 608, 11, 51186], "temperature": 0.0, "avg_logprob": -0.1040763094805289, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.02495398186147213}, {"id": 28, "seek": 16704, "start": 183.48, "end": 192.68, "text": " autorzy postanowili zej\u015b\u0107 znacznie g\u0142\u0119biej, zdecydowali si\u0119 podzieli\u0107 same obliczenia, ale wewn\u0105trz ka\u017cdej pojedynczej warstwy typu transformer.", "tokens": [51186, 19510, 1229, 2183, 282, 305, 2312, 5277, 44536, 15397, 14875, 2766, 18117, 1274, 7392, 73, 11, 49749, 1344, 67, 305, 5103, 3244, 2497, 42280, 12757, 912, 1111, 1050, 14320, 11, 6775, 321, 895, 1611, 6903, 89, 21912, 1479, 73, 714, 40543, 2534, 9680, 73, 1516, 372, 9726, 2125, 84, 31782, 13, 51646], "temperature": 0.0, "avg_logprob": -0.1040763094805289, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.02495398186147213}, {"id": 29, "seek": 19268, "start": 192.68, "end": 198.44, "text": " Czyli zamiast dawa\u0107 ka\u017cdemu kucharzowi osobn\u0105 cz\u0119\u015b\u0107 przepisu do wykonania po kolei,", "tokens": [50364, 37099, 710, 4526, 525, 1120, 25234, 21912, 10730, 84, 350, 31084, 89, 24503, 41518, 13113, 47149, 30829, 25871, 360, 46702, 5609, 714, 18303, 72, 11, 50652], "temperature": 0.0, "avg_logprob": -0.09427116994988428, "compression_ratio": 1.498360655737705, "no_speech_prob": 0.0323815755546093}, {"id": 30, "seek": 19268, "start": 198.44, "end": 203.84, "text": " posadzili wszystkich przy jednym wielkim stole i kazali im pracowa\u0107 nad tym samym daniem jednocze\u015bnie.", "tokens": [50652, 1366, 345, 89, 2312, 34234, 6501, 5232, 12996, 20570, 25112, 16326, 741, 30623, 5103, 566, 22404, 11445, 12617, 8107, 3247, 4199, 3277, 4907, 5232, 26694, 1381, 12221, 13, 50922], "temperature": 0.0, "avg_logprob": -0.09427116994988428, "compression_ratio": 1.498360655737705, "no_speech_prob": 0.0323815755546093}, {"id": 31, "seek": 19268, "start": 203.84, "end": 212.64000000000001, "text": " To jest idealna analogia. W podej\u015bciu Pipeline jeden kucharz najpierw kroi warzywa, a potem przekazuje je drugiemu, kt\u00f3ry miesza sos.", "tokens": [50922, 1407, 3492, 7157, 629, 16660, 654, 13, 343, 7468, 73, 6199, 84, 35396, 5440, 12906, 350, 31084, 89, 11212, 45119, 86, 45909, 72, 1516, 1229, 4151, 11, 257, 36513, 29785, 43317, 1506, 4110, 4907, 84, 11, 9913, 41543, 2394, 27226, 13, 51362], "temperature": 0.0, "avg_logprob": -0.09427116994988428, "compression_ratio": 1.498360655737705, "no_speech_prob": 0.0323815755546093}, {"id": 32, "seek": 19268, "start": 212.64000000000001, "end": 214.60000000000002, "text": " Jest sekwencja, jest czekanie?", "tokens": [51362, 24918, 17215, 15615, 34056, 11, 3492, 6472, 916, 7155, 30, 51460], "temperature": 0.0, "avg_logprob": -0.09427116994988428, "compression_ratio": 1.498360655737705, "no_speech_prob": 0.0323815755546093}, {"id": 33, "seek": 19268, "start": 214.60000000000002, "end": 221.20000000000002, "text": " W\u0142a\u015bnie. A tutaj w podej\u015bciu Intralayer obaj kucharze pracuj\u0105 nad sosem w tym samym czasie.", "tokens": [51460, 343, 5024, 12221, 13, 316, 12749, 261, 7468, 73, 6199, 84, 5681, 2155, 11167, 1111, 1805, 350, 31084, 1381, 22404, 13263, 12617, 27226, 443, 261, 8107, 3247, 4199, 42667, 13, 51790], "temperature": 0.0, "avg_logprob": -0.09427116994988428, "compression_ratio": 1.498360655737705, "no_speech_prob": 0.0323815755546093}, {"id": 34, "seek": 22120, "start": 221.2, "end": 227.2, "text": " Jeden sieka czosnek, drugi miesza pomidory, a na ko\u0144cu \u0142\u0105cz\u0105 to w jednym garnku. Dzia\u0142aj\u0105 r\u00f3wnolegle.", "tokens": [50364, 508, 6876, 2804, 2330, 6472, 329, 23255, 11, 4110, 72, 41543, 2394, 12991, 327, 827, 11, 257, 1667, 26470, 12032, 220, 43558, 1611, 281, 261, 5232, 12996, 25067, 5279, 13, 39448, 8908, 11133, 11416, 895, 4812, 22631, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13960644177028111, "compression_ratio": 1.3933933933933933, "no_speech_prob": 0.04074128344655037}, {"id": 35, "seek": 22120, "start": 227.2, "end": 229.32, "text": " To eliminuje momenty bezczynno\u015bci.", "tokens": [50664, 1407, 7892, 13008, 1623, 88, 10782, 6522, 77, 16438, 13, 50770], "temperature": 0.0, "avg_logprob": -0.13960644177028111, "compression_ratio": 1.3933933933933933, "no_speech_prob": 0.04074128344655037}, {"id": 36, "seek": 22120, "start": 229.32, "end": 230.16, "text": " Dok\u0142adnie.", "tokens": [50770, 29768, 10358, 2766, 13, 50812], "temperature": 0.0, "avg_logprob": -0.13960644177028111, "compression_ratio": 1.3933933933933933, "no_speech_prob": 0.04074128344655037}, {"id": 37, "seek": 22120, "start": 230.16, "end": 232.2, "text": " Bo tak sprytnie podzieli\u0107 prac\u0119.", "tokens": [50812, 3286, 991, 637, 627, 83, 2766, 2497, 42280, 12757, 22404, 1274, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13960644177028111, "compression_ratio": 1.3933933933933933, "no_speech_prob": 0.04074128344655037}, {"id": 38, "seek": 22120, "start": 232.2, "end": 235.76, "text": " Warstwa Transformer ma takie dwa g\u0142\u00f3wne klocki obliczeniowe.", "tokens": [50914, 3630, 372, 4151, 27938, 260, 463, 15963, 35045, 18117, 3901, 716, 350, 4102, 72, 1111, 1050, 42124, 6880, 13, 51092], "temperature": 0.0, "avg_logprob": -0.13960644177028111, "compression_ratio": 1.3933933933933933, "no_speech_prob": 0.04074128344655037}, {"id": 39, "seek": 22120, "start": 235.76, "end": 241.51999999999998, "text": " Block MLP, czyli Multilayer Perceptron i Block Self-Attention.", "tokens": [51092, 17500, 21601, 47, 11, 16591, 14665, 388, 11167, 3026, 1336, 2044, 741, 17500, 16348, 12, 38151, 1251, 13, 51380], "temperature": 0.0, "avg_logprob": -0.13960644177028111, "compression_ratio": 1.3933933933933933, "no_speech_prob": 0.04074128344655037}, {"id": 40, "seek": 22120, "start": 241.51999999999998, "end": 249.16, "text": " W bloku MLP, kt\u00f3ry w uproszczeniu sk\u0142ada si\u0119 z dw\u00f3ch du\u017cych operacji mno\u017cenia macierzy, czyli GEM, zrobili co\u015b bardzo sprytnego.", "tokens": [51380, 343, 888, 13275, 21601, 47, 11, 9913, 261, 493, 2635, 89, 66, 39651, 1110, 46217, 3244, 710, 27379, 812, 339, 1581, 7735, 339, 2208, 326, 4013, 275, 1771, 48830, 7912, 811, 1229, 11, 16591, 460, 6683, 11, 44399, 2312, 19241, 9034, 637, 627, 83, 11858, 13, 51762], "temperature": 0.0, "avg_logprob": -0.13960644177028111, "compression_ratio": 1.3933933933933933, "no_speech_prob": 0.04074128344655037}, {"id": 41, "seek": 22120, "start": 249.16, "end": 249.79999999999998, "text": " To znaczy?", "tokens": [51762, 1407, 36584, 30, 51794], "temperature": 0.0, "avg_logprob": -0.13960644177028111, "compression_ratio": 1.3933933933933933, "no_speech_prob": 0.04074128344655037}, {"id": 42, "seek": 24980, "start": 249.8, "end": 253.04000000000002, "text": " Pierwsz\u0105 macie\u017c WAK podzielili kolumnowo.", "tokens": [50364, 16676, 14358, 8925, 7912, 414, 1427, 343, 5340, 2497, 42280, 2312, 17818, 16449, 19941, 13, 50526], "temperature": 0.0, "avg_logprob": -0.12784619978916498, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.049967627972364426}, {"id": 43, "seek": 24980, "start": 253.04000000000002, "end": 257.28000000000003, "text": " Ka\u017cde GPU dosta\u0142o sw\u00f3j powiedzmy pionowy pasek.", "tokens": [50526, 10988, 1427, 1479, 18407, 274, 8638, 5249, 1693, 18999, 27617, 2226, 280, 313, 10089, 47125, 74, 13, 50738], "temperature": 0.0, "avg_logprob": -0.12784619978916498, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.049967627972364426}, {"id": 44, "seek": 24980, "start": 257.28000000000003, "end": 261.16, "text": " A drug\u0105 macie\u017c WAK z kolejnej operacji podzielili wierszowo.", "tokens": [50738, 316, 4110, 1611, 7912, 414, 1427, 343, 5340, 710, 23749, 11794, 2208, 13152, 2497, 42280, 2312, 261, 4890, 89, 19941, 13, 50932], "temperature": 0.0, "avg_logprob": -0.12784619978916498, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.049967627972364426}, {"id": 45, "seek": 24980, "start": 261.16, "end": 266.36, "text": " Czekaj, to brzmi troch\u0119 kontry intuicyjnie. Dlaczego Rastaka raz inaczej? Jaki jest w tym cel?", "tokens": [50932, 383, 19878, 1805, 11, 281, 738, 89, 3057, 24926, 14373, 627, 560, 84, 2632, 73, 2766, 13, 413, 75, 39329, 497, 525, 7849, 9639, 33230, 16920, 30, 508, 7421, 3492, 261, 8107, 9277, 30, 51192], "temperature": 0.0, "avg_logprob": -0.12784619978916498, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.049967627972364426}, {"id": 46, "seek": 24980, "start": 266.36, "end": 268.0, "text": " I tu jest ca\u0142a magia.", "tokens": [51192, 286, 2604, 3492, 1335, 5024, 2258, 654, 13, 51274], "temperature": 0.0, "avg_logprob": -0.12784619978916498, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.049967627972364426}, {"id": 47, "seek": 24980, "start": 268.0, "end": 275.28000000000003, "text": " Bo wyj\u015bcie z operacji mno\u017cenia przez macie\u017c podzielon\u0105 kolumnowo staje si\u0119 idealnym wej\u015bciem dla operacji mno\u017cenia przez macie\u017c podzielon\u0105 wierszowo.", "tokens": [51274, 3286, 4628, 73, 9815, 710, 2208, 13152, 275, 1771, 48830, 14064, 7912, 414, 1427, 2497, 42280, 266, 1611, 17818, 16449, 19941, 342, 11153, 3244, 7157, 12996, 321, 73, 9815, 76, 12285, 2208, 13152, 275, 1771, 48830, 14064, 7912, 414, 1427, 2497, 42280, 266, 1611, 261, 4890, 89, 19941, 13, 51638], "temperature": 0.0, "avg_logprob": -0.12784619978916498, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.049967627972364426}, {"id": 48, "seek": 24980, "start": 275.28000000000003, "end": 275.8, "text": " Aha.", "tokens": [51638, 27448, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12784619978916498, "compression_ratio": 1.6356877323420074, "no_speech_prob": 0.049967627972364426}, {"id": 49, "seek": 27580, "start": 275.8, "end": 284.2, "text": " Dzi\u0119ki temu obliczenia mog\u0105 p\u0142yn\u0105\u0107 dalej na tym samym GPU bez potrzeby wielkiej operacji tasowania danych pomi\u0119dzy wszystkimi procesorami.", "tokens": [50364, 413, 34546, 33346, 1111, 1050, 14320, 34123, 28695, 2534, 36374, 34257, 1667, 8107, 3247, 4199, 18407, 10782, 28577, 2322, 20570, 45145, 2208, 13152, 8023, 21308, 274, 34644, 12991, 49485, 14615, 10121, 17565, 284, 4526, 13, 50784], "temperature": 0.0, "avg_logprob": -0.08010264502631294, "compression_ratio": 1.4777070063694266, "no_speech_prob": 0.01808052323758602}, {"id": 50, "seek": 27580, "start": 284.2, "end": 290.68, "text": " To w\u0142a\u015bnie ta sztuczka sprawi\u0142a, \u017ce komunikacja mi\u0119dzy GPU zosta\u0142a zredukowana do absolutnego minimum.", "tokens": [50784, 1407, 14234, 1846, 262, 2682, 1311, 89, 2330, 22734, 72, 5024, 11, 3561, 45359, 1035, 23395, 33964, 18407, 23154, 5024, 710, 265, 769, 74, 40458, 360, 18757, 11858, 7285, 13, 51108], "temperature": 0.0, "avg_logprob": -0.08010264502631294, "compression_ratio": 1.4777070063694266, "no_speech_prob": 0.01808052323758602}, {"id": 51, "seek": 27580, "start": 290.68, "end": 299.88, "text": " Czyli ten sprytny zabieg geometryczny sprawia, \u017ce wyniki z pierwszego etapu idealnie pasuj\u0105 jako dane wej\u015bciowe do drugiego bez dodatkowego chaosu.", "tokens": [51108, 37099, 2064, 637, 627, 83, 1634, 24838, 20408, 18426, 3689, 1634, 22734, 654, 11, 3561, 31936, 9850, 710, 27623, 27725, 47634, 84, 7157, 2766, 1736, 13263, 17123, 49206, 321, 73, 6199, 6880, 360, 4110, 12200, 10782, 13886, 33525, 26576, 14158, 84, 13, 51568], "temperature": 0.0, "avg_logprob": -0.08010264502631294, "compression_ratio": 1.4777070063694266, "no_speech_prob": 0.01808052323758602}, {"id": 52, "seek": 27580, "start": 299.88, "end": 300.84000000000003, "text": " Dok\u0142adnie.", "tokens": [51568, 29768, 10358, 2766, 13, 51616], "temperature": 0.0, "avg_logprob": -0.08010264502631294, "compression_ratio": 1.4777070063694266, "no_speech_prob": 0.01808052323758602}, {"id": 53, "seek": 27580, "start": 300.84000000000003, "end": 303.48, "text": " A kiedy ta komunikacja w og\u00f3le jest potrzebna?", "tokens": [51616, 316, 18777, 1846, 45359, 1035, 23395, 261, 29229, 3492, 37595, 629, 30, 51748], "temperature": 0.0, "avg_logprob": -0.08010264502631294, "compression_ratio": 1.4777070063694266, "no_speech_prob": 0.01808052323758602}, {"id": 54, "seek": 30348, "start": 303.48, "end": 309.48, "text": " W\u0142a\u015bnie. Komunikacja, a konkretnie operacja AllReduce jest potrzebna dopiero na samym ko\u0144cu.", "tokens": [50364, 343, 5024, 12221, 13, 14286, 409, 1035, 23395, 11, 257, 36500, 2766, 2208, 23395, 1057, 20544, 4176, 3492, 37595, 629, 21900, 12030, 1667, 3247, 4199, 26470, 12032, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10512198517654116, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.007354253437370062}, {"id": 55, "seek": 30348, "start": 309.48, "end": 313.24, "text": " Po tym drugim mno\u017ceniu, \u017ceby zebra\u0107 cz\u0119\u015bciowy wyniki w jedn\u0105 ca\u0142o\u015b\u0107.", "tokens": [50664, 6165, 8107, 4110, 332, 275, 1771, 24930, 5951, 11, 11316, 47060, 2162, 41314, 10089, 31936, 9850, 261, 5232, 13113, 1335, 44742, 13, 50852], "temperature": 0.0, "avg_logprob": -0.10512198517654116, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.007354253437370062}, {"id": 56, "seek": 30348, "start": 313.24, "end": 315.88, "text": " Wspomnia\u0142e\u015b o operacji AllReduce.", "tokens": [50852, 343, 4952, 38131, 8908, 68, 1788, 277, 2208, 13152, 1057, 20544, 4176, 13, 50984], "temperature": 0.0, "avg_logprob": -0.10512198517654116, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.007354253437370062}, {"id": 57, "seek": 30348, "start": 315.88, "end": 322.40000000000003, "text": " Dla tych z nas, kt\u00f3rzy nie buduj\u0105 superkomputer\u00f3w na co dzie\u0144, czy mo\u017cemy to szybko zdefiniowa\u0107, co to w\u0142a\u015bciwie jest?", "tokens": [50984, 413, 875, 15180, 710, 5382, 11, 25382, 2838, 3265, 13263, 1687, 20557, 13849, 3901, 1667, 598, 47568, 11, 6430, 26500, 281, 36456, 4093, 710, 20595, 3812, 11445, 11, 598, 281, 50108, 3492, 30, 51310], "temperature": 0.0, "avg_logprob": -0.10512198517654116, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.007354253437370062}, {"id": 58, "seek": 30348, "start": 322.40000000000003, "end": 323.44, "text": " Jasne.", "tokens": [51310, 34023, 716, 13, 51362], "temperature": 0.0, "avg_logprob": -0.10512198517654116, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.007354253437370062}, {"id": 59, "seek": 30348, "start": 323.44, "end": 327.24, "text": " W najprostszych s\u0142owach to jak spotkanie zespo\u0142u.", "tokens": [51362, 343, 11212, 1424, 555, 45021, 15116, 305, 608, 281, 4207, 4008, 5225, 414, 710, 279, 2259, 24066, 13, 51552], "temperature": 0.0, "avg_logprob": -0.10512198517654116, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.007354253437370062}, {"id": 60, "seek": 30348, "start": 327.24, "end": 331.32, "text": " Ka\u017cdy pracowa\u0142 nad swoj\u0105 cz\u0119\u015bci\u0105 i przynosi sw\u00f3j wynik.", "tokens": [51552, 10988, 1427, 3173, 22404, 30105, 12617, 49194, 41314, 1611, 741, 6501, 16751, 72, 1693, 18999, 31936, 1035, 13, 51756], "temperature": 0.0, "avg_logprob": -0.10512198517654116, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.007354253437370062}, {"id": 61, "seek": 33132, "start": 331.32, "end": 342.68, "text": " W operacji AllReduce wszyscy dziel\u0105 si\u0119 swoimi wynikami i wsp\u00f3lnie obliczaj\u0105 jeden globalny rezultat, np. sum\u0119, kt\u00f3ry potem ka\u017cdy dostaje z powrotem.", "tokens": [50364, 343, 2208, 13152, 1057, 20544, 4176, 44232, 9758, 1187, 1611, 3244, 13291, 10121, 31936, 1035, 4526, 741, 47148, 2766, 1111, 1050, 89, 11133, 12906, 4338, 1634, 48060, 723, 267, 11, 33808, 13, 2408, 1274, 11, 9913, 36513, 31615, 20568, 11153, 710, 3388, 10536, 443, 13, 50932], "temperature": 0.0, "avg_logprob": -0.10391671022922873, "compression_ratio": 1.3785488958990537, "no_speech_prob": 0.018671805039048195}, {"id": 62, "seek": 33132, "start": 342.68, "end": 351.15999999999997, "text": " Ok. To jest kluczowy, ale te\u017c bardzo kosztowny moment synchronizacji. Ca\u0142a sztuka polega na tym, \u017ceby takich spotka\u0144 by\u0142o jak najmniej.", "tokens": [50932, 3477, 13, 1407, 3492, 9671, 1311, 89, 10089, 11, 6775, 9516, 9034, 19532, 2682, 648, 88, 1623, 5451, 14613, 590, 13152, 13, 7544, 5024, 262, 2682, 13599, 13208, 3680, 1667, 8107, 11, 11316, 29607, 4008, 2330, 5248, 14811, 4207, 11212, 47658, 13, 51356], "temperature": 0.0, "avg_logprob": -0.10391671022922873, "compression_ratio": 1.3785488958990537, "no_speech_prob": 0.018671805039048195}, {"id": 63, "seek": 33132, "start": 351.15999999999997, "end": 360.88, "text": " Rozumiem. Mamy wi\u0119c za\u0142atwion\u0105 cz\u0119\u015b\u0107 MLP, ale to tylko po\u0142owa sukces\u00f3w w warstwie transformer, co z tym s\u0142ynnym mechanizmem uwagi.", "tokens": [51356, 43313, 449, 4907, 13, 376, 7804, 16677, 7949, 1221, 267, 86, 313, 1611, 47149, 21601, 47, 11, 6775, 281, 13219, 714, 1221, 5528, 46432, 887, 3901, 261, 1516, 372, 8699, 31782, 11, 598, 710, 8107, 15116, 2534, 12996, 4236, 590, 17886, 23147, 20291, 13, 51842], "temperature": 0.0, "avg_logprob": -0.10391671022922873, "compression_ratio": 1.3785488958990537, "no_speech_prob": 0.018671805039048195}, {"id": 64, "seek": 36088, "start": 360.88, "end": 368.64, "text": " Sercem tych modeli. On przecie\u017c te\u017c generuje ogromne macierze. Jak poradzili sobie tam, \u017ceby nie zepsu\u0107 tej pi\u0119knej wydajno\u015bci?", "tokens": [50364, 4210, 26422, 15180, 2316, 72, 13, 1282, 8325, 40082, 9516, 1337, 13008, 34416, 298, 716, 7912, 811, 1381, 13, 15029, 1515, 345, 89, 2312, 13652, 7677, 11, 11316, 2838, 710, 10653, 84, 2162, 12573, 48085, 11794, 25984, 1805, 16438, 30, 50752], "temperature": 0.0, "avg_logprob": -0.09992478801085886, "compression_ratio": 1.39568345323741, "no_speech_prob": 0.055375345051288605}, {"id": 65, "seek": 36088, "start": 368.64, "end": 377.12, "text": " Tam wykorzystali naturaln\u0105 struktur\u0119 multi-head attention. Mechanizm uwagi w modelach transformer nie bez powodu nazywa si\u0119 wielog\u0142owym.", "tokens": [50752, 8540, 43606, 36049, 5103, 3303, 13113, 342, 31543, 1274, 4825, 12, 1934, 3202, 13, 30175, 590, 76, 23147, 20291, 261, 2316, 608, 31782, 2838, 10782, 3388, 34873, 20151, 88, 4151, 3244, 20570, 664, 1221, 31691, 13, 51176], "temperature": 0.0, "avg_logprob": -0.09992478801085886, "compression_ratio": 1.39568345323741, "no_speech_prob": 0.055375345051288605}, {"id": 66, "seek": 36088, "start": 377.12, "end": 383.6, "text": " Sk\u0142ada si\u0119 z wielu tzw. g\u0142\u00f3w uwagi, czyli attention heads, kt\u00f3re z natury dzia\u0142aj\u0105 niezale\u017cnie od siebie.", "tokens": [51176, 7324, 46217, 3244, 710, 40437, 256, 14406, 13, 18117, 3901, 23147, 20291, 11, 16591, 3202, 8050, 11, 8864, 710, 2249, 2598, 27121, 11133, 33511, 45494, 2766, 3611, 39137, 13, 51500], "temperature": 0.0, "avg_logprob": -0.09992478801085886, "compression_ratio": 1.39568345323741, "no_speech_prob": 0.055375345051288605}, {"id": 67, "seek": 38360, "start": 383.64000000000004, "end": 392.68, "text": " No tak. Autorzy po prostu podzielili te g\u0142owy mi\u0119dzy dost\u0119pne GPU. Ka\u017cdy procesor dosta\u0142 do obliczenia swoj\u0105, sprawiedliw\u0105 cz\u0119\u015b\u0107.", "tokens": [50366, 883, 991, 13, 6049, 284, 1229, 714, 19518, 2497, 42280, 2312, 535, 18117, 10089, 33964, 48209, 716, 18407, 13, 10988, 1427, 3173, 17565, 284, 274, 8638, 1221, 360, 1111, 1050, 14320, 49194, 11, 22734, 1091, 2081, 86, 1611, 47149, 13, 50818], "temperature": 0.0, "avg_logprob": -0.10373382414540937, "compression_ratio": 1.3416370106761566, "no_speech_prob": 0.03178452327847481}, {"id": 68, "seek": 38360, "start": 392.68, "end": 398.96000000000004, "text": " Macierze Quiry, Key i Value s\u0105 dzielone, a praca odbywa si\u0119 r\u00f3wnolegle.", "tokens": [50818, 5707, 811, 1381, 2326, 12781, 11, 12759, 741, 39352, 9015, 9758, 1187, 546, 11, 257, 582, 6628, 3611, 2322, 4151, 3244, 11416, 895, 4812, 22631, 13, 51132], "temperature": 0.0, "avg_logprob": -0.10373382414540937, "compression_ratio": 1.3416370106761566, "no_speech_prob": 0.03178452327847481}, {"id": 69, "seek": 38360, "start": 398.96000000000004, "end": 401.08000000000004, "text": " I znowu minimalna komunikacja.", "tokens": [51132, 286, 710, 3785, 84, 13206, 629, 45359, 1035, 23395, 13, 51238], "temperature": 0.0, "avg_logprob": -0.10373382414540937, "compression_ratio": 1.3416370106761566, "no_speech_prob": 0.03178452327847481}, {"id": 70, "seek": 38360, "start": 401.08000000000004, "end": 408.96000000000004, "text": " Znowu, podobnie jak w bloku MLP, komunikacja jest potrzebna dopiero na samym ko\u0144cu, \u017ceby po\u0142\u0105czy\u0107 wyniki ze wszystkich g\u0142\u00f3w.", "tokens": [51238, 1176, 3785, 84, 11, 43024, 2766, 4207, 261, 888, 13275, 21601, 47, 11, 45359, 1035, 23395, 3492, 37595, 629, 21900, 12030, 1667, 3247, 4199, 26470, 12032, 11, 11316, 714, 15926, 33967, 31936, 9850, 5277, 34234, 18117, 3901, 13, 51632], "temperature": 0.0, "avg_logprob": -0.10373382414540937, "compression_ratio": 1.3416370106761566, "no_speech_prob": 0.03178452327847481}, {"id": 71, "seek": 40896, "start": 408.96, "end": 413.47999999999996, "text": " A wi\u0119c to jest ich rozwi\u0105zanie paradoksu oceanu w szklance.", "tokens": [50364, 316, 16677, 281, 3492, 1893, 9544, 22620, 7155, 13480, 453, 15091, 7810, 84, 261, 7870, 7837, 719, 13, 50590], "temperature": 0.0, "avg_logprob": -0.11472592132770462, "compression_ratio": 1.4322580645161291, "no_speech_prob": 0.0069005293771624565}, {"id": 72, "seek": 40896, "start": 413.47999999999996, "end": 422.03999999999996, "text": " Zamiast pr\u00f3bowa\u0107 wla\u0107 go ca\u0142ego, znale\u017ali spos\u00f3b, by ka\u017cdy procesor wypi\u0142 swoj\u0105, idealnie odmierzon\u0105 porcj\u0119.", "tokens": [50590, 1176, 4526, 525, 8565, 65, 11445, 261, 875, 2162, 352, 35224, 6308, 11, 15397, 1220, 10659, 2081, 22904, 11, 538, 31615, 17565, 284, 4628, 22630, 1221, 49194, 11, 7157, 2766, 3611, 76, 811, 35296, 1611, 1515, 41960, 13, 51018], "temperature": 0.0, "avg_logprob": -0.11472592132770462, "compression_ratio": 1.4322580645161291, "no_speech_prob": 0.0069005293771624565}, {"id": 73, "seek": 40896, "start": 422.03999999999996, "end": 428.15999999999997, "text": " I to w tym samym czasie. Podsumowuj\u0105c, jak rzadko te procesory musz\u0105 si\u0119 ze sob\u0105 komunikowa\u0107?", "tokens": [51018, 286, 281, 261, 8107, 3247, 4199, 42667, 13, 12646, 82, 449, 305, 44733, 11, 4207, 367, 89, 345, 4093, 535, 17565, 827, 1038, 8925, 3244, 5277, 18253, 1611, 45359, 1035, 11445, 30, 51324], "temperature": 0.0, "avg_logprob": -0.11472592132770462, "compression_ratio": 1.4322580645161291, "no_speech_prob": 0.0069005293771624565}, {"id": 74, "seek": 40896, "start": 428.15999999999997, "end": 430.76, "text": " I tu dochodzimy do sedna ich sukcesu.", "tokens": [51324, 286, 2604, 9243, 378, 89, 13189, 360, 9643, 629, 1893, 46432, 887, 84, 13, 51454], "temperature": 0.0, "avg_logprob": -0.11472592132770462, "compression_ratio": 1.4322580645161291, "no_speech_prob": 0.0069005293771624565}, {"id": 75, "seek": 40896, "start": 430.76, "end": 438.67999999999995, "text": " W ca\u0142ej pojedynczej skomplikowanej warstwie transformer potrzebne s\u0105 tylko dwie operacje all reduce w przej\u015bciu do przodu.", "tokens": [51454, 343, 47631, 73, 714, 40543, 2534, 9680, 73, 1110, 298, 564, 1035, 23066, 73, 1516, 372, 8699, 31782, 37595, 716, 9015, 13219, 274, 8699, 2208, 29293, 439, 5407, 261, 8325, 73, 6199, 84, 360, 6541, 34873, 13, 51850], "temperature": 0.0, "avg_logprob": -0.11472592132770462, "compression_ratio": 1.4322580645161291, "no_speech_prob": 0.0069005293771624565}, {"id": 76, "seek": 43868, "start": 438.68, "end": 439.72, "text": " W forward pass.", "tokens": [50364, 343, 2128, 1320, 13, 50416], "temperature": 0.0, "avg_logprob": -0.16203656272282677, "compression_ratio": 1.4860557768924303, "no_speech_prob": 0.023584634065628052}, {"id": 77, "seek": 43868, "start": 439.72, "end": 446.32, "text": " Tak, forward pass i dwie w przej\u015bciu wstecz w backward pass. To jest niewiarygodnie ma\u0142o.", "tokens": [50416, 9118, 11, 2128, 1320, 741, 274, 8699, 261, 8325, 73, 6199, 84, 261, 2941, 3689, 261, 23897, 1320, 13, 1407, 3492, 43622, 29104, 21787, 2766, 463, 5249, 13, 50746], "temperature": 0.0, "avg_logprob": -0.16203656272282677, "compression_ratio": 1.4860557768924303, "no_speech_prob": 0.023584634065628052}, {"id": 78, "seek": 43868, "start": 446.32, "end": 447.36, "text": " Niewiarygodne.", "tokens": [50746, 426, 1093, 29104, 21787, 716, 13, 50798], "temperature": 0.0, "avg_logprob": -0.16203656272282677, "compression_ratio": 1.4860557768924303, "no_speech_prob": 0.023584634065628052}, {"id": 79, "seek": 43868, "start": 447.36, "end": 454.32, "text": " Co wi\u0119cej, i to te\u017c jest kluczowe, zainplementowali to wszystko w standardowym frameworku PyTorch.", "tokens": [50798, 3066, 26004, 11, 741, 281, 9516, 3492, 9671, 1311, 89, 6880, 11, 710, 491, 43704, 305, 5103, 281, 22607, 261, 3832, 31691, 8388, 84, 9953, 51, 284, 339, 13, 51146], "temperature": 0.0, "avg_logprob": -0.16203656272282677, "compression_ratio": 1.4860557768924303, "no_speech_prob": 0.023584634065628052}, {"id": 80, "seek": 43868, "start": 454.32, "end": 457.48, "text": " Nie musieli tworzy\u0107 nowego kompilatora.", "tokens": [51146, 12016, 1038, 23099, 46288, 27150, 586, 6308, 5207, 79, 388, 1639, 64, 13, 51304], "temperature": 0.0, "avg_logprob": -0.16203656272282677, "compression_ratio": 1.4860557768924303, "no_speech_prob": 0.023584634065628052}, {"id": 81, "seek": 43868, "start": 457.48, "end": 459.68, "text": " Czyli to by\u0142o praktyczne rozwi\u0105zanie.", "tokens": [51304, 37099, 281, 14811, 3206, 74, 874, 38491, 9544, 22620, 7155, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16203656272282677, "compression_ratio": 1.4860557768924303, "no_speech_prob": 0.023584634065628052}, {"id": 82, "seek": 43868, "start": 459.68, "end": 464.52, "text": " Bardzo. Wystarczy\u0142o doda\u0107 kilka prostych operacji komunikacyjnych.", "tokens": [51414, 38559, 13, 14458, 9710, 6522, 5249, 360, 2675, 2162, 36466, 10293, 16384, 2208, 13152, 45359, 1035, 31285, 9399, 13, 51656], "temperature": 0.0, "avg_logprob": -0.16203656272282677, "compression_ratio": 1.4860557768924303, "no_speech_prob": 0.023584634065628052}, {"id": 83, "seek": 46452, "start": 464.52, "end": 471.76, "text": " Pokazuj\u0105 w artykule, \u017ce to dos\u0142ownie kilka liniki kod\u00f3w Python'a, kt\u00f3re mo\u017cna wstawi\u0107 do istniej\u0105cego modelu.", "tokens": [50364, 14958, 921, 13263, 261, 594, 874, 74, 2271, 11, 3561, 281, 4491, 1221, 648, 414, 36466, 22896, 9850, 350, 378, 3901, 15329, 6, 64, 11, 8864, 17790, 261, 22580, 12757, 360, 1418, 2766, 8555, 384, 1571, 2316, 84, 13, 50726], "temperature": 0.0, "avg_logprob": -0.10754630623794184, "compression_ratio": 1.4749262536873156, "no_speech_prob": 0.08850480616092682}, {"id": 84, "seek": 46452, "start": 471.76, "end": 478.68, "text": " Ok, teoria jest elegancka, ale \u015bwiat AI jest pe\u0142en pi\u0119knych teorii, kt\u00f3re niestety, no, nie dzia\u0142aj\u0105 na du\u017c\u0105 skal\u0119.", "tokens": [50726, 3477, 11, 535, 8172, 3492, 1118, 1275, 39342, 11, 6775, 36425, 7318, 3492, 43205, 268, 48085, 9399, 40238, 5597, 11, 8864, 3867, 377, 2210, 11, 572, 11, 2838, 27121, 11133, 1667, 21783, 1611, 16890, 1274, 13, 51072], "temperature": 0.0, "avg_logprob": -0.10754630623794184, "compression_ratio": 1.4749262536873156, "no_speech_prob": 0.08850480616092682}, {"id": 85, "seek": 46452, "start": 478.68, "end": 483.24, "text": " Mamy tu do czynienia z setkami procesor\u00f3w, czy oni faktycznie to zbudowali?", "tokens": [51072, 376, 7804, 2604, 360, 6430, 77, 18811, 710, 992, 48737, 17565, 284, 3901, 11, 6430, 36317, 33647, 45586, 281, 710, 18281, 305, 5103, 30, 51300], "temperature": 0.0, "avg_logprob": -0.10754630623794184, "compression_ratio": 1.4749262536873156, "no_speech_prob": 0.08850480616092682}, {"id": 86, "seek": 46452, "start": 483.24, "end": 489.71999999999997, "text": " I udowodnili, \u017ce ten system nie rozpada si\u0119 pod w\u0142asnym ci\u0119\u017carem. Jak du\u017cy model uda\u0142o im si\u0119 tym uci\u0105gn\u0105\u0107?", "tokens": [51300, 286, 11727, 305, 378, 77, 2312, 11, 3561, 2064, 1185, 2838, 47576, 1538, 3244, 2497, 43572, 12996, 35484, 1427, 19183, 13, 15029, 1581, 7735, 2316, 44544, 5249, 566, 3244, 8107, 344, 34381, 4568, 36374, 30, 51624], "temperature": 0.0, "avg_logprob": -0.10754630623794184, "compression_ratio": 1.4749262536873156, "no_speech_prob": 0.08850480616092682}, {"id": 87, "seek": 46452, "start": 489.71999999999997, "end": 493.0, "text": " I to jest moment, w kt\u00f3rym ten artyku\u0142 naprawd\u0119 b\u0142yszczy.", "tokens": [51624, 286, 281, 3492, 1623, 11, 261, 30120, 2064, 594, 874, 5279, 1221, 20970, 272, 1221, 20589, 6522, 13, 51788], "temperature": 0.0, "avg_logprob": -0.10754630623794184, "compression_ratio": 1.4749262536873156, "no_speech_prob": 0.08850480616092682}, {"id": 88, "seek": 49300, "start": 493.0, "end": 498.52, "text": " Przeszli od teorii do praktyki na skal\u0119, kt\u00f3ra wtedy by\u0142a, no, niewyobra\u017calna.", "tokens": [50364, 2114, 89, 10430, 2081, 3611, 40238, 5597, 360, 3206, 74, 874, 2984, 1667, 16890, 1274, 11, 19456, 26959, 23936, 11, 572, 11, 43622, 88, 24393, 1427, 304, 629, 13, 50640], "temperature": 0.0, "avg_logprob": -0.12301937739054362, "compression_ratio": 1.2334801762114538, "no_speech_prob": 0.07526509463787079}, {"id": 89, "seek": 49300, "start": 498.52, "end": 506.24, "text": " Wytrenowali model w architekturze GPT-2, kt\u00f3ry mia\u0142 8,3 miliarda parametr\u00f3w.", "tokens": [50640, 343, 4328, 1095, 305, 5103, 2316, 261, 3912, 642, 2320, 374, 1381, 26039, 51, 12, 17, 11, 9913, 27989, 1649, 11, 18, 1962, 72, 19218, 6220, 27965, 3901, 13, 51026], "temperature": 0.0, "avg_logprob": -0.12301937739054362, "compression_ratio": 1.2334801762114538, "no_speech_prob": 0.07526509463787079}, {"id": 90, "seek": 49300, "start": 506.24, "end": 507.88, "text": " 8,3 miliarda.", "tokens": [51026, 1649, 11, 18, 1962, 72, 19218, 13, 51108], "temperature": 0.0, "avg_logprob": -0.12301937739054362, "compression_ratio": 1.2334801762114538, "no_speech_prob": 0.07526509463787079}, {"id": 91, "seek": 49300, "start": 507.88, "end": 513.24, "text": " U\u017cywaj\u0105c do tego klastra 512 procesor\u00f3w graficznych NVIDIA V100.", "tokens": [51108, 624, 7735, 86, 38757, 360, 8627, 9671, 525, 424, 1025, 4762, 17565, 284, 3901, 1295, 1786, 89, 9399, 426, 3958, 6914, 691, 6879, 13, 51376], "temperature": 0.0, "avg_logprob": -0.12301937739054362, "compression_ratio": 1.2334801762114538, "no_speech_prob": 0.07526509463787079}, {"id": 92, "seek": 49300, "start": 513.24, "end": 517.32, "text": " 512 GPU. To ju\u017c ma\u0142a elektrownia.", "tokens": [51376, 1025, 4762, 18407, 13, 1407, 10678, 463, 5024, 26991, 81, 648, 654, 13, 51580], "temperature": 0.0, "avg_logprob": -0.12301937739054362, "compression_ratio": 1.2334801762114538, "no_speech_prob": 0.07526509463787079}, {"id": 93, "seek": 51732, "start": 517.36, "end": 524.0400000000001, "text": " A jak z wydajno\u015bci\u0105? Czy te wszystkie procesory faktycznie ze sob\u0105 wsp\u00f3\u0142pracowa\u0142y, czy g\u0142\u00f3wnie czeka\u0142y na siebie nawzajem?", "tokens": [50366, 316, 4207, 710, 25984, 1805, 16438, 1611, 30, 19832, 535, 31723, 17565, 827, 33647, 45586, 5277, 18253, 1611, 39069, 1424, 326, 5528, 6825, 11, 6430, 18117, 812, 14215, 6472, 36361, 6825, 1667, 39137, 18969, 89, 1805, 443, 30, 50700], "temperature": 0.0, "avg_logprob": -0.11568372738287315, "compression_ratio": 1.4462025316455696, "no_speech_prob": 0.06465868651866913}, {"id": 94, "seek": 51732, "start": 524.0400000000001, "end": 526.7600000000001, "text": " Wyniki skalowania s\u0105 imponuj\u0105ce.", "tokens": [50700, 343, 2534, 9850, 16890, 21308, 9015, 704, 266, 13263, 384, 13, 50836], "temperature": 0.0, "avg_logprob": -0.11568372738287315, "compression_ratio": 1.4462025316455696, "no_speech_prob": 0.06465868651866913}, {"id": 95, "seek": 51732, "start": 526.7600000000001, "end": 533.44, "text": " Je\u015bli spojrzymy na rysunek jeden w artykule, zobaczymy wykres, kt\u00f3ry pokazuje niemal idealnie liniow\u0105 skalowalno\u015b\u0107.", "tokens": [50836, 37086, 8243, 73, 13047, 2226, 1667, 367, 749, 409, 916, 12906, 261, 594, 874, 74, 2271, 11, 37273, 2226, 39287, 495, 11, 9913, 13010, 43317, 2838, 5579, 7157, 2766, 287, 3812, 30297, 16890, 305, 304, 23293, 13, 51170], "temperature": 0.0, "avg_logprob": -0.11568372738287315, "compression_ratio": 1.4462025316455696, "no_speech_prob": 0.06465868651866913}, {"id": 96, "seek": 51732, "start": 533.44, "end": 537.4000000000001, "text": " Osi\u0105gn\u0119li moc obliczeniow\u0105 na poziomie 15-5 lops\u00f3w.", "tokens": [51170, 422, 7691, 1611, 4568, 1274, 2081, 34962, 1111, 1050, 42124, 30297, 1667, 38503, 40120, 2119, 12, 20, 287, 3370, 3901, 13, 51368], "temperature": 0.0, "avg_logprob": -0.11568372738287315, "compression_ratio": 1.4462025316455696, "no_speech_prob": 0.06465868651866913}, {"id": 97, "seek": 51732, "start": 537.4000000000001, "end": 539.12, "text": " 15-5 lops\u00f3w?", "tokens": [51368, 2119, 12, 20, 287, 3370, 3901, 30, 51454], "temperature": 0.0, "avg_logprob": -0.11568372738287315, "compression_ratio": 1.4462025316455696, "no_speech_prob": 0.06465868651866913}, {"id": 98, "seek": 51732, "start": 539.12, "end": 547.08, "text": " Tak. Co stanowi\u0142o a\u017c 76% wydajno\u015bci w por\u00f3wnaniu do teoretycznego, idealnego liniowego wzrostu.", "tokens": [51454, 9118, 13, 3066, 27984, 24503, 5249, 48134, 24733, 4, 25984, 1805, 16438, 261, 1515, 812, 895, 25849, 360, 535, 418, 874, 3689, 11858, 11, 7157, 11858, 287, 3812, 26576, 24809, 27494, 84, 13, 51852], "temperature": 0.0, "avg_logprob": -0.11568372738287315, "compression_ratio": 1.4462025316455696, "no_speech_prob": 0.06465868651866913}, {"id": 99, "seek": 54708, "start": 547.08, "end": 557.96, "text": " Czekaj, 76% to rzeczywi\u015bcie \u015bwietny wynik in\u017cynieryjny, ale to wci\u0105\u017c oznacza, \u017ce tracimy prawie 1,4 mocy obliczeniowej na sam\u0105 komunikacj\u0119 i synchronizacj\u0119.", "tokens": [50364, 383, 19878, 1805, 11, 24733, 4, 281, 44922, 8299, 39083, 1634, 31936, 1035, 294, 1427, 2534, 811, 88, 73, 1634, 11, 6775, 281, 261, 537, 27242, 277, 22672, 326, 2394, 11, 3561, 504, 326, 13189, 3206, 8699, 502, 11, 19, 705, 1344, 1111, 1050, 42124, 21091, 1667, 3247, 1611, 45359, 1035, 29924, 741, 19331, 590, 29924, 13, 50908], "temperature": 0.0, "avg_logprob": -0.10241498091281989, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.010898759588599205}, {"id": 100, "seek": 54708, "start": 557.96, "end": 566.1600000000001, "text": " Czy to nie jest argument za tym, \u017ce takie brutalne skalowanie zawsze b\u0119dzie nieefektywne i mo\u017ce powinni\u015bmy szuka\u0107 zupe\u0142nie innych, bardziej kompaktowych architektur?", "tokens": [50908, 19832, 281, 2838, 3492, 6770, 7949, 8107, 11, 3561, 15963, 17878, 716, 16890, 22028, 30964, 10562, 2838, 5666, 916, 874, 86, 716, 741, 12034, 27310, 3722, 10513, 7870, 13599, 2162, 49922, 36286, 11, 27209, 5207, 79, 5886, 19605, 3912, 642, 2320, 374, 30, 51318], "temperature": 0.0, "avg_logprob": -0.10241498091281989, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.010898759588599205}, {"id": 101, "seek": 54708, "start": 566.1600000000001, "end": 569.08, "text": " To jest doskona\u0142e pytanie. I tak i nie.", "tokens": [51318, 1407, 3492, 4491, 74, 4037, 19827, 36610, 13, 286, 991, 741, 2838, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10241498091281989, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.010898759588599205}, {"id": 102, "seek": 54708, "start": 569.08, "end": 574.84, "text": " W \u015bwiecie oblicze\u0144 rozproszonych osi\u0105gni\u0119cie 100% efektywno\u015bci jest praktycznie niemo\u017cliwe.", "tokens": [51464, 343, 40078, 4260, 1111, 1050, 49689, 9544, 1424, 329, 44479, 339, 3003, 11404, 70, 35938, 4260, 2319, 4, 31482, 916, 874, 20944, 6199, 3492, 3206, 74, 45586, 2838, 3280, 1427, 2081, 826, 13, 51752], "temperature": 0.0, "avg_logprob": -0.10241498091281989, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.010898759588599205}, {"id": 103, "seek": 57484, "start": 574.84, "end": 584.76, "text": " Zawsze jest jaki\u015b narzut. Wynik powy\u017cej 70% na tak ogromnej skali przy tak z\u0142o\u017conych obliczeniach by\u0142 absolutnym prze\u0142omem.", "tokens": [50364, 1176, 28354, 3492, 34721, 6714, 89, 325, 13, 343, 2534, 1035, 3388, 88, 38493, 5285, 4, 1667, 991, 34416, 298, 11794, 1110, 5103, 6501, 991, 710, 5249, 1427, 2526, 339, 1111, 1050, 42124, 608, 16673, 18757, 12996, 8325, 1221, 423, 76, 13, 50860], "temperature": 0.0, "avg_logprob": -0.09510105243627576, "compression_ratio": 1.4109589041095891, "no_speech_prob": 0.028184352442622185}, {"id": 104, "seek": 57484, "start": 584.76, "end": 592.48, "text": " Pokaza\u0142, \u017ce ta droga ma sens, \u017ce skalowanie horyzontalne jest mo\u017cliwe bez wpadania w pu\u0142apk\u0119 malej\u0105cych zysk\u00f3w.", "tokens": [50860, 14958, 12257, 1221, 11, 3561, 1846, 3789, 3680, 463, 2923, 11, 3561, 16890, 22028, 276, 827, 89, 896, 304, 716, 3492, 30854, 826, 10782, 32444, 345, 5609, 261, 2362, 1221, 569, 15724, 7133, 8555, 31306, 710, 749, 23849, 13, 51246], "temperature": 0.0, "avg_logprob": -0.09510105243627576, "compression_ratio": 1.4109589041095891, "no_speech_prob": 0.028184352442622185}, {"id": 105, "seek": 57484, "start": 592.48, "end": 596.64, "text": " Czyli to by\u0142 dow\u00f3d, \u017ce warto budowa\u0107 jeszcze wi\u0119ksze klastry?", "tokens": [51246, 37099, 281, 16673, 9459, 17081, 11, 3561, 31830, 3265, 11445, 14168, 29968, 1381, 9671, 525, 627, 30, 51454], "temperature": 0.0, "avg_logprob": -0.09510105243627576, "compression_ratio": 1.4109589041095891, "no_speech_prob": 0.028184352442622185}, {"id": 106, "seek": 57484, "start": 596.64, "end": 597.52, "text": " Dok\u0142adnie tak.", "tokens": [51454, 29768, 10358, 2766, 991, 13, 51498], "temperature": 0.0, "avg_logprob": -0.09510105243627576, "compression_ratio": 1.4109589041095891, "no_speech_prob": 0.028184352442622185}, {"id": 107, "seek": 57484, "start": 597.52, "end": 603.2, "text": " Dobrze. Czyli metoda jest wydajna, pozwala u\u017cywa\u0107 setek GPU niemal bez strat.", "tokens": [51498, 29679, 13503, 13, 37099, 1131, 13449, 3492, 25984, 1805, 629, 11, 40557, 5159, 34097, 25234, 992, 916, 18407, 2838, 5579, 10782, 23674, 13, 51782], "temperature": 0.0, "avg_logprob": -0.09510105243627576, "compression_ratio": 1.4109589041095891, "no_speech_prob": 0.028184352442622185}, {"id": 108, "seek": 60320, "start": 603.2, "end": 607.76, "text": " Ale pozostaje kluczowe pytanie dla kogo\u015b, kto chce z tych modeli korzysta\u0107.", "tokens": [50364, 9366, 21281, 555, 11153, 9671, 1311, 89, 6880, 36610, 12285, 350, 23515, 1788, 11, 23780, 28928, 710, 15180, 2316, 72, 14784, 49590, 2162, 13, 50592], "temperature": 0.0, "avg_logprob": -0.1033805671491121, "compression_ratio": 1.3725490196078431, "no_speech_prob": 0.0019100871868431568}, {"id": 109, "seek": 60320, "start": 607.76, "end": 609.88, "text": " Czy one s\u0105 faktycznie lepsze?", "tokens": [50592, 19832, 472, 9015, 33647, 45586, 476, 1878, 1381, 30, 50698], "temperature": 0.0, "avg_logprob": -0.1033805671491121, "compression_ratio": 1.3725490196078431, "no_speech_prob": 0.0019100871868431568}, {"id": 110, "seek": 60320, "start": 609.88, "end": 614.9200000000001, "text": " Czy ta ca\u0142a dodatkowa moc obliczeniowa prze\u0142o\u017cy\u0142a si\u0119 na wy\u017csz\u0105 jako\u015b\u0107 i lepsze wyniki?", "tokens": [50698, 19832, 1846, 1335, 5024, 13886, 33525, 5528, 34962, 1111, 1050, 42124, 5528, 8325, 5249, 7735, 5024, 3244, 1667, 4628, 1427, 82, 8925, 17123, 7753, 741, 476, 1878, 1381, 31936, 9850, 30, 50950], "temperature": 0.0, "avg_logprob": -0.1033805671491121, "compression_ratio": 1.3725490196078431, "no_speech_prob": 0.0019100871868431568}, {"id": 111, "seek": 60320, "start": 614.9200000000001, "end": 618.8000000000001, "text": " Zdecydowanie tak. I to na kilku frontach.", "tokens": [50950, 1176, 1479, 1344, 67, 22028, 991, 13, 286, 281, 1667, 5128, 5279, 1868, 608, 13, 51144], "temperature": 0.0, "avg_logprob": -0.1033805671491121, "compression_ratio": 1.3725490196078431, "no_speech_prob": 0.0019100871868431568}, {"id": 112, "seek": 60320, "start": 618.8000000000001, "end": 625.4000000000001, "text": " Zacznijmy od tego gigantycznego modelu GPT2 z 8-3 miliardami parametr\u00f3w.", "tokens": [51144, 1176, 14875, 77, 1718, 2226, 3611, 8627, 8741, 394, 17466, 11858, 2316, 84, 26039, 51, 17, 710, 1649, 12, 18, 1962, 72, 515, 4526, 6220, 27965, 3901, 13, 51474], "temperature": 0.0, "avg_logprob": -0.1033805671491121, "compression_ratio": 1.3725490196078431, "no_speech_prob": 0.0019100871868431568}, {"id": 113, "seek": 60320, "start": 625.4000000000001, "end": 631.44, "text": " Ustanowi\u0142 on nowe rekordy, czyli State of the Art, na dw\u00f3ch kluczowych benchmarkach j\u0119zykowych.", "tokens": [51474, 624, 18758, 24503, 1221, 322, 586, 68, 33881, 765, 88, 11, 16591, 4533, 295, 264, 5735, 11, 1667, 27379, 812, 339, 9671, 1311, 89, 19605, 18927, 608, 49055, 74, 19605, 13, 51776], "temperature": 0.0, "avg_logprob": -0.1033805671491121, "compression_ratio": 1.3725490196078431, "no_speech_prob": 0.0019100871868431568}, {"id": 114, "seek": 63144, "start": 631.44, "end": 636.1600000000001, "text": " Na wiki tekst 103 osi\u0105gn\u0105\u0142 Perplexity na poziomie 10-8.", "tokens": [50364, 6056, 261, 9850, 16624, 372, 48784, 3003, 11404, 4568, 1611, 1221, 3026, 18945, 507, 1667, 38503, 40120, 1266, 12, 23, 13, 50600], "temperature": 0.0, "avg_logprob": -0.13003209095127535, "compression_ratio": 1.4813559322033898, "no_speech_prob": 0.01914776675403118}, {"id": 115, "seek": 63144, "start": 636.1600000000001, "end": 638.08, "text": " Zatrzymajmy si\u0119 na chwil\u0119.", "tokens": [50600, 1176, 267, 13047, 1696, 73, 2226, 3244, 1667, 41941, 1274, 13, 50696], "temperature": 0.0, "avg_logprob": -0.13003209095127535, "compression_ratio": 1.4813559322033898, "no_speech_prob": 0.01914776675403118}, {"id": 116, "seek": 63144, "start": 638.08, "end": 644.0400000000001, "text": " Perplexity to metryka, kt\u00f3r\u0105 cz\u0119sto widzimy w artyku\u0142ach o modelach j\u0119zykowych.", "tokens": [50696, 3026, 18945, 507, 281, 1131, 627, 2330, 11, 37415, 34369, 27486, 13189, 261, 594, 874, 5279, 1221, 608, 277, 2316, 608, 49055, 74, 19605, 13, 50994], "temperature": 0.0, "avg_logprob": -0.13003209095127535, "compression_ratio": 1.4813559322033898, "no_speech_prob": 0.01914776675403118}, {"id": 117, "seek": 63144, "start": 644.0400000000001, "end": 648.9200000000001, "text": " Niszcza warto\u015b\u0107 jest lepsza, ale co ona tak naprawd\u0119 nam m\u00f3wi o modelu?", "tokens": [50994, 426, 271, 89, 41524, 31830, 7753, 3492, 476, 1878, 2394, 11, 6775, 598, 20325, 991, 20970, 8835, 24592, 277, 2316, 84, 30, 51238], "temperature": 0.0, "avg_logprob": -0.13003209095127535, "compression_ratio": 1.4813559322033898, "no_speech_prob": 0.01914776675403118}, {"id": 118, "seek": 63144, "start": 648.9200000000001, "end": 654.4000000000001, "text": " To jest miara tego, jak bardzo model jest zdziwiony tekstem, kt\u00f3ry widzi.", "tokens": [51238, 1407, 3492, 2752, 2419, 8627, 11, 4207, 9034, 2316, 3492, 16221, 3992, 86, 46184, 16624, 1099, 11, 9913, 5274, 3992, 13, 51512], "temperature": 0.0, "avg_logprob": -0.13003209095127535, "compression_ratio": 1.4813559322033898, "no_speech_prob": 0.01914776675403118}, {"id": 119, "seek": 63144, "start": 654.4000000000001, "end": 659.4000000000001, "text": " I mi ni\u017csza Perplexity, tym model jest pewniejszy swoich przewidywa\u0144 kolejnych s\u0142\u00f3w.", "tokens": [51512, 286, 2752, 28502, 82, 2394, 3026, 18945, 507, 11, 8107, 2316, 3492, 520, 895, 7764, 7706, 13291, 480, 39758, 38836, 4151, 5248, 23749, 9399, 15116, 3901, 13, 51762], "temperature": 0.0, "avg_logprob": -0.13003209095127535, "compression_ratio": 1.4813559322033898, "no_speech_prob": 0.01914776675403118}, {"id": 120, "seek": 63144, "start": 659.4000000000001, "end": 661.0400000000001, "text": " Jakby jego niepewno\u015b\u0107.", "tokens": [51762, 15029, 2322, 26542, 2838, 494, 20944, 7753, 13, 51844], "temperature": 0.0, "avg_logprob": -0.13003209095127535, "compression_ratio": 1.4813559322033898, "no_speech_prob": 0.01914776675403118}, {"id": 121, "seek": 66104, "start": 661.04, "end": 664.48, "text": " W\u0142a\u015bnie. Mo\u017cna o tym my\u015ble\u0107 jako niepewno\u015b\u0107 modelu,", "tokens": [50364, 343, 5024, 12221, 13, 44736, 629, 277, 8107, 48633, 306, 2162, 17123, 2838, 494, 20944, 7753, 2316, 84, 11, 50536], "temperature": 0.0, "avg_logprob": -0.12448406842798969, "compression_ratio": 1.4360655737704917, "no_speech_prob": 0.02068755403161049}, {"id": 122, "seek": 66104, "start": 664.48, "end": 672.36, "text": " a skok z poprzedniego rekordu, kt\u00f3ry wynosi\u0142 15-8 do 10-8 to jest ogromna redukcja tej niepewno\u015bci.", "tokens": [50536, 257, 1110, 453, 710, 1665, 81, 11312, 2766, 1571, 33881, 28655, 11, 9913, 31936, 21521, 1221, 2119, 12, 23, 360, 1266, 12, 23, 281, 3492, 34416, 298, 629, 2783, 74, 34056, 12573, 2838, 494, 20944, 6199, 13, 50930], "temperature": 0.0, "avg_logprob": -0.12448406842798969, "compression_ratio": 1.4360655737704917, "no_speech_prob": 0.02068755403161049}, {"id": 123, "seek": 66104, "start": 672.36, "end": 675.48, "text": " Model po prostu znacznie lepiej rozumia\u0142 struktur\u0119 j\u0119zyka.", "tokens": [50930, 17105, 714, 19518, 15397, 14875, 2766, 476, 39699, 48797, 8908, 342, 31543, 1274, 42309, 40940, 13, 51086], "temperature": 0.0, "avg_logprob": -0.12448406842798969, "compression_ratio": 1.4360655737704917, "no_speech_prob": 0.02068755403161049}, {"id": 124, "seek": 66104, "start": 675.48, "end": 676.68, "text": " A drugi benchmark?", "tokens": [51086, 316, 4110, 72, 18927, 30, 51146], "temperature": 0.0, "avg_logprob": -0.12448406842798969, "compression_ratio": 1.4360655737704917, "no_speech_prob": 0.02068755403161049}, {"id": 125, "seek": 66104, "start": 676.68, "end": 680.8, "text": " To lambada, kt\u00f3ry testuje rozumienie d\u0142ugiego kontekstu,", "tokens": [51146, 1407, 10097, 1538, 11, 9913, 1500, 13008, 48797, 27385, 274, 34077, 12200, 14373, 916, 372, 84, 11, 51352], "temperature": 0.0, "avg_logprob": -0.12448406842798969, "compression_ratio": 1.4360655737704917, "no_speech_prob": 0.02068755403161049}, {"id": 126, "seek": 66104, "start": 680.8, "end": 684.56, "text": " ka\u017c\u0105c modelowi przewidzie\u0107 ostatnie s\u0142owo w d\u0142ugim akapicie.", "tokens": [51352, 21912, 1611, 66, 2316, 24503, 39758, 327, 21214, 32686, 2766, 15116, 19941, 261, 274, 34077, 332, 9308, 569, 28434, 13, 51540], "temperature": 0.0, "avg_logprob": -0.12448406842798969, "compression_ratio": 1.4360655737704917, "no_speech_prob": 0.02068755403161049}, {"id": 127, "seek": 66104, "start": 684.56, "end": 690.1999999999999, "text": " Tam osi\u0105gn\u0105\u0142 celno\u015b\u0107 66,5% r\u00f3wnie\u017c pobijaj\u0105c poprzedni rekord.", "tokens": [51540, 8540, 3003, 11404, 4568, 1611, 1221, 9277, 23293, 21126, 11, 20, 4, 20532, 714, 30418, 38757, 1665, 81, 11312, 3722, 33881, 765, 13, 51822], "temperature": 0.0, "avg_logprob": -0.12448406842798969, "compression_ratio": 1.4360655737704917, "no_speech_prob": 0.02068755403161049}, {"id": 128, "seek": 69020, "start": 690.24, "end": 694.32, "text": " Co wi\u0119cej, rysunek sze\u015b\u0107 w artykule pokazuje co\u015b fascynuj\u0105cego.", "tokens": [50366, 3066, 26004, 11, 367, 749, 409, 916, 262, 1381, 7753, 261, 594, 874, 74, 2271, 13010, 43317, 19241, 30632, 1344, 77, 13263, 384, 1571, 13, 50570], "temperature": 0.0, "avg_logprob": -0.12739518400910613, "compression_ratio": 1.41156462585034, "no_speech_prob": 0.010002342984080315}, {"id": 129, "seek": 69020, "start": 694.32, "end": 700.36, "text": " Wi\u0119ksze modele nie tylko osi\u0105gaj\u0105 lepsze wyniki ko\u0144cowe, ale te\u017c ucz\u0105 si\u0119 znacznie szybciej.", "tokens": [50570, 30127, 1694, 1381, 4391, 306, 2838, 13219, 3003, 11404, 70, 11133, 476, 1878, 1381, 31936, 9850, 26470, 66, 6880, 11, 6775, 9516, 35403, 1611, 3244, 15397, 14875, 2766, 36456, 4260, 73, 13, 50872], "temperature": 0.0, "avg_logprob": -0.12739518400910613, "compression_ratio": 1.41156462585034, "no_speech_prob": 0.010002342984080315}, {"id": 130, "seek": 69020, "start": 700.36, "end": 701.12, "text": " Aha.", "tokens": [50872, 27448, 13, 50910], "temperature": 0.0, "avg_logprob": -0.12739518400910613, "compression_ratio": 1.41156462585034, "no_speech_prob": 0.010002342984080315}, {"id": 131, "seek": 69020, "start": 701.12, "end": 706.6400000000001, "text": " Krzywa b\u0142\u0119du dla modelu 8-3B opada o wiele gwa\u0142towniej ni\u017c dla mniejszych modeli.", "tokens": [50910, 6332, 1229, 4151, 272, 46564, 769, 12285, 2316, 84, 1649, 12, 18, 33, 999, 1538, 277, 33137, 290, 44603, 30401, 7764, 28502, 12285, 39513, 45021, 2316, 72, 13, 51186], "temperature": 0.0, "avg_logprob": -0.12739518400910613, "compression_ratio": 1.41156462585034, "no_speech_prob": 0.010002342984080315}, {"id": 132, "seek": 69020, "start": 706.6400000000001, "end": 709.4000000000001, "text": " Czyli dla GPT-2 wszystko posz\u0142o g\u0142adko.", "tokens": [51186, 37099, 12285, 26039, 51, 12, 17, 22607, 1366, 89, 5249, 290, 10358, 4093, 13, 51324], "temperature": 0.0, "avg_logprob": -0.12739518400910613, "compression_ratio": 1.41156462585034, "no_speech_prob": 0.010002342984080315}, {"id": 133, "seek": 69020, "start": 709.4000000000001, "end": 711.4000000000001, "text": " Wi\u0119kszy model lepszy wyniki.", "tokens": [51324, 30127, 1694, 1229, 2316, 476, 1878, 1229, 31936, 9850, 13, 51424], "temperature": 0.0, "avg_logprob": -0.12739518400910613, "compression_ratio": 1.41156462585034, "no_speech_prob": 0.010002342984080315}, {"id": 134, "seek": 69020, "start": 711.4000000000001, "end": 716.32, "text": " Ale badali te\u017c drug\u0105, r\u00f3wnie wa\u017cn\u0105 architektur\u0119, czyli BERT, jak posz\u0142o tam.", "tokens": [51424, 9366, 1578, 5103, 9516, 4110, 1611, 11, 11416, 14215, 27777, 13113, 3912, 642, 2320, 374, 1274, 11, 16591, 363, 31479, 11, 4207, 1366, 89, 5249, 7677, 13, 51670], "temperature": 0.0, "avg_logprob": -0.12739518400910613, "compression_ratio": 1.41156462585034, "no_speech_prob": 0.010002342984080315}, {"id": 135, "seek": 71632, "start": 716.32, "end": 722.08, "text": " I tu dochodzimy do chyba najciekawszego, niemal przypadkowego odkrycia w ca\u0142ym tym artykule.", "tokens": [50364, 286, 2604, 9243, 378, 89, 13189, 360, 31532, 11212, 4260, 74, 1607, 15453, 6308, 11, 2838, 5579, 33100, 74, 26576, 3611, 43298, 2755, 261, 35224, 4199, 8107, 594, 874, 74, 2271, 13, 50652], "temperature": 0.0, "avg_logprob": -0.09213651384626116, "compression_ratio": 1.4984709480122325, "no_speech_prob": 0.009610475972294807}, {"id": 136, "seek": 71632, "start": 722.08, "end": 728.84, "text": " Wcze\u015bniejsze pr\u00f3by powi\u0119kszania modelu BERT, podejmowane przez innych badaczy, ko\u0144czy\u0142y si\u0119 katastrof\u0105.", "tokens": [50652, 343, 9680, 37511, 82, 1381, 8565, 2322, 3388, 5034, 1694, 89, 5609, 2316, 84, 363, 31479, 11, 7468, 35195, 23066, 14064, 36286, 1578, 14691, 11, 26470, 6522, 6825, 3244, 16536, 525, 340, 69, 1611, 13, 50990], "temperature": 0.0, "avg_logprob": -0.09213651384626116, "compression_ratio": 1.4984709480122325, "no_speech_prob": 0.009610475972294807}, {"id": 137, "seek": 71632, "start": 728.84, "end": 730.5200000000001, "text": " Chwila, jak to katastrof\u0105.", "tokens": [50990, 761, 86, 7371, 11, 4207, 281, 16536, 525, 340, 69, 1611, 13, 51074], "temperature": 0.0, "avg_logprob": -0.09213651384626116, "compression_ratio": 1.4984709480122325, "no_speech_prob": 0.009610475972294807}, {"id": 138, "seek": 71632, "start": 730.5200000000001, "end": 735.6400000000001, "text": " M\u00f3wisz, \u017ce wi\u0119kszy i teoretycznie pot\u0119\u017cniejszy model by\u0142 gorszy od swojego mniejszego poprzednika,", "tokens": [51074, 376, 3901, 23848, 11, 3561, 29968, 1229, 741, 535, 418, 45586, 1847, 1274, 1427, 10402, 7706, 2316, 16673, 290, 830, 1229, 3611, 13291, 39738, 39513, 15453, 6308, 1665, 81, 11312, 77, 5439, 11, 51330], "temperature": 0.0, "avg_logprob": -0.09213651384626116, "compression_ratio": 1.4984709480122325, "no_speech_prob": 0.009610475972294807}, {"id": 139, "seek": 71632, "start": 735.6400000000001, "end": 737.8000000000001, "text": " to przeczy ca\u0142ej idei skalowania.", "tokens": [51330, 281, 8325, 6522, 47631, 73, 1153, 72, 16890, 21308, 13, 51438], "temperature": 0.0, "avg_logprob": -0.09213651384626116, "compression_ratio": 1.4984709480122325, "no_speech_prob": 0.009610475972294807}, {"id": 140, "seek": 71632, "start": 737.8000000000001, "end": 738.88, "text": " Dok\u0142adnie.", "tokens": [51438, 29768, 10358, 2766, 13, 51492], "temperature": 0.0, "avg_logprob": -0.09213651384626116, "compression_ratio": 1.4984709480122325, "no_speech_prob": 0.009610475972294807}, {"id": 141, "seek": 71632, "start": 738.88, "end": 745.24, "text": " Okazywa\u0142o si\u0119, \u017ce po przekroczeniu pewnego rozmiaru model stawa\u0142 si\u0119 niestabilny w trakcie treningu.", "tokens": [51492, 3477, 33235, 4151, 5249, 3244, 11, 3561, 714, 29785, 24174, 39651, 25889, 11858, 9544, 3057, 16870, 2316, 342, 10449, 1221, 3244, 3867, 377, 5177, 1634, 261, 944, 74, 4260, 2192, 773, 84, 13, 51810], "temperature": 0.0, "avg_logprob": -0.09213651384626116, "compression_ratio": 1.4984709480122325, "no_speech_prob": 0.009610475972294807}, {"id": 142, "seek": 74524, "start": 745.28, "end": 748.92, "text": " Jego b\u0142\u0105d, zamiast male\u0107, nagle eksplodowa\u0142.", "tokens": [50366, 508, 6308, 272, 15926, 67, 11, 710, 4526, 525, 7133, 2162, 11, 297, 15088, 30724, 564, 378, 30105, 13, 50548], "temperature": 0.0, "avg_logprob": -0.1188230807064501, "compression_ratio": 1.3961661341853036, "no_speech_prob": 0.006243779323995113}, {"id": 143, "seek": 74524, "start": 748.92, "end": 749.8, "text": " Wow.", "tokens": [50548, 3153, 13, 50592], "temperature": 0.0, "avg_logprob": -0.1188230807064501, "compression_ratio": 1.3961661341853036, "no_speech_prob": 0.006243779323995113}, {"id": 144, "seek": 74524, "start": 749.8, "end": 753.76, "text": " Wyniki zamiast si\u0119 poprawia\u0107, ulega\u0142y po gorszeniu.", "tokens": [50592, 343, 2534, 9850, 710, 4526, 525, 3244, 1665, 5131, 654, 2162, 11, 344, 306, 3680, 6825, 714, 290, 830, 39651, 13, 50790], "temperature": 0.0, "avg_logprob": -0.1188230807064501, "compression_ratio": 1.3961661341853036, "no_speech_prob": 0.006243779323995113}, {"id": 145, "seek": 74524, "start": 753.76, "end": 763.5600000000001, "text": " A zesp\u00f3\u0142 Megatron LM nie tylko umo\u017cliwi\u0142 fizyczne skalowanie BERT-a, ale te\u017c przy okazji zdiagnozowa\u0142, dlaczego wcze\u015bniej si\u0119 to nie udawa\u0142o.", "tokens": [50790, 316, 710, 13361, 16181, 9986, 267, 2044, 46529, 2838, 13219, 1105, 78, 1427, 2081, 6253, 1221, 21000, 17466, 716, 16890, 22028, 363, 31479, 12, 64, 11, 6775, 9516, 6501, 3133, 921, 4013, 710, 4504, 559, 1771, 89, 30105, 11, 37873, 39329, 40785, 3244, 281, 2838, 11727, 10449, 5249, 13, 51280], "temperature": 0.0, "avg_logprob": -0.1188230807064501, "compression_ratio": 1.3961661341853036, "no_speech_prob": 0.006243779323995113}, {"id": 146, "seek": 74524, "start": 763.5600000000001, "end": 765.32, "text": " To rodzi bardzo wa\u017cne pytanie.", "tokens": [51280, 1407, 8685, 3992, 9034, 46110, 36610, 13, 51368], "temperature": 0.0, "avg_logprob": -0.1188230807064501, "compression_ratio": 1.3961661341853036, "no_speech_prob": 0.006243779323995113}, {"id": 147, "seek": 74524, "start": 765.32, "end": 767.0, "text": " Co by\u0142o przyczyn\u0105?", "tokens": [51368, 3066, 14811, 6501, 6522, 13113, 30, 51452], "temperature": 0.0, "avg_logprob": -0.1188230807064501, "compression_ratio": 1.3961661341853036, "no_speech_prob": 0.006243779323995113}, {"id": 148, "seek": 74524, "start": 767.0, "end": 772.52, "text": " Jaka\u015b fundamentalna wada w architekturze BERT, kt\u00f3ra ujawnia\u0142a si\u0119 dopiero w du\u017cej skali?", "tokens": [51452, 508, 7849, 1788, 8088, 629, 261, 1538, 261, 3912, 642, 2320, 374, 1381, 363, 31479, 11, 19456, 344, 2938, 895, 25605, 3244, 21900, 12030, 261, 1581, 38493, 1110, 5103, 30, 51728], "temperature": 0.0, "avg_logprob": -0.1188230807064501, "compression_ratio": 1.3961661341853036, "no_speech_prob": 0.006243779323995113}, {"id": 149, "seek": 74524, "start": 772.52, "end": 774.92, "text": " Odpowied\u017a jest zaskakuj\u0105ca.", "tokens": [51728, 12210, 14701, 1091, 10659, 3492, 710, 3863, 514, 13263, 496, 13, 51848], "temperature": 0.0, "avg_logprob": -0.1188230807064501, "compression_ratio": 1.3961661341853036, "no_speech_prob": 0.006243779323995113}, {"id": 150, "seek": 77492, "start": 774.92, "end": 776.56, "text": " W swojej prostocie.", "tokens": [50364, 343, 29489, 73, 10293, 905, 414, 13, 50446], "temperature": 0.0, "avg_logprob": -0.12399339327847, "compression_ratio": 1.3833865814696487, "no_speech_prob": 0.018919577822089195}, {"id": 151, "seek": 77492, "start": 776.56, "end": 782.04, "text": " Problemem nie by\u0142 sam rozmiar, a drobny wydawa\u0142oby si\u0119 detal w architekturze.", "tokens": [50446, 11676, 443, 2838, 16673, 3247, 9544, 3057, 289, 11, 257, 3789, 65, 1634, 25984, 10449, 1221, 13944, 3244, 33185, 261, 3912, 642, 2320, 374, 1381, 13, 50720], "temperature": 0.0, "avg_logprob": -0.12399339327847, "compression_ratio": 1.3833865814696487, "no_speech_prob": 0.018919577822089195}, {"id": 152, "seek": 77492, "start": 782.04, "end": 782.8399999999999, "text": " Jaki?", "tokens": [50720, 508, 7421, 30, 50760], "temperature": 0.0, "avg_logprob": -0.12399339327847, "compression_ratio": 1.3833865814696487, "no_speech_prob": 0.018919577822089195}, {"id": 153, "seek": 77492, "start": 782.8399999999999, "end": 793.4799999999999, "text": " Kolejno\u015b\u0107 operacji Layer Normalization i tak zwanych Residual Connections, czyli tych po\u0142\u0105cze\u0144, kt\u00f3re przepuszczaj\u0105 informacje z wcze\u015bniejszych etap\u00f3w.", "tokens": [50760, 591, 4812, 73, 23293, 2208, 13152, 35166, 21277, 2144, 741, 991, 11873, 34644, 5015, 327, 901, 11653, 626, 11, 16591, 15180, 714, 15926, 9680, 5248, 11, 8864, 30829, 22378, 3689, 11133, 1356, 29293, 710, 40785, 45021, 47634, 3901, 13, 51292], "temperature": 0.0, "avg_logprob": -0.12399339327847, "compression_ratio": 1.3833865814696487, "no_speech_prob": 0.018919577822089195}, {"id": 154, "seek": 77492, "start": 793.4799999999999, "end": 794.5999999999999, "text": " \u017bartujesz?", "tokens": [51292, 29804, 446, 4579, 10430, 30, 51348], "temperature": 0.0, "avg_logprob": -0.12399339327847, "compression_ratio": 1.3833865814696487, "no_speech_prob": 0.018919577822089195}, {"id": 155, "seek": 77492, "start": 794.5999999999999, "end": 802.68, "text": " Czyli najwi\u0119kszym problemem w skalowaniu BERT-a nie by\u0142a jaka\u015b fundamentalna granica mo\u017cliwo\u015bci AI, tylko z\u0142a kolejno\u015b\u0107 klock\u00f3w?", "tokens": [51348, 37099, 48636, 1694, 26681, 1154, 443, 261, 16890, 305, 25849, 363, 31479, 12, 64, 2838, 23936, 4207, 64, 1788, 8088, 629, 9370, 2262, 30854, 36476, 7318, 11, 13219, 710, 5024, 23749, 23293, 350, 4102, 3901, 30, 51752], "temperature": 0.0, "avg_logprob": -0.12399339327847, "compression_ratio": 1.3833865814696487, "no_speech_prob": 0.018919577822089195}, {"id": 156, "seek": 77492, "start": 802.68, "end": 803.8399999999999, "text": " Dok\u0142adnie tak.", "tokens": [51752, 29768, 10358, 2766, 991, 13, 51810], "temperature": 0.0, "avg_logprob": -0.12399339327847, "compression_ratio": 1.3833865814696487, "no_speech_prob": 0.018919577822089195}, {"id": 157, "seek": 80384, "start": 803.88, "end": 810.24, "text": " To brzmi jak odkrycie, na kt\u00f3re kto\u015b m\u00f3g\u0142 wpa\u015b\u0107 w przez przypadek, a kt\u00f3re zmieni\u0142o wszystko.", "tokens": [50366, 1407, 738, 89, 3057, 4207, 3611, 43298, 4260, 11, 1667, 8864, 32982, 275, 14047, 1221, 261, 4306, 7753, 261, 14064, 41780, 762, 74, 11, 257, 8864, 17020, 35462, 5249, 22607, 13, 50684], "temperature": 0.0, "avg_logprob": -0.10481103037444639, "compression_ratio": 1.3629893238434163, "no_speech_prob": 0.004650530871003866}, {"id": 158, "seek": 80384, "start": 810.24, "end": 813.32, "text": " To pokazuje, jak diabe\u0142 tkwi w szczeg\u00f3\u0142ach.", "tokens": [50684, 1407, 13010, 43317, 11, 4207, 1026, 4488, 1221, 256, 74, 6253, 261, 22090, 1146, 16181, 608, 13, 50838], "temperature": 0.0, "avg_logprob": -0.10481103037444639, "compression_ratio": 1.3629893238434163, "no_speech_prob": 0.004650530871003866}, {"id": 159, "seek": 80384, "start": 813.32, "end": 814.64, "text": " Dok\u0142adnie tak.", "tokens": [50838, 29768, 10358, 2766, 991, 13, 50904], "temperature": 0.0, "avg_logprob": -0.10481103037444639, "compression_ratio": 1.3629893238434163, "no_speech_prob": 0.004650530871003866}, {"id": 160, "seek": 80384, "start": 814.64, "end": 818.0400000000001, "text": " Na rysunku 7 wida\u0107 to jak na d\u0142oni.", "tokens": [50904, 6056, 367, 749, 49910, 1614, 261, 46898, 281, 4207, 1667, 44042, 17049, 13, 51074], "temperature": 0.0, "avg_logprob": -0.10481103037444639, "compression_ratio": 1.3629893238434163, "no_speech_prob": 0.004650530871003866}, {"id": 161, "seek": 80384, "start": 818.0400000000001, "end": 822.72, "text": " Pokazuje on schematycznie star\u0105 architektur\u0119 i now\u0105 poprawion\u0105.", "tokens": [51074, 14958, 43317, 322, 956, 8615, 17466, 2766, 3543, 1611, 3912, 642, 2320, 374, 1274, 741, 586, 1611, 1665, 5131, 313, 1611, 13, 51308], "temperature": 0.0, "avg_logprob": -0.10481103037444639, "compression_ratio": 1.3629893238434163, "no_speech_prob": 0.004650530871003866}, {"id": 162, "seek": 80384, "start": 822.72, "end": 824.48, "text": " R\u00f3\u017cnica jest subtelna.", "tokens": [51308, 497, 812, 1427, 32687, 3492, 7257, 338, 629, 13, 51396], "temperature": 0.0, "avg_logprob": -0.10481103037444639, "compression_ratio": 1.3629893238434163, "no_speech_prob": 0.004650530871003866}, {"id": 163, "seek": 80384, "start": 824.48, "end": 828.24, "text": " To tylko przesuni\u0119cie bloku Layer Norm w inne miejsce.", "tokens": [51396, 1407, 13219, 6541, 279, 409, 5034, 4260, 888, 13275, 35166, 8702, 261, 24170, 38122, 13, 51584], "temperature": 0.0, "avg_logprob": -0.10481103037444639, "compression_ratio": 1.3629893238434163, "no_speech_prob": 0.004650530871003866}, {"id": 164, "seek": 80384, "start": 828.24, "end": 830.44, "text": " Ale z ogromnymi konsekwencjami.", "tokens": [51584, 9366, 710, 34416, 298, 31813, 47020, 74, 15615, 66, 73, 4526, 13, 51694], "temperature": 0.0, "avg_logprob": -0.10481103037444639, "compression_ratio": 1.3629893238434163, "no_speech_prob": 0.004650530871003866}, {"id": 165, "seek": 83044, "start": 830.44, "end": 831.48, "text": " Ogromnymi.", "tokens": [50364, 422, 861, 298, 1634, 3057, 13, 50416], "temperature": 0.0, "avg_logprob": -0.11844395228794642, "compression_ratio": 1.482490272373541, "no_speech_prob": 0.37302055954933167}, {"id": 166, "seek": 83044, "start": 831.48, "end": 835.36, "text": " Wykres obok dowodzi, jak kluczowa jest ta zmiana.", "tokens": [50416, 14458, 74, 495, 1111, 453, 9459, 14543, 11, 4207, 9671, 1311, 89, 5528, 3492, 1846, 17020, 8497, 13, 50610], "temperature": 0.0, "avg_logprob": -0.11844395228794642, "compression_ratio": 1.482490272373541, "no_speech_prob": 0.37302055954933167}, {"id": 167, "seek": 83044, "start": 835.36, "end": 841.36, "text": " Czerwona linia pokazuje, co dzia\u0142o si\u0119 podczas treningu wi\u0119kszego modelu na starej architekturze.", "tokens": [50610, 383, 4527, 86, 4037, 22896, 654, 13010, 43317, 11, 598, 9758, 654, 5249, 3244, 2497, 30989, 2192, 773, 84, 29968, 27725, 2316, 84, 1667, 22432, 73, 3912, 642, 2320, 374, 1381, 13, 50910], "temperature": 0.0, "avg_logprob": -0.11844395228794642, "compression_ratio": 1.482490272373541, "no_speech_prob": 0.37302055954933167}, {"id": 168, "seek": 83044, "start": 841.36, "end": 846.0, "text": " Po pewnym czasie trening si\u0119 za\u0142amywa\u0142, b\u0142\u0105d gwa\u0142townier\u00f3z.", "tokens": [50910, 6165, 47160, 4199, 42667, 2192, 773, 3244, 7949, 1221, 7804, 44603, 11, 272, 15926, 67, 290, 44603, 30401, 811, 812, 89, 13, 51142], "temperature": 0.0, "avg_logprob": -0.11844395228794642, "compression_ratio": 1.482490272373541, "no_speech_prob": 0.37302055954933167}, {"id": 169, "seek": 83044, "start": 846.0, "end": 847.1600000000001, "text": " A niebieska?", "tokens": [51142, 316, 2838, 23177, 2330, 30, 51200], "temperature": 0.0, "avg_logprob": -0.11844395228794642, "compression_ratio": 1.482490272373541, "no_speech_prob": 0.37302055954933167}, {"id": 170, "seek": 83044, "start": 847.1600000000001, "end": 857.24, "text": " A niebieska linia dla tej samej wielko\u015bci modelu, ale z now\u0105 poprawion\u0105 architektur\u0105 pokazuje stabilny spadek b\u0142\u0119du przez ca\u0142y czas.", "tokens": [51200, 316, 2838, 23177, 2330, 22896, 654, 12285, 12573, 912, 73, 20570, 4093, 6199, 2316, 84, 11, 6775, 710, 586, 1611, 1665, 5131, 313, 1611, 3912, 642, 2320, 374, 1611, 13010, 43317, 11652, 1634, 637, 762, 74, 272, 46564, 769, 14064, 35226, 13190, 13, 51704], "temperature": 0.0, "avg_logprob": -0.11844395228794642, "compression_ratio": 1.482490272373541, "no_speech_prob": 0.37302055954933167}, {"id": 171, "seek": 85724, "start": 857.24, "end": 863.72, "text": " To by\u0142o fundamentalne odkrycie, kt\u00f3re pozwoli\u0142o na reszcie stabilnie trenowa\u0107 gigantyczne modele typu BERT.", "tokens": [50364, 1407, 14811, 8088, 716, 3611, 43298, 4260, 11, 8864, 40557, 9384, 5249, 1667, 725, 89, 4260, 11652, 2766, 23136, 11445, 8741, 394, 17466, 716, 4391, 306, 2125, 84, 363, 31479, 13, 50688], "temperature": 0.0, "avg_logprob": -0.11778573144840289, "compression_ratio": 1.4233128834355828, "no_speech_prob": 0.29583489894866943}, {"id": 172, "seek": 85724, "start": 863.72, "end": 865.0, "text": " Niesamowite.", "tokens": [50688, 426, 530, 335, 305, 642, 13, 50752], "temperature": 0.0, "avg_logprob": -0.11778573144840289, "compression_ratio": 1.4233128834355828, "no_speech_prob": 0.29583489894866943}, {"id": 173, "seek": 85724, "start": 865.0, "end": 874.5600000000001, "text": " Czyli przyszli rozwi\u0105za\u0107 problem in\u017cynteryjny, a przy okazji dokonali odkrycia na poziomie samej architektury AI, kt\u00f3re odblokowa\u0142o ca\u0142\u0105 ga\u0142\u0105\u017a bada\u0144.", "tokens": [50752, 37099, 44018, 2081, 9544, 18234, 35873, 1154, 294, 1427, 2534, 12733, 73, 1634, 11, 257, 6501, 3133, 921, 4013, 360, 18295, 5103, 3611, 43298, 2755, 1667, 38503, 40120, 912, 73, 3912, 642, 2320, 2598, 7318, 11, 8864, 3611, 5199, 453, 5528, 5249, 1335, 15926, 5959, 15926, 10659, 272, 1538, 5248, 13, 51230], "temperature": 0.0, "avg_logprob": -0.11778573144840289, "compression_ratio": 1.4233128834355828, "no_speech_prob": 0.29583489894866943}, {"id": 174, "seek": 85724, "start": 874.5600000000001, "end": 875.64, "text": " W\u0142a\u015bnie.", "tokens": [51230, 343, 5024, 12221, 13, 51284], "temperature": 0.0, "avg_logprob": -0.11778573144840289, "compression_ratio": 1.4233128834355828, "no_speech_prob": 0.29583489894866943}, {"id": 175, "seek": 85724, "start": 875.64, "end": 878.44, "text": " I to odkrycie natychmiast przynios\u0142o owoce.", "tokens": [51284, 286, 281, 3611, 43298, 4260, 2249, 16384, 3057, 525, 6501, 77, 2717, 5249, 277, 6120, 384, 13, 51424], "temperature": 0.0, "avg_logprob": -0.11778573144840289, "compression_ratio": 1.4233128834355828, "no_speech_prob": 0.29583489894866943}, {"id": 176, "seek": 85724, "start": 878.44, "end": 886.76, "text": " Po wprowadzeniu tej poprawki model BERT o wielko\u015bci 3,9 miliarda parametr\u00f3w r\u00f3wnie\u017c osi\u0105gn\u0105 wyniki State of the Art.", "tokens": [51424, 6165, 46733, 39651, 12573, 1665, 5131, 2984, 2316, 363, 31479, 277, 20570, 4093, 6199, 805, 11, 24, 1962, 72, 19218, 6220, 27965, 3901, 20532, 3003, 11404, 4568, 1611, 31936, 9850, 4533, 295, 264, 5735, 13, 51840], "temperature": 0.0, "avg_logprob": -0.11778573144840289, "compression_ratio": 1.4233128834355828, "no_speech_prob": 0.29583489894866943}, {"id": 177, "seek": 88676, "start": 886.76, "end": 894.96, "text": " Na przyk\u0142ad na zadaniu czytania ze zrozumieniem o nazwie RISE uzyska\u0142 99% celno\u015bci, co by\u0142o nowym rekordem.", "tokens": [50364, 6056, 23144, 1667, 42788, 25849, 6430, 83, 5609, 5277, 710, 27857, 449, 1053, 4907, 277, 20151, 8699, 497, 19413, 16851, 749, 2330, 1221, 11803, 4, 9277, 16438, 11, 598, 14811, 586, 4199, 33881, 765, 443, 13, 50774], "temperature": 0.0, "avg_logprob": -0.10866069153651295, "compression_ratio": 1.3773584905660377, "no_speech_prob": 0.00599297322332859}, {"id": 178, "seek": 88676, "start": 894.96, "end": 896.16, "text": " Okej.", "tokens": [50774, 29094, 73, 13, 50834], "temperature": 0.0, "avg_logprob": -0.10866069153651295, "compression_ratio": 1.3773584905660377, "no_speech_prob": 0.00599297322332859}, {"id": 179, "seek": 88676, "start": 896.16, "end": 908.24, "text": " Wniosek jest wi\u0119c taki, \u017ce sukces Megatron LM to nie tylko genialna in\u017cynieria system\u00f3w, ale tak\u017ce fundamentalny wgl\u0105d w to, jak budowa\u0107 stabilne, bardzo g\u0142\u0119bokie sieci Transformer.", "tokens": [50834, 343, 3722, 541, 74, 3492, 16677, 20065, 11, 3561, 46432, 887, 9986, 267, 2044, 46529, 281, 2838, 13219, 48228, 629, 294, 1427, 2534, 811, 654, 1185, 3901, 11, 6775, 23306, 8088, 1634, 261, 7191, 18962, 261, 281, 11, 4207, 3265, 11445, 11652, 716, 11, 9034, 18117, 1274, 21666, 414, 2804, 537, 27938, 260, 13, 51438], "temperature": 0.0, "avg_logprob": -0.10866069153651295, "compression_ratio": 1.3773584905660377, "no_speech_prob": 0.00599297322332859}, {"id": 180, "seek": 88676, "start": 908.24, "end": 911.48, "text": " A wi\u0119c co to wszystko oznacza podsumowuj\u0105c?", "tokens": [51438, 316, 16677, 598, 281, 22607, 277, 22672, 326, 2394, 31925, 449, 305, 44733, 30, 51600], "temperature": 0.0, "avg_logprob": -0.10866069153651295, "compression_ratio": 1.3773584905660377, "no_speech_prob": 0.00599297322332859}, {"id": 181, "seek": 88676, "start": 911.48, "end": 915.48, "text": " Megatron LM to nie jest tylko chwytliwa nazwa pot\u0119\u017cnego modelu.", "tokens": [51600, 9986, 267, 2044, 46529, 281, 2838, 3492, 13219, 26237, 4328, 2081, 4151, 20151, 4151, 1847, 1274, 1427, 11858, 2316, 84, 13, 51800], "temperature": 0.0, "avg_logprob": -0.10866069153651295, "compression_ratio": 1.3773584905660377, "no_speech_prob": 0.00599297322332859}, {"id": 182, "seek": 88676, "start": 915.48, "end": 916.68, "text": " Zdecydowanie nie.", "tokens": [51800, 1176, 1479, 1344, 67, 22028, 2838, 13, 51860], "temperature": 0.0, "avg_logprob": -0.10866069153651295, "compression_ratio": 1.3773584905660377, "no_speech_prob": 0.00599297322332859}, {"id": 183, "seek": 91668, "start": 916.68, "end": 924.5999999999999, "text": " To przede wszystkim prosta i niezwykle skuteczna technika model paralelizm, kt\u00f3ra pozwoli\u0142a prze\u0142ama\u0107 barier\u0119 pami\u0119ci GPU.", "tokens": [50364, 1407, 44786, 30481, 582, 8638, 741, 33511, 9726, 14677, 1110, 1169, 3689, 629, 1537, 5439, 2316, 26009, 338, 590, 76, 11, 19456, 40557, 9384, 5024, 8325, 1221, 2404, 2162, 2159, 811, 1274, 31088, 537, 18407, 13, 50760], "temperature": 0.0, "avg_logprob": -0.07852827585660495, "compression_ratio": 1.4487951807228916, "no_speech_prob": 0.005699590314179659}, {"id": 184, "seek": 91668, "start": 924.5999999999999, "end": 927.56, "text": " Rozwi\u0105za\u0142a ten problem oceanu w szklance.", "tokens": [50760, 43313, 18234, 2394, 5024, 2064, 1154, 7810, 84, 261, 7870, 7837, 719, 13, 50908], "temperature": 0.0, "avg_logprob": -0.07852827585660495, "compression_ratio": 1.4487951807228916, "no_speech_prob": 0.005699590314179659}, {"id": 185, "seek": 91668, "start": 927.56, "end": 933.88, "text": " Umo\u017cliwi\u0142a trenowanie modeli oskali, kt\u00f3ra wcze\u015bniej by\u0142a kompletnie nieosi\u0105galna i co r\u00f3wnie wa\u017cne.", "tokens": [50908, 3301, 78, 1427, 2081, 6253, 5024, 23136, 22028, 2316, 72, 3003, 74, 5103, 11, 19456, 40785, 23936, 5207, 14657, 2766, 2838, 329, 11404, 9800, 629, 741, 598, 11416, 14215, 46110, 13, 51224], "temperature": 0.0, "avg_logprob": -0.07852827585660495, "compression_ratio": 1.4487951807228916, "no_speech_prob": 0.005699590314179659}, {"id": 186, "seek": 91668, "start": 933.88, "end": 939.56, "text": " Przy okazji odkryto kluczowy element, kt\u00f3ry stabilizuje architektur\u0119 BERT przy du\u017cych rozmiarach.", "tokens": [51224, 39590, 3133, 921, 4013, 3611, 43298, 1353, 9671, 1311, 89, 10089, 4478, 11, 9913, 11652, 590, 13008, 3912, 642, 2320, 374, 1274, 363, 31479, 6501, 1581, 7735, 339, 9544, 3057, 289, 608, 13, 51508], "temperature": 0.0, "avg_logprob": -0.07852827585660495, "compression_ratio": 1.4487951807228916, "no_speech_prob": 0.005699590314179659}, {"id": 187, "seek": 91668, "start": 939.56, "end": 945.3599999999999, "text": " Je\u015bli po\u0142\u0105czymy to z szerszym obrazem, ta praca po prostu otworzy\u0142a drog\u0119 dla ca\u0142ej bran\u017cy.", "tokens": [51508, 37086, 714, 15926, 6522, 2226, 281, 710, 7870, 433, 26681, 22798, 24313, 11, 1846, 582, 6628, 714, 19518, 4337, 28321, 1229, 5024, 3789, 70, 1274, 12285, 47631, 73, 12029, 7735, 13, 51798], "temperature": 0.0, "avg_logprob": -0.07852827585660495, "compression_ratio": 1.4487951807228916, "no_speech_prob": 0.005699590314179659}, {"id": 188, "seek": 94536, "start": 945.36, "end": 952.0, "text": " To dzi\u0119ki niej i podobnym technikom, kt\u00f3re na nie bazowa\u0142y nied\u0142ugo p\u00f3\u017aniej powsta\u0142y jeszcze wi\u0119ksze modele.", "tokens": [50364, 1407, 45003, 2838, 73, 741, 43024, 12996, 1537, 1035, 298, 11, 8864, 1667, 2838, 27147, 5528, 6825, 32488, 1221, 20746, 36968, 3388, 9140, 6825, 14168, 29968, 1381, 4391, 306, 13, 50696], "temperature": 0.0, "avg_logprob": -0.12039344094016335, "compression_ratio": 1.4428152492668622, "no_speech_prob": 0.145516499876976}, {"id": 189, "seek": 94536, "start": 952.0, "end": 953.4, "text": " Czyli to by\u0142 ten kamie\u0144 milowy?", "tokens": [50696, 37099, 281, 16673, 2064, 9727, 414, 5248, 1962, 10089, 30, 50766], "temperature": 0.0, "avg_logprob": -0.12039344094016335, "compression_ratio": 1.4428152492668622, "no_speech_prob": 0.145516499876976}, {"id": 190, "seek": 94536, "start": 953.4, "end": 958.16, "text": " Tak. Zreszt\u0105 sam artyku\u0142 wspomina, \u017ce w wsp\u00f3\u0142pracy z Microsoftem, u\u017cywaj\u0105c Megatrona,", "tokens": [50766, 9118, 13, 1176, 495, 2682, 1611, 3247, 594, 874, 5279, 1221, 17757, 49217, 11, 3561, 261, 39069, 1424, 2551, 710, 8116, 443, 11, 34097, 86, 38757, 9986, 267, 2044, 64, 11, 51004], "temperature": 0.0, "avg_logprob": -0.12039344094016335, "compression_ratio": 1.4428152492668622, "no_speech_prob": 0.145516499876976}, {"id": 191, "seek": 94536, "start": 958.16, "end": 962.96, "text": " wytremowano model Turing NLG maj\u0105cy 17 miliard\u00f3w parametr\u00f3w.", "tokens": [51004, 261, 4328, 2579, 305, 3730, 2316, 314, 1345, 426, 43, 38, 26064, 1344, 3282, 1962, 72, 515, 3901, 6220, 27965, 3901, 13, 51244], "temperature": 0.0, "avg_logprob": -0.12039344094016335, "compression_ratio": 1.4428152492668622, "no_speech_prob": 0.145516499876976}, {"id": 192, "seek": 94536, "start": 962.96, "end": 965.2, "text": " To by\u0142 bezpo\u015bredni nast\u0119pca.", "tokens": [51244, 1407, 16673, 10782, 2259, 1788, 986, 3722, 39662, 496, 13, 51356], "temperature": 0.0, "avg_logprob": -0.12039344094016335, "compression_ratio": 1.4428152492668622, "no_speech_prob": 0.145516499876976}, {"id": 193, "seek": 94536, "start": 965.2, "end": 974.24, "text": " Megatron LM dostarci\u0142 plan i narz\u0119dzia, kt\u00f3re nap\u0119dzi\u0142y ten wy\u015bcig w skalowaniu AI, prowadz\u0105c nas prosto do modeli, z kt\u00f3rymi pracujemy dzisiaj.", "tokens": [51356, 9986, 267, 2044, 46529, 20568, 289, 537, 1221, 1393, 741, 6714, 89, 6298, 40395, 11, 8864, 9296, 6298, 3992, 6825, 2064, 4628, 1788, 66, 328, 261, 16890, 305, 25849, 7318, 11, 36590, 8925, 66, 5382, 10293, 78, 360, 2316, 72, 11, 710, 9913, 3057, 22404, 21767, 25772, 13, 51808], "temperature": 0.0, "avg_logprob": -0.12039344094016335, "compression_ratio": 1.4428152492668622, "no_speech_prob": 0.145516499876976}, {"id": 194, "seek": 97424, "start": 974.24, "end": 981.76, "text": " Czyli to jeden z tych fundamentalnych artyku\u0142\u00f3w, do kt\u00f3rego wci\u0105\u017c wracamy, nawet je\u015bli technologia posz\u0142a ju\u017c naprz\u00f3d?", "tokens": [50364, 37099, 281, 12906, 710, 15180, 8088, 9399, 594, 874, 5279, 1221, 3901, 11, 360, 46951, 261, 537, 27242, 928, 326, 7804, 11, 22696, 25630, 1537, 24103, 1366, 89, 5024, 10678, 9296, 19390, 17081, 30, 50740], "temperature": 0.0, "avg_logprob": -0.10083006367538914, "compression_ratio": 1.3937282229965158, "no_speech_prob": 0.004942165222018957}, {"id": 195, "seek": 97424, "start": 981.76, "end": 986.8, "text": " Zdecydowanie. Wyznaczy\u0142 standardy i pokaza\u0142, \u017ce to jest mo\u017cliwe.", "tokens": [50740, 1176, 1479, 1344, 67, 22028, 13, 14458, 22672, 14691, 1221, 3832, 88, 741, 13010, 12257, 1221, 11, 3561, 281, 3492, 30854, 826, 13, 50992], "temperature": 0.0, "avg_logprob": -0.10083006367538914, "compression_ratio": 1.3937282229965158, "no_speech_prob": 0.004942165222018957}, {"id": 196, "seek": 97424, "start": 986.8, "end": 993.6, "text": " Artyku\u0142 wspomina te\u017c o przysz\u0142ych kierunkach bada\u0144, takich jak dalsze skalowanie czy knowledge distillation.", "tokens": [50992, 1587, 874, 5279, 1221, 17757, 49217, 9516, 277, 44018, 47655, 38767, 3197, 608, 272, 1538, 5248, 11, 29607, 4207, 274, 1124, 1381, 16890, 22028, 6430, 3601, 42923, 399, 13, 51332], "temperature": 0.0, "avg_logprob": -0.10083006367538914, "compression_ratio": 1.3937282229965158, "no_speech_prob": 0.004942165222018957}, {"id": 197, "seek": 97424, "start": 993.6, "end": 994.84, "text": " Destylacja wiedzy?", "tokens": [51332, 16339, 5088, 23395, 46894, 1229, 30, 51394], "temperature": 0.0, "avg_logprob": -0.10083006367538914, "compression_ratio": 1.3937282229965158, "no_speech_prob": 0.004942165222018957}, {"id": 198, "seek": 97424, "start": 994.84, "end": 998.0, "text": " Tak. I to prowadzi do ciekawej my\u015bli na koniec.", "tokens": [51394, 9118, 13, 286, 281, 36590, 3992, 360, 30596, 2330, 826, 73, 452, 15350, 1667, 5897, 35733, 13, 51552], "temperature": 0.0, "avg_logprob": -0.10083006367538914, "compression_ratio": 1.3937282229965158, "no_speech_prob": 0.004942165222018957}, {"id": 199, "seek": 97424, "start": 998.0, "end": 999.4, "text": " Zaintrykowa\u0142a\u015b mnie.", "tokens": [51552, 1176, 5114, 627, 74, 5528, 5024, 1788, 17661, 13, 51622], "temperature": 0.0, "avg_logprob": -0.10083006367538914, "compression_ratio": 1.3937282229965158, "no_speech_prob": 0.004942165222018957}, {"id": 200, "seek": 99940, "start": 999.4, "end": 1009.56, "text": " Przez lata, nap\u0119dzani takimi pracami jak Megatron LM, skupiali\u015bmy si\u0119 na budowaniu coraz wi\u0119kszych, cyfrowych m\u00f3zg\u00f3w, kt\u00f3re wymagaj\u0105 setek GPU do dzia\u0142ania.", "tokens": [50364, 2114, 1381, 89, 46722, 11, 9296, 6298, 89, 3782, 991, 10121, 22404, 4526, 4207, 9986, 267, 2044, 46529, 11, 1110, 1010, 831, 72, 10513, 3244, 1667, 3265, 305, 25849, 25899, 29968, 28051, 11, 3185, 69, 1892, 16384, 32515, 89, 70, 3901, 11, 8864, 29764, 559, 11133, 992, 916, 18407, 360, 27121, 5609, 13, 50872], "temperature": 0.0, "avg_logprob": -0.09299035330076476, "compression_ratio": 1.4371069182389937, "no_speech_prob": 0.13536977767944336}, {"id": 201, "seek": 99940, "start": 1009.56, "end": 1011.92, "text": " Osi\u0105gn\u0119li\u015bmy w tym mistrzostwo.", "tokens": [50872, 422, 7691, 1611, 4568, 1274, 38452, 261, 8107, 3544, 19390, 555, 6120, 13, 50990], "temperature": 0.0, "avg_logprob": -0.09299035330076476, "compression_ratio": 1.4371069182389937, "no_speech_prob": 0.13536977767944336}, {"id": 202, "seek": 99940, "start": 1011.92, "end": 1012.68, "text": " To prawda.", "tokens": [50990, 1407, 43607, 13, 51028], "temperature": 0.0, "avg_logprob": -0.09299035330076476, "compression_ratio": 1.4371069182389937, "no_speech_prob": 0.13536977767944336}, {"id": 203, "seek": 99940, "start": 1012.68, "end": 1019.92, "text": " Ale mo\u017ce teraz, gdy ju\u017c wiemy, jak je budowa\u0107, nast\u0119pnym wielkim wyzwaniem jest co\u015b zupe\u0142nie odwrotnego.", "tokens": [51028, 9366, 12034, 16854, 11, 28405, 10678, 3355, 2226, 11, 4207, 1506, 3265, 11445, 11, 39662, 12996, 20570, 25112, 4628, 89, 7916, 4907, 3492, 19241, 49922, 3611, 7449, 310, 11858, 13, 51390], "temperature": 0.0, "avg_logprob": -0.09299035330076476, "compression_ratio": 1.4371069182389937, "no_speech_prob": 0.13536977767944336}, {"id": 204, "seek": 99940, "start": 1019.92, "end": 1027.48, "text": " Czy nie powinni\u015bmy si\u0119 teraz skupi\u0107 na tym, jak destylowa\u0107 ich ogromn\u0105 wiedz\u0119 do znacznie mniejszych, bardziej wydajnych modeli?", "tokens": [51390, 19832, 2838, 27310, 3722, 10513, 3244, 16854, 1110, 1010, 12757, 1667, 8107, 11, 4207, 2677, 5088, 11445, 1893, 34416, 298, 13113, 46894, 11052, 360, 15397, 14875, 2766, 39513, 45021, 11, 27209, 25984, 1805, 9399, 2316, 72, 30, 51768], "temperature": 0.0, "avg_logprob": -0.09299035330076476, "compression_ratio": 1.4371069182389937, "no_speech_prob": 0.13536977767944336}, {"id": 205, "seek": 102748, "start": 1027.52, "end": 1030.44, "text": " Modeli, z kt\u00f3rych ka\u017cdy m\u00f3g\u0142by korzysta\u0107 na co dzie\u0144.", "tokens": [50366, 17105, 72, 11, 710, 30382, 31615, 275, 14047, 34635, 14784, 49590, 2162, 1667, 598, 47568, 13, 50512], "temperature": 0.0, "avg_logprob": -0.154031503200531, "compression_ratio": 1.2989690721649485, "no_speech_prob": 0.015697337687015533}, {"id": 206, "seek": 102748, "start": 1030.44, "end": 1031.56, "text": " W\u0142a\u015bnie.", "tokens": [50512, 343, 5024, 12221, 13, 50568], "temperature": 0.0, "avg_logprob": -0.154031503200531, "compression_ratio": 1.2989690721649485, "no_speech_prob": 0.015697337687015533}, {"id": 207, "seek": 102748, "start": 1031.56, "end": 1045.92, "text": " Na przyk\u0142ad na swoim telefonie, bez dost\u0119pu do superkomputera, mo\u017ce era budowania katedr dobiega ko\u0144ca, a zaczyna si\u0119 era budowania z nich u\u017cytecznych, podr\u0119cznych narz\u0119dzi.", "tokens": [50568, 6056, 23144, 1667, 13291, 332, 26812, 414, 11, 10782, 48209, 84, 360, 1687, 20557, 2582, 1663, 11, 12034, 4249, 3265, 21308, 350, 770, 81, 360, 7392, 3680, 26470, 496, 11, 257, 43811, 629, 3244, 4249, 3265, 21308, 710, 25570, 34097, 975, 3689, 9399, 11, 15305, 1274, 3689, 9399, 6714, 89, 6298, 3992, 13, 51286], "temperature": 0.0, "avg_logprob": -0.154031503200531, "compression_ratio": 1.2989690721649485, "no_speech_prob": 0.015697337687015533}], "language": "pl"}