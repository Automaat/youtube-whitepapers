{"text": " Czy w \u015bwiecie sztucznej inteligencji zawsze? No wiesz, wi\u0119kszy znaczy lepszy. Od lat panowa\u0142a taka mantra, \u017ce droga do pot\u0119\u017cniejszej AI prowadzi przez coraz wi\u0119ksze modele gigantyczne zbiory danych i nie wyobra\u017caln\u0105 moc obliczeniow\u0105. A co je\u015bli istnieje inna droga? Wyobra\u017cmy sobie model, kt\u00f3ry zrzuca wyzwanie tym gigantom nie przez surow\u0105 moc, ale przez takie sprytne, celowe wychowanie. To jest, wiesz, dok\u0142adnie to pytanie, kt\u00f3re le\u017cy\u0142 serca przy\u0142omowej pracy naukowej, kt\u00f3r\u0105 dzisiaj bierzemy na warsztat. Tytu\u0142 to Multitask Prompted Training Enables Zero Shot Task Generalization. Fascynuj\u0105cy tekst autorstwa Wiktora Sanha i naprawd\u0119 imponuj\u0105cej mi\u0119dzynarodowej grupy badaczy. To jest co\u015b, co naprawd\u0119 wstrz\u0105sne\u0142o posadami my\u015blenia o rozwoju AI. Dok\u0142adnie, a naszym selem dzisiaj jest zrozumie\u0107, w jaki spos\u00f3b model, kt\u00f3ry nazwali Ty Zero, zdo\u0142a\u0142 dokona\u0107 czego\u015b, co wydaje si\u0119 niemo\u017cliwe, czyli prze\u015bcign\u0105\u0107 znacznie, znacznie wi\u0119ksze modele. Zg\u0142\u0119bimy jego, wydaje si\u0119, rewolucyjn\u0105 metod\u0119 treningu. Zobaczymy, jakie przynios\u0142a zaskakuj\u0105ce rezultaty. No i co to wszystko oznacza dla przysz\u0142o\u015bci? Spr\u00f3bujemy wydoby\u0107 z tego esencj\u0119 skupiaj\u0105c si\u0119 na kluczowej tezie tej unikalnej metodologii i co najwa\u017cniejsze praktycznych implikacjach. Tak, m\u00f3wi\u0105c kr\u00f3tko, to chyba jest historia o tym, jak Finesia mo\u017ce wygra\u0107 z brutaln\u0105 si\u0142\u0105. OK, zacznijmy od samego pocz\u0105tku, od kontekstu. Wszyscy s\u0142yszymy o pot\u0119\u017cnych modelach j\u0119zykowych, a kr\u00f3\u0142em tej d\u017cungli przez d\u0142ugi czas by\u0142, no nie ukrywajmy, GPT-3. I panowa\u0142o takie niemal dogmatyczne przekonanie, \u017ce kluczem do sukcesu jest po prostu skala. Wi\u0119cej parametr\u00f3w, wi\u0119cej danych, wi\u0119cej mocy. A ta praca proponuje zupe\u0142nie inn\u0105 filozofi\u0119. \u017beby j\u0105 zrozumie\u0107, musimy najpierw dotkn\u0105\u0107 zjawiska, kt\u00f3re nazywamy Zero Shot Generalization. To jest jeden z najbardziej fascynuj\u0105cych aspekt\u00f3w du\u017cych modeli j\u0119zykowych. Mo\u017ce najlepiej wyja\u015bni\u0107 to przez analogi\u0119. Wyobra\u017amy sobie kogo\u015b, kto nauczy\u0142 si\u0119 biegle, czyta\u0107 i pisa\u0107, studiuj\u0105c tysi\u0105ce ksi\u0105\u017cek. Nigdy nikt go formalnie nie uczy\u0142, jak streszcza\u0107 powie\u015bci, ani pisa\u0107 wiersze. Jednak po prostu dzi\u0119ki opanowaniu j\u0119zyka na tak wysokim poziomie nagle, nagle potrafi to robi\u0107. Modele AI, trenowane g\u0142\u00f3wnie do pozornie prostego zadania, czyli przewidywania nast\u0119pnego s\u0142owa, nabywaj\u0105 podobnych nieoczekiwanych umiej\u0119tno\u015bci. I dotychczasowa teoria m\u00f3wi\u0142a, \u017ce to efekt czego\u015b, co nazywamy niejawnym uczeniem wielozadaniowym, czyli Implicit Multitask Learning, prawda? Dok\u0142adnie. \u017be model, przetwarzaj\u0105c te gigantyczne ilo\u015bci tekstu z Internetu, niejako przy okazji uczy si\u0119 wzorc\u00f3w. No w\u0142a\u015bnie. Na przyk\u0142ad analizuj\u0105c miliony w\u0105tk\u00f3w na forach internetowych, uczy si\u0119 formatu pyta\u0144 i odpowiedzi. Przetwarzaj\u0105c ca\u0142\u0105 wikipedi\u0119, ch\u0142onie wzorce strzeszczania informacji. Ale to wszystko dzieje si\u0119 w spos\u00f3b niezamierzony, to produkt uboczny. Troch\u0119 jak nauka o grawitacji przez, no nie wiem, przypadkowe upuskanie rzeczy. W ko\u0144cu si\u0119 za\u0142apie o co chodzi, ale to nie jest najbardziej efektywna metoda. I tu dochodzimy do sedna, bo autorzy tej pracy zadaj\u0105 fundamentalne i w sumie bardzo proste pytanie. A co je\u015bli nie zostawimy tego przypadkowi? Co je\u015bli zamiast biernie liczy\u0107 na tak\u0105, wiesz, chaotyczn\u0105 nauk\u0119, jawnie i celowo nauczymy model wykonywania setek r\u00f3\u017cnych zada\u0144 jednocze\u015bnie? W\u0142a\u015bnie. Co je\u015bli damy mu starannie przygotowany program nauczania, zamiast kaza\u0107 mu b\u0142\u0105dzi\u0107 po bibliotece? Dok\u0142adnie. I to jest w\u0142a\u015bnie centralna hipoteza tej pracy. To, co nazywaj\u0105 multitask prompted training, czyli ten jawny trening wielozadaniowy z u\u017cyciem prompt\u00f3w, kt\u00f3ry mo\u017ce bezpo\u015brednio i, co wa\u017cne, znacznie efektywnie wywo\u0142a\u0107 pot\u0119\u017cne zdolno\u015bci zero shot generalization. Ich teza idzie nawet dalej. Twierdz\u0105, \u017ce takie podej\u015bcie pozwala tworzy\u0107 modele nie tylko mniejsze i wydajniejsze, ale te\u017c, co jest kluczowe, znacznie bardziej odporne. Odporne na co? Na r\u00f3\u017cnice w sposobie, w jakim my ludzie formu\u0142ujemy dla nich polecenia. OK, to brzmi rewolucyjnie, ale od razu nasuwa mi si\u0119 pytanie o logistyk\u0119. Jak w praktyce nauczy\u0107 jeden model tak wielu r\u00f3\u017cnych rzeczy naraz? No bo analiza sentymentu to jedno, strzeszczanie drugie, to przecie\u017c tradycyjnie wymaga\u0142o zupe\u0142nie innych format\u00f3w, innych architektur. To musia\u0142o wymaga\u0107 jakiego\u015b ujednolicenia. I kluczem do tego takim uniwersalnym j\u0119zykiem okaza\u0142y si\u0119 prompty, czyli po prostu tekstowe polecenia. Zesp\u00f3\u0142 badawczy postanowi\u0142 przekszta\u0142ci\u0107 dziesi\u0105tki r\u00f3\u017cnych zada\u0144 NLP w jeden sp\u00f3jny format. Tekst wyj\u015bciowy, kt\u00f3ry jest poleceniem i oczekiwany tekst wyj\u015bciowy, czyli odpowied\u017a. Wszystko sprowadzili do schematu tekst do tekstu. I nagle problem strzeszczania i problem t\u0142umaczenia dla modelu wygl\u0105daj\u0105 tak samo. Dostaje tekst, ma wygenerowa\u0107 inny tekst. Ale to, co jest u mnie naprawd\u0119 fascynuj\u0105ce, to nie sam pomys\u0142, ale spos\u00f3b, w jaki go zrealizowali. Bo oni nie zamkn\u0119li si\u0119 w laboratorium, prawda? Zrobili co\u015b radykalnego, stworzyli otwarte narz\u0119dzie prompt source i zaprosili globaln\u0105 spo\u0142eczno\u015b\u0107 naukow\u0105 do wsp\u00f3\u0142pracy. Efekt jest niesamowity. W projekt zaanga\u017cowa\u0142o si\u0119 36 autor\u00f3w z 24 instytucji w o\u015bmiu krajach. To jest prawdziwie oddolny, otwarty wysi\u0142ek. I ten spo\u0142eczno\u015bciowy charakter przyni\u00f3s\u0142 co\u015b bezcennego. R\u00f3\u017cnorodno\u015b\u0107. Nie wiarygodn\u0105 r\u00f3\u017cnorodno\u015b\u0107. Co\u015b, czego ma\u0142a grupa badaczy nigdy by nie osi\u0105gn\u0119\u0142a. Zebrali ogromn\u0105 kolekcj\u0119. Nazwali j\u0105 Public Pool of Prompts, w skr\u00f3cie P3. Dla ka\u017cdego zadania mieli mn\u00f3stwo wariant\u00f3w prompt\u00f3w od bardzo formalnych, po niezwykle kreatywne, potoczne, a nawet no dziwaczne. Pami\u0119tam ten \u015bwietny przyk\u0142ad z pracy, kt\u00f3ry to doskonale ilustruje. Dla zadania identyfikacji parafrazy, czyli sprawdzania czy dwa zdania znacz\u0105 to samo. Standardowy prompt brzmia\u0142by pewnie jako\u015b tak. Czy zdanie A oznacza to samo, co zdanie B. Ale w P3 znalaz\u0142 si\u0119 te\u017c prompt stylizowany na wiadomo\u015b\u0107 od administratora serwisu Quora. Co\u015b w stylu. Jestem administratorem na stronie Quora. Mam dwa posty. Czy mog\u0119 po\u0142\u0105czy\u0107 te pytania? Dok\u0142adnie. I to jest fundamentalna r\u00f3\u017cnica. No tak, bo to testuje co\u015b znacznie g\u0142\u0119bszego. Tak. Model uczy si\u0119 nie tylko rozpoznawa\u0107 parafraz\u0119, ale te\u017c rozumie\u0107 kontekst, intencje ukryt\u0105 w bardzo r\u00f3\u017cnych formach komunikacji. Togo zmusza do g\u0142\u0119bszego zrozumienia samego zadania, a nie tylko, wiesz, powierzchownego do pasowywania s\u0142\u00f3w kluczowych. Czyli ta r\u00f3\u017cnorodno\u015b\u0107 okaza\u0142a si\u0119 tym tajnym sk\u0142adnikiem? Precywujnie. A co do samego motelu, co te\u017c jest wa\u017cne, nie budowali go od zera. Wykorzystali istniej\u0105c\u0105 architektur\u0119 typu encoder-decoder, a konkretnie model T5 plus LMM o 11 miliardach parametr\u00f3w. I ten gotowy model poddali procesowi fine tuning na tej ogromnej, wielozadaniowej i niezwykle zr\u00f3\u017cnicowanej mieszance prompt\u00f3w z P3. Czyli nie budowali nowego wielkiego silnika, wzi\u0119li istniej\u0105cy bardzo dobry silnik i poddali go takiemu wszechstronnemu programowi treningowemu. To wszystko brzmi imponuj\u0105co w teorii. Ale ca\u0142a ta praca by\u0142aby na nic, gdyby nie prze\u0142o\u017cy\u0142o si\u0119 to na twarde wyniki. Jak te zero faktycznie wypad\u0142 w starciu z zadaniami, kt\u00f3rych nigdy wcze\u015bniej nie widzia\u0142. Wyniki by\u0142y m\u00f3wi\u0105c wprost rewelacyjne i dla wielu by\u0142y szokem. Model T0 testowany na zadaniach, na kt\u00f3rych nie by\u0142 trenowany, dor\u00f3wna\u0142 lub przewy\u017cszy\u0142 wydajno\u015b\u0107 znacznie wi\u0119kszego modelu GPT-3. A GPT-3 ma 175 miliard\u00f3w parametr\u00f3w. T0 okaza\u0142 si\u0119 lepszy na 9 z 11 testowanych zada\u0144. W filach, fila, zatrzymajmy si\u0119 tutaj na moment, bo to jest absolutnie kluczowe. M\u00f3wimy o modelu, kt\u00f3ry jest ile? Jaki\u015b 16 razy mniejszy. Oko\u0142o 16 razy mniejszy, tak. To jakby wyspecjalizowany, zwinny zawodnik wagi \u015bredniej, wszed\u0142 na ring i metodycznie pokona\u0142 o ci\u0119\u017ca\u0142ego olbrzymia, kt\u00f3ry polega\u0142 wy\u0142\u0105cznie na swojej masie. To kompletnie podwa\u017ca ca\u0142\u0105 filozofi\u0119 Bigger is better. Dok\u0142adnie. To by\u0142 ten moment, w kt\u00f3rym wielu badaczy zrozumia\u0142o, \u017ce istnieje inna droga. Co ciekawe, T0 szczeg\u00f3lnie dobrze poradzi\u0142 sobie z zadaniami typu Natural Language Inference, NLI. To s\u0105 zadania, kt\u00f3re sprawdzaj\u0105 zdolno\u015b\u0107 do logicznego rozumowania na podstawie tekstu. A ani on, ani GPT-3 nie byli na nich bezpo\u015brednio trenowani. Osi\u0105gn\u0105 te\u017c \u015bwietne wyniki na bardzo trudnym benchmarku Big Bench, cz\u0119sto pokonuj\u0105c modele nawet 6 razy wi\u0119ksze od siebie. To prowadzi mnie do kolejnego pytania. Ka\u017cdy, kto pracowa\u0142 z du\u017cymi modelami wie, jak potrafi\u0105 by\u0107 kapryczne. Zmienisz jedno s\u0142owo w poleceniu i odpowied\u017a jest bezu\u017cyteczna. Czy te \u015bwietne wyniki T0 nie zale\u017ca\u0142y od znalezienia jednego idealnie sformu\u0142owanego z\u0142otego pr\u0105ptu? To jest kluczowe pytanie o odporno\u015bci i autorzy dok\u0142adnie to wbadali. Analiza przynios\u0142a kilka bardzo wa\u017cnych wniosk\u00f3w. Po pierwsze, im wi\u0119cej r\u00f3\u017cnych wariant\u00f3w pr\u0105pt\u00f3w na jedno zadanie u\u017cyto w treningu, tym model by\u0142 nie tylko lepszy, ale te\u017c bardziej stabilny. Znacz\u0105co zmniejsza\u0142a si\u0119 wariancja wynik\u00f3w w zale\u017cno\u015bci od u\u017cytego polecenia. Innymi s\u0142owy r\u00f3\u017cnorodno\u015b\u0107 w edukacji przek\u0142ada si\u0119 na elastyczno\u015b\u0107 i niezawodno\u015b\u0107 w dzia\u0142aniu. Model nie uczy si\u0119 sztuczek, tylko faktycznej umiej\u0119tno\u015bci. Tak. Po drugie, zwi\u0119kszenie liczby r\u00f3\u017cnych zada\u0144 w mikse treningowym r\u00f3wnie\u017c poprawi\u0142o median\u0119 wynik\u00f3w, ale co ciekawe niekoniecznie wp\u0142yn\u0119\u0142o na stabilno\u015b\u0107. To sugeruje, \u017ce g\u0142\u0119bia, czyli wiele pr\u0105pt\u00f3w na zadanie, jest wa\u017cniejsza dla odporno\u015bci ni\u017c szeroko\u015b\u0107, czyli wiele r\u00f3\u017cnych zada\u0144. Ciekowe. I na koniec zrobili bezpo\u015brednie por\u00f3wnanie z GPT-3 na zbiorze danych RTE. Okaza\u0142o si\u0119, \u017ce GPT-3 by\u0142 niezwykle wra\u017cliwy. Osi\u0105ga\u0142 dobre wyniki praktycznie tylko na jednym konkretnym pr\u0105bcie. A T-0 by\u0142 znacznie bardziej odporny, daj\u0105c solidne, powtarzalne wyniki dla wielu r\u00f3\u017cnych sformu\u0142owa\u0144. Dobrze. To wszystko uk\u0142ada si\u0119 w tak\u0105 sp\u00f3jn\u0105 ca\u0142o\u015b\u0107. Mamy mniejszy, wydajniejszy i bardziej odporny model. Spr\u00f3bujmy teraz po\u0142\u0105czy\u0107 te kropki. Jakie s\u0105 z tego wnioski? Dlaczego ta praca jest tak wa\u017cna dla ca\u0142ej dziedziny i? Je\u015bli spojrzymy na szerszy obraz, to badanie pokazuje alternatywn\u0105, by\u0107 mo\u017ce bardziej zr\u00f3wnowa\u017con\u0105 i inteligentn\u0105 \u015bcie\u017ck\u0119 rozwoju. Zamiast bezko\u0144ca skalowa\u0107 modele, co wi\u0105\u017ce si\u0119 z astronomicznymi kosztami i ogromnym zu\u017cyciem energii, mo\u017cna osi\u0105gn\u0105\u0107 por\u00f3wnywalne, a czasem lepsze rezultaty. Kluczem jest inteligentniejszy nadzorowany training, czyli supervised training na bardzo zr\u00f3\u017cnicowanych zadaniach. To jest, wiesz, fundamentalne przej\u015bcie od paradigmatu brutalnej si\u0142y do paradigmatu finezji. Warto te\u017c wspomnie\u0107, \u017ce mniej wi\u0119cej w tym samym czasie inny zesp\u00f3\u0142 z Google opublikowa\u0142 prac\u0119 o modelu Flan. On bazowa\u0142 na bardzo podobne pomy\u015ble. Jak T0 wypada w tym por\u00f3wnaniu? Czy to by\u0142 taki wy\u015bcig, w kt\u00f3rym obie dru\u017cyny wpad\u0142y na met\u0119 r\u00f3wnocze\u015bnie? To bardzo dobre pytanie, bo pokazuje, jak wa\u017cne s\u0105 niuanse. Cho\u0107 idea by\u0142a podobna, diabe\u0142t kwi\u0142\u0142 w szczeg\u00f3\u0142ach. T0 okaza\u0142 si\u0119 wyra\u017anie lepszy od modeli Flan o por\u00f3wnywalnej wielko\u015bci. I autorzu pracy o T0 sugeruj\u0105 dwa kluczowe czynniki, kt\u00f3re mog\u0142y o tym zadecydowa\u0107. Po pierwsze, architektura i metoda pretreningu. T0 bazuje na architekturze encoder decoder i pretreningu zwany masked language modeling. To historycznie jest bardziej efektywna strategia dla zada\u0144 typu fine tuning ni\u017c standardowe modelowanie j\u0119zykowe decoder only, na kt\u00f3rym bazowa\u0142 Flan. Czyli sam fundament, na kt\u00f3rym budowano, by\u0142 troch\u0119 solidniejszy dla tego konkretnego celu. A ten drugi czynnik? Drugim i by\u0107 mo\u017ce wa\u017cniejszym czynnikiem by\u0142a w\u0142a\u015bnie ta niesamowita organiczna r\u00f3\u017cnorodno\u015b\u0107 prompt\u00f3w zebranych dzi\u0119ki prompt source. Warto to zobrazowa\u0107. Flan m\u00f3g\u0142 mie\u0107 dla danego zadania 10 szablon\u00f3w. W wi\u0119kszo\u015bci stworzone automatycznie, na przyk\u0142ad, przet\u0142umacz to zdanie na j\u0119zyk francuski. A dzi\u0119ki prompt source T0 mia\u0142o mo\u017ce z 50 wariant\u00f3w. W tym, jakby to zabrzmia\u0142o po francusku, potrzebuje angielskiej wersji tego tekstu albo wyobra\u017a sobie, \u017ce jeste\u015b t\u0142umaczem. Ta ludzka kreatywno\u015b\u0107 i r\u00f3\u017cnorodno\u015b\u0107 to jako\u015bciowa, a nie tylko ilo\u015bciowa przewaga. To doskonale pokazuje si\u0142\u0119 wsp\u00f3\u0142pracy spo\u0142eczno\u015bci i to, \u017ce ludzka pomys\u0142owo\u015b\u0107 w formu\u0142owaniu problem\u00f3w jest zasobem, kt\u00f3rego nie da si\u0119 \u0142atwo zautomatyzowa\u0107. Oczywi\u015bcie \u017cadna praca nie jest pozbawiona ogranicze\u0144. Czy autorzy wskazuj\u0105 na jakie\u015b otwarte pytania? Oczywi\u015bcie i to jest oznaka dobrej nauki. Sami wskazuj\u0105, \u017ce ich wyniki, cho\u0107 imponuj\u0105ce, rodz\u0105 nowe pytania. Na przyk\u0142ad zaobserwowali, \u017ce proste zwi\u0119kszanie liczby zada\u0144 w miksie treningowym nie zawsze zmniejsza\u0142o wahania w wynikach. Zrozumienie, dlaczego tak si\u0119 dzieje, kt\u00f3re zadania najlepiej ze sob\u0105 wsp\u00f3\u0142graj\u0105, a kt\u00f3re si\u0119 powiedzmy kwuc\u0105, to pole do dalszych, fascynuj\u0105cych bada\u0144. To troch\u0119 jak tworzenie idealnego programu nauczania. Trzeba widzie\u0107, kt\u00f3re przedmioty wzajemnie si\u0119 wzmacniaj\u0105. Podsumowuj\u0105c model T0 i ca\u0142a stoj\u0105ca za nim metodologia, to pot\u0119\u017cny dow\u00f3d na to, \u017ce jawne uczenie wielozadaniowe z u\u017cyciem r\u00f3\u017cnorodnych tworzonych przez ludzi pr\u0105pt\u00f3w jest niezwykle skuteczn\u0105 metod\u0105. Nie liczy si\u0119 tylko rozmiar, ale przede wszystkim jako\u015b\u0107 i r\u00f3\u017cnorodno\u015b\u0107 tej edukacji. To taka zmiana perspektywy z chodowania cyfrowego potwora na wychowywanie inteligentnego agenta. I to prowadzi nas do ostatniej szerszej my\u015bli. Autorzy Teziro stworzyli sw\u00f3j zbi\u00f3r pr\u0105pt\u00f3w dzi\u0119ki bezprecedensowej globalnej wsp\u00f3\u0142precy. W miar\u0119 jak sztuczna inteligencja staje si\u0119 coraz bardziej integraln\u0105 cz\u0119\u015bci\u0105 naszego \u017cycia, jak\u0105 rol\u0119 powinna odgrywa\u0107 w\u0142a\u015bnie taka \u015bwiadoma ludzka wsp\u00f3\u0142paca w uczeniu tych modeli? To jest prowokuj\u0105ce pytanie, bo zastan\u00f3wmy si\u0119 nad tym, czy zamiast pozwala\u0107 modelom uczy\u0107 si\u0119 z ca\u0142ego chaotycznego, cz\u0119sto stronniczego i nie b\u00f3jmy si\u0119 tego s\u0142owa toksycznego internetu, powinni\u015bmy i\u015b\u0107 w stron\u0119 bardziej celowego, wsp\u00f3lnego podej\u015bcia do ich treningu. Czy to mog\u0142oby prowadzi\u0107 do tworzenia AI, kt\u00f3ra jest nie tylko inteligentniejsza w technicznym sensie, ale te\u017c bezpieczniejsza, bardziej niezawodna i lepiej dostosowana do z\u0142o\u017conych ludzkich warto\u015bci? Praca nad Te Zero zdaje si\u0119 mocno sugerowa\u0107, \u017ce odpowied\u017a grzmi tak.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.56, "text": " Czy w \u015bwiecie sztucznej inteligencji zawsze?", "tokens": [50364, 19832, 261, 40078, 4260, 262, 2682, 1311, 89, 11794, 24777, 3213, 19649, 30964, 30, 50542], "temperature": 0.0, "avg_logprob": -0.13940170342973668, "compression_ratio": 1.4654545454545456, "no_speech_prob": 0.013090062886476517}, {"id": 1, "seek": 0, "start": 3.7600000000000002, "end": 6.16, "text": " No wiesz, wi\u0119kszy znaczy lepszy.", "tokens": [50552, 883, 261, 15347, 11, 29968, 1229, 36584, 476, 1878, 1229, 13, 50672], "temperature": 0.0, "avg_logprob": -0.13940170342973668, "compression_ratio": 1.4654545454545456, "no_speech_prob": 0.013090062886476517}, {"id": 2, "seek": 0, "start": 6.36, "end": 11.200000000000001, "text": " Od lat panowa\u0142a taka mantra, \u017ce droga do pot\u0119\u017cniejszej AI prowadzi przez coraz wi\u0119ksze", "tokens": [50682, 12210, 4465, 2462, 5528, 5024, 28017, 32094, 11, 3561, 3789, 3680, 360, 1847, 1274, 1427, 30295, 16920, 7318, 36590, 3992, 14064, 25899, 29968, 1381, 50924], "temperature": 0.0, "avg_logprob": -0.13940170342973668, "compression_ratio": 1.4654545454545456, "no_speech_prob": 0.013090062886476517}, {"id": 3, "seek": 0, "start": 11.4, "end": 16.4, "text": " modele gigantyczne zbiory danych i nie wyobra\u017caln\u0105 moc obliczeniow\u0105.", "tokens": [50934, 4391, 306, 8741, 394, 17466, 716, 710, 5614, 827, 274, 34644, 741, 2838, 4628, 24393, 1427, 304, 13113, 34962, 1111, 1050, 42124, 30297, 13, 51184], "temperature": 0.0, "avg_logprob": -0.13940170342973668, "compression_ratio": 1.4654545454545456, "no_speech_prob": 0.013090062886476517}, {"id": 4, "seek": 0, "start": 16.6, "end": 19.32, "text": " A co je\u015bli istnieje inna droga?", "tokens": [51194, 316, 598, 25630, 1418, 2766, 2884, 294, 629, 3789, 3680, 30, 51330], "temperature": 0.0, "avg_logprob": -0.13940170342973668, "compression_ratio": 1.4654545454545456, "no_speech_prob": 0.013090062886476517}, {"id": 5, "seek": 0, "start": 19.52, "end": 20.96, "text": " Wyobra\u017cmy sobie model, kt\u00f3ry", "tokens": [51340, 14458, 24393, 1427, 2226, 13652, 2316, 11, 9913, 51412], "temperature": 0.0, "avg_logprob": -0.13940170342973668, "compression_ratio": 1.4654545454545456, "no_speech_prob": 0.013090062886476517}, {"id": 6, "seek": 0, "start": 21.16, "end": 28.28, "text": " zrzuca wyzwanie tym gigantom nie przez surow\u0105 moc, ale przez takie sprytne, celowe wychowanie.", "tokens": [51422, 710, 81, 11728, 496, 4628, 14406, 7155, 8107, 8741, 394, 298, 2838, 14064, 1022, 30297, 34962, 11, 6775, 14064, 15963, 637, 627, 83, 716, 11, 9277, 6880, 4628, 339, 22028, 13, 51778], "temperature": 0.0, "avg_logprob": -0.13940170342973668, "compression_ratio": 1.4654545454545456, "no_speech_prob": 0.013090062886476517}, {"id": 7, "seek": 2828, "start": 28.44, "end": 30.48, "text": " To jest, wiesz, dok\u0142adnie to pytanie, kt\u00f3re", "tokens": [50372, 1407, 3492, 11, 261, 15347, 11, 45864, 2766, 281, 36610, 11, 8864, 50474], "temperature": 0.0, "avg_logprob": -0.16053012979441675, "compression_ratio": 1.4227129337539433, "no_speech_prob": 0.031232323497533798}, {"id": 8, "seek": 2828, "start": 30.68, "end": 34.84, "text": " le\u017cy\u0142 serca przy\u0142omowej pracy naukowej, kt\u00f3r\u0105 dzisiaj bierzemy na warsztat.", "tokens": [50484, 476, 7735, 1221, 816, 496, 6501, 1221, 298, 21091, 35591, 35616, 74, 21091, 11, 37415, 25772, 272, 34602, 3633, 1667, 13718, 2682, 267, 13, 50692], "temperature": 0.0, "avg_logprob": -0.16053012979441675, "compression_ratio": 1.4227129337539433, "no_speech_prob": 0.031232323497533798}, {"id": 9, "seek": 2828, "start": 35.04, "end": 40.760000000000005, "text": " Tytu\u0142 to Multitask Prompted Training Enables Zero Shot Task Generalization.", "tokens": [50702, 314, 4328, 84, 1221, 281, 14665, 270, 3863, 15833, 25383, 20620, 2193, 2965, 17182, 28845, 30428, 6996, 2144, 13, 50988], "temperature": 0.0, "avg_logprob": -0.16053012979441675, "compression_ratio": 1.4227129337539433, "no_speech_prob": 0.031232323497533798}, {"id": 10, "seek": 2828, "start": 40.96, "end": 47.6, "text": " Fascynuj\u0105cy tekst autorstwa Wiktora Sanha i naprawd\u0119 imponuj\u0105cej mi\u0119dzynarodowej grupy badaczy.", "tokens": [50998, 479, 296, 1344, 77, 13263, 1344, 16624, 372, 19510, 372, 4151, 343, 9874, 3252, 5271, 1641, 741, 20970, 704, 266, 13263, 20811, 33964, 20062, 378, 21091, 12740, 88, 1578, 14691, 13, 51330], "temperature": 0.0, "avg_logprob": -0.16053012979441675, "compression_ratio": 1.4227129337539433, "no_speech_prob": 0.031232323497533798}, {"id": 11, "seek": 2828, "start": 47.8, "end": 52.28, "text": " To jest co\u015b, co naprawd\u0119 wstrz\u0105sne\u0142o posadami my\u015blenia o rozwoju AI.", "tokens": [51340, 1407, 3492, 19241, 11, 598, 20970, 261, 9733, 8925, 82, 716, 5249, 1366, 345, 4526, 48633, 6698, 654, 277, 9544, 6120, 8954, 7318, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16053012979441675, "compression_ratio": 1.4227129337539433, "no_speech_prob": 0.031232323497533798}, {"id": 12, "seek": 2828, "start": 52.480000000000004, "end": 57.88, "text": " Dok\u0142adnie, a naszym selem dzisiaj jest zrozumie\u0107, w jaki spos\u00f3b model,", "tokens": [51574, 29768, 10358, 2766, 11, 257, 48094, 369, 10386, 25772, 3492, 710, 27857, 449, 414, 2162, 11, 261, 24492, 22904, 2316, 11, 51844], "temperature": 0.0, "avg_logprob": -0.16053012979441675, "compression_ratio": 1.4227129337539433, "no_speech_prob": 0.031232323497533798}, {"id": 13, "seek": 5788, "start": 57.92, "end": 63.040000000000006, "text": " kt\u00f3ry nazwali Ty Zero, zdo\u0142a\u0142 dokona\u0107 czego\u015b, co wydaje si\u0119 niemo\u017cliwe,", "tokens": [50366, 9913, 20151, 40054, 5569, 17182, 11, 710, 2595, 5024, 1221, 25037, 4037, 2162, 36559, 1788, 11, 598, 49165, 3244, 2838, 3280, 1427, 2081, 826, 11, 50622], "temperature": 0.0, "avg_logprob": -0.12118034362792969, "compression_ratio": 1.4459016393442623, "no_speech_prob": 0.0352243036031723}, {"id": 14, "seek": 5788, "start": 63.24, "end": 66.44, "text": " czyli prze\u015bcign\u0105\u0107 znacznie, znacznie wi\u0119ksze modele.", "tokens": [50632, 16591, 8325, 1788, 66, 788, 36374, 15397, 14875, 2766, 11, 15397, 14875, 2766, 29968, 1381, 4391, 306, 13, 50792], "temperature": 0.0, "avg_logprob": -0.12118034362792969, "compression_ratio": 1.4459016393442623, "no_speech_prob": 0.0352243036031723}, {"id": 15, "seek": 5788, "start": 66.64, "end": 70.56, "text": " Zg\u0142\u0119bimy jego, wydaje si\u0119, rewolucyjn\u0105 metod\u0119 treningu.", "tokens": [50802, 1176, 70, 46564, 65, 13189, 26542, 11, 49165, 3244, 11, 319, 48481, 1311, 88, 73, 13113, 1131, 378, 1274, 2192, 773, 84, 13, 50998], "temperature": 0.0, "avg_logprob": -0.12118034362792969, "compression_ratio": 1.4459016393442623, "no_speech_prob": 0.0352243036031723}, {"id": 16, "seek": 5788, "start": 70.76, "end": 74.0, "text": " Zobaczymy, jakie przynios\u0142a zaskakuj\u0105ce rezultaty.", "tokens": [51008, 1176, 996, 14691, 2226, 11, 22124, 6501, 77, 2717, 5024, 710, 3863, 514, 13263, 384, 48060, 723, 21398, 13, 51170], "temperature": 0.0, "avg_logprob": -0.12118034362792969, "compression_ratio": 1.4459016393442623, "no_speech_prob": 0.0352243036031723}, {"id": 17, "seek": 5788, "start": 74.2, "end": 76.36, "text": " No i co to wszystko oznacza dla przysz\u0142o\u015bci?", "tokens": [51180, 883, 741, 598, 281, 22607, 277, 22672, 326, 2394, 12285, 44018, 35059, 30, 51288], "temperature": 0.0, "avg_logprob": -0.12118034362792969, "compression_ratio": 1.4459016393442623, "no_speech_prob": 0.0352243036031723}, {"id": 18, "seek": 5788, "start": 76.56, "end": 81.52000000000001, "text": " Spr\u00f3bujemy wydoby\u0107 z tego esencj\u0119 skupiaj\u0105c si\u0119 na kluczowej tezie tej", "tokens": [51298, 7702, 14216, 21767, 25984, 13944, 2162, 710, 8627, 785, 22660, 11115, 1110, 1010, 48125, 66, 3244, 1667, 9671, 1311, 89, 21091, 535, 3283, 12573, 51546], "temperature": 0.0, "avg_logprob": -0.12118034362792969, "compression_ratio": 1.4459016393442623, "no_speech_prob": 0.0352243036031723}, {"id": 19, "seek": 5788, "start": 81.72, "end": 85.84, "text": " unikalnej metodologii i co najwa\u017cniejsze praktycznych implikacjach.", "tokens": [51556, 517, 41216, 11794, 1131, 378, 1132, 5597, 741, 598, 11212, 27111, 44258, 3206, 74, 874, 3689, 9399, 8484, 1035, 326, 45059, 13, 51762], "temperature": 0.0, "avg_logprob": -0.12118034362792969, "compression_ratio": 1.4459016393442623, "no_speech_prob": 0.0352243036031723}, {"id": 20, "seek": 8584, "start": 85.96000000000001, "end": 91.64, "text": " Tak, m\u00f3wi\u0105c kr\u00f3tko, to chyba jest historia o tym, jak Finesia mo\u017ce wygra\u0107 z brutaln\u0105 si\u0142\u0105.", "tokens": [50370, 9118, 11, 46591, 66, 42366, 83, 4093, 11, 281, 31532, 3492, 18385, 277, 8107, 11, 4207, 479, 1652, 654, 12034, 4628, 20735, 2162, 710, 17878, 13113, 1511, 15926, 13, 50654], "temperature": 0.0, "avg_logprob": -0.1433838006221887, "compression_ratio": 1.4, "no_speech_prob": 0.009697524830698967}, {"id": 21, "seek": 8584, "start": 91.84, "end": 94.64, "text": " OK, zacznijmy od samego pocz\u0105tku, od kontekstu.", "tokens": [50664, 2264, 11, 710, 14875, 77, 1718, 2226, 3611, 912, 1571, 43959, 11, 3611, 14373, 916, 372, 84, 13, 50804], "temperature": 0.0, "avg_logprob": -0.1433838006221887, "compression_ratio": 1.4, "no_speech_prob": 0.009697524830698967}, {"id": 22, "seek": 8584, "start": 94.84, "end": 99.92, "text": " Wszyscy s\u0142yszymy o pot\u0119\u017cnych modelach j\u0119zykowych, a kr\u00f3\u0142em tej d\u017cungli przez d\u0142ugi czas", "tokens": [50814, 343, 15453, 38966, 15116, 749, 1229, 2226, 277, 1847, 1274, 1427, 9399, 2316, 608, 49055, 74, 19605, 11, 257, 42366, 11126, 12573, 274, 1427, 1063, 2081, 14064, 44042, 24780, 13190, 51068], "temperature": 0.0, "avg_logprob": -0.1433838006221887, "compression_ratio": 1.4, "no_speech_prob": 0.009697524830698967}, {"id": 23, "seek": 8584, "start": 100.12, "end": 102.56, "text": " by\u0142, no nie ukrywajmy, GPT-3.", "tokens": [51078, 16673, 11, 572, 2838, 26769, 47705, 1805, 2226, 11, 26039, 51, 12, 18, 13, 51200], "temperature": 0.0, "avg_logprob": -0.1433838006221887, "compression_ratio": 1.4, "no_speech_prob": 0.009697524830698967}, {"id": 24, "seek": 8584, "start": 102.76, "end": 109.0, "text": " I panowa\u0142o takie niemal dogmatyczne przekonanie, \u017ce kluczem do sukcesu jest po prostu skala.", "tokens": [51210, 286, 2462, 5528, 5249, 15963, 2838, 5579, 3000, 15677, 17466, 716, 29785, 266, 7155, 11, 3561, 9671, 1311, 24313, 360, 46432, 887, 84, 3492, 714, 19518, 1110, 5159, 13, 51522], "temperature": 0.0, "avg_logprob": -0.1433838006221887, "compression_ratio": 1.4, "no_speech_prob": 0.009697524830698967}, {"id": 25, "seek": 8584, "start": 109.2, "end": 112.28, "text": " Wi\u0119cej parametr\u00f3w, wi\u0119cej danych, wi\u0119cej mocy.", "tokens": [51532, 30127, 20811, 6220, 27965, 3901, 11, 26004, 274, 34644, 11, 26004, 705, 1344, 13, 51686], "temperature": 0.0, "avg_logprob": -0.1433838006221887, "compression_ratio": 1.4, "no_speech_prob": 0.009697524830698967}, {"id": 26, "seek": 8584, "start": 112.48, "end": 115.48, "text": " A ta praca proponuje zupe\u0142nie inn\u0105 filozofi\u0119.", "tokens": [51696, 316, 1846, 582, 6628, 2365, 266, 13008, 49922, 7714, 1611, 1387, 15151, 2670, 5034, 13, 51846], "temperature": 0.0, "avg_logprob": -0.1433838006221887, "compression_ratio": 1.4, "no_speech_prob": 0.009697524830698967}, {"id": 27, "seek": 11548, "start": 115.68, "end": 121.2, "text": " \u017beby j\u0105 zrozumie\u0107, musimy najpierw dotkn\u0105\u0107 zjawiska, kt\u00f3re nazywamy Zero Shot", "tokens": [50374, 46864, 2322, 35692, 710, 27857, 449, 414, 2162, 11, 43449, 11212, 45119, 86, 5893, 5457, 36374, 710, 22199, 21945, 11, 8864, 20151, 27112, 7804, 17182, 28845, 50650], "temperature": 0.0, "avg_logprob": -0.1318887925483811, "compression_ratio": 1.3898305084745763, "no_speech_prob": 0.0009742982219904661}, {"id": 28, "seek": 11548, "start": 121.4, "end": 125.16, "text": " Generalization. To jest jeden z najbardziej", "tokens": [50660, 6996, 2144, 13, 1407, 3492, 12906, 710, 41857, 50848], "temperature": 0.0, "avg_logprob": -0.1318887925483811, "compression_ratio": 1.3898305084745763, "no_speech_prob": 0.0009742982219904661}, {"id": 29, "seek": 11548, "start": 125.36, "end": 129.16, "text": " fascynuj\u0105cych aspekt\u00f3w du\u017cych modeli j\u0119zykowych.", "tokens": [50858, 30632, 1344, 77, 13263, 31306, 382, 23533, 3901, 1581, 7735, 339, 2316, 72, 49055, 74, 19605, 13, 51048], "temperature": 0.0, "avg_logprob": -0.1318887925483811, "compression_ratio": 1.3898305084745763, "no_speech_prob": 0.0009742982219904661}, {"id": 30, "seek": 11548, "start": 129.36, "end": 132.32, "text": " Mo\u017ce najlepiej wyja\u015bni\u0107 to przez analogi\u0119.", "tokens": [51058, 43774, 41903, 39699, 4628, 2938, 1788, 3722, 2162, 281, 14064, 16660, 5034, 13, 51206], "temperature": 0.0, "avg_logprob": -0.1318887925483811, "compression_ratio": 1.3898305084745763, "no_speech_prob": 0.0009742982219904661}, {"id": 31, "seek": 11548, "start": 132.52, "end": 138.2, "text": " Wyobra\u017amy sobie kogo\u015b, kto nauczy\u0142 si\u0119 biegle, czyta\u0107 i pisa\u0107, studiuj\u0105c tysi\u0105ce ksi\u0105\u017cek.", "tokens": [51216, 14458, 24393, 10659, 2226, 13652, 350, 23515, 1788, 11, 23780, 49103, 1229, 1221, 3244, 272, 20408, 306, 11, 6430, 42931, 741, 280, 3837, 2162, 11, 972, 72, 44733, 38156, 11404, 384, 39311, 916, 13, 51500], "temperature": 0.0, "avg_logprob": -0.1318887925483811, "compression_ratio": 1.3898305084745763, "no_speech_prob": 0.0009742982219904661}, {"id": 32, "seek": 11548, "start": 138.4, "end": 143.12, "text": " Nigdy nikt go formalnie nie uczy\u0142, jak streszcza\u0107 powie\u015bci, ani pisa\u0107 wiersze.", "tokens": [51510, 39554, 3173, 297, 9874, 352, 9860, 2766, 2838, 344, 6522, 1221, 11, 4207, 342, 495, 89, 66, 35873, 3388, 414, 6199, 11, 40477, 280, 3837, 2162, 261, 4890, 1381, 13, 51746], "temperature": 0.0, "avg_logprob": -0.1318887925483811, "compression_ratio": 1.3898305084745763, "no_speech_prob": 0.0009742982219904661}, {"id": 33, "seek": 14312, "start": 143.32, "end": 150.12, "text": " Jednak po prostu dzi\u0119ki opanowaniu j\u0119zyka na tak wysokim poziomie nagle, nagle potrafi to robi\u0107.", "tokens": [50374, 27076, 16852, 714, 19518, 45003, 999, 282, 305, 25849, 42309, 40940, 1667, 991, 27062, 453, 332, 38503, 40120, 297, 15088, 11, 297, 15088, 1847, 10437, 72, 281, 46900, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1514814349188321, "compression_ratio": 1.4385964912280702, "no_speech_prob": 0.0012288183206692338}, {"id": 34, "seek": 14312, "start": 150.32, "end": 157.08, "text": " Modele AI, trenowane g\u0142\u00f3wnie do pozornie prostego zadania, czyli przewidywania nast\u0119pnego s\u0142owa,", "tokens": [50724, 20500, 306, 7318, 11, 23136, 23066, 18117, 812, 14215, 360, 21281, 1865, 414, 10293, 6308, 42788, 5609, 11, 16591, 39758, 327, 27112, 5609, 39662, 11858, 15116, 5528, 11, 51062], "temperature": 0.0, "avg_logprob": -0.1514814349188321, "compression_ratio": 1.4385964912280702, "no_speech_prob": 0.0012288183206692338}, {"id": 35, "seek": 14312, "start": 157.28, "end": 160.44, "text": " nabywaj\u0105 podobnych nieoczekiwanych umiej\u0119tno\u015bci.", "tokens": [51072, 297, 2509, 86, 11133, 43024, 9399, 2838, 905, 89, 14753, 86, 34644, 1105, 7764, 46788, 16438, 13, 51230], "temperature": 0.0, "avg_logprob": -0.1514814349188321, "compression_ratio": 1.4385964912280702, "no_speech_prob": 0.0012288183206692338}, {"id": 36, "seek": 14312, "start": 160.64000000000001, "end": 165.32, "text": " I dotychczasowa teoria m\u00f3wi\u0142a, \u017ce to efekt czego\u015b, co nazywamy niejawnym uczeniem", "tokens": [51240, 286, 5893, 16384, 30989, 5528, 535, 8172, 24592, 5024, 11, 3561, 281, 31482, 8192, 36559, 1788, 11, 598, 20151, 27112, 7804, 2838, 2938, 895, 4199, 344, 66, 2904, 4907, 51474], "temperature": 0.0, "avg_logprob": -0.1514814349188321, "compression_ratio": 1.4385964912280702, "no_speech_prob": 0.0012288183206692338}, {"id": 37, "seek": 14312, "start": 165.52, "end": 169.24, "text": " wielozadaniowym, czyli Implicit Multitask Learning, prawda?", "tokens": [51484, 20570, 15151, 345, 3782, 31691, 11, 16591, 4331, 4770, 270, 14665, 270, 3863, 15205, 11, 43607, 30, 51670], "temperature": 0.0, "avg_logprob": -0.1514814349188321, "compression_ratio": 1.4385964912280702, "no_speech_prob": 0.0012288183206692338}, {"id": 38, "seek": 14312, "start": 169.44, "end": 170.16, "text": " Dok\u0142adnie.", "tokens": [51680, 29768, 10358, 2766, 13, 51716], "temperature": 0.0, "avg_logprob": -0.1514814349188321, "compression_ratio": 1.4385964912280702, "no_speech_prob": 0.0012288183206692338}, {"id": 39, "seek": 17016, "start": 170.35999999999999, "end": 175.76, "text": " \u017be model, przetwarzaj\u0105c te gigantyczne ilo\u015bci tekstu z Internetu, niejako", "tokens": [50374, 46864, 2316, 11, 6541, 302, 31991, 38757, 535, 8741, 394, 17466, 716, 1930, 44468, 16624, 372, 84, 710, 7703, 84, 11, 2838, 73, 18501, 50644], "temperature": 0.0, "avg_logprob": -0.16797621287996806, "compression_ratio": 1.45, "no_speech_prob": 0.028493987396359444}, {"id": 40, "seek": 17016, "start": 175.96, "end": 178.07999999999998, "text": " przy okazji uczy si\u0119 wzorc\u00f3w.", "tokens": [50654, 6501, 3133, 921, 4013, 344, 6522, 3244, 24809, 284, 29268, 13, 50760], "temperature": 0.0, "avg_logprob": -0.16797621287996806, "compression_ratio": 1.45, "no_speech_prob": 0.028493987396359444}, {"id": 41, "seek": 17016, "start": 178.28, "end": 178.76, "text": " No w\u0142a\u015bnie.", "tokens": [50770, 883, 14234, 13, 50794], "temperature": 0.0, "avg_logprob": -0.16797621287996806, "compression_ratio": 1.45, "no_speech_prob": 0.028493987396359444}, {"id": 42, "seek": 17016, "start": 178.96, "end": 185.64, "text": " Na przyk\u0142ad analizuj\u0105c miliony w\u0105tk\u00f3w na forach internetowych, uczy si\u0119 formatu pyta\u0144 i odpowiedzi.", "tokens": [50804, 6056, 23144, 2624, 590, 44733, 1962, 46184, 261, 23430, 23849, 1667, 337, 608, 4705, 19605, 11, 344, 6522, 3244, 7877, 84, 10664, 1328, 5248, 741, 36574, 3992, 13, 51138], "temperature": 0.0, "avg_logprob": -0.16797621287996806, "compression_ratio": 1.45, "no_speech_prob": 0.028493987396359444}, {"id": 43, "seek": 17016, "start": 185.84, "end": 190.76, "text": " Przetwarzaj\u0105c ca\u0142\u0105 wikipedi\u0119, ch\u0142onie wzorce strzeszczania informacji.", "tokens": [51148, 2114, 40399, 31991, 38757, 1335, 15926, 261, 1035, 647, 292, 5034, 11, 417, 1221, 32242, 24809, 284, 384, 1056, 89, 10430, 3689, 5609, 1356, 13152, 13, 51394], "temperature": 0.0, "avg_logprob": -0.16797621287996806, "compression_ratio": 1.45, "no_speech_prob": 0.028493987396359444}, {"id": 44, "seek": 17016, "start": 190.96, "end": 196.12, "text": " Ale to wszystko dzieje si\u0119 w spos\u00f3b niezamierzony, to produkt uboczny.", "tokens": [51404, 9366, 281, 22607, 17953, 2884, 3244, 261, 22904, 2838, 28915, 34602, 2526, 11, 281, 42816, 26709, 905, 89, 1634, 13, 51662], "temperature": 0.0, "avg_logprob": -0.16797621287996806, "compression_ratio": 1.45, "no_speech_prob": 0.028493987396359444}, {"id": 45, "seek": 19612, "start": 196.32, "end": 200.84, "text": " Troch\u0119 jak nauka o grawitacji przez, no nie wiem, przypadkowe upuskanie rzeczy.", "tokens": [50374, 19406, 23006, 4207, 35616, 2330, 277, 1295, 86, 270, 13152, 14064, 11, 572, 2838, 26522, 11, 33100, 74, 6880, 493, 301, 5225, 414, 26297, 13, 50600], "temperature": 0.0, "avg_logprob": -0.11869336429395173, "compression_ratio": 1.4743589743589745, "no_speech_prob": 0.05889986827969551}, {"id": 46, "seek": 19612, "start": 201.04, "end": 204.96, "text": " W ko\u0144cu si\u0119 za\u0142apie o co chodzi, ale to nie jest najbardziej efektywna metoda.", "tokens": [50610, 343, 26470, 12032, 3244, 7949, 1221, 569, 414, 277, 598, 23998, 11, 6775, 281, 2838, 3492, 41857, 31482, 916, 874, 86, 629, 1131, 13449, 13, 50806], "temperature": 0.0, "avg_logprob": -0.11869336429395173, "compression_ratio": 1.4743589743589745, "no_speech_prob": 0.05889986827969551}, {"id": 47, "seek": 19612, "start": 205.16, "end": 211.44, "text": " I tu dochodzimy do sedna, bo autorzy tej pracy zadaj\u0105 fundamentalne i w sumie bardzo proste pytanie.", "tokens": [50816, 286, 2604, 9243, 378, 89, 13189, 360, 9643, 629, 11, 748, 19510, 1229, 12573, 35591, 710, 1538, 8555, 8088, 716, 741, 261, 2408, 414, 9034, 10293, 68, 36610, 13, 51130], "temperature": 0.0, "avg_logprob": -0.11869336429395173, "compression_ratio": 1.4743589743589745, "no_speech_prob": 0.05889986827969551}, {"id": 48, "seek": 19612, "start": 211.64000000000001, "end": 214.12, "text": " A co je\u015bli nie zostawimy tego przypadkowi?", "tokens": [51140, 316, 598, 25630, 2838, 31873, 1607, 13189, 8627, 33100, 74, 24503, 30, 51264], "temperature": 0.0, "avg_logprob": -0.11869336429395173, "compression_ratio": 1.4743589743589745, "no_speech_prob": 0.05889986827969551}, {"id": 49, "seek": 19612, "start": 214.32, "end": 220.20000000000002, "text": " Co je\u015bli zamiast biernie liczy\u0107 na tak\u0105, wiesz, chaotyczn\u0105 nauk\u0119, jawnie i celowo", "tokens": [51274, 3066, 25630, 710, 4526, 525, 272, 811, 2766, 6169, 27150, 1667, 31069, 11, 261, 15347, 11, 6294, 6737, 3689, 13113, 35616, 15724, 11, 2784, 14215, 741, 9277, 19941, 51568], "temperature": 0.0, "avg_logprob": -0.11869336429395173, "compression_ratio": 1.4743589743589745, "no_speech_prob": 0.05889986827969551}, {"id": 50, "seek": 19612, "start": 220.4, "end": 223.52, "text": " nauczymy model wykonywania setek r\u00f3\u017cnych zada\u0144 jednocze\u015bnie?", "tokens": [51578, 49103, 1229, 2226, 2316, 39287, 2526, 86, 5609, 992, 916, 42602, 710, 1538, 5248, 5232, 26694, 1381, 12221, 30, 51734], "temperature": 0.0, "avg_logprob": -0.11869336429395173, "compression_ratio": 1.4743589743589745, "no_speech_prob": 0.05889986827969551}, {"id": 51, "seek": 22352, "start": 223.64000000000001, "end": 227.64000000000001, "text": " W\u0142a\u015bnie. Co je\u015bli damy mu starannie przygotowany program nauczania,", "tokens": [50370, 343, 5024, 12221, 13, 3066, 25630, 2422, 88, 2992, 3543, 43433, 35914, 23341, 1461, 49103, 89, 5609, 11, 50570], "temperature": 0.0, "avg_logprob": -0.16071774409367487, "compression_ratio": 1.4219858156028369, "no_speech_prob": 0.007797591388225555}, {"id": 52, "seek": 22352, "start": 227.84, "end": 230.04000000000002, "text": " zamiast kaza\u0107 mu b\u0142\u0105dzi\u0107 po bibliotece?", "tokens": [50580, 710, 4526, 525, 350, 12257, 2162, 2992, 272, 15926, 67, 28496, 714, 34344, 1370, 384, 30, 50690], "temperature": 0.0, "avg_logprob": -0.16071774409367487, "compression_ratio": 1.4219858156028369, "no_speech_prob": 0.007797591388225555}, {"id": 53, "seek": 22352, "start": 230.24, "end": 230.92000000000002, "text": " Dok\u0142adnie.", "tokens": [50700, 29768, 10358, 2766, 13, 50734], "temperature": 0.0, "avg_logprob": -0.16071774409367487, "compression_ratio": 1.4219858156028369, "no_speech_prob": 0.007797591388225555}, {"id": 54, "seek": 22352, "start": 231.12, "end": 234.32000000000002, "text": " I to jest w\u0142a\u015bnie centralna hipoteza tej pracy.", "tokens": [50744, 286, 281, 3492, 14234, 5777, 629, 8103, 1370, 2394, 12573, 35591, 13, 50904], "temperature": 0.0, "avg_logprob": -0.16071774409367487, "compression_ratio": 1.4219858156028369, "no_speech_prob": 0.007797591388225555}, {"id": 55, "seek": 22352, "start": 234.52, "end": 237.96, "text": " To, co nazywaj\u0105 multitask prompted training,", "tokens": [50914, 1407, 11, 598, 20151, 27112, 11133, 42338, 3863, 31042, 3097, 11, 51086], "temperature": 0.0, "avg_logprob": -0.16071774409367487, "compression_ratio": 1.4219858156028369, "no_speech_prob": 0.007797591388225555}, {"id": 56, "seek": 22352, "start": 238.16000000000003, "end": 243.72, "text": " czyli ten jawny trening wielozadaniowy z u\u017cyciem prompt\u00f3w, kt\u00f3ry mo\u017ce bezpo\u015brednio i,", "tokens": [51096, 16591, 2064, 2784, 43682, 2192, 773, 20570, 15151, 345, 3782, 10089, 710, 34097, 4260, 76, 12391, 3901, 11, 9913, 12034, 10782, 2259, 1788, 986, 41084, 741, 11, 51374], "temperature": 0.0, "avg_logprob": -0.16071774409367487, "compression_ratio": 1.4219858156028369, "no_speech_prob": 0.007797591388225555}, {"id": 57, "seek": 22352, "start": 243.92000000000002, "end": 250.04000000000002, "text": " co wa\u017cne, znacznie efektywnie wywo\u0142a\u0107 pot\u0119\u017cne zdolno\u015bci zero shot generalization.", "tokens": [51384, 598, 46110, 11, 15397, 14875, 2766, 31482, 916, 874, 14215, 4628, 6120, 5024, 2162, 1847, 1274, 1427, 716, 16221, 401, 16438, 4018, 3347, 2674, 2144, 13, 51690], "temperature": 0.0, "avg_logprob": -0.16071774409367487, "compression_ratio": 1.4219858156028369, "no_speech_prob": 0.007797591388225555}, {"id": 58, "seek": 25004, "start": 250.23999999999998, "end": 251.76, "text": " Ich teza idzie nawet dalej.", "tokens": [50374, 3141, 535, 2394, 4496, 3283, 22696, 34257, 13, 50450], "temperature": 0.0, "avg_logprob": -0.13554766134250384, "compression_ratio": 1.4685534591194969, "no_speech_prob": 0.004954751115292311}, {"id": 59, "seek": 25004, "start": 251.95999999999998, "end": 256.2, "text": " Twierdz\u0105, \u017ce takie podej\u015bcie pozwala tworzy\u0107 modele nie tylko mniejsze i", "tokens": [50460, 2574, 811, 67, 8925, 11, 3561, 15963, 7468, 73, 9815, 40557, 5159, 46288, 27150, 4391, 306, 2838, 13219, 275, 44258, 741, 50672], "temperature": 0.0, "avg_logprob": -0.13554766134250384, "compression_ratio": 1.4685534591194969, "no_speech_prob": 0.004954751115292311}, {"id": 60, "seek": 25004, "start": 256.4, "end": 259.96, "text": " wydajniejsze, ale te\u017c, co jest kluczowe, znacznie bardziej odporne.", "tokens": [50682, 25984, 1805, 44258, 11, 6775, 9516, 11, 598, 3492, 9671, 1311, 89, 6880, 11, 15397, 14875, 2766, 27209, 3611, 2816, 716, 13, 50860], "temperature": 0.0, "avg_logprob": -0.13554766134250384, "compression_ratio": 1.4685534591194969, "no_speech_prob": 0.004954751115292311}, {"id": 61, "seek": 25004, "start": 260.15999999999997, "end": 261.28, "text": " Odporne na co?", "tokens": [50870, 12210, 2816, 716, 1667, 598, 30, 50926], "temperature": 0.0, "avg_logprob": -0.13554766134250384, "compression_ratio": 1.4685534591194969, "no_speech_prob": 0.004954751115292311}, {"id": 62, "seek": 25004, "start": 261.48, "end": 266.24, "text": " Na r\u00f3\u017cnice w sposobie, w jakim my ludzie formu\u0142ujemy dla nich polecenia.", "tokens": [50936, 6056, 19637, 77, 573, 261, 20443, 996, 414, 11, 261, 49410, 452, 37025, 1254, 84, 1221, 21767, 12285, 25570, 13208, 13037, 654, 13, 51174], "temperature": 0.0, "avg_logprob": -0.13554766134250384, "compression_ratio": 1.4685534591194969, "no_speech_prob": 0.004954751115292311}, {"id": 63, "seek": 25004, "start": 266.44, "end": 270.59999999999997, "text": " OK, to brzmi rewolucyjnie, ale od razu nasuwa mi si\u0119 pytanie o logistyk\u0119.", "tokens": [51184, 2264, 11, 281, 738, 89, 3057, 319, 48481, 1311, 88, 73, 2766, 11, 6775, 3611, 367, 8813, 5382, 84, 4151, 2752, 3244, 36610, 277, 3565, 38618, 15724, 13, 51392], "temperature": 0.0, "avg_logprob": -0.13554766134250384, "compression_ratio": 1.4685534591194969, "no_speech_prob": 0.004954751115292311}, {"id": 64, "seek": 25004, "start": 270.8, "end": 275.52, "text": " Jak w praktyce nauczy\u0107 jeden model tak wielu r\u00f3\u017cnych rzeczy naraz?", "tokens": [51402, 15029, 261, 3206, 74, 874, 384, 49103, 27150, 12906, 2316, 991, 40437, 42602, 26297, 6714, 921, 30, 51638], "temperature": 0.0, "avg_logprob": -0.13554766134250384, "compression_ratio": 1.4685534591194969, "no_speech_prob": 0.004954751115292311}, {"id": 65, "seek": 25004, "start": 275.71999999999997, "end": 278.96, "text": " No bo analiza sentymentu to jedno, strzeszczanie drugie,", "tokens": [51648, 883, 748, 2624, 13427, 2279, 88, 518, 84, 281, 5232, 1771, 11, 1056, 89, 10430, 3689, 7155, 4110, 414, 11, 51810], "temperature": 0.0, "avg_logprob": -0.13554766134250384, "compression_ratio": 1.4685534591194969, "no_speech_prob": 0.004954751115292311}, {"id": 66, "seek": 27896, "start": 279.12, "end": 283.76, "text": " to przecie\u017c tradycyjnie wymaga\u0142o zupe\u0142nie innych format\u00f3w, innych architektur.", "tokens": [50372, 281, 8325, 40082, 504, 880, 42949, 2766, 29764, 9286, 5249, 49922, 36286, 7877, 3901, 11, 36286, 3912, 642, 2320, 374, 13, 50604], "temperature": 0.0, "avg_logprob": -0.09046892325083415, "compression_ratio": 1.4826388888888888, "no_speech_prob": 0.0021760647650808096}, {"id": 67, "seek": 27896, "start": 283.96, "end": 286.12, "text": " To musia\u0142o wymaga\u0107 jakiego\u015b ujednolicenia.", "tokens": [50614, 1407, 1038, 654, 5249, 29764, 9286, 2162, 4207, 12200, 1788, 344, 40543, 77, 7940, 268, 654, 13, 50722], "temperature": 0.0, "avg_logprob": -0.09046892325083415, "compression_ratio": 1.4826388888888888, "no_speech_prob": 0.0021760647650808096}, {"id": 68, "seek": 27896, "start": 286.32, "end": 291.76, "text": " I kluczem do tego takim uniwersalnym j\u0119zykiem okaza\u0142y si\u0119 prompty,", "tokens": [50732, 286, 9671, 1311, 24313, 360, 8627, 31732, 36435, 5364, 304, 12996, 49055, 26116, 3133, 12257, 6825, 3244, 12391, 88, 11, 51004], "temperature": 0.0, "avg_logprob": -0.09046892325083415, "compression_ratio": 1.4826388888888888, "no_speech_prob": 0.0021760647650808096}, {"id": 69, "seek": 27896, "start": 291.96, "end": 294.4, "text": " czyli po prostu tekstowe polecenia.", "tokens": [51014, 16591, 714, 19518, 16624, 372, 6880, 13208, 13037, 654, 13, 51136], "temperature": 0.0, "avg_logprob": -0.09046892325083415, "compression_ratio": 1.4826388888888888, "no_speech_prob": 0.0021760647650808096}, {"id": 70, "seek": 27896, "start": 294.59999999999997, "end": 301.24, "text": " Zesp\u00f3\u0142 badawczy postanowi\u0142 przekszta\u0142ci\u0107 dziesi\u0105tki r\u00f3\u017cnych zada\u0144 NLP w jeden sp\u00f3jny format.", "tokens": [51146, 1176, 13361, 16181, 272, 1538, 86, 6522, 2183, 282, 24503, 1221, 29785, 15453, 46426, 39162, 9758, 530, 11404, 83, 2984, 42602, 710, 1538, 5248, 426, 45196, 261, 12906, 637, 18999, 1634, 7877, 13, 51478], "temperature": 0.0, "avg_logprob": -0.09046892325083415, "compression_ratio": 1.4826388888888888, "no_speech_prob": 0.0021760647650808096}, {"id": 71, "seek": 27896, "start": 301.44, "end": 306.88, "text": " Tekst wyj\u015bciowy, kt\u00f3ry jest poleceniem i oczekiwany tekst wyj\u015bciowy, czyli odpowied\u017a.", "tokens": [51488, 27821, 372, 4628, 73, 6199, 10089, 11, 9913, 3492, 13208, 13037, 4907, 741, 277, 3689, 14753, 86, 1325, 16624, 372, 4628, 73, 6199, 10089, 11, 16591, 36574, 10659, 13, 51760], "temperature": 0.0, "avg_logprob": -0.09046892325083415, "compression_ratio": 1.4826388888888888, "no_speech_prob": 0.0021760647650808096}, {"id": 72, "seek": 30688, "start": 307.08, "end": 310.4, "text": " Wszystko sprowadzili do schematu tekst do tekstu.", "tokens": [50374, 343, 10424, 4093, 637, 1892, 345, 89, 2312, 360, 956, 8615, 84, 16624, 372, 360, 16624, 372, 84, 13, 50540], "temperature": 0.0, "avg_logprob": -0.13755542417115804, "compression_ratio": 1.4454828660436136, "no_speech_prob": 0.00458495831117034}, {"id": 73, "seek": 30688, "start": 310.6, "end": 316.2, "text": " I nagle problem strzeszczania i problem t\u0142umaczenia dla modelu wygl\u0105daj\u0105 tak samo.", "tokens": [50550, 286, 297, 15088, 1154, 1056, 89, 10430, 3689, 5609, 741, 1154, 256, 49166, 326, 14320, 12285, 2316, 84, 27947, 18962, 11133, 991, 36422, 13, 50830], "temperature": 0.0, "avg_logprob": -0.13755542417115804, "compression_ratio": 1.4454828660436136, "no_speech_prob": 0.00458495831117034}, {"id": 74, "seek": 30688, "start": 316.4, "end": 318.96, "text": " Dostaje tekst, ma wygenerowa\u0107 inny tekst.", "tokens": [50840, 413, 555, 11153, 16624, 372, 11, 463, 4628, 21848, 11445, 294, 1634, 16624, 372, 13, 50968], "temperature": 0.0, "avg_logprob": -0.13755542417115804, "compression_ratio": 1.4454828660436136, "no_speech_prob": 0.00458495831117034}, {"id": 75, "seek": 30688, "start": 319.15999999999997, "end": 324.96, "text": " Ale to, co jest u mnie naprawd\u0119 fascynuj\u0105ce, to nie sam pomys\u0142, ale spos\u00f3b, w jaki go zrealizowali.", "tokens": [50978, 9366, 281, 11, 598, 3492, 344, 17661, 20970, 30632, 1344, 77, 13263, 384, 11, 281, 2838, 3247, 12991, 39508, 11, 6775, 22904, 11, 261, 24492, 352, 710, 9342, 590, 305, 5103, 13, 51268], "temperature": 0.0, "avg_logprob": -0.13755542417115804, "compression_ratio": 1.4454828660436136, "no_speech_prob": 0.00458495831117034}, {"id": 76, "seek": 30688, "start": 325.15999999999997, "end": 327.84, "text": " Bo oni nie zamkn\u0119li si\u0119 w laboratorium, prawda?", "tokens": [51278, 3286, 36317, 2838, 19876, 5457, 1274, 2081, 3244, 261, 5938, 41679, 11, 43607, 30, 51412], "temperature": 0.0, "avg_logprob": -0.13755542417115804, "compression_ratio": 1.4454828660436136, "no_speech_prob": 0.00458495831117034}, {"id": 77, "seek": 30688, "start": 328.04, "end": 333.28, "text": " Zrobili co\u015b radykalnego, stworzyli otwarte narz\u0119dzie prompt source i zaprosili", "tokens": [51422, 1176, 16614, 2312, 19241, 367, 880, 19990, 11858, 11, 342, 28321, 1229, 2081, 4337, 86, 11026, 6714, 89, 42643, 12391, 4009, 741, 14223, 2635, 2312, 51684], "temperature": 0.0, "avg_logprob": -0.13755542417115804, "compression_ratio": 1.4454828660436136, "no_speech_prob": 0.00458495831117034}, {"id": 78, "seek": 30688, "start": 333.48, "end": 336.28, "text": " globaln\u0105 spo\u0142eczno\u015b\u0107 naukow\u0105 do wsp\u00f3\u0142pracy.", "tokens": [51694, 4338, 13113, 36851, 89, 23293, 35616, 74, 30297, 360, 39069, 1424, 2551, 13, 51834], "temperature": 0.0, "avg_logprob": -0.13755542417115804, "compression_ratio": 1.4454828660436136, "no_speech_prob": 0.00458495831117034}, {"id": 79, "seek": 33628, "start": 336.47999999999996, "end": 338.2, "text": " Efekt jest niesamowity.", "tokens": [50374, 31840, 8192, 3492, 48100, 335, 305, 507, 13, 50460], "temperature": 0.0, "avg_logprob": -0.1288247807820638, "compression_ratio": 1.3491525423728814, "no_speech_prob": 0.0028443546034395695}, {"id": 80, "seek": 33628, "start": 338.4, "end": 343.76, "text": " W projekt zaanga\u017cowa\u0142o si\u0119 36 autor\u00f3w z 24 instytucji w o\u015bmiu krajach.", "tokens": [50470, 343, 26261, 7949, 656, 18264, 5528, 5249, 3244, 8652, 19510, 3901, 710, 4022, 1058, 4328, 1311, 4013, 261, 277, 1788, 3057, 84, 28248, 45059, 13, 50738], "temperature": 0.0, "avg_logprob": -0.1288247807820638, "compression_ratio": 1.3491525423728814, "no_speech_prob": 0.0028443546034395695}, {"id": 81, "seek": 33628, "start": 343.96, "end": 347.64, "text": " To jest prawdziwie oddolny, otwarty wysi\u0142ek.", "tokens": [50748, 1407, 3492, 41175, 3992, 8699, 3611, 67, 401, 1634, 11, 4337, 29587, 88, 27062, 40622, 916, 13, 50932], "temperature": 0.0, "avg_logprob": -0.1288247807820638, "compression_ratio": 1.3491525423728814, "no_speech_prob": 0.0028443546034395695}, {"id": 82, "seek": 33628, "start": 347.84, "end": 351.32, "text": " I ten spo\u0142eczno\u015bciowy charakter przyni\u00f3s\u0142 co\u015b bezcennego.", "tokens": [50942, 286, 2064, 36851, 89, 16438, 10089, 1290, 33557, 6501, 3722, 12994, 1221, 19241, 10782, 13037, 11858, 13, 51116], "temperature": 0.0, "avg_logprob": -0.1288247807820638, "compression_ratio": 1.3491525423728814, "no_speech_prob": 0.0028443546034395695}, {"id": 83, "seek": 33628, "start": 351.52, "end": 352.67999999999995, "text": " R\u00f3\u017cnorodno\u015b\u0107.", "tokens": [51126, 497, 812, 1427, 19048, 378, 23293, 13, 51184], "temperature": 0.0, "avg_logprob": -0.1288247807820638, "compression_ratio": 1.3491525423728814, "no_speech_prob": 0.0028443546034395695}, {"id": 84, "seek": 33628, "start": 352.88, "end": 354.88, "text": " Nie wiarygodn\u0105 r\u00f3\u017cnorodno\u015b\u0107.", "tokens": [51194, 12016, 26393, 822, 21787, 13113, 19637, 19048, 378, 23293, 13, 51294], "temperature": 0.0, "avg_logprob": -0.1288247807820638, "compression_ratio": 1.3491525423728814, "no_speech_prob": 0.0028443546034395695}, {"id": 85, "seek": 33628, "start": 355.08, "end": 358.44, "text": " Co\u015b, czego ma\u0142a grupa badaczy nigdy by nie osi\u0105gn\u0119\u0142a.", "tokens": [51304, 3066, 1788, 11, 36559, 463, 5024, 12740, 64, 1578, 14691, 26996, 3173, 538, 2838, 3003, 11404, 4568, 1274, 5024, 13, 51472], "temperature": 0.0, "avg_logprob": -0.1288247807820638, "compression_ratio": 1.3491525423728814, "no_speech_prob": 0.0028443546034395695}, {"id": 86, "seek": 33628, "start": 358.64, "end": 360.35999999999996, "text": " Zebrali ogromn\u0105 kolekcj\u0119.", "tokens": [51482, 4853, 1443, 5103, 34416, 298, 13113, 18303, 74, 41960, 13, 51568], "temperature": 0.0, "avg_logprob": -0.1288247807820638, "compression_ratio": 1.3491525423728814, "no_speech_prob": 0.0028443546034395695}, {"id": 87, "seek": 33628, "start": 360.55999999999995, "end": 364.35999999999996, "text": " Nazwali j\u0105 Public Pool of Prompts, w skr\u00f3cie P3.", "tokens": [51578, 11870, 40054, 35692, 9489, 46188, 295, 15833, 39280, 11, 261, 1110, 11721, 4260, 430, 18, 13, 51768], "temperature": 0.0, "avg_logprob": -0.1288247807820638, "compression_ratio": 1.3491525423728814, "no_speech_prob": 0.0028443546034395695}, {"id": 88, "seek": 36436, "start": 364.52000000000004, "end": 368.56, "text": " Dla ka\u017cdego zadania mieli mn\u00f3stwo wariant\u00f3w prompt\u00f3w od bardzo formalnych,", "tokens": [50372, 413, 875, 21912, 67, 6308, 42788, 5609, 41214, 275, 77, 45052, 6120, 1516, 5798, 3901, 12391, 3901, 3611, 9034, 9860, 9399, 11, 50574], "temperature": 0.0, "avg_logprob": -0.13612791317612377, "compression_ratio": 1.4621212121212122, "no_speech_prob": 0.013591772876679897}, {"id": 89, "seek": 36436, "start": 368.76, "end": 373.16, "text": " po niezwykle kreatywne, potoczne, a nawet no dziwaczne.", "tokens": [50584, 714, 33511, 9726, 14677, 350, 620, 27112, 716, 11, 1847, 905, 43077, 11, 257, 22696, 572, 31981, 86, 14875, 716, 13, 50804], "temperature": 0.0, "avg_logprob": -0.13612791317612377, "compression_ratio": 1.4621212121212122, "no_speech_prob": 0.013591772876679897}, {"id": 90, "seek": 36436, "start": 373.36, "end": 377.68, "text": " Pami\u0119tam ten \u015bwietny przyk\u0142ad z pracy, kt\u00f3ry to doskonale ilustruje.", "tokens": [50814, 430, 23806, 37323, 2064, 8299, 39083, 1634, 23144, 710, 35591, 11, 9913, 281, 4491, 18295, 1220, 1930, 381, 894, 2884, 13, 51030], "temperature": 0.0, "avg_logprob": -0.13612791317612377, "compression_ratio": 1.4621212121212122, "no_speech_prob": 0.013591772876679897}, {"id": 91, "seek": 36436, "start": 377.88, "end": 383.08000000000004, "text": " Dla zadania identyfikacji parafrazy, czyli sprawdzania czy dwa zdania znacz\u0105 to samo.", "tokens": [51040, 413, 875, 42788, 5609, 2473, 88, 31230, 13152, 1690, 69, 424, 1229, 11, 16591, 46192, 89, 5609, 6430, 35045, 16221, 5609, 15397, 326, 8925, 281, 36422, 13, 51300], "temperature": 0.0, "avg_logprob": -0.13612791317612377, "compression_ratio": 1.4621212121212122, "no_speech_prob": 0.013591772876679897}, {"id": 92, "seek": 36436, "start": 383.28000000000003, "end": 386.04, "text": " Standardowy prompt brzmia\u0142by pewnie jako\u015b tak.", "tokens": [51310, 21298, 10089, 12391, 738, 89, 29958, 34635, 520, 14215, 17123, 1788, 991, 13, 51448], "temperature": 0.0, "avg_logprob": -0.13612791317612377, "compression_ratio": 1.4621212121212122, "no_speech_prob": 0.013591772876679897}, {"id": 93, "seek": 36436, "start": 386.24, "end": 389.48, "text": " Czy zdanie A oznacza to samo, co zdanie B.", "tokens": [51458, 19832, 16221, 7155, 316, 277, 22672, 326, 2394, 281, 36422, 11, 598, 16221, 7155, 363, 13, 51620], "temperature": 0.0, "avg_logprob": -0.13612791317612377, "compression_ratio": 1.4621212121212122, "no_speech_prob": 0.013591772876679897}, {"id": 94, "seek": 38948, "start": 389.72, "end": 395.56, "text": " Ale w P3 znalaz\u0142 si\u0119 te\u017c prompt stylizowany na wiadomo\u015b\u0107 od administratora serwisu Quora.", "tokens": [50376, 9366, 261, 430, 18, 710, 4660, 921, 1221, 3244, 9516, 12391, 23736, 590, 23341, 1667, 26393, 40633, 7753, 3611, 25529, 64, 816, 86, 25871, 2326, 3252, 13, 50668], "temperature": 0.0, "avg_logprob": -0.15499944556249332, "compression_ratio": 1.4342105263157894, "no_speech_prob": 0.01641554944217205}, {"id": 95, "seek": 38948, "start": 395.76, "end": 396.68, "text": " Co\u015b w stylu.", "tokens": [50678, 3066, 1788, 261, 7952, 2781, 13, 50724], "temperature": 0.0, "avg_logprob": -0.15499944556249332, "compression_ratio": 1.4342105263157894, "no_speech_prob": 0.01641554944217205}, {"id": 96, "seek": 38948, "start": 396.88, "end": 399.16, "text": " Jestem administratorem na stronie Quora.", "tokens": [50734, 24918, 443, 25529, 443, 1667, 1056, 32242, 2326, 3252, 13, 50848], "temperature": 0.0, "avg_logprob": -0.15499944556249332, "compression_ratio": 1.4342105263157894, "no_speech_prob": 0.01641554944217205}, {"id": 97, "seek": 38948, "start": 399.36, "end": 402.0, "text": " Mam dwa posty. Czy mog\u0119 po\u0142\u0105czy\u0107 te pytania?", "tokens": [50858, 19899, 35045, 2183, 88, 13, 19832, 41737, 714, 15926, 33967, 535, 25878, 5609, 30, 50990], "temperature": 0.0, "avg_logprob": -0.15499944556249332, "compression_ratio": 1.4342105263157894, "no_speech_prob": 0.01641554944217205}, {"id": 98, "seek": 38948, "start": 402.20000000000005, "end": 405.12, "text": " Dok\u0142adnie. I to jest fundamentalna r\u00f3\u017cnica.", "tokens": [51000, 29768, 10358, 2766, 13, 286, 281, 3492, 8088, 629, 19637, 32687, 13, 51146], "temperature": 0.0, "avg_logprob": -0.15499944556249332, "compression_ratio": 1.4342105263157894, "no_speech_prob": 0.01641554944217205}, {"id": 99, "seek": 38948, "start": 405.32, "end": 408.32, "text": " No tak, bo to testuje co\u015b znacznie g\u0142\u0119bszego.", "tokens": [51156, 883, 991, 11, 748, 281, 1500, 13008, 19241, 15397, 14875, 2766, 18117, 1274, 929, 27725, 13, 51306], "temperature": 0.0, "avg_logprob": -0.15499944556249332, "compression_ratio": 1.4342105263157894, "no_speech_prob": 0.01641554944217205}, {"id": 100, "seek": 38948, "start": 408.52000000000004, "end": 409.52000000000004, "text": " Tak.", "tokens": [51316, 9118, 13, 51366], "temperature": 0.0, "avg_logprob": -0.15499944556249332, "compression_ratio": 1.4342105263157894, "no_speech_prob": 0.01641554944217205}, {"id": 101, "seek": 38948, "start": 409.72, "end": 414.8, "text": " Model uczy si\u0119 nie tylko rozpoznawa\u0107 parafraz\u0119, ale te\u017c rozumie\u0107 kontekst,", "tokens": [51376, 17105, 344, 6522, 3244, 2838, 13219, 9544, 2259, 35458, 25234, 1690, 69, 424, 11052, 11, 6775, 9516, 48797, 414, 2162, 14373, 916, 372, 11, 51630], "temperature": 0.0, "avg_logprob": -0.15499944556249332, "compression_ratio": 1.4342105263157894, "no_speech_prob": 0.01641554944217205}, {"id": 102, "seek": 38948, "start": 415.0, "end": 419.28000000000003, "text": " intencje ukryt\u0105 w bardzo r\u00f3\u017cnych formach komunikacji.", "tokens": [51640, 560, 22660, 2884, 26769, 627, 83, 1611, 261, 9034, 42602, 1254, 608, 45359, 1035, 13152, 13, 51854], "temperature": 0.0, "avg_logprob": -0.15499944556249332, "compression_ratio": 1.4342105263157894, "no_speech_prob": 0.01641554944217205}, {"id": 103, "seek": 41948, "start": 419.48, "end": 423.48, "text": " Togo zmusza do g\u0142\u0119bszego zrozumienia samego zadania, a nie tylko, wiesz,", "tokens": [50364, 314, 23515, 17020, 301, 2394, 360, 18117, 1274, 929, 27725, 710, 27857, 449, 18811, 912, 1571, 42788, 5609, 11, 257, 2838, 13219, 11, 261, 15347, 11, 50564], "temperature": 0.0, "avg_logprob": -0.1264975012802496, "compression_ratio": 1.4123076923076923, "no_speech_prob": 0.0012155203148722649}, {"id": 104, "seek": 41948, "start": 423.68, "end": 426.04, "text": " powierzchownego do pasowywania s\u0142\u00f3w kluczowych.", "tokens": [50574, 3388, 34602, 339, 648, 6308, 360, 1736, 10089, 86, 5609, 15116, 3901, 9671, 1311, 89, 19605, 13, 50692], "temperature": 0.0, "avg_logprob": -0.1264975012802496, "compression_ratio": 1.4123076923076923, "no_speech_prob": 0.0012155203148722649}, {"id": 105, "seek": 41948, "start": 426.24, "end": 429.16, "text": " Czyli ta r\u00f3\u017cnorodno\u015b\u0107 okaza\u0142a si\u0119 tym tajnym sk\u0142adnikiem?", "tokens": [50702, 37099, 1846, 19637, 19048, 378, 23293, 3133, 12257, 5024, 3244, 8107, 256, 1805, 12996, 1110, 10358, 13123, 4907, 30, 50848], "temperature": 0.0, "avg_logprob": -0.1264975012802496, "compression_ratio": 1.4123076923076923, "no_speech_prob": 0.0012155203148722649}, {"id": 106, "seek": 41948, "start": 429.36, "end": 434.8, "text": " Precywujnie. A co do samego motelu, co te\u017c jest wa\u017cne, nie budowali go od zera.", "tokens": [50858, 6001, 1344, 86, 4579, 2766, 13, 316, 598, 360, 912, 1571, 2184, 34813, 11, 598, 9516, 3492, 46110, 11, 2838, 3265, 305, 5103, 352, 3611, 710, 1663, 13, 51130], "temperature": 0.0, "avg_logprob": -0.1264975012802496, "compression_ratio": 1.4123076923076923, "no_speech_prob": 0.0012155203148722649}, {"id": 107, "seek": 41948, "start": 435.0, "end": 438.92, "text": " Wykorzystali istniej\u0105c\u0105 architektur\u0119 typu encoder-decoder,", "tokens": [51140, 14458, 19339, 36049, 5103, 1418, 2766, 8555, 32557, 3912, 642, 2320, 374, 1274, 2125, 84, 2058, 19866, 12, 42821, 19866, 11, 51336], "temperature": 0.0, "avg_logprob": -0.1264975012802496, "compression_ratio": 1.4123076923076923, "no_speech_prob": 0.0012155203148722649}, {"id": 108, "seek": 41948, "start": 439.12, "end": 444.24, "text": " a konkretnie model T5 plus LMM o 11 miliardach parametr\u00f3w.", "tokens": [51346, 257, 36500, 2766, 2316, 314, 20, 1804, 46529, 44, 277, 2975, 1962, 72, 515, 608, 6220, 27965, 3901, 13, 51602], "temperature": 0.0, "avg_logprob": -0.1264975012802496, "compression_ratio": 1.4123076923076923, "no_speech_prob": 0.0012155203148722649}, {"id": 109, "seek": 41948, "start": 444.44, "end": 448.44, "text": " I ten gotowy model poddali procesowi fine tuning na tej ogromnej,", "tokens": [51612, 286, 2064, 658, 10089, 2316, 2497, 67, 5103, 17565, 24503, 2489, 15164, 1667, 12573, 34416, 298, 11794, 11, 51812], "temperature": 0.0, "avg_logprob": -0.1264975012802496, "compression_ratio": 1.4123076923076923, "no_speech_prob": 0.0012155203148722649}, {"id": 110, "seek": 44844, "start": 448.44, "end": 452.52, "text": " wielozadaniowej i niezwykle zr\u00f3\u017cnicowanej mieszance prompt\u00f3w z P3.", "tokens": [50364, 20570, 15151, 345, 3782, 21091, 741, 33511, 9726, 14677, 710, 11721, 1427, 7692, 23066, 73, 33039, 719, 12391, 3901, 710, 430, 18, 13, 50568], "temperature": 0.0, "avg_logprob": -0.12413054793628293, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.006552783772349358}, {"id": 111, "seek": 44844, "start": 452.71999999999997, "end": 457.88, "text": " Czyli nie budowali nowego wielkiego silnika, wzi\u0119li istniej\u0105cy bardzo dobry", "tokens": [50578, 37099, 2838, 3265, 305, 5103, 586, 6308, 20570, 42349, 3425, 77, 5439, 11, 261, 16706, 2081, 1418, 2766, 8555, 1344, 9034, 35884, 50836], "temperature": 0.0, "avg_logprob": -0.12413054793628293, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.006552783772349358}, {"id": 112, "seek": 44844, "start": 458.08, "end": 463.4, "text": " silnik i poddali go takiemu wszechstronnemu programowi treningowemu.", "tokens": [50846, 3425, 13123, 741, 2497, 67, 5103, 352, 991, 4907, 84, 37647, 19439, 372, 2044, 25989, 84, 1461, 24503, 2192, 773, 305, 37552, 13, 51112], "temperature": 0.0, "avg_logprob": -0.12413054793628293, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.006552783772349358}, {"id": 113, "seek": 44844, "start": 463.6, "end": 465.48, "text": " To wszystko brzmi imponuj\u0105co w teorii.", "tokens": [51122, 1407, 22607, 738, 89, 3057, 704, 266, 13263, 1291, 261, 40238, 5597, 13, 51216], "temperature": 0.0, "avg_logprob": -0.12413054793628293, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.006552783772349358}, {"id": 114, "seek": 44844, "start": 465.68, "end": 471.48, "text": " Ale ca\u0142a ta praca by\u0142aby na nic, gdyby nie prze\u0142o\u017cy\u0142o si\u0119 to na twarde wyniki.", "tokens": [51226, 9366, 1335, 5024, 1846, 582, 6628, 16673, 2509, 1667, 6201, 11, 28405, 2322, 2838, 8325, 5249, 7735, 5249, 3244, 281, 1667, 683, 10866, 31936, 9850, 13, 51516], "temperature": 0.0, "avg_logprob": -0.12413054793628293, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.006552783772349358}, {"id": 115, "seek": 44844, "start": 471.68, "end": 474.04, "text": " Jak te zero faktycznie wypad\u0142 w starciu z", "tokens": [51526, 15029, 535, 710, 2032, 33647, 45586, 4628, 13647, 1221, 261, 3543, 30795, 710, 51644], "temperature": 0.0, "avg_logprob": -0.12413054793628293, "compression_ratio": 1.4328358208955223, "no_speech_prob": 0.006552783772349358}, {"id": 116, "seek": 47404, "start": 474.04, "end": 476.16, "text": " zadaniami, kt\u00f3rych nigdy wcze\u015bniej nie widzia\u0142.", "tokens": [50364, 710, 11338, 15568, 11, 30382, 26996, 3173, 40785, 2838, 27486, 8908, 13, 50470], "temperature": 0.0, "avg_logprob": -0.1413861443014706, "compression_ratio": 1.4366666666666668, "no_speech_prob": 0.16083694994449615}, {"id": 117, "seek": 47404, "start": 476.36, "end": 480.84000000000003, "text": " Wyniki by\u0142y m\u00f3wi\u0105c wprost rewelacyjne i dla wielu by\u0142y szokem.", "tokens": [50480, 343, 2534, 9850, 26366, 46591, 66, 261, 1424, 555, 319, 45512, 31285, 716, 741, 12285, 40437, 26366, 7870, 453, 443, 13, 50704], "temperature": 0.0, "avg_logprob": -0.1413861443014706, "compression_ratio": 1.4366666666666668, "no_speech_prob": 0.16083694994449615}, {"id": 118, "seek": 47404, "start": 481.04, "end": 485.28000000000003, "text": " Model T0 testowany na zadaniach, na kt\u00f3rych nie by\u0142 trenowany,", "tokens": [50714, 17105, 314, 15, 1500, 23341, 1667, 42788, 3782, 608, 11, 1667, 30382, 2838, 16673, 23136, 23341, 11, 50926], "temperature": 0.0, "avg_logprob": -0.1413861443014706, "compression_ratio": 1.4366666666666668, "no_speech_prob": 0.16083694994449615}, {"id": 119, "seek": 47404, "start": 485.48, "end": 490.6, "text": " dor\u00f3wna\u0142 lub przewy\u017cszy\u0142 wydajno\u015b\u0107 znacznie wi\u0119kszego modelu GPT-3.", "tokens": [50936, 26313, 3901, 629, 1221, 15980, 39758, 88, 1427, 7706, 1221, 25984, 1805, 23293, 15397, 14875, 2766, 29968, 27725, 2316, 84, 26039, 51, 12, 18, 13, 51192], "temperature": 0.0, "avg_logprob": -0.1413861443014706, "compression_ratio": 1.4366666666666668, "no_speech_prob": 0.16083694994449615}, {"id": 120, "seek": 47404, "start": 490.8, "end": 494.24, "text": " A GPT-3 ma 175 miliard\u00f3w parametr\u00f3w.", "tokens": [51202, 316, 26039, 51, 12, 18, 463, 41165, 1962, 72, 515, 3901, 6220, 27965, 3901, 13, 51374], "temperature": 0.0, "avg_logprob": -0.1413861443014706, "compression_ratio": 1.4366666666666668, "no_speech_prob": 0.16083694994449615}, {"id": 121, "seek": 47404, "start": 494.44, "end": 497.88, "text": " T0 okaza\u0142 si\u0119 lepszy na 9 z 11 testowanych zada\u0144.", "tokens": [51384, 314, 15, 3133, 12257, 1221, 3244, 476, 1878, 1229, 1667, 1722, 710, 2975, 1500, 23341, 339, 710, 1538, 5248, 13, 51556], "temperature": 0.0, "avg_logprob": -0.1413861443014706, "compression_ratio": 1.4366666666666668, "no_speech_prob": 0.16083694994449615}, {"id": 122, "seek": 47404, "start": 498.08000000000004, "end": 501.68, "text": " W filach, fila, zatrzymajmy si\u0119 tutaj na moment, bo to jest absolutnie kluczowe.", "tokens": [51566, 343, 1387, 608, 11, 1387, 64, 11, 35802, 13047, 1696, 73, 2226, 3244, 12749, 1667, 1623, 11, 748, 281, 3492, 18757, 2766, 9671, 1311, 89, 6880, 13, 51746], "temperature": 0.0, "avg_logprob": -0.1413861443014706, "compression_ratio": 1.4366666666666668, "no_speech_prob": 0.16083694994449615}, {"id": 123, "seek": 50168, "start": 501.88, "end": 504.40000000000003, "text": " M\u00f3wimy o modelu, kt\u00f3ry jest ile?", "tokens": [50374, 376, 3901, 13189, 277, 2316, 84, 11, 9913, 3492, 15465, 30, 50500], "temperature": 0.0, "avg_logprob": -0.1541888211902819, "compression_ratio": 1.4320557491289199, "no_speech_prob": 0.016654808074235916}, {"id": 124, "seek": 50168, "start": 504.6, "end": 507.12, "text": " Jaki\u015b 16 razy mniejszy.", "tokens": [50510, 508, 7421, 1788, 3165, 9639, 88, 39513, 7706, 13, 50636], "temperature": 0.0, "avg_logprob": -0.1541888211902819, "compression_ratio": 1.4320557491289199, "no_speech_prob": 0.016654808074235916}, {"id": 125, "seek": 50168, "start": 507.32, "end": 510.0, "text": " Oko\u0142o 16 razy mniejszy, tak.", "tokens": [50646, 3477, 78, 5249, 3165, 9639, 88, 39513, 7706, 11, 991, 13, 50780], "temperature": 0.0, "avg_logprob": -0.1541888211902819, "compression_ratio": 1.4320557491289199, "no_speech_prob": 0.016654808074235916}, {"id": 126, "seek": 50168, "start": 510.2, "end": 514.88, "text": " To jakby wyspecjalizowany, zwinny zawodnik wagi \u015bredniej,", "tokens": [50790, 1407, 28976, 27062, 494, 66, 22600, 590, 23341, 11, 11873, 259, 1634, 28165, 378, 13123, 261, 20291, 8299, 986, 10402, 11, 51024], "temperature": 0.0, "avg_logprob": -0.1541888211902819, "compression_ratio": 1.4320557491289199, "no_speech_prob": 0.016654808074235916}, {"id": 127, "seek": 50168, "start": 515.08, "end": 520.44, "text": " wszed\u0142 na ring i metodycznie pokona\u0142 o ci\u0119\u017ca\u0142ego olbrzymia, kt\u00f3ry polega\u0142", "tokens": [51034, 37647, 11312, 1221, 1667, 4875, 741, 1131, 843, 19923, 13010, 4037, 1221, 277, 35484, 35075, 1221, 6308, 2545, 1443, 26681, 654, 11, 9913, 13208, 3680, 1221, 51302], "temperature": 0.0, "avg_logprob": -0.1541888211902819, "compression_ratio": 1.4320557491289199, "no_speech_prob": 0.016654808074235916}, {"id": 128, "seek": 50168, "start": 520.64, "end": 522.24, "text": " wy\u0142\u0105cznie na swojej masie.", "tokens": [51312, 4628, 15926, 19923, 1667, 29489, 73, 2300, 414, 13, 51392], "temperature": 0.0, "avg_logprob": -0.1541888211902819, "compression_ratio": 1.4320557491289199, "no_speech_prob": 0.016654808074235916}, {"id": 129, "seek": 50168, "start": 522.44, "end": 526.0, "text": " To kompletnie podwa\u017ca ca\u0142\u0105 filozofi\u0119 Bigger is better.", "tokens": [51402, 1407, 5207, 14657, 2766, 2497, 27111, 64, 1335, 15926, 1387, 15151, 2670, 5034, 5429, 1321, 307, 1101, 13, 51580], "temperature": 0.0, "avg_logprob": -0.1541888211902819, "compression_ratio": 1.4320557491289199, "no_speech_prob": 0.016654808074235916}, {"id": 130, "seek": 50168, "start": 526.2, "end": 531.2, "text": " Dok\u0142adnie. To by\u0142 ten moment, w kt\u00f3rym wielu badaczy zrozumia\u0142o, \u017ce istnieje inna droga.", "tokens": [51590, 29768, 10358, 2766, 13, 1407, 16673, 2064, 1623, 11, 261, 30120, 40437, 1578, 14691, 710, 27857, 449, 654, 5249, 11, 3561, 1418, 2766, 2884, 294, 629, 3789, 3680, 13, 51840], "temperature": 0.0, "avg_logprob": -0.1541888211902819, "compression_ratio": 1.4320557491289199, "no_speech_prob": 0.016654808074235916}, {"id": 131, "seek": 53120, "start": 531.4000000000001, "end": 538.12, "text": " Co ciekawe, T0 szczeg\u00f3lnie dobrze poradzi\u0142 sobie z zadaniami typu Natural Language Inference, NLI.", "tokens": [50374, 3066, 30596, 2330, 826, 11, 314, 15, 49624, 2766, 28335, 1515, 345, 3992, 1221, 13652, 710, 710, 11338, 15568, 2125, 84, 20137, 24445, 682, 5158, 11, 426, 48718, 13, 50710], "temperature": 0.0, "avg_logprob": -0.1347287727968536, "compression_ratio": 1.372168284789644, "no_speech_prob": 0.0018208712572231889}, {"id": 132, "seek": 53120, "start": 538.32, "end": 543.48, "text": " To s\u0105 zadania, kt\u00f3re sprawdzaj\u0105 zdolno\u015b\u0107 do logicznego rozumowania na podstawie tekstu.", "tokens": [50720, 1407, 9015, 42788, 5609, 11, 8864, 46192, 89, 11133, 16221, 401, 23293, 360, 9952, 89, 11858, 48797, 21308, 1667, 43443, 414, 16624, 372, 84, 13, 50978], "temperature": 0.0, "avg_logprob": -0.1347287727968536, "compression_ratio": 1.372168284789644, "no_speech_prob": 0.0018208712572231889}, {"id": 133, "seek": 53120, "start": 543.6800000000001, "end": 547.9200000000001, "text": " A ani on, ani GPT-3 nie byli na nich bezpo\u015brednio trenowani.", "tokens": [50988, 316, 40477, 322, 11, 40477, 26039, 51, 12, 18, 2838, 538, 2081, 1667, 25570, 10782, 2259, 1788, 986, 41084, 23136, 305, 3782, 13, 51200], "temperature": 0.0, "avg_logprob": -0.1347287727968536, "compression_ratio": 1.372168284789644, "no_speech_prob": 0.0018208712572231889}, {"id": 134, "seek": 53120, "start": 548.12, "end": 552.24, "text": " Osi\u0105gn\u0105 te\u017c \u015bwietne wyniki na bardzo trudnym benchmarku Big Bench,", "tokens": [51210, 422, 7691, 1611, 4568, 1611, 9516, 8299, 39083, 716, 31936, 9850, 1667, 9034, 32007, 12996, 18927, 84, 5429, 3964, 339, 11, 51416], "temperature": 0.0, "avg_logprob": -0.1347287727968536, "compression_ratio": 1.372168284789644, "no_speech_prob": 0.0018208712572231889}, {"id": 135, "seek": 53120, "start": 552.44, "end": 555.6400000000001, "text": " cz\u0119sto pokonuj\u0105c modele nawet 6 razy wi\u0119ksze od siebie.", "tokens": [51426, 34369, 13010, 266, 44733, 4391, 306, 22696, 1386, 9639, 88, 29968, 1381, 3611, 39137, 13, 51586], "temperature": 0.0, "avg_logprob": -0.1347287727968536, "compression_ratio": 1.372168284789644, "no_speech_prob": 0.0018208712572231889}, {"id": 136, "seek": 53120, "start": 555.84, "end": 558.0400000000001, "text": " To prowadzi mnie do kolejnego pytania.", "tokens": [51596, 1407, 36590, 3992, 17661, 360, 23749, 11858, 25878, 5609, 13, 51706], "temperature": 0.0, "avg_logprob": -0.1347287727968536, "compression_ratio": 1.372168284789644, "no_speech_prob": 0.0018208712572231889}, {"id": 137, "seek": 55804, "start": 558.24, "end": 563.0799999999999, "text": " Ka\u017cdy, kto pracowa\u0142 z du\u017cymi modelami wie, jak potrafi\u0105 by\u0107 kapryczne.", "tokens": [50374, 10988, 1427, 3173, 11, 23780, 22404, 30105, 710, 1581, 7735, 3057, 2316, 4526, 3355, 11, 4207, 1847, 10437, 11404, 15069, 13816, 627, 38491, 13, 50616], "temperature": 0.0, "avg_logprob": -0.11051512498121996, "compression_ratio": 1.3903345724907064, "no_speech_prob": 0.004515758715569973}, {"id": 138, "seek": 55804, "start": 563.28, "end": 567.8399999999999, "text": " Zmienisz jedno s\u0142owo w poleceniu i odpowied\u017a jest bezu\u017cyteczna.", "tokens": [50626, 1176, 76, 1053, 23848, 5232, 1771, 15116, 19941, 261, 13208, 13037, 5951, 741, 36574, 10659, 3492, 10782, 84, 7735, 975, 3689, 629, 13, 50854], "temperature": 0.0, "avg_logprob": -0.11051512498121996, "compression_ratio": 1.3903345724907064, "no_speech_prob": 0.004515758715569973}, {"id": 139, "seek": 55804, "start": 568.04, "end": 575.92, "text": " Czy te \u015bwietne wyniki T0 nie zale\u017ca\u0142y od znalezienia jednego idealnie sformu\u0142owanego z\u0142otego pr\u0105ptu?", "tokens": [50864, 19832, 535, 8299, 39083, 716, 31936, 9850, 314, 15, 2838, 710, 1220, 35075, 6825, 3611, 15397, 37646, 18811, 5232, 11858, 7157, 2766, 262, 837, 84, 1221, 37345, 6308, 31614, 310, 6308, 582, 1611, 662, 84, 30, 51258], "temperature": 0.0, "avg_logprob": -0.11051512498121996, "compression_ratio": 1.3903345724907064, "no_speech_prob": 0.004515758715569973}, {"id": 140, "seek": 55804, "start": 576.12, "end": 580.9599999999999, "text": " To jest kluczowe pytanie o odporno\u015bci i autorzy dok\u0142adnie to wbadali.", "tokens": [51268, 1407, 3492, 9671, 1311, 89, 6880, 36610, 277, 3611, 2816, 16438, 741, 19510, 1229, 45864, 2766, 281, 261, 27580, 5103, 13, 51510], "temperature": 0.0, "avg_logprob": -0.11051512498121996, "compression_ratio": 1.3903345724907064, "no_speech_prob": 0.004515758715569973}, {"id": 141, "seek": 55804, "start": 581.16, "end": 584.36, "text": " Analiza przynios\u0142a kilka bardzo wa\u017cnych wniosk\u00f3w.", "tokens": [51520, 16128, 13427, 6501, 77, 2717, 5024, 36466, 9034, 27777, 9399, 45368, 2717, 23849, 13, 51680], "temperature": 0.0, "avg_logprob": -0.11051512498121996, "compression_ratio": 1.3903345724907064, "no_speech_prob": 0.004515758715569973}, {"id": 142, "seek": 58436, "start": 584.5600000000001, "end": 589.72, "text": " Po pierwsze, im wi\u0119cej r\u00f3\u017cnych wariant\u00f3w pr\u0105pt\u00f3w na jedno zadanie u\u017cyto w treningu,", "tokens": [50374, 6165, 45994, 11, 566, 26004, 42602, 1516, 5798, 3901, 582, 1611, 662, 3901, 1667, 5232, 1771, 42788, 7155, 34097, 1353, 261, 2192, 773, 84, 11, 50632], "temperature": 0.0, "avg_logprob": -0.09925459336864856, "compression_ratio": 1.4817518248175183, "no_speech_prob": 0.002623731503263116}, {"id": 143, "seek": 58436, "start": 589.92, "end": 593.52, "text": " tym model by\u0142 nie tylko lepszy, ale te\u017c bardziej stabilny.", "tokens": [50642, 8107, 2316, 16673, 2838, 13219, 476, 1878, 1229, 11, 6775, 9516, 27209, 11652, 1634, 13, 50822], "temperature": 0.0, "avg_logprob": -0.09925459336864856, "compression_ratio": 1.4817518248175183, "no_speech_prob": 0.002623731503263116}, {"id": 144, "seek": 58436, "start": 593.72, "end": 598.16, "text": " Znacz\u0105co zmniejsza\u0142a si\u0119 wariancja wynik\u00f3w w zale\u017cno\u015bci od u\u017cytego polecenia.", "tokens": [50832, 1176, 77, 326, 8925, 1291, 17020, 30295, 2394, 5024, 3244, 1516, 952, 34056, 31936, 1035, 3901, 261, 710, 45494, 16438, 3611, 34097, 975, 1571, 13208, 13037, 654, 13, 51054], "temperature": 0.0, "avg_logprob": -0.09925459336864856, "compression_ratio": 1.4817518248175183, "no_speech_prob": 0.002623731503263116}, {"id": 145, "seek": 58436, "start": 598.36, "end": 605.32, "text": " Innymi s\u0142owy r\u00f3\u017cnorodno\u015b\u0107 w edukacji przek\u0142ada si\u0119 na elastyczno\u015b\u0107 i niezawodno\u015b\u0107 w dzia\u0142aniu.", "tokens": [51064, 682, 31813, 15116, 10089, 19637, 19048, 378, 23293, 261, 1257, 2034, 13152, 29785, 46217, 3244, 1667, 806, 9820, 3689, 23293, 741, 33511, 1607, 378, 23293, 261, 27121, 25849, 13, 51412], "temperature": 0.0, "avg_logprob": -0.09925459336864856, "compression_ratio": 1.4817518248175183, "no_speech_prob": 0.002623731503263116}, {"id": 146, "seek": 58436, "start": 605.52, "end": 609.64, "text": " Model nie uczy si\u0119 sztuczek, tylko faktycznej umiej\u0119tno\u015bci.", "tokens": [51422, 17105, 2838, 344, 6522, 3244, 262, 2682, 1311, 19878, 11, 13219, 33647, 874, 3689, 11794, 1105, 7764, 46788, 16438, 13, 51628], "temperature": 0.0, "avg_logprob": -0.09925459336864856, "compression_ratio": 1.4817518248175183, "no_speech_prob": 0.002623731503263116}, {"id": 147, "seek": 60964, "start": 609.8, "end": 616.36, "text": " Tak. Po drugie, zwi\u0119kszenie liczby r\u00f3\u017cnych zada\u0144 w mikse treningowym r\u00f3wnie\u017c poprawi\u0142o median\u0119 wynik\u00f3w,", "tokens": [50372, 9118, 13, 6165, 4110, 414, 11, 11873, 5034, 1694, 16778, 6169, 89, 2322, 42602, 710, 1538, 5248, 261, 23959, 405, 2192, 773, 31691, 20532, 1665, 5131, 72, 5249, 1205, 952, 1274, 31936, 1035, 3901, 11, 50700], "temperature": 0.0, "avg_logprob": -0.12698005580302305, "compression_ratio": 1.4867549668874172, "no_speech_prob": 0.023023413494229317}, {"id": 148, "seek": 60964, "start": 616.56, "end": 619.64, "text": " ale co ciekawe niekoniecznie wp\u0142yn\u0119\u0142o na stabilno\u015b\u0107.", "tokens": [50710, 6775, 598, 30596, 2330, 826, 2838, 18295, 414, 19923, 32444, 1221, 2534, 1274, 5249, 1667, 11652, 23293, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12698005580302305, "compression_ratio": 1.4867549668874172, "no_speech_prob": 0.023023413494229317}, {"id": 149, "seek": 60964, "start": 619.84, "end": 627.8, "text": " To sugeruje, \u017ce g\u0142\u0119bia, czyli wiele pr\u0105pt\u00f3w na zadanie, jest wa\u017cniejsza dla odporno\u015bci ni\u017c szeroko\u015b\u0107, czyli wiele r\u00f3\u017cnych zada\u0144.", "tokens": [50874, 1407, 459, 1321, 13008, 11, 3561, 18117, 1274, 26975, 11, 16591, 33137, 582, 1611, 662, 3901, 1667, 42788, 7155, 11, 3492, 27777, 30295, 2394, 12285, 3611, 2816, 16438, 28502, 36160, 13704, 7753, 11, 16591, 33137, 42602, 710, 1538, 5248, 13, 51272], "temperature": 0.0, "avg_logprob": -0.12698005580302305, "compression_ratio": 1.4867549668874172, "no_speech_prob": 0.023023413494229317}, {"id": 150, "seek": 60964, "start": 628.0, "end": 628.96, "text": " Ciekowe.", "tokens": [51282, 383, 19487, 6880, 13, 51330], "temperature": 0.0, "avg_logprob": -0.12698005580302305, "compression_ratio": 1.4867549668874172, "no_speech_prob": 0.023023413494229317}, {"id": 151, "seek": 60964, "start": 629.16, "end": 633.96, "text": " I na koniec zrobili bezpo\u015brednie por\u00f3wnanie z GPT-3 na zbiorze danych RTE.", "tokens": [51340, 286, 1667, 5897, 35733, 44399, 2312, 10782, 2259, 1788, 986, 2766, 1515, 812, 895, 7155, 710, 26039, 51, 12, 18, 1667, 710, 33362, 1381, 274, 34644, 497, 13639, 13, 51580], "temperature": 0.0, "avg_logprob": -0.12698005580302305, "compression_ratio": 1.4867549668874172, "no_speech_prob": 0.023023413494229317}, {"id": 152, "seek": 60964, "start": 634.16, "end": 637.4, "text": " Okaza\u0142o si\u0119, \u017ce GPT-3 by\u0142 niezwykle wra\u017cliwy.", "tokens": [51590, 3477, 12257, 5249, 3244, 11, 3561, 26039, 51, 12, 18, 16673, 33511, 9726, 14677, 7843, 1427, 2081, 9726, 13, 51752], "temperature": 0.0, "avg_logprob": -0.12698005580302305, "compression_ratio": 1.4867549668874172, "no_speech_prob": 0.023023413494229317}, {"id": 153, "seek": 63740, "start": 637.6, "end": 641.6, "text": " Osi\u0105ga\u0142 dobre wyniki praktycznie tylko na jednym konkretnym pr\u0105bcie.", "tokens": [50374, 422, 7691, 1611, 3680, 1221, 41959, 31936, 9850, 3206, 74, 45586, 13219, 1667, 5232, 12996, 36500, 12996, 582, 1611, 65, 4260, 13, 50574], "temperature": 0.0, "avg_logprob": -0.11637547810872396, "compression_ratio": 1.4802259887005649, "no_speech_prob": 0.020346589386463165}, {"id": 154, "seek": 63740, "start": 641.8, "end": 648.52, "text": " A T-0 by\u0142 znacznie bardziej odporny, daj\u0105c solidne, powtarzalne wyniki dla wielu r\u00f3\u017cnych sformu\u0142owa\u0144.", "tokens": [50584, 316, 314, 12, 15, 16673, 15397, 14875, 2766, 27209, 3611, 2816, 1634, 11, 1120, 8555, 66, 5100, 716, 11, 3388, 23480, 89, 304, 716, 31936, 9850, 12285, 40437, 42602, 262, 837, 84, 1221, 5528, 5248, 13, 50920], "temperature": 0.0, "avg_logprob": -0.11637547810872396, "compression_ratio": 1.4802259887005649, "no_speech_prob": 0.020346589386463165}, {"id": 155, "seek": 63740, "start": 648.72, "end": 651.36, "text": " Dobrze. To wszystko uk\u0142ada si\u0119 w tak\u0105 sp\u00f3jn\u0105 ca\u0142o\u015b\u0107.", "tokens": [50930, 29679, 13503, 13, 1407, 22607, 26769, 46217, 3244, 261, 31069, 637, 18999, 13113, 1335, 44742, 13, 51062], "temperature": 0.0, "avg_logprob": -0.11637547810872396, "compression_ratio": 1.4802259887005649, "no_speech_prob": 0.020346589386463165}, {"id": 156, "seek": 63740, "start": 651.56, "end": 654.8, "text": " Mamy mniejszy, wydajniejszy i bardziej odporny model.", "tokens": [51072, 376, 7804, 39513, 7706, 11, 25984, 1805, 10402, 7706, 741, 27209, 3611, 2816, 1634, 2316, 13, 51234], "temperature": 0.0, "avg_logprob": -0.11637547810872396, "compression_ratio": 1.4802259887005649, "no_speech_prob": 0.020346589386463165}, {"id": 157, "seek": 63740, "start": 655.0, "end": 656.68, "text": " Spr\u00f3bujmy teraz po\u0142\u0105czy\u0107 te kropki.", "tokens": [51244, 7702, 14216, 4579, 2226, 16854, 714, 15926, 33967, 535, 350, 1513, 2984, 13, 51328], "temperature": 0.0, "avg_logprob": -0.11637547810872396, "compression_ratio": 1.4802259887005649, "no_speech_prob": 0.020346589386463165}, {"id": 158, "seek": 63740, "start": 656.88, "end": 658.16, "text": " Jakie s\u0105 z tego wnioski?", "tokens": [51338, 15029, 414, 9015, 710, 8627, 45368, 2717, 2984, 30, 51402], "temperature": 0.0, "avg_logprob": -0.11637547810872396, "compression_ratio": 1.4802259887005649, "no_speech_prob": 0.020346589386463165}, {"id": 159, "seek": 63740, "start": 658.36, "end": 661.0, "text": " Dlaczego ta praca jest tak wa\u017cna dla ca\u0142ej dziedziny i?", "tokens": [51412, 413, 75, 39329, 1846, 582, 6628, 3492, 991, 27777, 629, 12285, 47631, 73, 9758, 15338, 3519, 741, 30, 51544], "temperature": 0.0, "avg_logprob": -0.11637547810872396, "compression_ratio": 1.4802259887005649, "no_speech_prob": 0.020346589386463165}, {"id": 160, "seek": 63740, "start": 661.1999999999999, "end": 667.0799999999999, "text": " Je\u015bli spojrzymy na szerszy obraz, to badanie pokazuje alternatywn\u0105, by\u0107 mo\u017ce bardziej zr\u00f3wnowa\u017con\u0105", "tokens": [51554, 37086, 8243, 73, 13047, 2226, 1667, 7870, 433, 1229, 22798, 89, 11, 281, 1578, 7155, 13010, 43317, 5400, 21398, 895, 1611, 11, 15069, 12034, 27209, 710, 11721, 895, 5528, 1427, 266, 1611, 51848], "temperature": 0.0, "avg_logprob": -0.11637547810872396, "compression_ratio": 1.4802259887005649, "no_speech_prob": 0.020346589386463165}, {"id": 161, "seek": 66708, "start": 667.08, "end": 669.0400000000001, "text": " i inteligentn\u0105 \u015bcie\u017ck\u0119 rozwoju.", "tokens": [50364, 741, 24777, 25002, 13113, 8299, 40082, 15724, 9544, 6120, 8954, 13, 50462], "temperature": 0.0, "avg_logprob": -0.1216823367253403, "compression_ratio": 1.473529411764706, "no_speech_prob": 0.01681801863014698}, {"id": 162, "seek": 66708, "start": 669.24, "end": 675.4000000000001, "text": " Zamiast bezko\u0144ca skalowa\u0107 modele, co wi\u0105\u017ce si\u0119 z astronomicznymi kosztami i ogromnym zu\u017cyciem energii,", "tokens": [50472, 1176, 4526, 525, 10782, 4093, 5248, 496, 16890, 11445, 4391, 306, 11, 598, 261, 11404, 2875, 3244, 710, 26302, 17946, 31813, 19532, 2682, 4526, 741, 34416, 298, 12996, 2164, 7735, 4260, 76, 10575, 5597, 11, 50780], "temperature": 0.0, "avg_logprob": -0.1216823367253403, "compression_ratio": 1.473529411764706, "no_speech_prob": 0.01681801863014698}, {"id": 163, "seek": 66708, "start": 675.6, "end": 679.0, "text": " mo\u017cna osi\u0105gn\u0105\u0107 por\u00f3wnywalne, a czasem lepsze rezultaty.", "tokens": [50790, 17790, 3003, 11404, 4568, 36374, 1515, 812, 895, 27112, 304, 716, 11, 257, 13190, 443, 476, 1878, 1381, 48060, 723, 21398, 13, 50960], "temperature": 0.0, "avg_logprob": -0.1216823367253403, "compression_ratio": 1.473529411764706, "no_speech_prob": 0.01681801863014698}, {"id": 164, "seek": 66708, "start": 679.2, "end": 685.6, "text": " Kluczem jest inteligentniejszy nadzorowany training, czyli supervised training na bardzo zr\u00f3\u017cnicowanych zadaniach.", "tokens": [50970, 16053, 1311, 24313, 3492, 24777, 25002, 10402, 7706, 12617, 89, 284, 23341, 3097, 11, 16591, 46533, 3097, 1667, 9034, 710, 11721, 1427, 7692, 23341, 339, 42788, 3782, 608, 13, 51290], "temperature": 0.0, "avg_logprob": -0.1216823367253403, "compression_ratio": 1.473529411764706, "no_speech_prob": 0.01681801863014698}, {"id": 165, "seek": 66708, "start": 685.8000000000001, "end": 690.5600000000001, "text": " To jest, wiesz, fundamentalne przej\u015bcie od paradigmatu brutalnej si\u0142y do paradigmatu finezji.", "tokens": [51300, 1407, 3492, 11, 261, 15347, 11, 8088, 716, 8325, 73, 9815, 3611, 13480, 328, 15677, 84, 17878, 11794, 1511, 6825, 360, 13480, 328, 15677, 84, 2489, 89, 4013, 13, 51538], "temperature": 0.0, "avg_logprob": -0.1216823367253403, "compression_ratio": 1.473529411764706, "no_speech_prob": 0.01681801863014698}, {"id": 166, "seek": 66708, "start": 690.76, "end": 695.5600000000001, "text": " Warto te\u017c wspomnie\u0107, \u017ce mniej wi\u0119cej w tym samym czasie inny zesp\u00f3\u0142 z Google", "tokens": [51548, 343, 15864, 9516, 17757, 298, 2766, 2162, 11, 3561, 39513, 26004, 261, 8107, 3247, 4199, 42667, 294, 1634, 710, 13361, 16181, 710, 3329, 51788], "temperature": 0.0, "avg_logprob": -0.1216823367253403, "compression_ratio": 1.473529411764706, "no_speech_prob": 0.01681801863014698}, {"id": 167, "seek": 69556, "start": 695.56, "end": 698.4, "text": " opublikowa\u0142 prac\u0119 o modelu Flan.", "tokens": [50364, 999, 48620, 30105, 22404, 1274, 277, 2316, 84, 3235, 282, 13, 50506], "temperature": 0.0, "avg_logprob": -0.13081400641079607, "compression_ratio": 1.3992805755395683, "no_speech_prob": 0.20924456417560577}, {"id": 168, "seek": 69556, "start": 698.5999999999999, "end": 700.9599999999999, "text": " On bazowa\u0142 na bardzo podobne pomy\u015ble.", "tokens": [50516, 1282, 27147, 30105, 1667, 9034, 43024, 716, 280, 8488, 1788, 306, 13, 50634], "temperature": 0.0, "avg_logprob": -0.13081400641079607, "compression_ratio": 1.3992805755395683, "no_speech_prob": 0.20924456417560577}, {"id": 169, "seek": 69556, "start": 701.16, "end": 703.4, "text": " Jak T0 wypada w tym por\u00f3wnaniu?", "tokens": [50644, 15029, 314, 15, 46392, 1538, 261, 8107, 1515, 812, 895, 25849, 30, 50756], "temperature": 0.0, "avg_logprob": -0.13081400641079607, "compression_ratio": 1.3992805755395683, "no_speech_prob": 0.20924456417560577}, {"id": 170, "seek": 69556, "start": 703.5999999999999, "end": 708.0, "text": " Czy to by\u0142 taki wy\u015bcig, w kt\u00f3rym obie dru\u017cyny wpad\u0142y na met\u0119 r\u00f3wnocze\u015bnie?", "tokens": [50766, 19832, 281, 16673, 20065, 4628, 1788, 66, 328, 11, 261, 30120, 1111, 414, 38864, 7735, 1634, 261, 13647, 6825, 1667, 1131, 1274, 11416, 895, 905, 1381, 12221, 30, 50986], "temperature": 0.0, "avg_logprob": -0.13081400641079607, "compression_ratio": 1.3992805755395683, "no_speech_prob": 0.20924456417560577}, {"id": 171, "seek": 69556, "start": 708.1999999999999, "end": 711.76, "text": " To bardzo dobre pytanie, bo pokazuje, jak wa\u017cne s\u0105 niuanse.", "tokens": [50996, 1407, 9034, 41959, 36610, 11, 748, 13010, 43317, 11, 4207, 46110, 9015, 3867, 6139, 405, 13, 51174], "temperature": 0.0, "avg_logprob": -0.13081400641079607, "compression_ratio": 1.3992805755395683, "no_speech_prob": 0.20924456417560577}, {"id": 172, "seek": 69556, "start": 711.9599999999999, "end": 715.3199999999999, "text": " Cho\u0107 idea by\u0142a podobna, diabe\u0142t kwi\u0142\u0142 w szczeg\u00f3\u0142ach.", "tokens": [51184, 12366, 2162, 1558, 23936, 43024, 629, 11, 1026, 4488, 1221, 83, 350, 6253, 1221, 1221, 261, 22090, 1146, 16181, 608, 13, 51352], "temperature": 0.0, "avg_logprob": -0.13081400641079607, "compression_ratio": 1.3992805755395683, "no_speech_prob": 0.20924456417560577}, {"id": 173, "seek": 69556, "start": 715.52, "end": 720.52, "text": " T0 okaza\u0142 si\u0119 wyra\u017anie lepszy od modeli Flan o por\u00f3wnywalnej wielko\u015bci.", "tokens": [51362, 314, 15, 3133, 12257, 1221, 3244, 4628, 424, 10659, 2766, 476, 1878, 1229, 3611, 2316, 72, 3235, 282, 277, 1515, 812, 895, 27112, 304, 11794, 20570, 4093, 6199, 13, 51612], "temperature": 0.0, "avg_logprob": -0.13081400641079607, "compression_ratio": 1.3992805755395683, "no_speech_prob": 0.20924456417560577}, {"id": 174, "seek": 72052, "start": 720.72, "end": 726.0799999999999, "text": " I autorzu pracy o T0 sugeruj\u0105 dwa kluczowe czynniki, kt\u00f3re mog\u0142y o tym zadecydowa\u0107.", "tokens": [50374, 286, 19510, 11728, 35591, 277, 314, 15, 459, 1321, 13263, 35045, 9671, 1311, 89, 6880, 6430, 26384, 9850, 11, 8864, 13172, 6825, 277, 8107, 710, 762, 1344, 67, 11445, 13, 50642], "temperature": 0.0, "avg_logprob": -0.11669322046740302, "compression_ratio": 1.5256410256410255, "no_speech_prob": 0.12443647533655167}, {"id": 175, "seek": 72052, "start": 726.28, "end": 729.48, "text": " Po pierwsze, architektura i metoda pretreningu.", "tokens": [50652, 6165, 45994, 11, 3912, 642, 2320, 2991, 741, 1131, 13449, 1162, 265, 773, 84, 13, 50812], "temperature": 0.0, "avg_logprob": -0.11669322046740302, "compression_ratio": 1.5256410256410255, "no_speech_prob": 0.12443647533655167}, {"id": 176, "seek": 72052, "start": 729.68, "end": 735.4, "text": " T0 bazuje na architekturze encoder decoder i pretreningu zwany masked language modeling.", "tokens": [50822, 314, 15, 27147, 13008, 1667, 3912, 642, 2320, 374, 1381, 2058, 19866, 979, 19866, 741, 1162, 265, 773, 84, 11873, 1325, 45249, 2856, 15983, 13, 51108], "temperature": 0.0, "avg_logprob": -0.11669322046740302, "compression_ratio": 1.5256410256410255, "no_speech_prob": 0.12443647533655167}, {"id": 177, "seek": 72052, "start": 735.6, "end": 738.92, "text": " To historycznie jest bardziej efektywna strategia dla zada\u0144 typu", "tokens": [51118, 1407, 2503, 19923, 3492, 27209, 31482, 916, 874, 86, 629, 5464, 654, 12285, 710, 1538, 5248, 2125, 84, 51284], "temperature": 0.0, "avg_logprob": -0.11669322046740302, "compression_ratio": 1.5256410256410255, "no_speech_prob": 0.12443647533655167}, {"id": 178, "seek": 72052, "start": 739.12, "end": 744.3199999999999, "text": " fine tuning ni\u017c standardowe modelowanie j\u0119zykowe decoder only, na kt\u00f3rym bazowa\u0142 Flan.", "tokens": [51294, 2489, 15164, 28502, 3832, 6880, 2316, 22028, 49055, 74, 6880, 979, 19866, 787, 11, 1667, 30120, 27147, 30105, 3235, 282, 13, 51554], "temperature": 0.0, "avg_logprob": -0.11669322046740302, "compression_ratio": 1.5256410256410255, "no_speech_prob": 0.12443647533655167}, {"id": 179, "seek": 72052, "start": 744.52, "end": 749.92, "text": " Czyli sam fundament, na kt\u00f3rym budowano, by\u0142 troch\u0119 solidniejszy dla tego konkretnego celu.", "tokens": [51564, 37099, 3247, 6073, 11, 1667, 30120, 3265, 305, 3730, 11, 16673, 24926, 5100, 10402, 7706, 12285, 8627, 36500, 11858, 9277, 84, 13, 51834], "temperature": 0.0, "avg_logprob": -0.11669322046740302, "compression_ratio": 1.5256410256410255, "no_speech_prob": 0.12443647533655167}, {"id": 180, "seek": 74992, "start": 750.12, "end": 751.12, "text": " A ten drugi czynnik?", "tokens": [50374, 316, 2064, 4110, 72, 6430, 77, 13123, 30, 50424], "temperature": 0.0, "avg_logprob": -0.15228010057569383, "compression_ratio": 1.4914675767918089, "no_speech_prob": 0.008324560709297657}, {"id": 181, "seek": 74992, "start": 751.3199999999999, "end": 755.8399999999999, "text": " Drugim i by\u0107 mo\u017ce wa\u017cniejszym czynnikiem by\u0142a w\u0142a\u015bnie ta niesamowita", "tokens": [50434, 35806, 332, 741, 15069, 12034, 27777, 10402, 7706, 76, 6430, 77, 13123, 4907, 23936, 14234, 1846, 48100, 335, 305, 2786, 50660], "temperature": 0.0, "avg_logprob": -0.15228010057569383, "compression_ratio": 1.4914675767918089, "no_speech_prob": 0.008324560709297657}, {"id": 182, "seek": 74992, "start": 756.04, "end": 759.76, "text": " organiczna r\u00f3\u017cnorodno\u015b\u0107 prompt\u00f3w zebranych dzi\u0119ki prompt source.", "tokens": [50670, 10220, 35458, 19637, 19048, 378, 23293, 12391, 3901, 5277, 1443, 34644, 45003, 12391, 4009, 13, 50856], "temperature": 0.0, "avg_logprob": -0.15228010057569383, "compression_ratio": 1.4914675767918089, "no_speech_prob": 0.008324560709297657}, {"id": 183, "seek": 74992, "start": 759.9599999999999, "end": 764.52, "text": " Warto to zobrazowa\u0107. Flan m\u00f3g\u0142 mie\u0107 dla danego zadania 10 szablon\u00f3w.", "tokens": [50866, 343, 15864, 281, 710, 24393, 89, 11445, 13, 3235, 282, 275, 14047, 1221, 35612, 12285, 3277, 6308, 42788, 5609, 1266, 7870, 455, 14864, 3901, 13, 51094], "temperature": 0.0, "avg_logprob": -0.15228010057569383, "compression_ratio": 1.4914675767918089, "no_speech_prob": 0.008324560709297657}, {"id": 184, "seek": 74992, "start": 764.7199999999999, "end": 769.92, "text": " W wi\u0119kszo\u015bci stworzone automatycznie, na przyk\u0142ad, przet\u0142umacz to zdanie na j\u0119zyk francuski.", "tokens": [51104, 343, 29968, 4765, 6199, 342, 28321, 16896, 28034, 17466, 2766, 11, 1667, 23144, 11, 6541, 302, 49166, 14875, 281, 16221, 7155, 1667, 49055, 74, 431, 282, 1149, 2984, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15228010057569383, "compression_ratio": 1.4914675767918089, "no_speech_prob": 0.008324560709297657}, {"id": 185, "seek": 74992, "start": 770.12, "end": 774.04, "text": " A dzi\u0119ki prompt source T0 mia\u0142o mo\u017ce z 50 wariant\u00f3w.", "tokens": [51374, 316, 45003, 12391, 4009, 314, 15, 21290, 5249, 12034, 710, 2625, 1516, 5798, 3901, 13, 51570], "temperature": 0.0, "avg_logprob": -0.15228010057569383, "compression_ratio": 1.4914675767918089, "no_speech_prob": 0.008324560709297657}, {"id": 186, "seek": 74992, "start": 774.24, "end": 777.3199999999999, "text": " W tym, jakby to zabrzmia\u0142o po francusku,", "tokens": [51580, 343, 8107, 11, 28976, 281, 24838, 19390, 29958, 5249, 714, 431, 282, 1149, 5279, 11, 51734], "temperature": 0.0, "avg_logprob": -0.15228010057569383, "compression_ratio": 1.4914675767918089, "no_speech_prob": 0.008324560709297657}, {"id": 187, "seek": 77732, "start": 777.44, "end": 782.12, "text": " potrzebuje angielskiej wersji tego tekstu albo wyobra\u017a sobie, \u017ce jeste\u015b t\u0142umaczem.", "tokens": [50370, 28577, 6021, 2884, 2562, 1187, 5161, 7764, 261, 433, 4013, 8627, 16624, 372, 84, 22622, 4628, 24393, 10659, 13652, 11, 3561, 25255, 1788, 256, 49166, 14875, 443, 13, 50604], "temperature": 0.0, "avg_logprob": -0.08611901601155598, "compression_ratio": 1.5044776119402985, "no_speech_prob": 0.013309174217283726}, {"id": 188, "seek": 77732, "start": 782.32, "end": 786.96, "text": " Ta ludzka kreatywno\u015b\u0107 i r\u00f3\u017cnorodno\u015b\u0107 to jako\u015bciowa, a nie tylko ilo\u015bciowa przewaga.", "tokens": [50614, 6551, 15946, 89, 2330, 350, 620, 88, 20944, 7753, 741, 19637, 19048, 378, 23293, 281, 17123, 6199, 5528, 11, 257, 2838, 13219, 1930, 44468, 5528, 39758, 9286, 13, 50846], "temperature": 0.0, "avg_logprob": -0.08611901601155598, "compression_ratio": 1.5044776119402985, "no_speech_prob": 0.013309174217283726}, {"id": 189, "seek": 77732, "start": 787.1600000000001, "end": 791.24, "text": " To doskonale pokazuje si\u0142\u0119 wsp\u00f3\u0142pracy spo\u0142eczno\u015bci i to, \u017ce ludzka", "tokens": [50856, 1407, 4491, 18295, 1220, 13010, 43317, 1511, 46564, 39069, 1424, 2551, 36851, 89, 16438, 741, 281, 11, 3561, 15946, 89, 2330, 51060], "temperature": 0.0, "avg_logprob": -0.08611901601155598, "compression_ratio": 1.5044776119402985, "no_speech_prob": 0.013309174217283726}, {"id": 190, "seek": 77732, "start": 791.44, "end": 796.44, "text": " pomys\u0142owo\u015b\u0107 w formu\u0142owaniu problem\u00f3w jest zasobem, kt\u00f3rego nie da si\u0119 \u0142atwo zautomatyzowa\u0107.", "tokens": [51070, 12991, 39508, 19941, 7753, 261, 1254, 84, 1221, 305, 25849, 1154, 3901, 3492, 26530, 996, 443, 11, 46951, 2838, 1120, 3244, 47759, 6120, 710, 1375, 298, 21398, 89, 11445, 13, 51320], "temperature": 0.0, "avg_logprob": -0.08611901601155598, "compression_ratio": 1.5044776119402985, "no_speech_prob": 0.013309174217283726}, {"id": 191, "seek": 77732, "start": 796.6400000000001, "end": 799.72, "text": " Oczywi\u015bcie \u017cadna praca nie jest pozbawiona ogranicze\u0144.", "tokens": [51330, 42980, 39628, 629, 582, 6628, 2838, 3492, 21281, 65, 1607, 21758, 34416, 30732, 49689, 13, 51484], "temperature": 0.0, "avg_logprob": -0.08611901601155598, "compression_ratio": 1.5044776119402985, "no_speech_prob": 0.013309174217283726}, {"id": 192, "seek": 77732, "start": 799.9200000000001, "end": 802.44, "text": " Czy autorzy wskazuj\u0105 na jakie\u015b otwarte pytania?", "tokens": [51494, 19832, 19510, 1229, 261, 5161, 921, 13263, 1667, 31163, 4337, 86, 11026, 25878, 5609, 30, 51620], "temperature": 0.0, "avg_logprob": -0.08611901601155598, "compression_ratio": 1.5044776119402985, "no_speech_prob": 0.013309174217283726}, {"id": 193, "seek": 77732, "start": 802.6400000000001, "end": 806.12, "text": " Oczywi\u015bcie i to jest oznaka dobrej nauki.", "tokens": [51630, 42980, 741, 281, 3492, 277, 22672, 7849, 41959, 73, 35616, 2984, 13, 51804], "temperature": 0.0, "avg_logprob": -0.08611901601155598, "compression_ratio": 1.5044776119402985, "no_speech_prob": 0.013309174217283726}, {"id": 194, "seek": 80612, "start": 806.24, "end": 811.32, "text": " Sami wskazuj\u0105, \u017ce ich wyniki, cho\u0107 imponuj\u0105ce, rodz\u0105 nowe pytania.", "tokens": [50370, 44029, 261, 5161, 921, 13263, 11, 3561, 1893, 31936, 9850, 11, 1586, 2162, 704, 266, 13263, 384, 11, 8685, 8925, 586, 68, 25878, 5609, 13, 50624], "temperature": 0.0, "avg_logprob": -0.11346244812011719, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.044249873608350754}, {"id": 195, "seek": 80612, "start": 811.52, "end": 815.16, "text": " Na przyk\u0142ad zaobserwowali, \u017ce proste zwi\u0119kszanie liczby zada\u0144 w miksie", "tokens": [50634, 6056, 23144, 7949, 16537, 260, 34354, 5103, 11, 3561, 10293, 68, 11873, 5034, 1694, 89, 7155, 6169, 89, 2322, 710, 1538, 5248, 261, 275, 23292, 414, 50816], "temperature": 0.0, "avg_logprob": -0.11346244812011719, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.044249873608350754}, {"id": 196, "seek": 80612, "start": 815.36, "end": 818.92, "text": " treningowym nie zawsze zmniejsza\u0142o wahania w wynikach.", "tokens": [50826, 2192, 773, 31691, 2838, 30964, 17020, 30295, 2394, 5249, 31979, 5609, 261, 31936, 1035, 608, 13, 51004], "temperature": 0.0, "avg_logprob": -0.11346244812011719, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.044249873608350754}, {"id": 197, "seek": 80612, "start": 819.12, "end": 823.64, "text": " Zrozumienie, dlaczego tak si\u0119 dzieje, kt\u00f3re zadania najlepiej ze sob\u0105 wsp\u00f3\u0142graj\u0105,", "tokens": [51014, 1176, 27857, 449, 27385, 11, 37873, 39329, 991, 3244, 17953, 2884, 11, 8864, 42788, 5609, 41903, 39699, 5277, 18253, 1611, 39069, 20735, 8555, 11, 51240], "temperature": 0.0, "avg_logprob": -0.11346244812011719, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.044249873608350754}, {"id": 198, "seek": 80612, "start": 823.84, "end": 828.64, "text": " a kt\u00f3re si\u0119 powiedzmy kwuc\u0105, to pole do dalszych, fascynuj\u0105cych bada\u0144.", "tokens": [51250, 257, 8864, 3244, 27617, 2226, 350, 86, 1311, 1611, 11, 281, 13208, 360, 274, 1124, 28051, 11, 30632, 1344, 77, 13263, 31306, 272, 1538, 5248, 13, 51490], "temperature": 0.0, "avg_logprob": -0.11346244812011719, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.044249873608350754}, {"id": 199, "seek": 80612, "start": 828.84, "end": 831.8, "text": " To troch\u0119 jak tworzenie idealnego programu nauczania.", "tokens": [51500, 1407, 24926, 4207, 46288, 16778, 7157, 11858, 1461, 84, 49103, 89, 5609, 13, 51648], "temperature": 0.0, "avg_logprob": -0.11346244812011719, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.044249873608350754}, {"id": 200, "seek": 80612, "start": 832.0, "end": 834.96, "text": " Trzeba widzie\u0107, kt\u00f3re przedmioty wzajemnie si\u0119 wzmacniaj\u0105.", "tokens": [51658, 1765, 1381, 4231, 5274, 21214, 11, 8864, 18334, 3057, 6737, 24809, 1805, 443, 2766, 3244, 24809, 37065, 12679, 8555, 13, 51806], "temperature": 0.0, "avg_logprob": -0.11346244812011719, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.044249873608350754}, {"id": 201, "seek": 83496, "start": 835.2, "end": 841.32, "text": " Podsumowuj\u0105c model T0 i ca\u0142a stoj\u0105ca za nim metodologia, to pot\u0119\u017cny dow\u00f3d na to,", "tokens": [50376, 12646, 82, 449, 305, 44733, 2316, 314, 15, 741, 1335, 5024, 22784, 8555, 496, 7949, 24887, 1131, 378, 24103, 11, 281, 1847, 1274, 1427, 1634, 9459, 17081, 1667, 281, 11, 50682], "temperature": 0.0, "avg_logprob": -0.1281560308793012, "compression_ratio": 1.4190140845070423, "no_speech_prob": 0.002373815979808569}, {"id": 202, "seek": 83496, "start": 841.52, "end": 847.0, "text": " \u017ce jawne uczenie wielozadaniowe z u\u017cyciem r\u00f3\u017cnorodnych tworzonych przez ludzi pr\u0105pt\u00f3w", "tokens": [50692, 3561, 18162, 716, 344, 39043, 20570, 15151, 345, 3782, 6880, 710, 34097, 4260, 76, 19637, 19048, 378, 9399, 46288, 44479, 339, 14064, 29586, 582, 1611, 662, 3901, 50966], "temperature": 0.0, "avg_logprob": -0.1281560308793012, "compression_ratio": 1.4190140845070423, "no_speech_prob": 0.002373815979808569}, {"id": 203, "seek": 83496, "start": 847.2, "end": 849.6, "text": " jest niezwykle skuteczn\u0105 metod\u0105.", "tokens": [50976, 3492, 33511, 9726, 14677, 1110, 1169, 3689, 13113, 1131, 378, 1611, 13, 51096], "temperature": 0.0, "avg_logprob": -0.1281560308793012, "compression_ratio": 1.4190140845070423, "no_speech_prob": 0.002373815979808569}, {"id": 204, "seek": 83496, "start": 849.8000000000001, "end": 855.24, "text": " Nie liczy si\u0119 tylko rozmiar, ale przede wszystkim jako\u015b\u0107 i r\u00f3\u017cnorodno\u015b\u0107 tej edukacji.", "tokens": [51106, 12016, 6169, 1229, 3244, 13219, 9544, 3057, 289, 11, 6775, 44786, 30481, 17123, 7753, 741, 19637, 19048, 378, 23293, 12573, 1257, 2034, 13152, 13, 51378], "temperature": 0.0, "avg_logprob": -0.1281560308793012, "compression_ratio": 1.4190140845070423, "no_speech_prob": 0.002373815979808569}, {"id": 205, "seek": 83496, "start": 855.44, "end": 862.64, "text": " To taka zmiana perspektywy z chodowania cyfrowego potwora na wychowywanie inteligentnego agenta.", "tokens": [51388, 1407, 28017, 17020, 8497, 868, 32659, 874, 9726, 710, 417, 378, 21308, 3185, 69, 1892, 6308, 1847, 86, 3252, 1667, 4628, 339, 10089, 86, 7155, 24777, 25002, 11858, 623, 8938, 13, 51748], "temperature": 0.0, "avg_logprob": -0.1281560308793012, "compression_ratio": 1.4190140845070423, "no_speech_prob": 0.002373815979808569}, {"id": 206, "seek": 86264, "start": 862.84, "end": 866.12, "text": " I to prowadzi nas do ostatniej szerszej my\u015bli.", "tokens": [50374, 286, 281, 36590, 3992, 5382, 360, 32686, 10402, 7870, 433, 16920, 452, 15350, 13, 50538], "temperature": 0.0, "avg_logprob": -0.13702076099537036, "compression_ratio": 1.4191419141914192, "no_speech_prob": 0.002556327497586608}, {"id": 207, "seek": 86264, "start": 866.3199999999999, "end": 872.4399999999999, "text": " Autorzy Teziro stworzyli sw\u00f3j zbi\u00f3r pr\u0105pt\u00f3w dzi\u0119ki bezprecedensowej globalnej wsp\u00f3\u0142precy.", "tokens": [50548, 6049, 284, 1229, 1989, 89, 5182, 342, 28321, 1229, 2081, 1693, 18999, 710, 5614, 15614, 582, 1611, 662, 3901, 45003, 10782, 3712, 1232, 694, 21091, 4338, 11794, 39069, 3712, 1344, 13, 50854], "temperature": 0.0, "avg_logprob": -0.13702076099537036, "compression_ratio": 1.4191419141914192, "no_speech_prob": 0.002556327497586608}, {"id": 208, "seek": 86264, "start": 872.64, "end": 877.28, "text": " W miar\u0119 jak sztuczna inteligencja staje si\u0119 coraz bardziej integraln\u0105 cz\u0119\u015bci\u0105 naszego \u017cycia,", "tokens": [50864, 343, 2752, 289, 1274, 4207, 262, 2682, 1311, 35458, 24777, 3213, 34056, 342, 11153, 3244, 25899, 27209, 11573, 13113, 41314, 1611, 44517, 44343, 11, 51096], "temperature": 0.0, "avg_logprob": -0.13702076099537036, "compression_ratio": 1.4191419141914192, "no_speech_prob": 0.002556327497586608}, {"id": 209, "seek": 86264, "start": 877.48, "end": 883.04, "text": " jak\u0105 rol\u0119 powinna odgrywa\u0107 w\u0142a\u015bnie taka \u015bwiadoma ludzka wsp\u00f3\u0142paca w uczeniu tych modeli?", "tokens": [51106, 46719, 34109, 1274, 27310, 629, 3611, 70, 627, 25234, 14234, 28017, 21485, 345, 6440, 15946, 89, 2330, 39069, 79, 6628, 261, 344, 66, 39651, 15180, 2316, 72, 30, 51384], "temperature": 0.0, "avg_logprob": -0.13702076099537036, "compression_ratio": 1.4191419141914192, "no_speech_prob": 0.002556327497586608}, {"id": 210, "seek": 86264, "start": 883.24, "end": 888.72, "text": " To jest prowokuj\u0105ce pytanie, bo zastan\u00f3wmy si\u0119 nad tym, czy zamiast pozwala\u0107 modelom", "tokens": [51394, 1407, 3492, 45553, 453, 13263, 384, 36610, 11, 748, 36746, 282, 3901, 2226, 3244, 12617, 8107, 11, 6430, 710, 4526, 525, 40557, 5159, 2162, 2316, 298, 51668], "temperature": 0.0, "avg_logprob": -0.13702076099537036, "compression_ratio": 1.4191419141914192, "no_speech_prob": 0.002556327497586608}, {"id": 211, "seek": 88872, "start": 888.72, "end": 896.28, "text": " uczy\u0107 si\u0119 z ca\u0142ego chaotycznego, cz\u0119sto stronniczego i nie b\u00f3jmy si\u0119 tego s\u0142owa toksycznego internetu,", "tokens": [50364, 344, 33967, 3244, 710, 35224, 6308, 6294, 6737, 3689, 11858, 11, 34369, 45766, 7692, 27725, 741, 2838, 272, 18999, 2226, 3244, 8627, 15116, 5528, 281, 1694, 17466, 11858, 4705, 84, 11, 50742], "temperature": 0.0, "avg_logprob": -0.1478693869806105, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.4173824191093445}, {"id": 212, "seek": 88872, "start": 896.48, "end": 901.6, "text": " powinni\u015bmy i\u015b\u0107 w stron\u0119 bardziej celowego, wsp\u00f3lnego podej\u015bcia do ich treningu.", "tokens": [50752, 27310, 3722, 10513, 741, 7753, 261, 45766, 1274, 27209, 9277, 26576, 11, 47148, 11858, 7468, 73, 1788, 2755, 360, 1893, 2192, 773, 84, 13, 51008], "temperature": 0.0, "avg_logprob": -0.1478693869806105, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.4173824191093445}, {"id": 213, "seek": 88872, "start": 901.8000000000001, "end": 907.44, "text": " Czy to mog\u0142oby prowadzi\u0107 do tworzenia AI, kt\u00f3ra jest nie tylko inteligentniejsza w technicznym sensie,", "tokens": [51018, 19832, 281, 13172, 1221, 13944, 36590, 28496, 360, 46288, 14320, 7318, 11, 19456, 3492, 2838, 13219, 24777, 25002, 30295, 2394, 261, 1537, 17946, 12996, 2923, 414, 11, 51300], "temperature": 0.0, "avg_logprob": -0.1478693869806105, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.4173824191093445}, {"id": 214, "seek": 88872, "start": 907.64, "end": 911.28, "text": " ale te\u017c bezpieczniejsza, bardziej niezawodna i", "tokens": [51310, 6775, 9516, 47153, 3689, 30295, 2394, 11, 27209, 33511, 1607, 378, 629, 741, 51492], "temperature": 0.0, "avg_logprob": -0.1478693869806105, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.4173824191093445}, {"id": 215, "seek": 88872, "start": 911.48, "end": 915.1600000000001, "text": " lepiej dostosowana do z\u0142o\u017conych ludzkich warto\u015bci?", "tokens": [51502, 476, 39699, 20568, 329, 40458, 360, 710, 5249, 1427, 2526, 339, 15946, 30154, 480, 31830, 6199, 30, 51686], "temperature": 0.0, "avg_logprob": -0.1478693869806105, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.4173824191093445}, {"id": 216, "seek": 91516, "start": 915.16, "end": 919.1999999999999, "text": " Praca nad Te Zero zdaje si\u0119 mocno sugerowa\u0107, \u017ce odpowied\u017a grzmi tak.", "tokens": [50372, 2114, 6628, 12617, 1989, 17182, 16221, 11153, 3244, 34962, 1771, 459, 1321, 11445, 11, 3561, 36574, 10659, 677, 89, 3057, 991, 13, 50566], "temperature": 0.0, "avg_logprob": -0.3681666564941406, "compression_ratio": 0.9113924050632911, "no_speech_prob": 0.031741149723529816}], "language": "pl"}