{"text": " Witajcie w T-Deep Dive. Dzisiaj mamy, wydaje mi si\u0119 na stole, co\u015b, co mo\u017ce naprawd\u0119 zatrz\u0105\u015b\u0107 posadami wsp\u00f3\u0142czesnej sztucznej inteligencji. Co\u015b du\u017cego? Co\u015b bardzo du\u017cego. Wszyscy \u017cyjemy od lat w erze modeli Transformer. One nap\u0119dzaj\u0105 GPT, generuj\u0105 obrazy, t\u0142umocz\u0105, s\u0105 dos\u0142ownie wsz\u0119dzie. Ale maj\u0105 t\u0119 jedn\u0105 fundamentaln\u0105 wad\u0119. Wszyscy o niej wiedz\u0105, ale tak troch\u0119 staraj\u0105 si\u0119 j\u0105 omija\u0107. Kwadratowa z\u0142o\u017cono\u015b\u0107. Dok\u0142adnie. Im d\u0142u\u017csza sekwencja tekst, film, fragment DNA, tym wolniej i dro\u017cej to wszystko dzia\u0142a. I to nie liniowo, a wyk\u0142adnicz\u0105. To jak super samoch\u00f3d, kt\u00f3ry jest niesamowicie szybki, ale na ka\u017cdy kolejny kilometr pali coraz wi\u0119cej, a\u017c w ko\u0144cu po prostu staje. I dlatego w\u0142a\u015bnie dzisiaj przyjrzymy si\u0119 do g\u0142\u0119bnie pracy naukowej, kt\u00f3ra nie pr\u00f3buje tego problemu obej\u015b\u0107, ale rozwi\u0105za\u0107 u samych podstaw. M\u00f3wi\u0119 tu o artykule, kt\u00f3ry zrobi\u0142 mn\u00f3stwo szumu. Mamba. Linear Time Sequence Modeling with Selective State Spaces. Autorstwa Alberta Gui Tridao. I to nie jest, jak m\u00f3wisz, jaka\u015b drobna optymalizacja. To propozycja zupe\u0142nie nowej architektury. W\u0142a\u015bnie. I to jest nasze kluczowe pytanie na dzi\u015b. Czy to jest prawdziwa rewolucja? Czy Mamba to ten mityczny Transformer Killer, na kt\u00f3rego czekamy? Czy mo\u017ce, no wiesz, kolejna bardzo sprytna alternatywa, kt\u00f3ra znajdzie swoj\u0105 nisz\u0119? Chcemy zrozumie\u0107, co sprawia, \u017ce jest tak szybka, co wchniemy si\u0119 okrok. Zrozumiemy, dlaczego poprzednie pr\u00f3by, tak zwane State Space Models, czyli SSM, poleg\u0142y na polu j\u0119zykowym. Potem zanurkujemy w samoserce Mamby, ten ca\u0142y mechanizm selekcji, kt\u00f3ry jest jej prawdziwym prze\u0142omem. A potem pewnie in\u017cynieria, kt\u00f3ra za tym stoi. Tak, ten majsterszczyk, kt\u00f3ry pozwoli\u0142 po\u0142\u0105czy\u0107 selektywno\u015b\u0107 z niesamowit\u0105 pr\u0119dko\u015bci\u0105. A na koniec oczywi\u015bcie dowody, twarde dane wyniki. Zobaczymy, czy to tylko teoria, czy ju\u017c praktyka. Dobra, to zatrzymajmy si\u0119 na moment przy tych starych modelach SSM. Skoro by\u0142y tak wydajne, tak szybkie, to co im w\u0142a\u015bciwie stan\u0119\u0142o na drodze? Dlaczego nie mamy ich dzisiaj w ka\u017cdym czat bocie? A to jest idealne pytanie na start, bo ono prowadzi nas do sedna. T\u0105 wad\u0105 by\u0142a ich natura, kt\u00f3r\u0105 w artykule okre\u015blaj\u0105 jako Linear Time Invariance. W skr\u00f3cie LTI. Liniowa niezmienno\u015b\u0107 w czasie. Dok\u0142adnie. Wyobra\u017a sobie tak\u0105 lini\u0119 produkcyjn\u0105 w fabryce. Robot wykonuje na niej w k\u00f3\u0142ko t\u0119 sam\u0105 czynno\u015b\u0107. Nie obchodzi go, czy produkt jest czerwony, niebieski, ma\u0142y, du\u017cy. On zawsze robi to samo. Tak dzia\u0142a\u0142y modele LTI. Ich parametry by\u0142y sta\u0142e. Niezmienne, niezale\u017cne od tre\u015bci. Czyli to by\u0142o super dop\u00f3ki dane by\u0142y jednolite. Jak na przyk\u0142ad, nie wiem, sygna\u0142 audio, gdzie fizyka d\u017awi\u0119ku jest sta\u0142a. Dok\u0142adnie tak, ale j\u0119zyk. Albo genomika, to jest zupe\u0142nie inna bajka. Tu kontekst jest kr\u00f3lem. Znaczenie s\u0142owa zale\u017cy od tego, co by\u0142o pi\u0119\u0107 s\u0142\u00f3w wcze\u015bniej. Model musi rozumie\u0107, \u017ce zamek w zdaniu kr\u00f3l mieszka w zamku, to nie to samo, co zaci\u0105\u0142 mi si\u0119 zamek w kurtce. A modele LTI by\u0142y na to \u015blepe. W artykule jest ten \u015bwietny test, kt\u00f3ry to obna\u017ca, prawda? Zadanie selekty w coping. Tak, to jest genialne w swojej prostocie. W takiej normalnej wersji, coping, model dostaje sekwencj\u0119 i ma skopiowa\u0107 tokeny, kt\u00f3re s\u0105, powiedzmy, co dziesi\u0119\u0107 miejsc. OK. I model LTI mo\u017ce si\u0119 tego nauczy\u0107. Dzia\u0142a jak ten robot, mechanicznie kopiuje co dziesi\u0105ty element. Ale w selekty w coping zasady si\u0119 zmieniaj\u0105 i to diametralnie. Model ma skopiowa\u0107 tylko te tokeny, kt\u00f3re s\u0105 specjalnie oznaczone, a ich pozycje s\u0105 zupe\u0142nie losowe. Musi aktywnie wybra\u0107, co zapami\u0119ta\u0107, a co ola\u0107. I tu modele LTI kompletnie zawodz\u0105. Nie s\u0105, jak to pi\u0119knie uj\u0119li autorzy, content aware. Nie s\u0105 \u015bwiadome tre\u015bci. OK, czyli stare modele by\u0142y \u015blepe na tre\u015b\u0107. To jak gu i da\u0142, dali mambie jej. No, metaforyczne oczy. Jak j\u0105 nauczyli odr\u00f3\u017cnia\u0107 ziarno od plef? I to jest ten moment, w kt\u00f3rym pojawia si\u0119 ta prosta, ale absolutnie genialna zmiana. Zamiast sta\u0142ych, niezmiennych parametr\u00f3w, te kluczowe elementy mamby w artykule oznaczone jako Delta, B i C, sta\u0142y si\u0119 dynamicznymi funkcjami danych wejszczowych. Czyli dla ka\u017cdego tokena model liczy je na nowo. Dla ka\u017cdego jednego tokena. Model na nowo oblicza, jak ma si\u0119 zachowa\u0107. To ju\u017c nie jest ten g\u0142upi robot z fabryki. To bardziej mistrz rzemios\u0142a, kt\u00f3ry ogl\u0105da ka\u017cdy element i decyduje. Oten jest wa\u017cny, wzmacniam jego znaczenie, a potem, a ten to szum, ignoruj\u0119. To troch\u0119 tak, jak my czytamy ksi\u0105\u017ck\u0119. Nie pami\u0119tamy ka\u017cdego s\u0142owa z pierwszego rozdzia\u0142u, ale mamy w g\u0142owie takie skompresowane poczucie tego, co si\u0119 sta\u0142o, kim s\u0105 bohaterowie. Mamba uczy si\u0119 tworzy\u0107 taki zwarty, u\u017cyteczny stan umys\u0142u. To jest idealna analogia, bo Transformers z mechanizmem attention dzia\u0142a zupe\u0142nie inaczej. On trzyma wszystko w pami\u0119ci. Wszystko. \u017beby zrozumie\u0107 stron\u0119 setn\u0105, on w zasadzie por\u00f3wnuje ka\u017cde nowe s\u0142owo z ka\u017cdym s\u0142owem z poprzednich 99 stron. To pot\u0119\u017cne, ale no absurdalnie nieefektywne. Amamba. Amamba kompresuje wiedz\u0119 stycz 99 stron do zwi\u0119z\u0142ego stanu i na jego podstawie interpretuje setn\u0105 stron\u0119. Co jest fascynuj\u0105ce, to nawi\u0105zuje do klasycznych mechanizm\u00f3w, kt\u00f3re znamy z sieci RNN, jak LSTM, do gating mechanisms. Czyli ten parametr delta to jest w zasadzie taki dynamiczny potencjometr skupienia. Tak. Dobre okre\u015blenie. Mo\u017ce podkr\u0119ci\u0107 uwag\u0119 na bie\u017c\u0105cym s\u0142owie, m\u00f3wi\u0105c to jest superwa\u017cne. Zapomnij na chwil\u0119 o przesz\u0142o\u015bci albo go wyciszy\u0107, \u017ceby zachowa\u0107 to, co by\u0142o wcze\u015bniej. W\u0142a\u015bnie tak to dzia\u0142a. Du\u017ca warto\u015b\u0107 delta resetuje stan i skupia si\u0119 na imowe informacji. Ma\u0142a warto\u015b\u0107 delta skutecznie ignoruje bie\u017c\u0105cy token i podtrzymuje pami\u0119\u0107 o przesz\u0142o\u015bci. To daje modelowi zdolno\u015b\u0107 do filtrowania kontekstu, do rozumienia granic na przyk\u0142ad ko\u0144ca zdania. Czekaj, czekaj, ale ta ca\u0142a dynamika, to uzale\u017cnienie od tre\u015bci, to brzni, jakby\u015bmy w\u0142a\u015bnie nie wyrzucili zab\u00f3r t\u0119 g\u0142\u00f3wn\u0105 zalet\u0119 starych modeli SSM, ich szybko\u015b\u0107. Przecie\u017c one by\u0142y szybkie, bo mo\u017cna je by\u0142o oblicza\u0107 za pomoc\u0105 super zoptymalizowanych operacji splotu, czyli convolutions, a te dzia\u0142aj\u0105 tylko przy sta\u0142ych parametrach. Wygl\u0105da mi to na ogromny kompromis. I tu dochodzimy do drugiego aktu g\u0119mszo w tej pracy. Tym razem czysto in\u017cynierskiego. OK. Autorzy doskonale wiedzieli, \u017ce trac\u0105 mo\u017cliwo\u015b\u0107 u\u017cycia splot\u00f3w. Musieli wr\u00f3ci\u0107 do oblicze\u0144 rekurrencyjnych, kt\u00f3re z natury s\u0105 no sekwencyjne i po prostu wolne. Ale zamiast u\u017cywa\u0107 standardowej implementacji stworzyli co\u015b, co nazywaj\u0105 hardware aware algorithm. Algorytm \u015bwiadomy sprz\u0119tu. Co to znaczy w praktyce? U\u017cyli operacji zwanej Parallel Scan to troch\u0119 jak b\u0142yskawiczne liczenie sumy bie\u017c\u0105cej w arkuszu kalkulacyjnym, ale zoptymalizowane do granic na GPU. To pozwala zachowa\u0107 pewne cechy rekurrencji, ale liczy\u0107 to lwnolegle. Ale kluczowy jest jak to zrobili. Dok\u0142adnie. Wiedzieli, \u017ce w\u0105skim gard\u0142em w GPU jest transfer danych mi\u0119dzy t\u0105 ogromn\u0105, ale woln\u0105 pami\u0119ci\u0105 HBM, a malutk\u0105, ale super szybk\u0105 pami\u0119ci\u0105 podr\u0119czn\u0105 SRAM. Wi\u0119c zaprojektowali sw\u00f3j algorytm tak, \u017ceby jak najwi\u0119cej danych wczyta\u0107 do tej szybkiej pami\u0119ci SRAM raz, zrobi\u0107 na nich wszystko, co trzeba i dopiero na ko\u0144cu zapisa\u0107 wynik. U\u017cywaj\u0105 do tego technik jak Kernel Fusion i Recomputation. Czyli zamiast zapisywa\u0107 wszystkie po\u015brednie wyniki do wolnej pami\u0119ci, co jest kosztowne, wol\u0105 je w razie potrzeby policzy\u0107 jeszcze raz. Bo to i tak jest szybsze, skoro dane s\u0105 ju\u017c pod r\u0119k\u0105 w tej ultra szybkiej pami\u0119ci. Dok\u0142adnie. I efekt jest taki, \u017ce ich implementacja na nowoczeznym GPU jak A100 jest od 20 do 40 razy szybsza ni\u017c standardowe podej\u015bcie do rekurrencji. To nie jest optymalizacja, to jest zupe\u0142nie inna liga. To brzmi prawie za dobrze, \u017ceby by\u0142o prawdziwe. Jest w tym jaki\u015b haczyk? Czy ta optymizacja nie jest przypadkiem tak uszyta na miar\u0119 pod karty NVD, \u017ce na innym sprz\u0119cie to nie zadzia\u0142a? To jest bardzo s\u0142uszna uwaga i tak to jest maister sztyg in\u017cynierii, kt\u00f3ry w pe\u0142ni wykorzystuje architektur\u0119 w sztu\u0142ocznych akcelerator\u00f3w. Ale ja bym tego nie nazwa\u0142a wad\u0105 raczej zalet\u0105. Autorzy nie stworzyli tylko jakiego\u015b teoretycznego modelu, stworzyli praktyczne narz\u0119dzie, kt\u00f3re dzia\u0142a tu i teraz na sprz\u0119cie, kt\u00f3rego u\u017cywa ca\u0142a bran\u017ca. A do tego sama architektura mamby jest prostsza. W Transformerze przeplatasz dwa r\u00f3\u017cne bloki, Multi-head Attention i MLP, a tu masz jeden homogeniczny blok, kt\u00f3ry po prostu powielasz. To jest, no, bardziej eleganckie. Wszystko to brzmi niezwykle elegancko na papierze. Ale w \u015bwiecie, a i, wiesz, teoria to jedno, a brutalna rzeczywisto\u015b\u0107 benchmark\u00f3w to drugie. Czy tw\u00f3rcy mamby maj\u0105 liczb\u0119, kt\u00f3re potwierdzaj\u0105 te obietnice? Maj\u0105 i to jakie. Zacznijmy od tych zada\u0144 syntetycznych. W tym Selective Coping, o kt\u00f3rym m\u00f3wili\u015bmy, mamba osi\u0105ga prawie 100 proc. dok\u0142adno\u015bci. Ale prawdziwym testem jest zadanie Induction Heads. To jest to, co uwa\u017ca si\u0119 za kluczowe dla in-context learning w LLM-ach. Tak, chodzi o to, \u017ceby model zauwa\u017cy\u0142 wzorzec, np. par\u0119 s\u0142\u00f3w Harry Potter. I gdy potem zobaczy samochary, \u017ceby przewidzia\u0142, \u017ce nast\u0119pne b\u0119dzie Potter. Czekaj, upewnij si\u0119, \u017ce dobrze to rozumiem. Oni trenowali model na sekwencjach d\u0142ugo\u015bci, powiedzmy, kilku k\u0142opot pit\u00f3w, a on potem potrafi\u0142 poprawnie zastosowa\u0107 te logik\u0119 do sekwencji d\u0142ugo\u015bci ca\u0142ej powie\u015bci. To, no, to brzmi niewiargodnie. A jednak mamba trenowana na sekwencjach o d\u0142ugo\u015bci 256 token\u00f3w. Potrafi\u0142a bezb\u0142\u0119dnie rozwi\u0105za\u0107 to zadanie na sekwencjach o d\u0142ugo\u015bci miliona token\u00f3w. Miliona. Miliona. To jest 4 tysi\u0105ce razy d\u0142u\u017cej. \u017baden inny model, w tym r\u00f3\u017cne warianty Attention nawet nie zbli\u017cy\u0142 si\u0119 do takiego wyniku. To pokazuje, \u017ce jej mechanizm kompresji stanu naprawd\u0119 dzia\u0142a i potrafi ekstrapolowa\u0107. To ju\u017c nie jest tylko poprawa wydajno\u015bci. To jest fundamentalnie nowa zdolno\u015b\u0107. A co z realnym \u015bwiatem, co z modelowaniem j\u0119zyka? Tutaj wyniki s\u0105 r\u00f3wnie mocne. Model Mamba 3B, czyli z 3 miliardami parametr\u00f3w, osi\u0105ga wyniki na poziomie modeli Transformer dwukrotnie wi\u0119kszych. Na przyk\u0142ad Pythia 7B. Dwukrotnie wi\u0119kszych? Tak. I to zar\u00f3wno w metrykach z pretrainingu, jak i w zadaniach downstream. I to jest, no, historyczny moment. To jest pierwszy model o z\u0142o\u017cono\u015bci liniowej, kt\u00f3ry naprawd\u0119 dor\u00f3wnuje jako\u015bci\u0105 najlepszym Transformerom. A co to znaczy w praktyce? Dla firmy, kt\u00f3ra buduje model AI, oznacza to, \u017ce mo\u017ce dosta\u0107 t\u0119 sam\u0105, mo\u017ce nawet lepsz\u0105 jako\u015b\u0107, u\u017cywaj\u0105c modelu o po\u0142ow\u0119 mniejszego. A mniejszy model to ni\u017csze koszty, szybsze odpowiedzi. Dok\u0142adnie tak. A wisienka na torcie jest taka, \u017ce przepustowo\u015b\u0107 mamby w trybie inferencji, czyli generowania odpowiedzi, jest pi\u0119ciokrotnie wy\u017csza, ni\u017c u Transformera o por\u00f3wnywalnej jako\u015bci. Pi\u0119ciokrotnie. Wow. A co z tymi dziedzinami, kt\u00f3re by\u0142y na pocz\u0105tku motywacj\u0105? Genomika, audio. Tam mamy do czynienia z naprawd\u0119 potwornie d\u0142ugimi sekwencjami. W genomice Mamba po prostu b\u0142yszczy. Przy analizie ludzkiego genomu, gdzie m\u00f3wimy o sekwencjach rz\u0119du miliona par zasad, ona nie tylko skaluje si\u0119 bez problemu. Jej wydajno\u015b\u0107, mierzona metryk\u0105 Perplexity, autentycznie ro\u015bnie wraz z d\u0142ugo\u015bci\u0105 kontekstu. Ro\u015bnie. U innych modeli zazwyczaj spada. No a spada\u0107. A Mamba potrafi efektywnie wykorzysta\u0107 ten dodatkowy kontekst, bo otwiltrowuje szum. Czyli badacz mo\u017ce teraz wrzuci\u0107 do modelu ca\u0142\u0105 sekwencj\u0119 genomu, a nie tylko fragmenty i szuka\u0107 wzorc\u00f3w na niespodkan\u0105 skal\u0119. To jest w\u0142a\u015bnie ten potencja\u0142. A w audio, w zadaniu generowania mowy ze zbioru SC09 Mamba ustanowi\u0142a nowy rekord State of the Art. Pobi\u0142a modele oparte na dyfuzje i sieciach GAN. Wska\u017anik b\u0142\u0119du FID zmniejsza\u0142a o ponad po\u0142ow\u0119 w stosunku do poprzedniego najlepszego modelu. To jest ogromny skok. Wi\u0119c po tym wszystkim, czy to czas, \u017ceby zacz\u0105\u0107 pisa\u0107 nekrolog dla architektury Transformer? Mamba jest bez w\u0105tpienia najsilniejszym kandydatem na taki og\u00f3lny model bazowy dla sekwencji, jakiego widzieli\u015bmy do tej pory. Zdolno\u015b\u0107 do liniowego skalowania przy zachowaniu jako\u015bci sota otwiera drzwi, kt\u00f3re dla Transformer\u00f3w by\u0142y po prostu zamkni\u0119te. Modelowanie ca\u0142ych genom\u00f3w, film\u00f3w w wysokiej rozdzielczo\u015bci. Analiza wielotomowych dokument\u00f3w bez dzielenia ich na kawa\u0142ki. To wszystko staje si\u0119 realne. Ale autorzy sami przyznaj\u0105, \u017ce to nie jest rozwi\u0105zanie uniwersalne, prawda? Wspominaj\u0105 o pewnym spektrum ci\u0105g\u0142e dyskretne. Czyli to nie jest tak, \u017ce Mamba jest lepsza we wszystkim. Tak i to jest bardzo wa\u017cna i uczciwa obserwacja. Ten mechanizm selekcji jest darem nie wios dla danych dyskretnych. Text DNA. Tam, gdzie znaczenie jest przypisane do konkretnych token\u00f3w. Ale w przypadku sygna\u0142\u00f3w ci\u0105g\u0142ych, jak surowe pr\u00f3bki audio, gdzie ka\u017cdy punkt jest silnie skorelowany z s\u0105siadami. Stare modele LTI ze swoj\u0105 niezmienno\u015bci\u0105 mog\u0105 mie\u0107 naturaln\u0105 przewag\u0119. Zreszt\u0105 w jednym z eksperyment\u00f3w na audio wy\u0142\u0105czenie selekcji w Mambie i powr\u00f3t do wariantu LTI da\u0142o lepsze wyniki. Czyli trzeba dobra\u0107 narz\u0119dzie do problemu i jest jeszcze jedna kwestia. Skalowanie. Te wszystkie wyniki dotycz\u0105 modeli do 3 miliard\u00f3w parametr\u00f3w. A co z gigantami? Z modelami 70B, 175B. Czy jest jaki\u015b pow\u00f3d, by s\u0105dzi\u0107, \u017ce Mamba nie uderzy w jak\u0105\u015b \u015bcian\u0119? To jest pytanie za milion dolar\u00f3w. I ten artyku\u0142 na nie jeszcze nie odpowiada. To oczylisty nast\u0119pny krok dla bada\u0144. Skalowanie transformer\u00f3w jest ju\u017c dobrze zrozumiane. Skalowanie Mamby to wci\u0105\u017c nieodkryte terytorium. Ale jej fundamentalna efektywno\u015b\u0107, zar\u00f3wno pami\u0119ciowa, jak i obliczeniowa, daje solidne podstawy, by s\u0105dzi\u0107, \u017ce b\u0119dzie si\u0119 skalowa\u0107 bardzo, bardzo dobrze. Mamba to realnie nowa architektura. Rozwi\u0105zuje kluczowy problem transformer\u00f3w dzi\u0119ki inteligentnej selekcji, a jednocze\u015bnie pozostaje b\u0142yskawiczna dzi\u0119ki genialnej in\u017cynierii i ma na to dowody w postaci rewelacyjnych wynik\u00f3w. Zdecydowanie. Ale my\u015bl\u0119, \u017ce na koniec warto zostawi\u0107 s\u0142uchaczy z tak\u0105 jedn\u0105, troch\u0119 prowokacyjn\u0105 my\u015bl\u0105. Zreszt\u0105 sugeruj\u0105 j\u0105 sami autorzy. S\u0142ucham. Wskazuj\u0105, \u017ce prawdziwym testem dla ka\u017cdej nowej architektury nie jest tylko wynik w pretreningu. Jest nim zdolno\u015b\u0107 do fine tuningu, instruction tuning, czy RLHF. I tu wracamy do tych niesamowitych wynik\u00f3w mamby w zadaniu induction heads. Bo one sugeruj\u0105, \u017ce Mamba mo\u017ce mie\u0107 pot\u0119\u017cne, wbudowane zdolno\u015bci do uczenia si\u0119 w kontek\u015bcie. To in-context learning. Dok\u0142adnie. I to prowadzi do g\u0142\u0119bszego pytania. Pytanie nie brzmi ju\u017c tylko, czy Mamba mo\u017ce dor\u00f3wna\u0107 transformerom w zadaniach, kt\u00f3re ju\u017c znamy. A jak? Czy jej fundamentalnie inny spos\u00f3b przetwarzania informacji oparty na inteligentnej kompresji kontekstu do stanu, a nie na przechowywaniu wszystkiego w pami\u0119ci jak attention, mo\u017ce odblokowa\u0107 zupe\u0142nie nowe, nieodkryte jeszcze mo\u017cliwo\u015bci? Czyli m\u00f3wisz, \u017ce prawdziwym prze\u0142omem mo\u017ce nie by\u0107 nawet szybko\u015b\u0107, ale zupe\u0142nie nowy spos\u00f3b my\u015blenia tych modeli. Przej\u015bcie od takiej brutalnej, si\u0142owej pami\u0119ci do czego\u015b, co bardziej przypomina inteligentne streszczenie i rozumienie. W\u0142a\u015bnie. By\u0107 mo\u017ce transformery ze swoj\u0105 metod\u0105 patrzenia na wszystko naraz osi\u0105gn\u0119\u0142y ju\u017c swoje poznawcze granice. A Mamba ze swoj\u0105 elegancj\u0105 i efektywno\u015bci\u0105 dopiero zaczyna nam pokazywa\u0107, co jest mo\u017cliwe. I to jest my\u015bl, kt\u00f3ra sprawia, \u017ce przysz\u0142o\u015b\u0107 tej dziedziny jest tak ekscytuj\u0105ca.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 1.52, "text": " Witajcie w T-Deep Dive.", "tokens": [50364, 42299, 47276, 261, 314, 12, 11089, 595, 413, 488, 13, 50440], "temperature": 0.0, "avg_logprob": -0.17680436972803718, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.009762290865182877}, {"id": 1, "seek": 0, "start": 1.72, "end": 6.44, "text": " Dzisiaj mamy, wydaje mi si\u0119 na stole, co\u015b, co mo\u017ce naprawd\u0119 zatrz\u0105\u015b\u0107 posadami", "tokens": [50450, 39448, 22356, 17335, 11, 49165, 2752, 3244, 1667, 16326, 11, 19241, 11, 598, 12034, 20970, 35802, 81, 8925, 7753, 1366, 345, 4526, 50686], "temperature": 0.0, "avg_logprob": -0.17680436972803718, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.009762290865182877}, {"id": 2, "seek": 0, "start": 6.640000000000001, "end": 8.2, "text": " wsp\u00f3\u0142czesnej sztucznej inteligencji.", "tokens": [50696, 39069, 3689, 279, 11794, 262, 2682, 1311, 89, 11794, 24777, 3213, 19649, 13, 50774], "temperature": 0.0, "avg_logprob": -0.17680436972803718, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.009762290865182877}, {"id": 3, "seek": 0, "start": 8.4, "end": 9.36, "text": " Co\u015b du\u017cego?", "tokens": [50784, 3066, 1788, 21783, 6308, 30, 50832], "temperature": 0.0, "avg_logprob": -0.17680436972803718, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.009762290865182877}, {"id": 4, "seek": 0, "start": 9.56, "end": 10.68, "text": " Co\u015b bardzo du\u017cego.", "tokens": [50842, 3066, 1788, 9034, 21783, 6308, 13, 50898], "temperature": 0.0, "avg_logprob": -0.17680436972803718, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.009762290865182877}, {"id": 5, "seek": 0, "start": 10.88, "end": 14.68, "text": " Wszyscy \u017cyjemy od lat w erze modeli Transformer.", "tokens": [50908, 343, 15453, 38966, 16136, 73, 3633, 3611, 4465, 261, 1189, 1381, 2316, 72, 27938, 260, 13, 51098], "temperature": 0.0, "avg_logprob": -0.17680436972803718, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.009762290865182877}, {"id": 6, "seek": 0, "start": 14.88, "end": 21.04, "text": " One nap\u0119dzaj\u0105 GPT, generuj\u0105 obrazy, t\u0142umocz\u0105, s\u0105 dos\u0142ownie wsz\u0119dzie.", "tokens": [51108, 1485, 9296, 6298, 89, 11133, 26039, 51, 11, 1337, 13263, 22798, 1229, 11, 256, 49166, 905, 8925, 11, 9015, 4491, 1221, 648, 414, 38322, 42643, 13, 51416], "temperature": 0.0, "avg_logprob": -0.17680436972803718, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.009762290865182877}, {"id": 7, "seek": 0, "start": 21.240000000000002, "end": 24.32, "text": " Ale maj\u0105 t\u0119 jedn\u0105 fundamentaln\u0105 wad\u0119.", "tokens": [51426, 9366, 26064, 32489, 5232, 13113, 8088, 13113, 261, 345, 1274, 13, 51580], "temperature": 0.0, "avg_logprob": -0.17680436972803718, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.009762290865182877}, {"id": 8, "seek": 0, "start": 24.52, "end": 27.72, "text": " Wszyscy o niej wiedz\u0105, ale tak troch\u0119 staraj\u0105 si\u0119 j\u0105 omija\u0107.", "tokens": [51590, 343, 15453, 38966, 277, 2838, 73, 46894, 8925, 11, 6775, 991, 24926, 3543, 11133, 3244, 35692, 3406, 20642, 2162, 13, 51750], "temperature": 0.0, "avg_logprob": -0.17680436972803718, "compression_ratio": 1.4107744107744107, "no_speech_prob": 0.009762290865182877}, {"id": 9, "seek": 2772, "start": 27.84, "end": 29.119999999999997, "text": " Kwadratowa z\u0142o\u017cono\u015b\u0107.", "tokens": [50370, 591, 86, 345, 4481, 5528, 710, 5249, 1427, 8957, 7753, 13, 50434], "temperature": 0.0, "avg_logprob": -0.14260380868693345, "compression_ratio": 1.3875432525951557, "no_speech_prob": 0.27724167704582214}, {"id": 10, "seek": 2772, "start": 29.32, "end": 35.8, "text": " Dok\u0142adnie. Im d\u0142u\u017csza sekwencja tekst, film, fragment DNA, tym wolniej i dro\u017cej to wszystko dzia\u0142a.", "tokens": [50444, 29768, 10358, 2766, 13, 4331, 274, 24066, 1427, 82, 2394, 17215, 15615, 34056, 16624, 372, 11, 2007, 11, 26424, 8272, 11, 8107, 20960, 10402, 741, 3789, 38493, 281, 22607, 37903, 13, 50768], "temperature": 0.0, "avg_logprob": -0.14260380868693345, "compression_ratio": 1.3875432525951557, "no_speech_prob": 0.27724167704582214}, {"id": 11, "seek": 2772, "start": 36.0, "end": 39.96, "text": " I to nie liniowo, a wyk\u0142adnicz\u0105.", "tokens": [50778, 286, 281, 2838, 287, 3812, 19941, 11, 257, 4628, 15317, 7692, 8925, 13, 50976], "temperature": 0.0, "avg_logprob": -0.14260380868693345, "compression_ratio": 1.3875432525951557, "no_speech_prob": 0.27724167704582214}, {"id": 12, "seek": 2772, "start": 40.16, "end": 42.32, "text": " To jak super samoch\u00f3d, kt\u00f3ry jest niesamowicie", "tokens": [50986, 1407, 4207, 1687, 3247, 8997, 17081, 11, 9913, 3492, 48100, 335, 305, 28434, 51094], "temperature": 0.0, "avg_logprob": -0.14260380868693345, "compression_ratio": 1.3875432525951557, "no_speech_prob": 0.27724167704582214}, {"id": 13, "seek": 2772, "start": 42.519999999999996, "end": 47.480000000000004, "text": " szybki, ale na ka\u017cdy kolejny kilometr pali coraz wi\u0119cej, a\u017c w ko\u0144cu po prostu staje.", "tokens": [51104, 36456, 2984, 11, 6775, 1667, 31615, 23749, 1634, 9677, 81, 3984, 72, 25899, 26004, 11, 48134, 261, 26470, 12032, 714, 19518, 342, 11153, 13, 51352], "temperature": 0.0, "avg_logprob": -0.14260380868693345, "compression_ratio": 1.3875432525951557, "no_speech_prob": 0.27724167704582214}, {"id": 14, "seek": 2772, "start": 47.68, "end": 52.239999999999995, "text": " I dlatego w\u0142a\u015bnie dzisiaj przyjrzymy si\u0119 do g\u0142\u0119bnie pracy naukowej, kt\u00f3ra nie pr\u00f3buje tego", "tokens": [51362, 286, 32205, 14234, 25772, 6501, 73, 13047, 2226, 3244, 360, 18117, 1274, 65, 2766, 35591, 35616, 74, 21091, 11, 19456, 2838, 8565, 6021, 2884, 8627, 51590], "temperature": 0.0, "avg_logprob": -0.14260380868693345, "compression_ratio": 1.3875432525951557, "no_speech_prob": 0.27724167704582214}, {"id": 15, "seek": 5224, "start": 52.24, "end": 56.2, "text": " problemu obej\u015b\u0107, ale rozwi\u0105za\u0107 u samych podstaw.", "tokens": [50364, 1154, 84, 36346, 44536, 11, 6775, 9544, 18234, 35873, 344, 3247, 16384, 43443, 13, 50562], "temperature": 0.0, "avg_logprob": -0.1574727932136216, "compression_ratio": 1.406153846153846, "no_speech_prob": 0.7048232555389404}, {"id": 16, "seek": 5224, "start": 56.400000000000006, "end": 58.88, "text": " M\u00f3wi\u0119 tu o artykule, kt\u00f3ry zrobi\u0142 mn\u00f3stwo szumu.", "tokens": [50572, 376, 3901, 5034, 2604, 277, 594, 874, 74, 2271, 11, 9913, 24483, 1221, 275, 77, 45052, 6120, 7870, 30034, 13, 50696], "temperature": 0.0, "avg_logprob": -0.1574727932136216, "compression_ratio": 1.406153846153846, "no_speech_prob": 0.7048232555389404}, {"id": 17, "seek": 5224, "start": 59.08, "end": 59.68, "text": " Mamba.", "tokens": [50706, 376, 23337, 13, 50736], "temperature": 0.0, "avg_logprob": -0.1574727932136216, "compression_ratio": 1.406153846153846, "no_speech_prob": 0.7048232555389404}, {"id": 18, "seek": 5224, "start": 59.88, "end": 64.08, "text": " Linear Time Sequence Modeling with Selective State Spaces.", "tokens": [50746, 14670, 289, 6161, 46859, 655, 6583, 11031, 365, 13638, 488, 4533, 1738, 2116, 13, 50956], "temperature": 0.0, "avg_logprob": -0.1574727932136216, "compression_ratio": 1.406153846153846, "no_speech_prob": 0.7048232555389404}, {"id": 19, "seek": 5224, "start": 64.28, "end": 66.48, "text": " Autorstwa Alberta Gui Tridao.", "tokens": [50966, 6049, 284, 372, 4151, 43279, 2694, 72, 1765, 2887, 78, 13, 51076], "temperature": 0.0, "avg_logprob": -0.1574727932136216, "compression_ratio": 1.406153846153846, "no_speech_prob": 0.7048232555389404}, {"id": 20, "seek": 5224, "start": 66.68, "end": 69.8, "text": " I to nie jest, jak m\u00f3wisz, jaka\u015b drobna optymalizacja.", "tokens": [51086, 286, 281, 2838, 3492, 11, 4207, 13489, 23848, 11, 4207, 64, 1788, 3789, 65, 629, 2427, 4199, 304, 590, 23395, 13, 51242], "temperature": 0.0, "avg_logprob": -0.1574727932136216, "compression_ratio": 1.406153846153846, "no_speech_prob": 0.7048232555389404}, {"id": 21, "seek": 5224, "start": 70.0, "end": 72.04, "text": " To propozycja zupe\u0142nie nowej architektury.", "tokens": [51252, 1407, 447, 2259, 1229, 34056, 49922, 586, 40779, 3912, 642, 2320, 2598, 13, 51354], "temperature": 0.0, "avg_logprob": -0.1574727932136216, "compression_ratio": 1.406153846153846, "no_speech_prob": 0.7048232555389404}, {"id": 22, "seek": 5224, "start": 72.24000000000001, "end": 72.84, "text": " W\u0142a\u015bnie.", "tokens": [51364, 343, 5024, 12221, 13, 51394], "temperature": 0.0, "avg_logprob": -0.1574727932136216, "compression_ratio": 1.406153846153846, "no_speech_prob": 0.7048232555389404}, {"id": 23, "seek": 5224, "start": 73.04, "end": 75.44, "text": " I to jest nasze kluczowe pytanie na dzi\u015b.", "tokens": [51404, 286, 281, 3492, 43394, 9671, 1311, 89, 6880, 36610, 1667, 31981, 1788, 13, 51524], "temperature": 0.0, "avg_logprob": -0.1574727932136216, "compression_ratio": 1.406153846153846, "no_speech_prob": 0.7048232555389404}, {"id": 24, "seek": 5224, "start": 75.64, "end": 77.44, "text": " Czy to jest prawdziwa rewolucja?", "tokens": [51534, 19832, 281, 3492, 41175, 3992, 4151, 319, 48481, 1311, 2938, 30, 51624], "temperature": 0.0, "avg_logprob": -0.1574727932136216, "compression_ratio": 1.406153846153846, "no_speech_prob": 0.7048232555389404}, {"id": 25, "seek": 5224, "start": 77.64, "end": 81.72, "text": " Czy Mamba to ten mityczny Transformer Killer, na kt\u00f3rego czekamy?", "tokens": [51634, 19832, 376, 23337, 281, 2064, 275, 507, 3689, 1634, 27938, 260, 39846, 11, 1667, 46951, 6472, 916, 7804, 30, 51838], "temperature": 0.0, "avg_logprob": -0.1574727932136216, "compression_ratio": 1.406153846153846, "no_speech_prob": 0.7048232555389404}, {"id": 26, "seek": 8172, "start": 81.92, "end": 86.6, "text": " Czy mo\u017ce, no wiesz, kolejna bardzo sprytna alternatywa, kt\u00f3ra znajdzie swoj\u0105 nisz\u0119?", "tokens": [50374, 19832, 12034, 11, 572, 261, 15347, 11, 23749, 629, 9034, 637, 627, 83, 629, 5400, 21398, 4151, 11, 19456, 27318, 13096, 49194, 297, 271, 11052, 30, 50608], "temperature": 0.0, "avg_logprob": -0.16143165826797484, "compression_ratio": 1.4377104377104377, "no_speech_prob": 0.0008285663207061589}, {"id": 27, "seek": 8172, "start": 86.8, "end": 92.0, "text": " Chcemy zrozumie\u0107, co sprawia, \u017ce jest tak szybka, co wchniemy si\u0119 okrok.", "tokens": [50618, 761, 384, 2226, 710, 27857, 449, 414, 2162, 11, 598, 22734, 654, 11, 3561, 3492, 991, 36456, 2330, 11, 598, 261, 1377, 414, 2226, 3244, 3133, 31621, 13, 50878], "temperature": 0.0, "avg_logprob": -0.16143165826797484, "compression_ratio": 1.4377104377104377, "no_speech_prob": 0.0008285663207061589}, {"id": 28, "seek": 8172, "start": 92.2, "end": 97.08, "text": " Zrozumiemy, dlaczego poprzednie pr\u00f3by, tak zwane State Space Models, czyli SSM,", "tokens": [50888, 1176, 27857, 449, 414, 2226, 11, 37873, 39329, 1665, 81, 11312, 2766, 8565, 2322, 11, 991, 11873, 1929, 4533, 8705, 6583, 1625, 11, 16591, 12238, 44, 11, 51132], "temperature": 0.0, "avg_logprob": -0.16143165826797484, "compression_ratio": 1.4377104377104377, "no_speech_prob": 0.0008285663207061589}, {"id": 29, "seek": 8172, "start": 97.28, "end": 99.4, "text": " poleg\u0142y na polu j\u0119zykowym.", "tokens": [51142, 714, 6363, 6825, 1667, 1180, 84, 49055, 74, 31691, 13, 51248], "temperature": 0.0, "avg_logprob": -0.16143165826797484, "compression_ratio": 1.4377104377104377, "no_speech_prob": 0.0008285663207061589}, {"id": 30, "seek": 8172, "start": 99.6, "end": 105.88, "text": " Potem zanurkujemy w samoserce Mamby, ten ca\u0142y mechanizm selekcji, kt\u00f3ry jest jej", "tokens": [51258, 9145, 443, 710, 282, 374, 74, 21767, 261, 3247, 329, 260, 384, 376, 2173, 88, 11, 2064, 35226, 4236, 590, 76, 23264, 74, 19649, 11, 9913, 3492, 28924, 51572], "temperature": 0.0, "avg_logprob": -0.16143165826797484, "compression_ratio": 1.4377104377104377, "no_speech_prob": 0.0008285663207061589}, {"id": 31, "seek": 8172, "start": 106.08, "end": 107.16, "text": " prawdziwym prze\u0142omem.", "tokens": [51582, 41175, 3992, 86, 4199, 8325, 1221, 423, 76, 13, 51636], "temperature": 0.0, "avg_logprob": -0.16143165826797484, "compression_ratio": 1.4377104377104377, "no_speech_prob": 0.0008285663207061589}, {"id": 32, "seek": 8172, "start": 107.16, "end": 109.28, "text": " A potem pewnie in\u017cynieria, kt\u00f3ra za tym stoi.", "tokens": [51636, 316, 36513, 520, 14215, 294, 1427, 2534, 811, 654, 11, 19456, 7949, 8107, 342, 4869, 13, 51742], "temperature": 0.0, "avg_logprob": -0.16143165826797484, "compression_ratio": 1.4377104377104377, "no_speech_prob": 0.0008285663207061589}, {"id": 33, "seek": 10928, "start": 109.28, "end": 114.92, "text": " Tak, ten majsterszczyk, kt\u00f3ry pozwoli\u0142 po\u0142\u0105czy\u0107 selektywno\u015b\u0107 z niesamowit\u0105 pr\u0119dko\u015bci\u0105.", "tokens": [50364, 9118, 11, 2064, 13673, 10130, 89, 6522, 74, 11, 9913, 40557, 9384, 1221, 714, 15926, 33967, 23264, 74, 874, 20944, 7753, 710, 48100, 335, 305, 270, 1611, 582, 6298, 4093, 50227, 13, 50646], "temperature": 0.0, "avg_logprob": -0.11060208450129003, "compression_ratio": 1.459375, "no_speech_prob": 0.004704633262008429}, {"id": 34, "seek": 10928, "start": 115.12, "end": 118.68, "text": " A na koniec oczywi\u015bcie dowody, twarde dane wyniki.", "tokens": [50656, 316, 1667, 5897, 35733, 23862, 9459, 843, 11, 683, 10866, 49206, 31936, 9850, 13, 50834], "temperature": 0.0, "avg_logprob": -0.11060208450129003, "compression_ratio": 1.459375, "no_speech_prob": 0.004704633262008429}, {"id": 35, "seek": 10928, "start": 118.88, "end": 121.76, "text": " Zobaczymy, czy to tylko teoria, czy ju\u017c praktyka.", "tokens": [50844, 1176, 996, 14691, 2226, 11, 6430, 281, 13219, 535, 8172, 11, 6430, 10678, 3206, 74, 874, 2330, 13, 50988], "temperature": 0.0, "avg_logprob": -0.11060208450129003, "compression_ratio": 1.459375, "no_speech_prob": 0.004704633262008429}, {"id": 36, "seek": 10928, "start": 121.96000000000001, "end": 124.12, "text": " Dobra, to zatrzymajmy si\u0119 na moment przy tych", "tokens": [50998, 413, 24393, 11, 281, 35802, 13047, 1696, 73, 2226, 3244, 1667, 1623, 6501, 15180, 51106], "temperature": 0.0, "avg_logprob": -0.11060208450129003, "compression_ratio": 1.459375, "no_speech_prob": 0.004704633262008429}, {"id": 37, "seek": 10928, "start": 124.32000000000001, "end": 125.72, "text": " starych modelach SSM.", "tokens": [51116, 342, 822, 339, 2316, 608, 12238, 44, 13, 51186], "temperature": 0.0, "avg_logprob": -0.11060208450129003, "compression_ratio": 1.459375, "no_speech_prob": 0.004704633262008429}, {"id": 38, "seek": 10928, "start": 125.92, "end": 129.6, "text": " Skoro by\u0142y tak wydajne, tak szybkie, to co im w\u0142a\u015bciwie stan\u0119\u0142o na drodze?", "tokens": [51196, 7324, 10780, 26366, 991, 25984, 1805, 716, 11, 991, 36456, 22872, 11, 281, 598, 566, 50108, 27984, 1274, 5249, 1667, 3789, 67, 1381, 30, 51380], "temperature": 0.0, "avg_logprob": -0.11060208450129003, "compression_ratio": 1.459375, "no_speech_prob": 0.004704633262008429}, {"id": 39, "seek": 10928, "start": 129.8, "end": 131.88, "text": " Dlaczego nie mamy ich dzisiaj w ka\u017cdym czat bocie?", "tokens": [51390, 413, 75, 39329, 2838, 17335, 1893, 25772, 261, 31615, 76, 6472, 267, 748, 4260, 30, 51494], "temperature": 0.0, "avg_logprob": -0.11060208450129003, "compression_ratio": 1.459375, "no_speech_prob": 0.004704633262008429}, {"id": 40, "seek": 10928, "start": 132.08, "end": 135.92000000000002, "text": " A to jest idealne pytanie na start, bo ono prowadzi nas do sedna.", "tokens": [51504, 316, 281, 3492, 7157, 716, 36610, 1667, 722, 11, 748, 322, 78, 36590, 3992, 5382, 360, 9643, 629, 13, 51696], "temperature": 0.0, "avg_logprob": -0.11060208450129003, "compression_ratio": 1.459375, "no_speech_prob": 0.004704633262008429}, {"id": 41, "seek": 13592, "start": 136.11999999999998, "end": 141.23999999999998, "text": " T\u0105 wad\u0105 by\u0142a ich natura, kt\u00f3r\u0105 w artykule okre\u015blaj\u0105 jako Linear Time Invariance.", "tokens": [50374, 314, 1611, 261, 345, 1611, 23936, 1893, 2249, 2991, 11, 37415, 261, 594, 874, 74, 2271, 3133, 265, 1788, 875, 8555, 17123, 14670, 289, 6161, 31124, 3504, 719, 13, 50630], "temperature": 0.0, "avg_logprob": -0.16992674012115036, "compression_ratio": 1.3686131386861313, "no_speech_prob": 0.053266994655132294}, {"id": 42, "seek": 13592, "start": 141.44, "end": 142.95999999999998, "text": " W skr\u00f3cie LTI.", "tokens": [50640, 343, 1110, 11721, 4260, 441, 5422, 13, 50716], "temperature": 0.0, "avg_logprob": -0.16992674012115036, "compression_ratio": 1.3686131386861313, "no_speech_prob": 0.053266994655132294}, {"id": 43, "seek": 13592, "start": 143.16, "end": 145.51999999999998, "text": " Liniowa niezmienno\u015b\u0107 w czasie.", "tokens": [50726, 441, 3812, 5528, 33511, 76, 1053, 23293, 261, 42667, 13, 50844], "temperature": 0.0, "avg_logprob": -0.16992674012115036, "compression_ratio": 1.3686131386861313, "no_speech_prob": 0.053266994655132294}, {"id": 44, "seek": 13592, "start": 145.72, "end": 149.07999999999998, "text": " Dok\u0142adnie. Wyobra\u017a sobie tak\u0105 lini\u0119 produkcyjn\u0105 w fabryce.", "tokens": [50854, 29768, 10358, 2766, 13, 14458, 24393, 10659, 13652, 31069, 287, 3812, 1274, 33699, 42949, 13113, 261, 5355, 627, 384, 13, 51022], "temperature": 0.0, "avg_logprob": -0.16992674012115036, "compression_ratio": 1.3686131386861313, "no_speech_prob": 0.053266994655132294}, {"id": 45, "seek": 13592, "start": 149.27999999999997, "end": 152.32, "text": " Robot wykonuje na niej w k\u00f3\u0142ko t\u0119 sam\u0105 czynno\u015b\u0107.", "tokens": [51032, 29601, 46702, 13008, 1667, 2838, 73, 261, 350, 16181, 4093, 32489, 3247, 1611, 6430, 77, 23293, 13, 51184], "temperature": 0.0, "avg_logprob": -0.16992674012115036, "compression_ratio": 1.3686131386861313, "no_speech_prob": 0.053266994655132294}, {"id": 46, "seek": 13592, "start": 152.51999999999998, "end": 156.2, "text": " Nie obchodzi go, czy produkt jest czerwony, niebieski, ma\u0142y, du\u017cy.", "tokens": [51194, 12016, 1111, 34616, 352, 11, 6430, 42816, 3492, 269, 4527, 86, 2526, 11, 2838, 23177, 2984, 11, 463, 6825, 11, 1581, 7735, 13, 51378], "temperature": 0.0, "avg_logprob": -0.16992674012115036, "compression_ratio": 1.3686131386861313, "no_speech_prob": 0.053266994655132294}, {"id": 47, "seek": 13592, "start": 156.39999999999998, "end": 158.76, "text": " On zawsze robi to samo.", "tokens": [51388, 1282, 30964, 47380, 281, 36422, 13, 51506], "temperature": 0.0, "avg_logprob": -0.16992674012115036, "compression_ratio": 1.3686131386861313, "no_speech_prob": 0.053266994655132294}, {"id": 48, "seek": 13592, "start": 158.95999999999998, "end": 160.48, "text": " Tak dzia\u0142a\u0142y modele LTI.", "tokens": [51516, 9118, 37903, 6825, 4391, 306, 441, 5422, 13, 51592], "temperature": 0.0, "avg_logprob": -0.16992674012115036, "compression_ratio": 1.3686131386861313, "no_speech_prob": 0.053266994655132294}, {"id": 49, "seek": 16048, "start": 160.64, "end": 163.11999999999998, "text": " Ich parametry by\u0142y sta\u0142e.", "tokens": [50372, 3141, 6220, 9889, 26366, 11135, 19827, 13, 50496], "temperature": 0.0, "avg_logprob": -0.15191902570276453, "compression_ratio": 1.4358108108108107, "no_speech_prob": 0.1739407181739807}, {"id": 50, "seek": 16048, "start": 163.32, "end": 166.04, "text": " Niezmienne, niezale\u017cne od tre\u015bci.", "tokens": [50506, 12016, 89, 76, 21262, 11, 33511, 45494, 716, 3611, 2192, 6199, 13, 50642], "temperature": 0.0, "avg_logprob": -0.15191902570276453, "compression_ratio": 1.4358108108108107, "no_speech_prob": 0.1739407181739807}, {"id": 51, "seek": 16048, "start": 166.23999999999998, "end": 169.35999999999999, "text": " Czyli to by\u0142o super dop\u00f3ki dane by\u0142y jednolite.", "tokens": [50652, 37099, 281, 14811, 1687, 21900, 812, 2984, 49206, 26366, 5232, 77, 401, 642, 13, 50808], "temperature": 0.0, "avg_logprob": -0.15191902570276453, "compression_ratio": 1.4358108108108107, "no_speech_prob": 0.1739407181739807}, {"id": 52, "seek": 16048, "start": 169.56, "end": 173.51999999999998, "text": " Jak na przyk\u0142ad, nie wiem, sygna\u0142 audio, gdzie fizyka d\u017awi\u0119ku jest sta\u0142a.", "tokens": [50818, 15029, 1667, 23144, 11, 2838, 26522, 11, 943, 70, 629, 1221, 6278, 11, 18922, 21000, 88, 2330, 274, 10659, 22423, 5279, 3492, 11135, 5024, 13, 51016], "temperature": 0.0, "avg_logprob": -0.15191902570276453, "compression_ratio": 1.4358108108108107, "no_speech_prob": 0.1739407181739807}, {"id": 53, "seek": 16048, "start": 173.72, "end": 176.39999999999998, "text": " Dok\u0142adnie tak, ale j\u0119zyk.", "tokens": [51026, 29768, 10358, 2766, 991, 11, 6775, 49055, 74, 13, 51160], "temperature": 0.0, "avg_logprob": -0.15191902570276453, "compression_ratio": 1.4358108108108107, "no_speech_prob": 0.1739407181739807}, {"id": 54, "seek": 16048, "start": 176.6, "end": 179.39999999999998, "text": " Albo genomika, to jest zupe\u0142nie inna bajka.", "tokens": [51170, 967, 1763, 1049, 298, 5439, 11, 281, 3492, 49922, 294, 629, 23589, 2330, 13, 51310], "temperature": 0.0, "avg_logprob": -0.15191902570276453, "compression_ratio": 1.4358108108108107, "no_speech_prob": 0.1739407181739807}, {"id": 55, "seek": 16048, "start": 179.6, "end": 181.72, "text": " Tu kontekst jest kr\u00f3lem.", "tokens": [51320, 7836, 14373, 916, 372, 3492, 42366, 10386, 13, 51426], "temperature": 0.0, "avg_logprob": -0.15191902570276453, "compression_ratio": 1.4358108108108107, "no_speech_prob": 0.1739407181739807}, {"id": 56, "seek": 16048, "start": 181.92, "end": 186.32, "text": " Znaczenie s\u0142owa zale\u017cy od tego, co by\u0142o pi\u0119\u0107 s\u0142\u00f3w wcze\u015bniej.", "tokens": [51436, 1176, 77, 326, 16778, 15116, 5528, 710, 37169, 3611, 8627, 11, 598, 14811, 32677, 2162, 15116, 3901, 40785, 13, 51656], "temperature": 0.0, "avg_logprob": -0.15191902570276453, "compression_ratio": 1.4358108108108107, "no_speech_prob": 0.1739407181739807}, {"id": 57, "seek": 16048, "start": 186.51999999999998, "end": 189.76, "text": " Model musi rozumie\u0107, \u017ce zamek w zdaniu kr\u00f3l mieszka w zamku,", "tokens": [51666, 17105, 37587, 48797, 414, 2162, 11, 3561, 710, 529, 74, 261, 16221, 25849, 42366, 75, 33039, 2330, 261, 19876, 5279, 11, 51828], "temperature": 0.0, "avg_logprob": -0.15191902570276453, "compression_ratio": 1.4358108108108107, "no_speech_prob": 0.1739407181739807}, {"id": 58, "seek": 18976, "start": 189.92, "end": 193.35999999999999, "text": " to nie to samo, co zaci\u0105\u0142 mi si\u0119 zamek w kurtce.", "tokens": [50372, 281, 2838, 281, 36422, 11, 598, 34430, 11404, 1221, 2752, 3244, 710, 529, 74, 261, 34701, 384, 13, 50544], "temperature": 0.0, "avg_logprob": -0.1705879635281033, "compression_ratio": 1.4580645161290322, "no_speech_prob": 0.0010870350524783134}, {"id": 59, "seek": 18976, "start": 193.56, "end": 195.67999999999998, "text": " A modele LTI by\u0142y na to \u015blepe.", "tokens": [50554, 316, 4391, 306, 441, 5422, 26366, 1667, 281, 8299, 306, 494, 13, 50660], "temperature": 0.0, "avg_logprob": -0.1705879635281033, "compression_ratio": 1.4580645161290322, "no_speech_prob": 0.0010870350524783134}, {"id": 60, "seek": 18976, "start": 195.88, "end": 198.95999999999998, "text": " W artykule jest ten \u015bwietny test, kt\u00f3ry to obna\u017ca, prawda?", "tokens": [50670, 343, 594, 874, 74, 2271, 3492, 2064, 8299, 39083, 1634, 1500, 11, 9913, 281, 1111, 629, 35075, 11, 43607, 30, 50824], "temperature": 0.0, "avg_logprob": -0.1705879635281033, "compression_ratio": 1.4580645161290322, "no_speech_prob": 0.0010870350524783134}, {"id": 61, "seek": 18976, "start": 199.16, "end": 200.79999999999998, "text": " Zadanie selekty w coping.", "tokens": [50834, 1176, 345, 7155, 23264, 74, 874, 261, 32893, 13, 50916], "temperature": 0.0, "avg_logprob": -0.1705879635281033, "compression_ratio": 1.4580645161290322, "no_speech_prob": 0.0010870350524783134}, {"id": 62, "seek": 18976, "start": 201.0, "end": 203.72, "text": " Tak, to jest genialne w swojej prostocie.", "tokens": [50926, 9118, 11, 281, 3492, 48228, 716, 261, 29489, 73, 10293, 905, 414, 13, 51062], "temperature": 0.0, "avg_logprob": -0.1705879635281033, "compression_ratio": 1.4580645161290322, "no_speech_prob": 0.0010870350524783134}, {"id": 63, "seek": 18976, "start": 203.92, "end": 208.39999999999998, "text": " W takiej normalnej wersji, coping, model dostaje sekwencj\u0119 i ma", "tokens": [51072, 343, 38941, 2710, 11794, 261, 433, 4013, 11, 32893, 11, 2316, 20568, 11153, 17215, 15615, 41960, 741, 463, 51296], "temperature": 0.0, "avg_logprob": -0.1705879635281033, "compression_ratio": 1.4580645161290322, "no_speech_prob": 0.0010870350524783134}, {"id": 64, "seek": 18976, "start": 208.6, "end": 212.32, "text": " skopiowa\u0107 tokeny, kt\u00f3re s\u0105, powiedzmy, co dziesi\u0119\u0107 miejsc.", "tokens": [51306, 1110, 404, 72, 11445, 281, 2653, 88, 11, 8864, 9015, 11, 27617, 2226, 11, 598, 9758, 530, 5034, 2162, 32754, 13, 51492], "temperature": 0.0, "avg_logprob": -0.1705879635281033, "compression_ratio": 1.4580645161290322, "no_speech_prob": 0.0010870350524783134}, {"id": 65, "seek": 18976, "start": 212.51999999999998, "end": 215.39999999999998, "text": " OK. I model LTI mo\u017ce si\u0119 tego nauczy\u0107.", "tokens": [51502, 2264, 13, 286, 2316, 441, 5422, 12034, 3244, 8627, 49103, 27150, 13, 51646], "temperature": 0.0, "avg_logprob": -0.1705879635281033, "compression_ratio": 1.4580645161290322, "no_speech_prob": 0.0010870350524783134}, {"id": 66, "seek": 18976, "start": 215.6, "end": 219.44, "text": " Dzia\u0142a jak ten robot, mechanicznie kopiuje co dziesi\u0105ty element.", "tokens": [51656, 39448, 25605, 4207, 2064, 7881, 11, 4236, 17946, 2766, 28920, 5951, 2884, 598, 9758, 530, 11404, 874, 4478, 13, 51848], "temperature": 0.0, "avg_logprob": -0.1705879635281033, "compression_ratio": 1.4580645161290322, "no_speech_prob": 0.0010870350524783134}, {"id": 67, "seek": 21944, "start": 219.68, "end": 223.28, "text": " Ale w selekty w coping zasady si\u0119 zmieniaj\u0105 i to diametralnie.", "tokens": [50376, 9366, 261, 23264, 74, 874, 261, 32893, 26530, 880, 3244, 17020, 18811, 8555, 741, 281, 7484, 302, 2155, 2766, 13, 50556], "temperature": 0.0, "avg_logprob": -0.17429796441808923, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.001504753832705319}, {"id": 68, "seek": 21944, "start": 223.48, "end": 227.35999999999999, "text": " Model ma skopiowa\u0107 tylko te tokeny, kt\u00f3re s\u0105 specjalnie oznaczone,", "tokens": [50566, 17105, 463, 1110, 404, 72, 11445, 13219, 535, 281, 2653, 88, 11, 8864, 9015, 46433, 2766, 277, 22672, 14875, 546, 11, 50760], "temperature": 0.0, "avg_logprob": -0.17429796441808923, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.001504753832705319}, {"id": 69, "seek": 21944, "start": 227.56, "end": 229.68, "text": " a ich pozycje s\u0105 zupe\u0142nie losowe.", "tokens": [50770, 257, 1893, 49358, 44261, 9015, 49922, 1750, 6880, 13, 50876], "temperature": 0.0, "avg_logprob": -0.17429796441808923, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.001504753832705319}, {"id": 70, "seek": 21944, "start": 229.88, "end": 233.0, "text": " Musi aktywnie wybra\u0107, co zapami\u0119ta\u0107, a co ola\u0107.", "tokens": [50886, 3569, 72, 9308, 874, 14215, 4628, 6198, 2162, 11, 598, 14223, 23806, 42931, 11, 257, 598, 277, 875, 2162, 13, 51042], "temperature": 0.0, "avg_logprob": -0.17429796441808923, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.001504753832705319}, {"id": 71, "seek": 21944, "start": 233.2, "end": 235.8, "text": " I tu modele LTI kompletnie zawodz\u0105.", "tokens": [51052, 286, 2604, 4391, 306, 441, 5422, 5207, 14657, 2766, 28165, 378, 8925, 13, 51182], "temperature": 0.0, "avg_logprob": -0.17429796441808923, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.001504753832705319}, {"id": 72, "seek": 21944, "start": 236.0, "end": 239.28, "text": " Nie s\u0105, jak to pi\u0119knie uj\u0119li autorzy, content aware.", "tokens": [51192, 12016, 9015, 11, 4207, 281, 48085, 2766, 344, 11115, 2081, 19510, 1229, 11, 2701, 3650, 13, 51356], "temperature": 0.0, "avg_logprob": -0.17429796441808923, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.001504753832705319}, {"id": 73, "seek": 21944, "start": 239.48, "end": 241.07999999999998, "text": " Nie s\u0105 \u015bwiadome tre\u015bci.", "tokens": [51366, 12016, 9015, 21485, 345, 423, 2192, 6199, 13, 51446], "temperature": 0.0, "avg_logprob": -0.17429796441808923, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.001504753832705319}, {"id": 74, "seek": 21944, "start": 241.28, "end": 245.28, "text": " OK, czyli stare modele by\u0142y \u015blepe na tre\u015b\u0107.", "tokens": [51456, 2264, 11, 16591, 22432, 4391, 306, 26366, 8299, 306, 494, 1667, 2192, 7753, 13, 51656], "temperature": 0.0, "avg_logprob": -0.17429796441808923, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.001504753832705319}, {"id": 75, "seek": 21944, "start": 245.48, "end": 248.96, "text": " To jak gu i da\u0142, dali mambie jej.", "tokens": [51666, 1407, 4207, 695, 741, 1120, 1221, 11, 274, 5103, 13524, 7392, 28924, 13, 51840], "temperature": 0.0, "avg_logprob": -0.17429796441808923, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.001504753832705319}, {"id": 76, "seek": 24896, "start": 249.08, "end": 250.88, "text": " No, metaforyczne oczy.", "tokens": [50370, 883, 11, 1131, 2792, 827, 38491, 277, 6522, 13, 50460], "temperature": 0.0, "avg_logprob": -0.15525904189061074, "compression_ratio": 1.3917910447761195, "no_speech_prob": 0.0008825387922115624}, {"id": 77, "seek": 24896, "start": 251.08, "end": 253.92000000000002, "text": " Jak j\u0105 nauczyli odr\u00f3\u017cnia\u0107 ziarno od plef?", "tokens": [50470, 15029, 35692, 49103, 1229, 2081, 3611, 11721, 1427, 12679, 2162, 710, 72, 1083, 78, 3611, 3362, 69, 30, 50612], "temperature": 0.0, "avg_logprob": -0.15525904189061074, "compression_ratio": 1.3917910447761195, "no_speech_prob": 0.0008825387922115624}, {"id": 78, "seek": 24896, "start": 254.12, "end": 260.36, "text": " I to jest ten moment, w kt\u00f3rym pojawia si\u0119 ta prosta, ale absolutnie genialna zmiana.", "tokens": [50622, 286, 281, 3492, 2064, 1623, 11, 261, 30120, 30655, 654, 3244, 1846, 582, 8638, 11, 6775, 18757, 2766, 48228, 629, 17020, 8497, 13, 50934], "temperature": 0.0, "avg_logprob": -0.15525904189061074, "compression_ratio": 1.3917910447761195, "no_speech_prob": 0.0008825387922115624}, {"id": 79, "seek": 24896, "start": 260.56, "end": 265.24, "text": " Zamiast sta\u0142ych, niezmiennych parametr\u00f3w, te kluczowe elementy mamby w", "tokens": [50944, 1176, 4526, 525, 11135, 47655, 11, 33511, 76, 1053, 9399, 6220, 27965, 3901, 11, 535, 9671, 1311, 89, 6880, 4478, 88, 13524, 2322, 261, 51178], "temperature": 0.0, "avg_logprob": -0.15525904189061074, "compression_ratio": 1.3917910447761195, "no_speech_prob": 0.0008825387922115624}, {"id": 80, "seek": 24896, "start": 265.44, "end": 271.96000000000004, "text": " artykule oznaczone jako Delta, B i C, sta\u0142y si\u0119 dynamicznymi funkcjami danych wejszczowych.", "tokens": [51188, 594, 874, 74, 2271, 277, 22672, 14875, 546, 17123, 18183, 11, 363, 741, 383, 11, 11135, 6825, 3244, 8546, 89, 31813, 26476, 66, 73, 4526, 274, 34644, 321, 73, 15453, 3689, 19605, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15525904189061074, "compression_ratio": 1.3917910447761195, "no_speech_prob": 0.0008825387922115624}, {"id": 81, "seek": 24896, "start": 272.16, "end": 275.64, "text": " Czyli dla ka\u017cdego tokena model liczy je na nowo.", "tokens": [51524, 37099, 12285, 21912, 67, 6308, 281, 2653, 64, 2316, 6169, 1229, 1506, 1667, 586, 78, 13, 51698], "temperature": 0.0, "avg_logprob": -0.15525904189061074, "compression_ratio": 1.3917910447761195, "no_speech_prob": 0.0008825387922115624}, {"id": 82, "seek": 27564, "start": 275.84, "end": 277.44, "text": " Dla ka\u017cdego jednego tokena.", "tokens": [50374, 413, 875, 21912, 67, 6308, 5232, 11858, 281, 2653, 64, 13, 50454], "temperature": 0.0, "avg_logprob": -0.11583958864212036, "compression_ratio": 1.4870967741935484, "no_speech_prob": 0.08347208052873611}, {"id": 83, "seek": 27564, "start": 277.64, "end": 280.47999999999996, "text": " Model na nowo oblicza, jak ma si\u0119 zachowa\u0107.", "tokens": [50464, 17105, 1667, 586, 78, 1111, 1050, 2394, 11, 4207, 463, 3244, 29303, 11445, 13, 50606], "temperature": 0.0, "avg_logprob": -0.11583958864212036, "compression_ratio": 1.4870967741935484, "no_speech_prob": 0.08347208052873611}, {"id": 84, "seek": 27564, "start": 280.68, "end": 283.12, "text": " To ju\u017c nie jest ten g\u0142upi robot z fabryki.", "tokens": [50616, 1407, 10678, 2838, 3492, 2064, 18117, 1010, 72, 7881, 710, 5355, 627, 2984, 13, 50738], "temperature": 0.0, "avg_logprob": -0.11583958864212036, "compression_ratio": 1.4870967741935484, "no_speech_prob": 0.08347208052873611}, {"id": 85, "seek": 27564, "start": 283.32, "end": 288.03999999999996, "text": " To bardziej mistrz rzemios\u0142a, kt\u00f3ry ogl\u0105da ka\u017cdy element i decyduje.", "tokens": [50748, 1407, 27209, 3544, 19390, 367, 24313, 2717, 5024, 11, 9913, 49424, 26398, 31615, 4478, 741, 979, 88, 769, 2884, 13, 50984], "temperature": 0.0, "avg_logprob": -0.11583958864212036, "compression_ratio": 1.4870967741935484, "no_speech_prob": 0.08347208052873611}, {"id": 86, "seek": 27564, "start": 288.24, "end": 293.2, "text": " Oten jest wa\u017cny, wzmacniam jego znaczenie, a potem, a ten to szum, ignoruj\u0119.", "tokens": [50994, 422, 1147, 3492, 27777, 1634, 11, 24809, 37065, 77, 2918, 26542, 15397, 326, 16778, 11, 257, 36513, 11, 257, 2064, 281, 7870, 449, 11, 14698, 18258, 13, 51242], "temperature": 0.0, "avg_logprob": -0.11583958864212036, "compression_ratio": 1.4870967741935484, "no_speech_prob": 0.08347208052873611}, {"id": 87, "seek": 27564, "start": 293.4, "end": 295.4, "text": " To troch\u0119 tak, jak my czytamy ksi\u0105\u017ck\u0119.", "tokens": [51252, 1407, 24926, 991, 11, 4207, 452, 6430, 83, 7804, 39311, 15724, 13, 51352], "temperature": 0.0, "avg_logprob": -0.11583958864212036, "compression_ratio": 1.4870967741935484, "no_speech_prob": 0.08347208052873611}, {"id": 88, "seek": 27564, "start": 295.59999999999997, "end": 299.4, "text": " Nie pami\u0119tamy ka\u017cdego s\u0142owa z pierwszego rozdzia\u0142u, ale mamy w g\u0142owie takie", "tokens": [51362, 12016, 31088, 83, 7804, 21912, 67, 6308, 15116, 5528, 710, 27623, 27725, 9544, 28168, 8908, 84, 11, 6775, 17335, 261, 18117, 13998, 15963, 51552], "temperature": 0.0, "avg_logprob": -0.11583958864212036, "compression_ratio": 1.4870967741935484, "no_speech_prob": 0.08347208052873611}, {"id": 89, "seek": 27564, "start": 299.59999999999997, "end": 303.56, "text": " skompresowane poczucie tego, co si\u0119 sta\u0142o, kim s\u0105 bohaterowie.", "tokens": [51562, 1110, 8586, 495, 23066, 26423, 1311, 414, 8627, 11, 598, 3244, 11135, 5249, 11, 10776, 9015, 748, 71, 771, 13998, 13, 51760], "temperature": 0.0, "avg_logprob": -0.11583958864212036, "compression_ratio": 1.4870967741935484, "no_speech_prob": 0.08347208052873611}, {"id": 90, "seek": 30356, "start": 303.76, "end": 307.96, "text": " Mamba uczy si\u0119 tworzy\u0107 taki zwarty, u\u017cyteczny stan umys\u0142u.", "tokens": [50374, 376, 23337, 344, 6522, 3244, 46288, 27150, 20065, 11873, 446, 88, 11, 34097, 975, 3689, 1634, 27984, 1105, 749, 24066, 13, 50584], "temperature": 0.0, "avg_logprob": -0.14025670006161645, "compression_ratio": 1.376425855513308, "no_speech_prob": 0.0037636184133589268}, {"id": 91, "seek": 30356, "start": 308.16, "end": 315.88, "text": " To jest idealna analogia, bo Transformers z mechanizmem attention dzia\u0142a zupe\u0142nie inaczej.", "tokens": [50594, 1407, 3492, 7157, 629, 16660, 654, 11, 748, 27938, 433, 710, 4236, 590, 17886, 3202, 37903, 49922, 33230, 16920, 13, 50980], "temperature": 0.0, "avg_logprob": -0.14025670006161645, "compression_ratio": 1.376425855513308, "no_speech_prob": 0.0037636184133589268}, {"id": 92, "seek": 30356, "start": 316.08, "end": 317.76, "text": " On trzyma wszystko w pami\u0119ci.", "tokens": [50990, 1282, 34573, 1696, 22607, 261, 31088, 537, 13, 51074], "temperature": 0.0, "avg_logprob": -0.14025670006161645, "compression_ratio": 1.376425855513308, "no_speech_prob": 0.0037636184133589268}, {"id": 93, "seek": 30356, "start": 317.96, "end": 324.84000000000003, "text": " Wszystko. \u017beby zrozumie\u0107 stron\u0119 setn\u0105, on w zasadzie por\u00f3wnuje ka\u017cde nowe s\u0142owo", "tokens": [51084, 343, 10424, 4093, 13, 46864, 2322, 710, 27857, 449, 414, 2162, 45766, 1274, 992, 13113, 11, 322, 261, 44585, 3283, 1515, 812, 895, 13008, 21912, 1479, 586, 68, 15116, 19941, 51428], "temperature": 0.0, "avg_logprob": -0.14025670006161645, "compression_ratio": 1.376425855513308, "no_speech_prob": 0.0037636184133589268}, {"id": 94, "seek": 30356, "start": 325.04, "end": 328.52, "text": " z ka\u017cdym s\u0142owem z poprzednich 99 stron.", "tokens": [51438, 710, 31615, 76, 15116, 305, 443, 710, 1665, 81, 11312, 77, 480, 11803, 45766, 13, 51612], "temperature": 0.0, "avg_logprob": -0.14025670006161645, "compression_ratio": 1.376425855513308, "no_speech_prob": 0.0037636184133589268}, {"id": 95, "seek": 30356, "start": 328.72, "end": 332.72, "text": " To pot\u0119\u017cne, ale no absurdalnie nieefektywne.", "tokens": [51622, 1407, 1847, 1274, 1427, 716, 11, 6775, 572, 19774, 304, 2766, 2838, 5666, 916, 874, 86, 716, 13, 51822], "temperature": 0.0, "avg_logprob": -0.14025670006161645, "compression_ratio": 1.376425855513308, "no_speech_prob": 0.0037636184133589268}, {"id": 96, "seek": 33272, "start": 332.88000000000005, "end": 333.84000000000003, "text": " Amamba.", "tokens": [50372, 2012, 23337, 13, 50420], "temperature": 0.0, "avg_logprob": -0.18671716543344352, "compression_ratio": 1.3579766536964981, "no_speech_prob": 0.005017660092562437}, {"id": 97, "seek": 33272, "start": 334.04, "end": 338.0, "text": " Amamba kompresuje wiedz\u0119 stycz 99 stron do", "tokens": [50430, 2012, 23337, 5207, 14508, 13008, 46894, 11052, 7952, 3689, 11803, 45766, 360, 50628], "temperature": 0.0, "avg_logprob": -0.18671716543344352, "compression_ratio": 1.3579766536964981, "no_speech_prob": 0.005017660092562437}, {"id": 98, "seek": 33272, "start": 338.20000000000005, "end": 342.88000000000005, "text": " zwi\u0119z\u0142ego stanu i na jego podstawie interpretuje setn\u0105 stron\u0119.", "tokens": [50638, 710, 22423, 89, 1221, 6308, 27984, 84, 741, 1667, 26542, 43443, 414, 7302, 13008, 992, 13113, 45766, 1274, 13, 50872], "temperature": 0.0, "avg_logprob": -0.18671716543344352, "compression_ratio": 1.3579766536964981, "no_speech_prob": 0.005017660092562437}, {"id": 99, "seek": 33272, "start": 343.08000000000004, "end": 347.88000000000005, "text": " Co jest fascynuj\u0105ce, to nawi\u0105zuje do klasycznych mechanizm\u00f3w, kt\u00f3re znamy z sieci RNN,", "tokens": [50882, 3066, 3492, 30632, 1344, 77, 13263, 384, 11, 281, 18969, 11404, 11728, 2884, 360, 9671, 5871, 3689, 9399, 4236, 590, 76, 3901, 11, 8864, 710, 5378, 88, 710, 2804, 537, 45702, 45, 11, 51122], "temperature": 0.0, "avg_logprob": -0.18671716543344352, "compression_ratio": 1.3579766536964981, "no_speech_prob": 0.005017660092562437}, {"id": 100, "seek": 33272, "start": 348.08000000000004, "end": 351.36, "text": " jak LSTM, do gating mechanisms.", "tokens": [51132, 4207, 441, 6840, 44, 11, 360, 290, 990, 15902, 13, 51296], "temperature": 0.0, "avg_logprob": -0.18671716543344352, "compression_ratio": 1.3579766536964981, "no_speech_prob": 0.005017660092562437}, {"id": 101, "seek": 33272, "start": 351.56, "end": 354.36, "text": " Czyli ten parametr delta to jest w zasadzie taki", "tokens": [51306, 37099, 2064, 6220, 27965, 8289, 281, 3492, 261, 44585, 3283, 20065, 51446], "temperature": 0.0, "avg_logprob": -0.18671716543344352, "compression_ratio": 1.3579766536964981, "no_speech_prob": 0.005017660092562437}, {"id": 102, "seek": 33272, "start": 354.56, "end": 357.56, "text": " dynamiczny potencjometr skupienia.", "tokens": [51456, 8546, 89, 1634, 1847, 22660, 73, 649, 81, 1110, 1010, 18811, 13, 51606], "temperature": 0.0, "avg_logprob": -0.18671716543344352, "compression_ratio": 1.3579766536964981, "no_speech_prob": 0.005017660092562437}, {"id": 103, "seek": 33272, "start": 357.76000000000005, "end": 358.84000000000003, "text": " Tak.", "tokens": [51616, 9118, 13, 51670], "temperature": 0.0, "avg_logprob": -0.18671716543344352, "compression_ratio": 1.3579766536964981, "no_speech_prob": 0.005017660092562437}, {"id": 104, "seek": 33272, "start": 359.04, "end": 360.08000000000004, "text": " Dobre okre\u015blenie.", "tokens": [51680, 29679, 265, 3133, 265, 1788, 6698, 414, 13, 51732], "temperature": 0.0, "avg_logprob": -0.18671716543344352, "compression_ratio": 1.3579766536964981, "no_speech_prob": 0.005017660092562437}, {"id": 105, "seek": 36008, "start": 360.24, "end": 364.84, "text": " Mo\u017ce podkr\u0119ci\u0107 uwag\u0119 na bie\u017c\u0105cym s\u0142owie, m\u00f3wi\u0105c to jest superwa\u017cne.", "tokens": [50372, 43774, 2497, 38553, 1274, 39162, 43696, 1667, 272, 414, 1427, 1611, 1344, 76, 15116, 13998, 11, 46591, 66, 281, 3492, 1687, 27111, 716, 13, 50602], "temperature": 0.0, "avg_logprob": -0.15128929550583298, "compression_ratio": 1.5047923322683705, "no_speech_prob": 0.010392600670456886}, {"id": 106, "seek": 36008, "start": 365.03999999999996, "end": 369.8, "text": " Zapomnij na chwil\u0119 o przesz\u0142o\u015bci albo go wyciszy\u0107, \u017ceby zachowa\u0107 to, co by\u0142o wcze\u015bniej.", "tokens": [50612, 34018, 38131, 1718, 1667, 41941, 1274, 277, 6541, 10430, 35059, 22622, 352, 4628, 26720, 27150, 11, 11316, 29303, 11445, 281, 11, 598, 14811, 40785, 13, 50850], "temperature": 0.0, "avg_logprob": -0.15128929550583298, "compression_ratio": 1.5047923322683705, "no_speech_prob": 0.010392600670456886}, {"id": 107, "seek": 36008, "start": 370.0, "end": 371.68, "text": " W\u0142a\u015bnie tak to dzia\u0142a.", "tokens": [50860, 343, 5024, 12221, 991, 281, 37903, 13, 50944], "temperature": 0.0, "avg_logprob": -0.15128929550583298, "compression_ratio": 1.5047923322683705, "no_speech_prob": 0.010392600670456886}, {"id": 108, "seek": 36008, "start": 371.88, "end": 376.91999999999996, "text": " Du\u017ca warto\u015b\u0107 delta resetuje stan i skupia si\u0119 na imowe informacji.", "tokens": [50954, 5153, 35075, 31830, 7753, 8289, 14322, 13008, 27984, 741, 1110, 1010, 654, 3244, 1667, 566, 6880, 1356, 13152, 13, 51206], "temperature": 0.0, "avg_logprob": -0.15128929550583298, "compression_ratio": 1.5047923322683705, "no_speech_prob": 0.010392600670456886}, {"id": 109, "seek": 36008, "start": 377.12, "end": 383.2, "text": " Ma\u0142a warto\u015b\u0107 delta skutecznie ignoruje bie\u017c\u0105cy token i podtrzymuje pami\u0119\u0107 o przesz\u0142o\u015bci.", "tokens": [51216, 4042, 5024, 31830, 7753, 8289, 1110, 1169, 19923, 14698, 13008, 272, 414, 1427, 1611, 1344, 14862, 741, 2497, 6903, 26681, 13008, 31088, 2162, 277, 6541, 10430, 35059, 13, 51520], "temperature": 0.0, "avg_logprob": -0.15128929550583298, "compression_ratio": 1.5047923322683705, "no_speech_prob": 0.010392600670456886}, {"id": 110, "seek": 36008, "start": 383.4, "end": 386.64, "text": " To daje modelowi zdolno\u015b\u0107 do filtrowania kontekstu, do", "tokens": [51530, 1407, 1120, 2884, 2316, 24503, 16221, 401, 23293, 360, 1387, 6903, 21308, 14373, 916, 372, 84, 11, 360, 51692], "temperature": 0.0, "avg_logprob": -0.15128929550583298, "compression_ratio": 1.5047923322683705, "no_speech_prob": 0.010392600670456886}, {"id": 111, "seek": 36008, "start": 386.84, "end": 389.4, "text": " rozumienia granic na przyk\u0142ad ko\u0144ca zdania.", "tokens": [51702, 48797, 18811, 9370, 299, 1667, 23144, 26470, 496, 16221, 5609, 13, 51830], "temperature": 0.0, "avg_logprob": -0.15128929550583298, "compression_ratio": 1.5047923322683705, "no_speech_prob": 0.010392600670456886}, {"id": 112, "seek": 38940, "start": 389.59999999999997, "end": 395.35999999999996, "text": " Czekaj, czekaj, ale ta ca\u0142a dynamika, to uzale\u017cnienie od tre\u015bci, to brzni, jakby\u015bmy", "tokens": [50374, 383, 19878, 1805, 11, 6472, 916, 1805, 11, 6775, 1846, 1335, 5024, 5999, 5439, 11, 281, 16851, 45494, 77, 27385, 3611, 2192, 6199, 11, 281, 738, 89, 3722, 11, 28976, 10513, 50662], "temperature": 0.0, "avg_logprob": -0.16006200963800604, "compression_ratio": 1.4294871794871795, "no_speech_prob": 0.0009556784061715007}, {"id": 113, "seek": 38940, "start": 395.56, "end": 401.12, "text": " w\u0142a\u015bnie nie wyrzucili zab\u00f3r t\u0119 g\u0142\u00f3wn\u0105 zalet\u0119 starych modeli SSM, ich szybko\u015b\u0107.", "tokens": [50672, 14234, 2838, 4628, 19390, 1311, 2312, 24838, 15614, 32489, 18117, 812, 895, 1611, 29599, 302, 1274, 342, 822, 339, 2316, 72, 12238, 44, 11, 1893, 36456, 4093, 7753, 13, 50950], "temperature": 0.0, "avg_logprob": -0.16006200963800604, "compression_ratio": 1.4294871794871795, "no_speech_prob": 0.0009556784061715007}, {"id": 114, "seek": 38940, "start": 401.32, "end": 405.67999999999995, "text": " Przecie\u017c one by\u0142y szybkie, bo mo\u017cna je by\u0142o oblicza\u0107 za pomoc\u0105 super zoptymalizowanych", "tokens": [50960, 2114, 1381, 40082, 472, 26366, 36456, 22872, 11, 748, 17790, 1506, 14811, 1111, 1050, 35873, 7949, 48962, 1611, 1687, 710, 404, 874, 5579, 590, 23341, 339, 51178], "temperature": 0.0, "avg_logprob": -0.16006200963800604, "compression_ratio": 1.4294871794871795, "no_speech_prob": 0.0009556784061715007}, {"id": 115, "seek": 38940, "start": 405.88, "end": 411.23999999999995, "text": " operacji splotu, czyli convolutions, a te dzia\u0142aj\u0105 tylko przy sta\u0142ych parametrach.", "tokens": [51188, 2208, 13152, 4732, 310, 84, 11, 16591, 3754, 15892, 11, 257, 535, 27121, 11133, 13219, 6501, 11135, 47655, 6220, 27965, 608, 13, 51456], "temperature": 0.0, "avg_logprob": -0.16006200963800604, "compression_ratio": 1.4294871794871795, "no_speech_prob": 0.0009556784061715007}, {"id": 116, "seek": 38940, "start": 411.44, "end": 413.15999999999997, "text": " Wygl\u0105da mi to na ogromny kompromis.", "tokens": [51466, 14458, 7191, 26398, 2752, 281, 1667, 34416, 298, 1634, 5207, 28722, 271, 13, 51552], "temperature": 0.0, "avg_logprob": -0.16006200963800604, "compression_ratio": 1.4294871794871795, "no_speech_prob": 0.0009556784061715007}, {"id": 117, "seek": 38940, "start": 413.35999999999996, "end": 416.79999999999995, "text": " I tu dochodzimy do drugiego aktu g\u0119mszo w tej pracy.", "tokens": [51562, 286, 2604, 9243, 378, 89, 13189, 360, 4110, 12200, 13680, 84, 290, 1274, 2592, 4765, 261, 12573, 35591, 13, 51734], "temperature": 0.0, "avg_logprob": -0.16006200963800604, "compression_ratio": 1.4294871794871795, "no_speech_prob": 0.0009556784061715007}, {"id": 118, "seek": 41680, "start": 416.96000000000004, "end": 419.12, "text": " Tym razem czysto in\u017cynierskiego.", "tokens": [50372, 314, 4199, 40225, 6430, 20875, 294, 1427, 2534, 4890, 42349, 13, 50480], "temperature": 0.0, "avg_logprob": -0.15645265966896119, "compression_ratio": 1.3656716417910448, "no_speech_prob": 0.004925742745399475}, {"id": 119, "seek": 41680, "start": 419.32, "end": 423.96000000000004, "text": " OK. Autorzy doskonale wiedzieli, \u017ce trac\u0105 mo\u017cliwo\u015b\u0107 u\u017cycia splot\u00f3w.", "tokens": [50490, 2264, 13, 6049, 284, 1229, 4491, 18295, 1220, 261, 15338, 23099, 11, 3561, 504, 326, 1611, 30854, 48847, 34097, 2755, 4732, 310, 3901, 13, 50722], "temperature": 0.0, "avg_logprob": -0.15645265966896119, "compression_ratio": 1.3656716417910448, "no_speech_prob": 0.004925742745399475}, {"id": 120, "seek": 41680, "start": 424.16, "end": 431.08000000000004, "text": " Musieli wr\u00f3ci\u0107 do oblicze\u0144 rekurrencyjnych, kt\u00f3re z natury s\u0105 no sekwencyjne i po prostu wolne.", "tokens": [50732, 3569, 23099, 928, 812, 39162, 360, 1111, 1050, 49689, 319, 33503, 1095, 42949, 9399, 11, 8864, 710, 2249, 2598, 9015, 572, 17215, 86, 3020, 73, 716, 741, 714, 19518, 20960, 716, 13, 51078], "temperature": 0.0, "avg_logprob": -0.15645265966896119, "compression_ratio": 1.3656716417910448, "no_speech_prob": 0.004925742745399475}, {"id": 121, "seek": 41680, "start": 431.28000000000003, "end": 437.96000000000004, "text": " Ale zamiast u\u017cywa\u0107 standardowej implementacji stworzyli co\u015b, co nazywaj\u0105 hardware aware algorithm.", "tokens": [51088, 9366, 710, 4526, 525, 34097, 25234, 3832, 21091, 4445, 13152, 342, 28321, 1229, 2081, 19241, 11, 598, 20151, 27112, 11133, 8837, 3650, 9284, 13, 51422], "temperature": 0.0, "avg_logprob": -0.15645265966896119, "compression_ratio": 1.3656716417910448, "no_speech_prob": 0.004925742745399475}, {"id": 122, "seek": 41680, "start": 438.16, "end": 441.32, "text": " Algorytm \u015bwiadomy sprz\u0119tu. Co to znaczy w praktyce?", "tokens": [51432, 35014, 827, 83, 76, 21485, 345, 8488, 6103, 11052, 9179, 13, 3066, 281, 36584, 261, 3206, 74, 874, 384, 30, 51590], "temperature": 0.0, "avg_logprob": -0.15645265966896119, "compression_ratio": 1.3656716417910448, "no_speech_prob": 0.004925742745399475}, {"id": 123, "seek": 44132, "start": 441.52, "end": 447.15999999999997, "text": " U\u017cyli operacji zwanej Parallel Scan to troch\u0119 jak b\u0142yskawiczne liczenie sumy bie\u017c\u0105cej", "tokens": [50374, 624, 7735, 2081, 2208, 13152, 11873, 1929, 73, 3457, 336, 338, 41177, 281, 24926, 4207, 272, 1221, 749, 74, 1607, 17946, 716, 6169, 16778, 2408, 88, 272, 414, 1427, 1611, 20811, 50656], "temperature": 0.0, "avg_logprob": -0.13266324377679206, "compression_ratio": 1.4267100977198697, "no_speech_prob": 0.010710413567721844}, {"id": 124, "seek": 44132, "start": 447.36, "end": 451.64, "text": " w arkuszu kalkulacyjnym, ale zoptymalizowane do granic na GPU.", "tokens": [50666, 261, 14408, 301, 11728, 34960, 425, 31285, 12996, 11, 6775, 710, 404, 874, 5579, 590, 23066, 360, 9370, 299, 1667, 18407, 13, 50880], "temperature": 0.0, "avg_logprob": -0.13266324377679206, "compression_ratio": 1.4267100977198697, "no_speech_prob": 0.010710413567721844}, {"id": 125, "seek": 44132, "start": 451.84, "end": 456.32, "text": " To pozwala zachowa\u0107 pewne cechy rekurrencji, ale liczy\u0107 to lwnolegle.", "tokens": [50890, 1407, 40557, 5159, 29303, 11445, 25889, 716, 1769, 28629, 319, 33503, 1095, 19649, 11, 6775, 6169, 27150, 281, 287, 895, 4812, 22631, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13266324377679206, "compression_ratio": 1.4267100977198697, "no_speech_prob": 0.010710413567721844}, {"id": 126, "seek": 44132, "start": 456.52, "end": 458.28, "text": " Ale kluczowy jest jak to zrobili.", "tokens": [51124, 9366, 9671, 1311, 89, 10089, 3492, 4207, 281, 44399, 2312, 13, 51212], "temperature": 0.0, "avg_logprob": -0.13266324377679206, "compression_ratio": 1.4267100977198697, "no_speech_prob": 0.010710413567721844}, {"id": 127, "seek": 44132, "start": 458.48, "end": 465.68, "text": " Dok\u0142adnie. Wiedzieli, \u017ce w\u0105skim gard\u0142em w GPU jest transfer danych mi\u0119dzy t\u0105 ogromn\u0105, ale woln\u0105", "tokens": [51222, 29768, 10358, 2766, 13, 343, 15338, 23099, 11, 3561, 261, 1611, 5161, 332, 5628, 11126, 261, 18407, 3492, 5003, 274, 34644, 33964, 32294, 34416, 298, 13113, 11, 6775, 20960, 13113, 51582], "temperature": 0.0, "avg_logprob": -0.13266324377679206, "compression_ratio": 1.4267100977198697, "no_speech_prob": 0.010710413567721844}, {"id": 128, "seek": 44132, "start": 465.88, "end": 470.8, "text": " pami\u0119ci\u0105 HBM, a malutk\u0105, ale super szybk\u0105 pami\u0119ci\u0105 podr\u0119czn\u0105 SRAM.", "tokens": [51592, 31088, 34381, 389, 18345, 11, 257, 2806, 325, 26304, 11, 6775, 1687, 36456, 26304, 31088, 34381, 15305, 1274, 3689, 13113, 20840, 2865, 13, 51838], "temperature": 0.0, "avg_logprob": -0.13266324377679206, "compression_ratio": 1.4267100977198697, "no_speech_prob": 0.010710413567721844}, {"id": 129, "seek": 47080, "start": 471.64, "end": 477.16, "text": " Wi\u0119c zaprojektowali sw\u00f3j algorytm tak, \u017ceby jak najwi\u0119cej danych wczyta\u0107 do tej", "tokens": [50406, 32508, 14223, 340, 14930, 305, 5103, 1693, 18999, 3501, 827, 83, 76, 991, 11, 11316, 4207, 48636, 20811, 274, 34644, 261, 6522, 42931, 360, 12573, 50682], "temperature": 0.0, "avg_logprob": -0.1275486867935931, "compression_ratio": 1.4621212121212122, "no_speech_prob": 0.0003398022090550512}, {"id": 130, "seek": 47080, "start": 477.36, "end": 484.08000000000004, "text": " szybkiej pami\u0119ci SRAM raz, zrobi\u0107 na nich wszystko, co trzeba i dopiero na ko\u0144cu zapisa\u0107 wynik.", "tokens": [50692, 36456, 45145, 31088, 537, 20840, 2865, 9639, 11, 31785, 1667, 25570, 22607, 11, 598, 25860, 741, 21900, 12030, 1667, 26470, 12032, 14223, 3837, 2162, 31936, 1035, 13, 51028], "temperature": 0.0, "avg_logprob": -0.1275486867935931, "compression_ratio": 1.4621212121212122, "no_speech_prob": 0.0003398022090550512}, {"id": 131, "seek": 47080, "start": 484.28000000000003, "end": 488.2, "text": " U\u017cywaj\u0105 do tego technik jak Kernel Fusion i Recomputation.", "tokens": [51038, 624, 7735, 86, 11133, 360, 8627, 1537, 1035, 4207, 40224, 338, 36721, 741, 1300, 1112, 2582, 399, 13, 51234], "temperature": 0.0, "avg_logprob": -0.1275486867935931, "compression_ratio": 1.4621212121212122, "no_speech_prob": 0.0003398022090550512}, {"id": 132, "seek": 47080, "start": 488.40000000000003, "end": 493.92, "text": " Czyli zamiast zapisywa\u0107 wszystkie po\u015brednie wyniki do wolnej pami\u0119ci, co jest kosztowne,", "tokens": [51244, 37099, 710, 4526, 525, 14223, 14169, 25234, 31723, 714, 1788, 986, 2766, 31936, 9850, 360, 20960, 11794, 31088, 537, 11, 598, 3492, 19532, 2682, 648, 68, 11, 51520], "temperature": 0.0, "avg_logprob": -0.1275486867935931, "compression_ratio": 1.4621212121212122, "no_speech_prob": 0.0003398022090550512}, {"id": 133, "seek": 47080, "start": 494.12, "end": 497.04, "text": " wol\u0105 je w razie potrzeby policzy\u0107 jeszcze raz.", "tokens": [51530, 20960, 1611, 1506, 261, 9639, 414, 28577, 2322, 6285, 27150, 14168, 9639, 13, 51676], "temperature": 0.0, "avg_logprob": -0.1275486867935931, "compression_ratio": 1.4621212121212122, "no_speech_prob": 0.0003398022090550512}, {"id": 134, "seek": 49704, "start": 497.24, "end": 501.88, "text": " Bo to i tak jest szybsze, skoro dane s\u0105 ju\u017c pod r\u0119k\u0105 w tej ultra szybkiej pami\u0119ci.", "tokens": [50374, 3286, 281, 741, 991, 3492, 30526, 929, 1381, 11, 1110, 10780, 49206, 9015, 10678, 2497, 41197, 26304, 261, 12573, 14808, 36456, 45145, 31088, 537, 13, 50606], "temperature": 0.0, "avg_logprob": -0.12394075012207031, "compression_ratio": 1.400749063670412, "no_speech_prob": 0.009019195102155209}, {"id": 135, "seek": 49704, "start": 502.08000000000004, "end": 508.76000000000005, "text": " Dok\u0142adnie. I efekt jest taki, \u017ce ich implementacja na nowoczeznym GPU jak A100 jest", "tokens": [50616, 29768, 10358, 2766, 13, 286, 31482, 8192, 3492, 20065, 11, 3561, 1893, 4445, 23395, 1667, 586, 905, 1381, 89, 12996, 18407, 4207, 316, 6879, 3492, 50950], "temperature": 0.0, "avg_logprob": -0.12394075012207031, "compression_ratio": 1.400749063670412, "no_speech_prob": 0.009019195102155209}, {"id": 136, "seek": 49704, "start": 508.96000000000004, "end": 513.32, "text": " od 20 do 40 razy szybsza ni\u017c standardowe podej\u015bcie do rekurrencji.", "tokens": [50960, 3611, 945, 360, 3356, 9639, 88, 30526, 929, 2394, 28502, 3832, 6880, 7468, 73, 9815, 360, 319, 33503, 1095, 19649, 13, 51178], "temperature": 0.0, "avg_logprob": -0.12394075012207031, "compression_ratio": 1.400749063670412, "no_speech_prob": 0.009019195102155209}, {"id": 137, "seek": 49704, "start": 513.52, "end": 516.84, "text": " To nie jest optymalizacja, to jest zupe\u0142nie inna liga.", "tokens": [51188, 1407, 2838, 3492, 2427, 4199, 304, 590, 23395, 11, 281, 3492, 49922, 294, 629, 287, 9900, 13, 51354], "temperature": 0.0, "avg_logprob": -0.12394075012207031, "compression_ratio": 1.400749063670412, "no_speech_prob": 0.009019195102155209}, {"id": 138, "seek": 49704, "start": 517.04, "end": 519.52, "text": " To brzmi prawie za dobrze, \u017ceby by\u0142o prawdziwe.", "tokens": [51364, 1407, 738, 89, 3057, 3206, 8699, 7949, 28335, 11, 11316, 14811, 41175, 3992, 826, 13, 51488], "temperature": 0.0, "avg_logprob": -0.12394075012207031, "compression_ratio": 1.400749063670412, "no_speech_prob": 0.009019195102155209}, {"id": 139, "seek": 49704, "start": 519.72, "end": 521.16, "text": " Jest w tym jaki\u015b haczyk?", "tokens": [51498, 24918, 261, 8107, 34721, 324, 6522, 74, 30, 51570], "temperature": 0.0, "avg_logprob": -0.12394075012207031, "compression_ratio": 1.400749063670412, "no_speech_prob": 0.009019195102155209}, {"id": 140, "seek": 52116, "start": 521.3199999999999, "end": 524.0799999999999, "text": " Czy ta optymizacja nie jest przypadkiem tak uszyta", "tokens": [50372, 19832, 1846, 2427, 4199, 590, 23395, 2838, 3492, 33100, 26116, 991, 505, 1229, 1328, 50510], "temperature": 0.0, "avg_logprob": -0.15655020171520756, "compression_ratio": 1.4773519163763067, "no_speech_prob": 0.0015926448395475745}, {"id": 141, "seek": 52116, "start": 524.28, "end": 528.12, "text": " na miar\u0119 pod karty NVD, \u017ce na innym sprz\u0119cie to nie zadzia\u0142a?", "tokens": [50520, 1667, 2752, 289, 1274, 2497, 29120, 88, 46512, 35, 11, 3561, 1667, 294, 12996, 6103, 11052, 4260, 281, 2838, 42788, 89, 25605, 30, 50712], "temperature": 0.0, "avg_logprob": -0.15655020171520756, "compression_ratio": 1.4773519163763067, "no_speech_prob": 0.0015926448395475745}, {"id": 142, "seek": 52116, "start": 528.3199999999999, "end": 532.68, "text": " To jest bardzo s\u0142uszna uwaga i tak to jest maister sztyg", "tokens": [50722, 1407, 3492, 9034, 15116, 22378, 629, 23147, 9286, 741, 991, 281, 3492, 463, 1964, 262, 2682, 18103, 50940], "temperature": 0.0, "avg_logprob": -0.15655020171520756, "compression_ratio": 1.4773519163763067, "no_speech_prob": 0.0015926448395475745}, {"id": 143, "seek": 52116, "start": 532.88, "end": 537.04, "text": " in\u017cynierii, kt\u00f3ry w pe\u0142ni wykorzystuje architektur\u0119 w sztu\u0142ocznych akcelerator\u00f3w.", "tokens": [50950, 294, 1427, 2534, 811, 5597, 11, 9913, 261, 43205, 3722, 43606, 36049, 13008, 3912, 642, 2320, 374, 1274, 261, 262, 2682, 84, 1221, 905, 89, 9399, 9308, 4933, 260, 1639, 3901, 13, 51158], "temperature": 0.0, "avg_logprob": -0.15655020171520756, "compression_ratio": 1.4773519163763067, "no_speech_prob": 0.0015926448395475745}, {"id": 144, "seek": 52116, "start": 537.24, "end": 540.64, "text": " Ale ja bym tego nie nazwa\u0142a wad\u0105 raczej zalet\u0105.", "tokens": [51168, 9366, 2784, 538, 76, 8627, 2838, 20151, 4151, 5024, 261, 345, 1611, 4129, 16920, 29599, 302, 1611, 13, 51338], "temperature": 0.0, "avg_logprob": -0.15655020171520756, "compression_ratio": 1.4773519163763067, "no_speech_prob": 0.0015926448395475745}, {"id": 145, "seek": 52116, "start": 540.8399999999999, "end": 546.8399999999999, "text": " Autorzy nie stworzyli tylko jakiego\u015b teoretycznego modelu, stworzyli praktyczne narz\u0119dzie, kt\u00f3re dzia\u0142a tu", "tokens": [51348, 6049, 284, 1229, 2838, 342, 28321, 1229, 2081, 13219, 4207, 12200, 1788, 535, 418, 874, 3689, 11858, 2316, 84, 11, 342, 28321, 1229, 2081, 3206, 74, 874, 38491, 6714, 89, 42643, 11, 8864, 37903, 2604, 51648], "temperature": 0.0, "avg_logprob": -0.15655020171520756, "compression_ratio": 1.4773519163763067, "no_speech_prob": 0.0015926448395475745}, {"id": 146, "seek": 54684, "start": 546.96, "end": 550.08, "text": " i teraz na sprz\u0119cie, kt\u00f3rego u\u017cywa ca\u0142a bran\u017ca.", "tokens": [50370, 741, 16854, 1667, 6103, 11052, 4260, 11, 46951, 34097, 4151, 1335, 5024, 12029, 35075, 13, 50526], "temperature": 0.0, "avg_logprob": -0.16229926956283464, "compression_ratio": 1.423728813559322, "no_speech_prob": 0.13226023316383362}, {"id": 147, "seek": 54684, "start": 550.2800000000001, "end": 553.48, "text": " A do tego sama architektura mamby jest prostsza.", "tokens": [50536, 316, 360, 8627, 17768, 3912, 642, 2320, 2991, 13524, 2322, 3492, 10293, 82, 2394, 13, 50696], "temperature": 0.0, "avg_logprob": -0.16229926956283464, "compression_ratio": 1.423728813559322, "no_speech_prob": 0.13226023316383362}, {"id": 148, "seek": 54684, "start": 553.6800000000001, "end": 558.6, "text": " W Transformerze przeplatasz dwa r\u00f3\u017cne bloki, Multi-head Attention i MLP,", "tokens": [50706, 343, 27938, 260, 1381, 8325, 39975, 19601, 35045, 47760, 888, 17056, 11, 29238, 12, 1934, 31858, 741, 21601, 47, 11, 50952], "temperature": 0.0, "avg_logprob": -0.16229926956283464, "compression_ratio": 1.423728813559322, "no_speech_prob": 0.13226023316383362}, {"id": 149, "seek": 54684, "start": 558.8000000000001, "end": 562.64, "text": " a tu masz jeden homogeniczny blok, kt\u00f3ry po prostu powielasz.", "tokens": [50962, 257, 2604, 2300, 89, 12906, 3655, 8799, 17946, 1634, 888, 453, 11, 9913, 714, 19518, 3388, 1187, 19601, 13, 51154], "temperature": 0.0, "avg_logprob": -0.16229926956283464, "compression_ratio": 1.423728813559322, "no_speech_prob": 0.13226023316383362}, {"id": 150, "seek": 54684, "start": 562.84, "end": 565.6800000000001, "text": " To jest, no, bardziej eleganckie.", "tokens": [51164, 1407, 3492, 11, 572, 11, 27209, 1118, 1275, 547, 414, 13, 51306], "temperature": 0.0, "avg_logprob": -0.16229926956283464, "compression_ratio": 1.423728813559322, "no_speech_prob": 0.13226023316383362}, {"id": 151, "seek": 54684, "start": 565.88, "end": 569.5600000000001, "text": " Wszystko to brzmi niezwykle elegancko na papierze.", "tokens": [51316, 343, 10424, 4093, 281, 738, 89, 3057, 33511, 9726, 14677, 1118, 1275, 41416, 1667, 37410, 1381, 13, 51500], "temperature": 0.0, "avg_logprob": -0.16229926956283464, "compression_ratio": 1.423728813559322, "no_speech_prob": 0.13226023316383362}, {"id": 152, "seek": 54684, "start": 569.76, "end": 576.2, "text": " Ale w \u015bwiecie, a i, wiesz, teoria to jedno, a brutalna rzeczywisto\u015b\u0107 benchmark\u00f3w to drugie.", "tokens": [51510, 9366, 261, 40078, 4260, 11, 257, 741, 11, 261, 15347, 11, 535, 8172, 281, 5232, 1771, 11, 257, 17878, 629, 26297, 86, 9334, 7753, 18927, 3901, 281, 4110, 414, 13, 51832], "temperature": 0.0, "avg_logprob": -0.16229926956283464, "compression_ratio": 1.423728813559322, "no_speech_prob": 0.13226023316383362}, {"id": 153, "seek": 57620, "start": 576.4000000000001, "end": 580.0400000000001, "text": " Czy tw\u00f3rcy mamby maj\u0105 liczb\u0119, kt\u00f3re potwierdzaj\u0105 te obietnice?", "tokens": [50374, 19832, 683, 15614, 1344, 13524, 2322, 26064, 6169, 89, 65, 1274, 11, 8864, 1847, 40717, 28168, 11133, 535, 1111, 1684, 77, 573, 30, 50556], "temperature": 0.0, "avg_logprob": -0.16547978469748903, "compression_ratio": 1.37987012987013, "no_speech_prob": 0.005661006551235914}, {"id": 154, "seek": 57620, "start": 580.24, "end": 582.24, "text": " Maj\u0105 i to jakie.", "tokens": [50566, 7048, 1611, 741, 281, 22124, 13, 50666], "temperature": 0.0, "avg_logprob": -0.16547978469748903, "compression_ratio": 1.37987012987013, "no_speech_prob": 0.005661006551235914}, {"id": 155, "seek": 57620, "start": 582.44, "end": 585.08, "text": " Zacznijmy od tych zada\u0144 syntetycznych.", "tokens": [50676, 1176, 14875, 77, 1718, 2226, 3611, 15180, 710, 1538, 5248, 23980, 2210, 3689, 9399, 13, 50808], "temperature": 0.0, "avg_logprob": -0.16547978469748903, "compression_ratio": 1.37987012987013, "no_speech_prob": 0.005661006551235914}, {"id": 156, "seek": 57620, "start": 585.2800000000001, "end": 591.12, "text": " W tym Selective Coping, o kt\u00f3rym m\u00f3wili\u015bmy, mamba osi\u0105ga prawie 100 proc. dok\u0142adno\u015bci.", "tokens": [50818, 343, 8107, 13638, 488, 11579, 278, 11, 277, 30120, 13489, 43912, 11, 13524, 4231, 3003, 11404, 3680, 3206, 8699, 2319, 9510, 13, 45864, 16438, 13, 51110], "temperature": 0.0, "avg_logprob": -0.16547978469748903, "compression_ratio": 1.37987012987013, "no_speech_prob": 0.005661006551235914}, {"id": 157, "seek": 57620, "start": 591.32, "end": 594.6, "text": " Ale prawdziwym testem jest zadanie Induction Heads.", "tokens": [51120, 9366, 41175, 3992, 86, 4199, 1500, 443, 3492, 42788, 7155, 2333, 27549, 634, 5834, 13, 51284], "temperature": 0.0, "avg_logprob": -0.16547978469748903, "compression_ratio": 1.37987012987013, "no_speech_prob": 0.005661006551235914}, {"id": 158, "seek": 57620, "start": 594.8000000000001, "end": 599.0400000000001, "text": " To jest to, co uwa\u017ca si\u0119 za kluczowe dla in-context learning w LLM-ach.", "tokens": [51294, 1407, 3492, 281, 11, 598, 48089, 64, 3244, 7949, 9671, 1311, 89, 6880, 12285, 294, 12, 9000, 3828, 2539, 261, 441, 43, 44, 12, 608, 13, 51506], "temperature": 0.0, "avg_logprob": -0.16547978469748903, "compression_ratio": 1.37987012987013, "no_speech_prob": 0.005661006551235914}, {"id": 159, "seek": 57620, "start": 599.24, "end": 604.8000000000001, "text": " Tak, chodzi o to, \u017ceby model zauwa\u017cy\u0142 wzorzec, np. par\u0119 s\u0142\u00f3w Harry Potter.", "tokens": [51516, 9118, 11, 23998, 277, 281, 11, 11316, 2316, 710, 1459, 4151, 7735, 1221, 24809, 284, 1381, 66, 11, 33808, 13, 971, 1274, 15116, 3901, 9378, 18115, 13, 51794], "temperature": 0.0, "avg_logprob": -0.16547978469748903, "compression_ratio": 1.37987012987013, "no_speech_prob": 0.005661006551235914}, {"id": 160, "seek": 60480, "start": 605.0, "end": 608.88, "text": " I gdy potem zobaczy samochary, \u017ceby przewidzia\u0142, \u017ce nast\u0119pne b\u0119dzie Potter.", "tokens": [50374, 286, 28405, 36513, 37273, 3247, 8997, 822, 11, 11316, 39758, 327, 43070, 11, 3561, 39662, 716, 10562, 18115, 13, 50568], "temperature": 0.0, "avg_logprob": -0.1767084441915916, "compression_ratio": 1.4851851851851852, "no_speech_prob": 0.005029695574194193}, {"id": 161, "seek": 60480, "start": 609.0799999999999, "end": 611.12, "text": " Czekaj, upewnij si\u0119, \u017ce dobrze to rozumiem.", "tokens": [50578, 383, 19878, 1805, 11, 493, 68, 895, 1718, 3244, 11, 3561, 28335, 281, 48797, 4907, 13, 50680], "temperature": 0.0, "avg_logprob": -0.1767084441915916, "compression_ratio": 1.4851851851851852, "no_speech_prob": 0.005029695574194193}, {"id": 162, "seek": 60480, "start": 611.3199999999999, "end": 616.4, "text": " Oni trenowali model na sekwencjach d\u0142ugo\u015bci, powiedzmy, kilku k\u0142opot pit\u00f3w,", "tokens": [50690, 1282, 72, 23136, 305, 5103, 2316, 1667, 17215, 15615, 66, 45059, 44042, 20746, 6199, 11, 27617, 2226, 11, 5128, 5279, 350, 1221, 45225, 10147, 3901, 11, 50944], "temperature": 0.0, "avg_logprob": -0.1767084441915916, "compression_ratio": 1.4851851851851852, "no_speech_prob": 0.005029695574194193}, {"id": 163, "seek": 60480, "start": 616.5999999999999, "end": 622.3199999999999, "text": " a on potem potrafi\u0142 poprawnie zastosowa\u0107 te logik\u0119 do sekwencji d\u0142ugo\u015bci ca\u0142ej powie\u015bci.", "tokens": [50954, 257, 322, 36513, 1847, 10437, 40622, 1665, 424, 14215, 36746, 329, 11445, 535, 3565, 1035, 1274, 360, 17215, 15615, 19649, 44042, 20746, 6199, 47631, 73, 3388, 414, 6199, 13, 51240], "temperature": 0.0, "avg_logprob": -0.1767084441915916, "compression_ratio": 1.4851851851851852, "no_speech_prob": 0.005029695574194193}, {"id": 164, "seek": 60480, "start": 622.52, "end": 624.8, "text": " To, no, to brzmi niewiargodnie.", "tokens": [51250, 1407, 11, 572, 11, 281, 738, 89, 3057, 43622, 72, 33544, 378, 2766, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1767084441915916, "compression_ratio": 1.4851851851851852, "no_speech_prob": 0.005029695574194193}, {"id": 165, "seek": 60480, "start": 625.0, "end": 630.56, "text": " A jednak mamba trenowana na sekwencjach o d\u0142ugo\u015bci 256 token\u00f3w.", "tokens": [51374, 316, 25897, 13524, 4231, 23136, 40458, 1667, 17215, 15615, 66, 45059, 277, 44042, 20746, 6199, 38882, 14862, 3901, 13, 51652], "temperature": 0.0, "avg_logprob": -0.1767084441915916, "compression_ratio": 1.4851851851851852, "no_speech_prob": 0.005029695574194193}, {"id": 166, "seek": 63056, "start": 630.8, "end": 635.7199999999999, "text": " Potrafi\u0142a bezb\u0142\u0119dnie rozwi\u0105za\u0107 to zadanie na sekwencjach o d\u0142ugo\u015bci miliona token\u00f3w.", "tokens": [50376, 9145, 10437, 72, 5024, 10782, 65, 1221, 6298, 2766, 9544, 18234, 35873, 281, 42788, 7155, 1667, 17215, 15615, 66, 45059, 277, 44042, 20746, 6199, 1962, 21758, 14862, 3901, 13, 50622], "temperature": 0.0, "avg_logprob": -0.1346183693321952, "compression_ratio": 1.4612676056338028, "no_speech_prob": 0.06028105691075325}, {"id": 167, "seek": 63056, "start": 635.92, "end": 636.7199999999999, "text": " Miliona.", "tokens": [50632, 7036, 21758, 13, 50672], "temperature": 0.0, "avg_logprob": -0.1346183693321952, "compression_ratio": 1.4612676056338028, "no_speech_prob": 0.06028105691075325}, {"id": 168, "seek": 63056, "start": 636.92, "end": 641.4, "text": " Miliona. To jest 4 tysi\u0105ce razy d\u0142u\u017cej.", "tokens": [50682, 7036, 21758, 13, 1407, 3492, 1017, 38156, 11404, 384, 9639, 88, 274, 24066, 38493, 13, 50906], "temperature": 0.0, "avg_logprob": -0.1346183693321952, "compression_ratio": 1.4612676056338028, "no_speech_prob": 0.06028105691075325}, {"id": 169, "seek": 63056, "start": 641.5999999999999, "end": 644.1999999999999, "text": " \u017baden inny model, w tym r\u00f3\u017cne", "tokens": [50916, 29804, 14771, 294, 1634, 2316, 11, 261, 8107, 47760, 51046], "temperature": 0.0, "avg_logprob": -0.1346183693321952, "compression_ratio": 1.4612676056338028, "no_speech_prob": 0.06028105691075325}, {"id": 170, "seek": 63056, "start": 644.4, "end": 648.3199999999999, "text": " warianty Attention nawet nie zbli\u017cy\u0142 si\u0119 do takiego wyniku.", "tokens": [51056, 1516, 5798, 88, 31858, 22696, 2838, 710, 32117, 7735, 1221, 3244, 360, 32296, 31936, 24320, 13, 51252], "temperature": 0.0, "avg_logprob": -0.1346183693321952, "compression_ratio": 1.4612676056338028, "no_speech_prob": 0.06028105691075325}, {"id": 171, "seek": 63056, "start": 648.52, "end": 654.2399999999999, "text": " To pokazuje, \u017ce jej mechanizm kompresji stanu naprawd\u0119 dzia\u0142a i potrafi ekstrapolowa\u0107.", "tokens": [51262, 1407, 13010, 43317, 11, 3561, 28924, 4236, 590, 76, 5207, 14508, 4013, 27984, 84, 20970, 37903, 741, 1847, 10437, 72, 13359, 19639, 12892, 11445, 13, 51548], "temperature": 0.0, "avg_logprob": -0.1346183693321952, "compression_ratio": 1.4612676056338028, "no_speech_prob": 0.06028105691075325}, {"id": 172, "seek": 63056, "start": 654.4399999999999, "end": 657.4, "text": " To ju\u017c nie jest tylko poprawa wydajno\u015bci.", "tokens": [51558, 1407, 10678, 2838, 3492, 13219, 1665, 424, 4151, 25984, 1805, 16438, 13, 51706], "temperature": 0.0, "avg_logprob": -0.1346183693321952, "compression_ratio": 1.4612676056338028, "no_speech_prob": 0.06028105691075325}, {"id": 173, "seek": 63056, "start": 657.5999999999999, "end": 659.92, "text": " To jest fundamentalnie nowa zdolno\u015b\u0107.", "tokens": [51716, 1407, 3492, 8088, 2766, 586, 64, 16221, 401, 23293, 13, 51832], "temperature": 0.0, "avg_logprob": -0.1346183693321952, "compression_ratio": 1.4612676056338028, "no_speech_prob": 0.06028105691075325}, {"id": 174, "seek": 65992, "start": 660.0799999999999, "end": 662.8399999999999, "text": " A co z realnym \u015bwiatem, co z modelowaniem j\u0119zyka?", "tokens": [50372, 316, 598, 710, 957, 12996, 21485, 26851, 11, 598, 710, 2316, 37345, 4907, 42309, 40940, 30, 50510], "temperature": 0.0, "avg_logprob": -0.16860873358590261, "compression_ratio": 1.4388489208633093, "no_speech_prob": 0.003876810660585761}, {"id": 175, "seek": 65992, "start": 663.04, "end": 665.1999999999999, "text": " Tutaj wyniki s\u0105 r\u00f3wnie mocne.", "tokens": [50520, 41819, 31936, 9850, 9015, 11416, 14215, 34962, 716, 13, 50628], "temperature": 0.0, "avg_logprob": -0.16860873358590261, "compression_ratio": 1.4388489208633093, "no_speech_prob": 0.003876810660585761}, {"id": 176, "seek": 65992, "start": 665.4, "end": 670.8399999999999, "text": " Model Mamba 3B, czyli z 3 miliardami parametr\u00f3w, osi\u0105ga wyniki na poziomie modeli", "tokens": [50638, 17105, 376, 23337, 805, 33, 11, 16591, 710, 805, 1962, 72, 515, 4526, 6220, 27965, 3901, 11, 3003, 11404, 3680, 31936, 9850, 1667, 38503, 40120, 2316, 72, 50910], "temperature": 0.0, "avg_logprob": -0.16860873358590261, "compression_ratio": 1.4388489208633093, "no_speech_prob": 0.003876810660585761}, {"id": 177, "seek": 65992, "start": 671.04, "end": 674.88, "text": " Transformer dwukrotnie wi\u0119kszych. Na przyk\u0142ad Pythia 7B.", "tokens": [50920, 27938, 260, 27379, 2034, 10536, 2766, 29968, 28051, 13, 6056, 23144, 9953, 392, 654, 1614, 33, 13, 51112], "temperature": 0.0, "avg_logprob": -0.16860873358590261, "compression_ratio": 1.4388489208633093, "no_speech_prob": 0.003876810660585761}, {"id": 178, "seek": 65992, "start": 675.0799999999999, "end": 676.12, "text": " Dwukrotnie wi\u0119kszych?", "tokens": [51122, 41448, 2034, 10536, 2766, 29968, 28051, 30, 51174], "temperature": 0.0, "avg_logprob": -0.16860873358590261, "compression_ratio": 1.4388489208633093, "no_speech_prob": 0.003876810660585761}, {"id": 179, "seek": 65992, "start": 676.3199999999999, "end": 681.24, "text": " Tak. I to zar\u00f3wno w metrykach z pretrainingu, jak i w zadaniach downstream.", "tokens": [51184, 9118, 13, 286, 281, 22675, 812, 20944, 261, 1131, 627, 41326, 710, 1162, 424, 1760, 84, 11, 4207, 741, 261, 42788, 3782, 608, 30621, 13, 51430], "temperature": 0.0, "avg_logprob": -0.16860873358590261, "compression_ratio": 1.4388489208633093, "no_speech_prob": 0.003876810660585761}, {"id": 180, "seek": 65992, "start": 681.4399999999999, "end": 684.36, "text": " I to jest, no, historyczny moment.", "tokens": [51440, 286, 281, 3492, 11, 572, 11, 2503, 3689, 1634, 1623, 13, 51586], "temperature": 0.0, "avg_logprob": -0.16860873358590261, "compression_ratio": 1.4388489208633093, "no_speech_prob": 0.003876810660585761}, {"id": 181, "seek": 65992, "start": 684.56, "end": 686.7199999999999, "text": " To jest pierwszy model o z\u0142o\u017cono\u015bci", "tokens": [51596, 1407, 3492, 34016, 2316, 277, 710, 5249, 1427, 8957, 6199, 51704], "temperature": 0.0, "avg_logprob": -0.16860873358590261, "compression_ratio": 1.4388489208633093, "no_speech_prob": 0.003876810660585761}, {"id": 182, "seek": 68672, "start": 686.72, "end": 690.64, "text": " liniowej, kt\u00f3ry naprawd\u0119 dor\u00f3wnuje jako\u015bci\u0105 najlepszym Transformerom.", "tokens": [50364, 287, 3812, 21091, 11, 9913, 20970, 26313, 812, 895, 13008, 17123, 50227, 41903, 1878, 26681, 27938, 260, 298, 13, 50560], "temperature": 0.0, "avg_logprob": -0.1343057015362908, "compression_ratio": 1.460377358490566, "no_speech_prob": 0.04565820470452309}, {"id": 183, "seek": 68672, "start": 690.84, "end": 692.5600000000001, "text": " A co to znaczy w praktyce?", "tokens": [50570, 316, 598, 281, 36584, 261, 3206, 74, 874, 384, 30, 50656], "temperature": 0.0, "avg_logprob": -0.1343057015362908, "compression_ratio": 1.460377358490566, "no_speech_prob": 0.04565820470452309}, {"id": 184, "seek": 68672, "start": 692.76, "end": 699.4, "text": " Dla firmy, kt\u00f3ra buduje model AI, oznacza to, \u017ce mo\u017ce dosta\u0107 t\u0119 sam\u0105, mo\u017ce nawet", "tokens": [50666, 413, 875, 12159, 2226, 11, 19456, 3265, 13008, 2316, 7318, 11, 277, 22672, 326, 2394, 281, 11, 3561, 12034, 274, 8638, 2162, 32489, 3247, 1611, 11, 12034, 22696, 50998], "temperature": 0.0, "avg_logprob": -0.1343057015362908, "compression_ratio": 1.460377358490566, "no_speech_prob": 0.04565820470452309}, {"id": 185, "seek": 68672, "start": 699.6, "end": 703.48, "text": " lepsz\u0105 jako\u015b\u0107, u\u017cywaj\u0105c modelu o po\u0142ow\u0119 mniejszego.", "tokens": [51008, 476, 1878, 8925, 17123, 7753, 11, 34097, 86, 38757, 2316, 84, 277, 714, 1221, 305, 1274, 39513, 15453, 6308, 13, 51202], "temperature": 0.0, "avg_logprob": -0.1343057015362908, "compression_ratio": 1.460377358490566, "no_speech_prob": 0.04565820470452309}, {"id": 186, "seek": 68672, "start": 703.6800000000001, "end": 707.2, "text": " A mniejszy model to ni\u017csze koszty, szybsze odpowiedzi.", "tokens": [51212, 316, 39513, 7706, 2316, 281, 28502, 82, 1381, 19532, 89, 874, 11, 30526, 929, 1381, 36574, 3992, 13, 51388], "temperature": 0.0, "avg_logprob": -0.1343057015362908, "compression_ratio": 1.460377358490566, "no_speech_prob": 0.04565820470452309}, {"id": 187, "seek": 68672, "start": 707.4, "end": 708.44, "text": " Dok\u0142adnie tak.", "tokens": [51398, 29768, 10358, 2766, 991, 13, 51450], "temperature": 0.0, "avg_logprob": -0.1343057015362908, "compression_ratio": 1.460377358490566, "no_speech_prob": 0.04565820470452309}, {"id": 188, "seek": 68672, "start": 708.64, "end": 712.76, "text": " A wisienka na torcie jest taka, \u017ce przepustowo\u015b\u0107 mamby w trybie", "tokens": [51460, 316, 9074, 1053, 2330, 1667, 3930, 4260, 3492, 28017, 11, 3561, 30829, 381, 19941, 7753, 13524, 2322, 261, 853, 7392, 51666], "temperature": 0.0, "avg_logprob": -0.1343057015362908, "compression_ratio": 1.460377358490566, "no_speech_prob": 0.04565820470452309}, {"id": 189, "seek": 71276, "start": 712.8, "end": 717.52, "text": " inferencji, czyli generowania odpowiedzi, jest pi\u0119ciokrotnie wy\u017csza,", "tokens": [50366, 13596, 268, 19649, 11, 16591, 1337, 21308, 36574, 3992, 11, 3492, 32677, 537, 453, 10536, 2766, 4628, 1427, 82, 2394, 11, 50602], "temperature": 0.0, "avg_logprob": -0.13891585180364505, "compression_ratio": 1.439102564102564, "no_speech_prob": 0.0812922790646553}, {"id": 190, "seek": 71276, "start": 717.72, "end": 720.28, "text": " ni\u017c u Transformera o por\u00f3wnywalnej jako\u015bci.", "tokens": [50612, 28502, 344, 27938, 1663, 277, 1515, 812, 895, 27112, 304, 11794, 17123, 6199, 13, 50740], "temperature": 0.0, "avg_logprob": -0.13891585180364505, "compression_ratio": 1.439102564102564, "no_speech_prob": 0.0812922790646553}, {"id": 191, "seek": 71276, "start": 720.48, "end": 721.64, "text": " Pi\u0119ciokrotnie.", "tokens": [50750, 430, 5034, 537, 453, 10536, 2766, 13, 50808], "temperature": 0.0, "avg_logprob": -0.13891585180364505, "compression_ratio": 1.439102564102564, "no_speech_prob": 0.0812922790646553}, {"id": 192, "seek": 71276, "start": 721.84, "end": 722.76, "text": " Wow.", "tokens": [50818, 3153, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13891585180364505, "compression_ratio": 1.439102564102564, "no_speech_prob": 0.0812922790646553}, {"id": 193, "seek": 71276, "start": 722.96, "end": 726.08, "text": " A co z tymi dziedzinami, kt\u00f3re by\u0142y na pocz\u0105tku motywacj\u0105?", "tokens": [50874, 316, 598, 710, 1104, 3057, 9758, 15338, 259, 4526, 11, 8864, 26366, 1667, 43959, 2184, 27112, 326, 8555, 30, 51030], "temperature": 0.0, "avg_logprob": -0.13891585180364505, "compression_ratio": 1.439102564102564, "no_speech_prob": 0.0812922790646553}, {"id": 194, "seek": 71276, "start": 726.28, "end": 727.8, "text": " Genomika, audio.", "tokens": [51040, 3632, 298, 5439, 11, 6278, 13, 51116], "temperature": 0.0, "avg_logprob": -0.13891585180364505, "compression_ratio": 1.439102564102564, "no_speech_prob": 0.0812922790646553}, {"id": 195, "seek": 71276, "start": 728.0, "end": 731.6, "text": " Tam mamy do czynienia z naprawd\u0119 potwornie d\u0142ugimi sekwencjami.", "tokens": [51126, 8540, 17335, 360, 6430, 77, 18811, 710, 20970, 1847, 86, 1865, 414, 274, 34077, 10121, 17215, 15615, 66, 73, 4526, 13, 51306], "temperature": 0.0, "avg_logprob": -0.13891585180364505, "compression_ratio": 1.439102564102564, "no_speech_prob": 0.0812922790646553}, {"id": 196, "seek": 71276, "start": 731.8, "end": 734.28, "text": " W genomice Mamba po prostu b\u0142yszczy.", "tokens": [51316, 343, 1049, 298, 573, 376, 23337, 714, 19518, 272, 1221, 20589, 6522, 13, 51440], "temperature": 0.0, "avg_logprob": -0.13891585180364505, "compression_ratio": 1.439102564102564, "no_speech_prob": 0.0812922790646553}, {"id": 197, "seek": 71276, "start": 734.48, "end": 738.52, "text": " Przy analizie ludzkiego genomu, gdzie m\u00f3wimy o sekwencjach rz\u0119du miliona par", "tokens": [51450, 39590, 2624, 590, 414, 15946, 30154, 12200, 1049, 298, 84, 11, 18922, 13489, 13189, 277, 17215, 15615, 66, 45059, 367, 11052, 769, 1962, 21758, 971, 51652], "temperature": 0.0, "avg_logprob": -0.13891585180364505, "compression_ratio": 1.439102564102564, "no_speech_prob": 0.0812922790646553}, {"id": 198, "seek": 71276, "start": 738.72, "end": 741.88, "text": " zasad, ona nie tylko skaluje si\u0119 bez problemu.", "tokens": [51662, 44585, 11, 20325, 2838, 13219, 1110, 304, 13008, 3244, 10782, 1154, 84, 13, 51820], "temperature": 0.0, "avg_logprob": -0.13891585180364505, "compression_ratio": 1.439102564102564, "no_speech_prob": 0.0812922790646553}, {"id": 199, "seek": 74188, "start": 742.12, "end": 747.24, "text": " Jej wydajno\u015b\u0107, mierzona metryk\u0105 Perplexity, autentycznie ro\u015bnie wraz z d\u0142ugo\u015bci\u0105", "tokens": [50376, 2588, 73, 25984, 1805, 23293, 11, 47448, 13383, 1131, 627, 26304, 3026, 18945, 507, 11, 1476, 4179, 19923, 744, 12221, 7843, 89, 710, 44042, 20746, 50227, 50632], "temperature": 0.0, "avg_logprob": -0.1393354659112508, "compression_ratio": 1.410344827586207, "no_speech_prob": 0.0012050183722749352}, {"id": 200, "seek": 74188, "start": 747.4399999999999, "end": 749.12, "text": " kontekstu. Ro\u015bnie.", "tokens": [50642, 14373, 916, 372, 84, 13, 3101, 12221, 13, 50726], "temperature": 0.0, "avg_logprob": -0.1393354659112508, "compression_ratio": 1.410344827586207, "no_speech_prob": 0.0012050183722749352}, {"id": 201, "seek": 74188, "start": 749.32, "end": 751.04, "text": " U innych modeli zazwyczaj spada.", "tokens": [50736, 624, 36286, 2316, 72, 710, 921, 9726, 3689, 1805, 637, 1538, 13, 50822], "temperature": 0.0, "avg_logprob": -0.1393354659112508, "compression_ratio": 1.410344827586207, "no_speech_prob": 0.0012050183722749352}, {"id": 202, "seek": 74188, "start": 751.24, "end": 752.16, "text": " No a spada\u0107.", "tokens": [50832, 883, 257, 637, 1538, 2162, 13, 50878], "temperature": 0.0, "avg_logprob": -0.1393354659112508, "compression_ratio": 1.410344827586207, "no_speech_prob": 0.0012050183722749352}, {"id": 203, "seek": 74188, "start": 752.36, "end": 757.48, "text": " A Mamba potrafi efektywnie wykorzysta\u0107 ten dodatkowy kontekst, bo otwiltrowuje szum.", "tokens": [50888, 316, 376, 23337, 1847, 10437, 72, 31482, 916, 874, 14215, 43606, 49590, 2162, 2064, 13886, 33525, 10089, 14373, 916, 372, 11, 748, 4337, 86, 2352, 1892, 13008, 7870, 449, 13, 51144], "temperature": 0.0, "avg_logprob": -0.1393354659112508, "compression_ratio": 1.410344827586207, "no_speech_prob": 0.0012050183722749352}, {"id": 204, "seek": 74188, "start": 757.68, "end": 762.08, "text": " Czyli badacz mo\u017ce teraz wrzuci\u0107 do modelu ca\u0142\u0105 sekwencj\u0119 genomu, a nie tylko", "tokens": [51154, 37099, 1578, 14875, 12034, 16854, 928, 11728, 39162, 360, 2316, 84, 1335, 15926, 17215, 15615, 41960, 1049, 298, 84, 11, 257, 2838, 13219, 51374], "temperature": 0.0, "avg_logprob": -0.1393354659112508, "compression_ratio": 1.410344827586207, "no_speech_prob": 0.0012050183722749352}, {"id": 205, "seek": 74188, "start": 762.28, "end": 765.84, "text": " fragmenty i szuka\u0107 wzorc\u00f3w na niespodkan\u0105 skal\u0119.", "tokens": [51384, 26424, 88, 741, 7870, 13599, 2162, 24809, 284, 29268, 1667, 48100, 43388, 5225, 1611, 16890, 1274, 13, 51562], "temperature": 0.0, "avg_logprob": -0.1393354659112508, "compression_ratio": 1.410344827586207, "no_speech_prob": 0.0012050183722749352}, {"id": 206, "seek": 74188, "start": 766.04, "end": 768.08, "text": " To jest w\u0142a\u015bnie ten potencja\u0142.", "tokens": [51572, 1407, 3492, 14234, 2064, 1847, 22660, 2938, 1221, 13, 51674], "temperature": 0.0, "avg_logprob": -0.1393354659112508, "compression_ratio": 1.410344827586207, "no_speech_prob": 0.0012050183722749352}, {"id": 207, "seek": 76808, "start": 768.2800000000001, "end": 774.6, "text": " A w audio, w zadaniu generowania mowy ze zbioru SC09 Mamba ustanowi\u0142a nowy rekord", "tokens": [50374, 316, 261, 6278, 11, 261, 42788, 25849, 1337, 21308, 275, 10089, 5277, 710, 33362, 84, 9028, 13811, 376, 23337, 26189, 282, 24503, 5024, 586, 88, 33881, 765, 50690], "temperature": 0.0, "avg_logprob": -0.15121600686050043, "compression_ratio": 1.3815384615384616, "no_speech_prob": 0.011834273114800453}, {"id": 208, "seek": 76808, "start": 774.8000000000001, "end": 775.5600000000001, "text": " State of the Art.", "tokens": [50700, 4533, 295, 264, 5735, 13, 50738], "temperature": 0.0, "avg_logprob": -0.15121600686050043, "compression_ratio": 1.3815384615384616, "no_speech_prob": 0.011834273114800453}, {"id": 209, "seek": 76808, "start": 775.76, "end": 779.1600000000001, "text": " Pobi\u0142a modele oparte na dyfuzje i sieciach GAN.", "tokens": [50748, 430, 19293, 5024, 4391, 306, 999, 11026, 1667, 14584, 69, 3334, 2884, 741, 2804, 537, 608, 460, 1770, 13, 50918], "temperature": 0.0, "avg_logprob": -0.15121600686050043, "compression_ratio": 1.3815384615384616, "no_speech_prob": 0.011834273114800453}, {"id": 210, "seek": 76808, "start": 779.36, "end": 784.8000000000001, "text": " Wska\u017anik b\u0142\u0119du FID zmniejsza\u0142a o ponad po\u0142ow\u0119 w stosunku do poprzedniego najlepszego", "tokens": [50928, 343, 20771, 10659, 13123, 272, 46564, 769, 479, 2777, 17020, 30295, 2394, 5024, 277, 9224, 345, 714, 1221, 305, 1274, 261, 43581, 49910, 360, 1665, 81, 11312, 2766, 1571, 41903, 1878, 27725, 51200], "temperature": 0.0, "avg_logprob": -0.15121600686050043, "compression_ratio": 1.3815384615384616, "no_speech_prob": 0.011834273114800453}, {"id": 211, "seek": 76808, "start": 785.0, "end": 786.48, "text": " modelu. To jest ogromny skok.", "tokens": [51210, 2316, 84, 13, 1407, 3492, 34416, 298, 1634, 1110, 453, 13, 51284], "temperature": 0.0, "avg_logprob": -0.15121600686050043, "compression_ratio": 1.3815384615384616, "no_speech_prob": 0.011834273114800453}, {"id": 212, "seek": 76808, "start": 786.6800000000001, "end": 791.36, "text": " Wi\u0119c po tym wszystkim, czy to czas, \u017ceby zacz\u0105\u0107 pisa\u0107 nekrolog dla architektury", "tokens": [51294, 32508, 714, 8107, 30481, 11, 6430, 281, 13190, 11, 11316, 34430, 8925, 2162, 280, 3837, 2162, 408, 74, 20978, 12285, 3912, 642, 2320, 2598, 51528], "temperature": 0.0, "avg_logprob": -0.15121600686050043, "compression_ratio": 1.3815384615384616, "no_speech_prob": 0.011834273114800453}, {"id": 213, "seek": 76808, "start": 791.5600000000001, "end": 792.24, "text": " Transformer?", "tokens": [51538, 27938, 260, 30, 51572], "temperature": 0.0, "avg_logprob": -0.15121600686050043, "compression_ratio": 1.3815384615384616, "no_speech_prob": 0.011834273114800453}, {"id": 214, "seek": 76808, "start": 792.44, "end": 796.72, "text": " Mamba jest bez w\u0105tpienia najsilniejszym kandydatem na taki og\u00f3lny model bazowy", "tokens": [51582, 376, 23337, 3492, 10782, 261, 23430, 79, 18811, 11212, 30605, 10402, 7706, 76, 350, 474, 6655, 26851, 1667, 20065, 5360, 15741, 1634, 2316, 27147, 10089, 51796], "temperature": 0.0, "avg_logprob": -0.15121600686050043, "compression_ratio": 1.3815384615384616, "no_speech_prob": 0.011834273114800453}, {"id": 215, "seek": 79672, "start": 796.72, "end": 799.52, "text": " dla sekwencji, jakiego widzieli\u015bmy do tej pory.", "tokens": [50364, 12285, 17215, 15615, 19649, 11, 4207, 12200, 27486, 23099, 10513, 360, 12573, 280, 827, 13, 50504], "temperature": 0.0, "avg_logprob": -0.10550994222814386, "compression_ratio": 1.5331412103746398, "no_speech_prob": 0.00871933251619339}, {"id": 216, "seek": 79672, "start": 799.72, "end": 804.12, "text": " Zdolno\u015b\u0107 do liniowego skalowania przy zachowaniu jako\u015bci sota otwiera drzwi, kt\u00f3re", "tokens": [50514, 1176, 67, 401, 23293, 360, 287, 3812, 26576, 16890, 21308, 6501, 29303, 305, 25849, 17123, 6199, 262, 5377, 4337, 86, 10609, 1224, 89, 6253, 11, 8864, 50734], "temperature": 0.0, "avg_logprob": -0.10550994222814386, "compression_ratio": 1.5331412103746398, "no_speech_prob": 0.00871933251619339}, {"id": 217, "seek": 79672, "start": 804.32, "end": 806.0, "text": " dla Transformer\u00f3w by\u0142y po prostu zamkni\u0119te.", "tokens": [50744, 12285, 27938, 260, 3901, 26366, 714, 19518, 19876, 74, 35938, 975, 13, 50828], "temperature": 0.0, "avg_logprob": -0.10550994222814386, "compression_ratio": 1.5331412103746398, "no_speech_prob": 0.00871933251619339}, {"id": 218, "seek": 79672, "start": 806.2, "end": 810.1600000000001, "text": " Modelowanie ca\u0142ych genom\u00f3w, film\u00f3w w wysokiej rozdzielczo\u015bci.", "tokens": [50838, 17105, 22028, 35226, 339, 1049, 298, 3901, 11, 2007, 3901, 261, 27062, 453, 7764, 9544, 28168, 1187, 3689, 44468, 13, 51036], "temperature": 0.0, "avg_logprob": -0.10550994222814386, "compression_ratio": 1.5331412103746398, "no_speech_prob": 0.00871933251619339}, {"id": 219, "seek": 79672, "start": 810.36, "end": 813.88, "text": " Analiza wielotomowych dokument\u00f3w bez dzielenia ich na kawa\u0142ki.", "tokens": [51046, 16128, 13427, 20570, 42939, 19605, 40858, 3901, 10782, 9758, 12844, 654, 1893, 1667, 350, 10449, 1221, 2984, 13, 51222], "temperature": 0.0, "avg_logprob": -0.10550994222814386, "compression_ratio": 1.5331412103746398, "no_speech_prob": 0.00871933251619339}, {"id": 220, "seek": 79672, "start": 814.08, "end": 816.0400000000001, "text": " To wszystko staje si\u0119 realne.", "tokens": [51232, 1407, 22607, 342, 11153, 3244, 957, 716, 13, 51330], "temperature": 0.0, "avg_logprob": -0.10550994222814386, "compression_ratio": 1.5331412103746398, "no_speech_prob": 0.00871933251619339}, {"id": 221, "seek": 79672, "start": 816.24, "end": 820.1600000000001, "text": " Ale autorzy sami przyznaj\u0105, \u017ce to nie jest rozwi\u0105zanie uniwersalne, prawda?", "tokens": [51340, 9366, 19510, 1229, 3247, 72, 6501, 35458, 8555, 11, 3561, 281, 2838, 3492, 9544, 22620, 7155, 36435, 5364, 304, 716, 11, 43607, 30, 51536], "temperature": 0.0, "avg_logprob": -0.10550994222814386, "compression_ratio": 1.5331412103746398, "no_speech_prob": 0.00871933251619339}, {"id": 222, "seek": 79672, "start": 820.36, "end": 823.32, "text": " Wspominaj\u0105 o pewnym spektrum ci\u0105g\u0142e dyskretne.", "tokens": [51546, 343, 4952, 49217, 8555, 277, 47160, 4199, 768, 2320, 6247, 42398, 70, 19827, 15243, 74, 1505, 716, 13, 51694], "temperature": 0.0, "avg_logprob": -0.10550994222814386, "compression_ratio": 1.5331412103746398, "no_speech_prob": 0.00871933251619339}, {"id": 223, "seek": 79672, "start": 823.52, "end": 826.24, "text": " Czyli to nie jest tak, \u017ce Mamba jest lepsza we wszystkim.", "tokens": [51704, 37099, 281, 2838, 3492, 991, 11, 3561, 376, 23337, 3492, 476, 1878, 2394, 321, 30481, 13, 51840], "temperature": 0.0, "avg_logprob": -0.10550994222814386, "compression_ratio": 1.5331412103746398, "no_speech_prob": 0.00871933251619339}, {"id": 224, "seek": 82624, "start": 826.44, "end": 829.52, "text": " Tak i to jest bardzo wa\u017cna i uczciwa obserwacja.", "tokens": [50374, 9118, 741, 281, 3492, 9034, 27777, 629, 741, 35403, 537, 4151, 12887, 86, 23395, 13, 50528], "temperature": 0.0, "avg_logprob": -0.13847849688192052, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.008242244832217693}, {"id": 225, "seek": 82624, "start": 829.72, "end": 833.72, "text": " Ten mechanizm selekcji jest darem nie wios dla danych dyskretnych.", "tokens": [50538, 9380, 4236, 590, 76, 23264, 74, 19649, 3492, 8955, 76, 2838, 261, 2717, 12285, 274, 34644, 15243, 74, 1505, 9399, 13, 50738], "temperature": 0.0, "avg_logprob": -0.13847849688192052, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.008242244832217693}, {"id": 226, "seek": 82624, "start": 833.92, "end": 835.32, "text": " Text DNA.", "tokens": [50748, 18643, 8272, 13, 50818], "temperature": 0.0, "avg_logprob": -0.13847849688192052, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.008242244832217693}, {"id": 227, "seek": 82624, "start": 835.52, "end": 839.16, "text": " Tam, gdzie znaczenie jest przypisane do konkretnych token\u00f3w.", "tokens": [50828, 8540, 11, 18922, 15397, 326, 16778, 3492, 6501, 40516, 1929, 360, 36500, 9399, 14862, 3901, 13, 51010], "temperature": 0.0, "avg_logprob": -0.13847849688192052, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.008242244832217693}, {"id": 228, "seek": 82624, "start": 839.36, "end": 843.0, "text": " Ale w przypadku sygna\u0142\u00f3w ci\u0105g\u0142ych, jak surowe pr\u00f3bki audio,", "tokens": [51020, 9366, 261, 41955, 943, 70, 629, 1221, 3901, 42398, 70, 47655, 11, 4207, 1022, 6880, 8565, 65, 2984, 6278, 11, 51202], "temperature": 0.0, "avg_logprob": -0.13847849688192052, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.008242244832217693}, {"id": 229, "seek": 82624, "start": 843.2, "end": 846.76, "text": " gdzie ka\u017cdy punkt jest silnie skorelowany z s\u0105siadami.", "tokens": [51212, 18922, 31615, 39561, 3492, 3425, 2766, 1110, 418, 14107, 1325, 710, 9015, 7691, 345, 4526, 13, 51390], "temperature": 0.0, "avg_logprob": -0.13847849688192052, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.008242244832217693}, {"id": 230, "seek": 82624, "start": 846.96, "end": 853.2, "text": " Stare modele LTI ze swoj\u0105 niezmienno\u015bci\u0105 mog\u0105 mie\u0107 naturaln\u0105 przewag\u0119.", "tokens": [51400, 745, 543, 4391, 306, 441, 5422, 5277, 49194, 33511, 76, 1053, 16438, 1611, 34123, 35612, 3303, 13113, 39758, 40748, 13, 51712], "temperature": 0.0, "avg_logprob": -0.13847849688192052, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.008242244832217693}, {"id": 231, "seek": 85320, "start": 853.44, "end": 855.76, "text": " Zreszt\u0105 w jednym z eksperyment\u00f3w na audio", "tokens": [50376, 1176, 495, 2682, 1611, 261, 5232, 12996, 710, 30724, 610, 88, 518, 3901, 1667, 6278, 50492], "temperature": 0.0, "avg_logprob": -0.14879732439594884, "compression_ratio": 1.3876221498371335, "no_speech_prob": 0.004634646233171225}, {"id": 232, "seek": 85320, "start": 855.96, "end": 861.44, "text": " wy\u0142\u0105czenie selekcji w Mambie i powr\u00f3t do wariantu LTI da\u0142o lepsze wyniki.", "tokens": [50502, 4628, 15926, 39043, 23264, 74, 19649, 261, 376, 2173, 414, 741, 3388, 11721, 83, 360, 1516, 5798, 84, 441, 5422, 1120, 5249, 476, 1878, 1381, 31936, 9850, 13, 50776], "temperature": 0.0, "avg_logprob": -0.14879732439594884, "compression_ratio": 1.3876221498371335, "no_speech_prob": 0.004634646233171225}, {"id": 233, "seek": 85320, "start": 861.6400000000001, "end": 865.44, "text": " Czyli trzeba dobra\u0107 narz\u0119dzie do problemu i jest jeszcze jedna kwestia.", "tokens": [50786, 37099, 25860, 360, 6198, 2162, 6714, 89, 42643, 360, 1154, 84, 741, 3492, 14168, 5232, 629, 42035, 654, 13, 50976], "temperature": 0.0, "avg_logprob": -0.14879732439594884, "compression_ratio": 1.3876221498371335, "no_speech_prob": 0.004634646233171225}, {"id": 234, "seek": 85320, "start": 865.6400000000001, "end": 870.0400000000001, "text": " Skalowanie. Te wszystkie wyniki dotycz\u0105 modeli do 3 miliard\u00f3w parametr\u00f3w.", "tokens": [50986, 7324, 304, 22028, 13, 1989, 31723, 31936, 9850, 5893, 17466, 1611, 2316, 72, 360, 805, 1962, 72, 515, 3901, 6220, 27965, 3901, 13, 51206], "temperature": 0.0, "avg_logprob": -0.14879732439594884, "compression_ratio": 1.3876221498371335, "no_speech_prob": 0.004634646233171225}, {"id": 235, "seek": 85320, "start": 870.24, "end": 871.4000000000001, "text": " A co z gigantami?", "tokens": [51216, 316, 598, 710, 8741, 394, 4526, 30, 51274], "temperature": 0.0, "avg_logprob": -0.14879732439594884, "compression_ratio": 1.3876221498371335, "no_speech_prob": 0.004634646233171225}, {"id": 236, "seek": 85320, "start": 871.6, "end": 875.36, "text": " Z modelami 70B, 175B.", "tokens": [51284, 1176, 2316, 4526, 5285, 33, 11, 41165, 33, 13, 51472], "temperature": 0.0, "avg_logprob": -0.14879732439594884, "compression_ratio": 1.3876221498371335, "no_speech_prob": 0.004634646233171225}, {"id": 237, "seek": 85320, "start": 875.5600000000001, "end": 879.2800000000001, "text": " Czy jest jaki\u015b pow\u00f3d, by s\u0105dzi\u0107, \u017ce Mamba nie uderzy w jak\u0105\u015b \u015bcian\u0119?", "tokens": [51482, 19832, 3492, 34721, 3388, 17081, 11, 538, 9015, 67, 28496, 11, 3561, 376, 23337, 2838, 344, 1068, 1229, 261, 46719, 1788, 220, 6199, 282, 1274, 30, 51668], "temperature": 0.0, "avg_logprob": -0.14879732439594884, "compression_ratio": 1.3876221498371335, "no_speech_prob": 0.004634646233171225}, {"id": 238, "seek": 85320, "start": 879.48, "end": 881.1600000000001, "text": " To jest pytanie za milion dolar\u00f3w.", "tokens": [51678, 1407, 3492, 36610, 7949, 1962, 313, 360, 2200, 3901, 13, 51762], "temperature": 0.0, "avg_logprob": -0.14879732439594884, "compression_ratio": 1.3876221498371335, "no_speech_prob": 0.004634646233171225}, {"id": 239, "seek": 88116, "start": 881.28, "end": 883.8399999999999, "text": " I ten artyku\u0142 na nie jeszcze nie odpowiada.", "tokens": [50370, 286, 2064, 594, 874, 5279, 1221, 1667, 2838, 14168, 2838, 24314, 39018, 13, 50498], "temperature": 0.0, "avg_logprob": -0.14003014237913367, "compression_ratio": 1.4795918367346939, "no_speech_prob": 0.10989516973495483}, {"id": 240, "seek": 88116, "start": 884.04, "end": 887.04, "text": " To oczylisty nast\u0119pny krok dla bada\u0144.", "tokens": [50508, 1407, 277, 6522, 75, 38618, 39662, 1634, 350, 31621, 12285, 272, 1538, 5248, 13, 50658], "temperature": 0.0, "avg_logprob": -0.14003014237913367, "compression_ratio": 1.4795918367346939, "no_speech_prob": 0.10989516973495483}, {"id": 241, "seek": 88116, "start": 887.24, "end": 890.3199999999999, "text": " Skalowanie transformer\u00f3w jest ju\u017c dobrze zrozumiane.", "tokens": [50668, 7324, 304, 22028, 31782, 3901, 3492, 10678, 28335, 710, 27857, 449, 21133, 13, 50822], "temperature": 0.0, "avg_logprob": -0.14003014237913367, "compression_ratio": 1.4795918367346939, "no_speech_prob": 0.10989516973495483}, {"id": 242, "seek": 88116, "start": 890.52, "end": 893.9599999999999, "text": " Skalowanie Mamby to wci\u0105\u017c nieodkryte terytorium.", "tokens": [50832, 7324, 304, 22028, 376, 2173, 88, 281, 261, 537, 27242, 2838, 378, 43298, 975, 1796, 4328, 284, 2197, 13, 51004], "temperature": 0.0, "avg_logprob": -0.14003014237913367, "compression_ratio": 1.4795918367346939, "no_speech_prob": 0.10989516973495483}, {"id": 243, "seek": 88116, "start": 894.16, "end": 898.92, "text": " Ale jej fundamentalna efektywno\u015b\u0107, zar\u00f3wno pami\u0119ciowa, jak i obliczeniowa,", "tokens": [51014, 9366, 28924, 8088, 629, 31482, 916, 874, 20944, 7753, 11, 22675, 812, 20944, 31088, 537, 5528, 11, 4207, 741, 1111, 1050, 42124, 5528, 11, 51252], "temperature": 0.0, "avg_logprob": -0.14003014237913367, "compression_ratio": 1.4795918367346939, "no_speech_prob": 0.10989516973495483}, {"id": 244, "seek": 88116, "start": 899.12, "end": 904.04, "text": " daje solidne podstawy, by s\u0105dzi\u0107, \u017ce b\u0119dzie si\u0119 skalowa\u0107 bardzo, bardzo dobrze.", "tokens": [51262, 1120, 2884, 5100, 716, 43443, 88, 11, 538, 9015, 67, 28496, 11, 3561, 10562, 3244, 16890, 11445, 9034, 11, 9034, 28335, 13, 51508], "temperature": 0.0, "avg_logprob": -0.14003014237913367, "compression_ratio": 1.4795918367346939, "no_speech_prob": 0.10989516973495483}, {"id": 245, "seek": 88116, "start": 904.24, "end": 906.9599999999999, "text": " Mamba to realnie nowa architektura.", "tokens": [51518, 376, 23337, 281, 957, 2766, 586, 64, 3912, 642, 2320, 2991, 13, 51654], "temperature": 0.0, "avg_logprob": -0.14003014237913367, "compression_ratio": 1.4795918367346939, "no_speech_prob": 0.10989516973495483}, {"id": 246, "seek": 88116, "start": 907.16, "end": 909.36, "text": " Rozwi\u0105zuje kluczowy problem transformer\u00f3w", "tokens": [51664, 43313, 22620, 13008, 9671, 1311, 89, 10089, 1154, 4088, 260, 3901, 51774], "temperature": 0.0, "avg_logprob": -0.14003014237913367, "compression_ratio": 1.4795918367346939, "no_speech_prob": 0.10989516973495483}, {"id": 247, "seek": 90936, "start": 909.36, "end": 914.04, "text": " dzi\u0119ki inteligentnej selekcji, a jednocze\u015bnie pozostaje b\u0142yskawiczna dzi\u0119ki", "tokens": [50364, 45003, 24777, 25002, 11794, 23264, 74, 19649, 11, 257, 5232, 26694, 1381, 12221, 21281, 555, 11153, 272, 1221, 749, 74, 1607, 17946, 629, 45003, 50598], "temperature": 0.0, "avg_logprob": -0.1281594885400979, "compression_ratio": 1.475, "no_speech_prob": 0.013748042285442352}, {"id": 248, "seek": 90936, "start": 914.24, "end": 918.28, "text": " genialnej in\u017cynierii i ma na to dowody w postaci rewelacyjnych wynik\u00f3w.", "tokens": [50608, 48228, 11794, 294, 1427, 2534, 811, 5597, 741, 463, 1667, 281, 9459, 843, 261, 2183, 22086, 319, 45512, 31285, 9399, 31936, 1035, 3901, 13, 50810], "temperature": 0.0, "avg_logprob": -0.1281594885400979, "compression_ratio": 1.475, "no_speech_prob": 0.013748042285442352}, {"id": 249, "seek": 90936, "start": 918.48, "end": 919.44, "text": " Zdecydowanie.", "tokens": [50820, 1176, 1479, 1344, 67, 22028, 13, 50868], "temperature": 0.0, "avg_logprob": -0.1281594885400979, "compression_ratio": 1.475, "no_speech_prob": 0.013748042285442352}, {"id": 250, "seek": 90936, "start": 919.64, "end": 923.52, "text": " Ale my\u015bl\u0119, \u017ce na koniec warto zostawi\u0107 s\u0142uchaczy z tak\u0105 jedn\u0105, troch\u0119", "tokens": [50878, 9366, 37730, 11, 3561, 1667, 5897, 35733, 31830, 31873, 1607, 12757, 15116, 625, 14691, 710, 31069, 5232, 13113, 11, 24926, 51072], "temperature": 0.0, "avg_logprob": -0.1281594885400979, "compression_ratio": 1.475, "no_speech_prob": 0.013748042285442352}, {"id": 251, "seek": 90936, "start": 923.72, "end": 927.28, "text": " prowokacyjn\u0105 my\u015bl\u0105. Zreszt\u0105 sugeruj\u0105 j\u0105 sami autorzy.", "tokens": [51082, 45553, 453, 31285, 13113, 452, 19212, 1611, 13, 1176, 495, 2682, 1611, 459, 1321, 13263, 35692, 3247, 72, 19510, 1229, 13, 51260], "temperature": 0.0, "avg_logprob": -0.1281594885400979, "compression_ratio": 1.475, "no_speech_prob": 0.013748042285442352}, {"id": 252, "seek": 90936, "start": 927.48, "end": 928.5600000000001, "text": " S\u0142ucham.", "tokens": [51270, 318, 1221, 625, 335, 13, 51324], "temperature": 0.0, "avg_logprob": -0.1281594885400979, "compression_ratio": 1.475, "no_speech_prob": 0.013748042285442352}, {"id": 253, "seek": 90936, "start": 928.76, "end": 931.44, "text": " Wskazuj\u0105, \u017ce prawdziwym testem dla ka\u017cdej nowej", "tokens": [51334, 343, 5161, 921, 13263, 11, 3561, 41175, 3992, 86, 4199, 1500, 443, 12285, 21912, 1479, 73, 586, 40779, 51468], "temperature": 0.0, "avg_logprob": -0.1281594885400979, "compression_ratio": 1.475, "no_speech_prob": 0.013748042285442352}, {"id": 254, "seek": 90936, "start": 931.64, "end": 934.4, "text": " architektury nie jest tylko wynik w pretreningu.", "tokens": [51478, 3912, 642, 2320, 2598, 2838, 3492, 13219, 31936, 1035, 261, 1162, 265, 773, 84, 13, 51616], "temperature": 0.0, "avg_logprob": -0.1281594885400979, "compression_ratio": 1.475, "no_speech_prob": 0.013748042285442352}, {"id": 255, "seek": 90936, "start": 934.6, "end": 938.36, "text": " Jest nim zdolno\u015b\u0107 do fine tuningu, instruction tuning,", "tokens": [51626, 24918, 24887, 16221, 401, 23293, 360, 2489, 15164, 84, 11, 10951, 15164, 11, 51814], "temperature": 0.0, "avg_logprob": -0.1281594885400979, "compression_ratio": 1.475, "no_speech_prob": 0.013748042285442352}, {"id": 256, "seek": 93836, "start": 938.4, "end": 940.12, "text": " czy RLHF.", "tokens": [50366, 6430, 497, 43, 39, 37, 13, 50452], "temperature": 0.0, "avg_logprob": -0.15015822003601462, "compression_ratio": 1.4185303514376997, "no_speech_prob": 0.007103004492819309}, {"id": 257, "seek": 93836, "start": 940.32, "end": 945.16, "text": " I tu wracamy do tych niesamowitych wynik\u00f3w mamby w zadaniu induction heads.", "tokens": [50462, 286, 2604, 928, 326, 7804, 360, 15180, 48100, 335, 305, 507, 339, 31936, 1035, 3901, 13524, 2322, 261, 42788, 25849, 33371, 8050, 13, 50704], "temperature": 0.0, "avg_logprob": -0.15015822003601462, "compression_ratio": 1.4185303514376997, "no_speech_prob": 0.007103004492819309}, {"id": 258, "seek": 93836, "start": 945.36, "end": 947.32, "text": " Bo one sugeruj\u0105, \u017ce Mamba mo\u017ce mie\u0107", "tokens": [50714, 3286, 472, 459, 1321, 13263, 11, 3561, 376, 23337, 12034, 35612, 50812], "temperature": 0.0, "avg_logprob": -0.15015822003601462, "compression_ratio": 1.4185303514376997, "no_speech_prob": 0.007103004492819309}, {"id": 259, "seek": 93836, "start": 947.52, "end": 950.4, "text": " pot\u0119\u017cne, wbudowane zdolno\u015bci do uczenia si\u0119 w kontek\u015bcie.", "tokens": [50822, 1847, 1274, 1427, 716, 11, 261, 18281, 23066, 16221, 401, 16438, 360, 344, 38517, 3244, 261, 14373, 916, 9815, 13, 50966], "temperature": 0.0, "avg_logprob": -0.15015822003601462, "compression_ratio": 1.4185303514376997, "no_speech_prob": 0.007103004492819309}, {"id": 260, "seek": 93836, "start": 950.6, "end": 951.76, "text": " To in-context learning.", "tokens": [50976, 1407, 294, 12, 9000, 3828, 2539, 13, 51034], "temperature": 0.0, "avg_logprob": -0.15015822003601462, "compression_ratio": 1.4185303514376997, "no_speech_prob": 0.007103004492819309}, {"id": 261, "seek": 93836, "start": 951.96, "end": 954.12, "text": " Dok\u0142adnie. I to prowadzi do g\u0142\u0119bszego pytania.", "tokens": [51044, 29768, 10358, 2766, 13, 286, 281, 36590, 3992, 360, 18117, 1274, 929, 27725, 25878, 5609, 13, 51152], "temperature": 0.0, "avg_logprob": -0.15015822003601462, "compression_ratio": 1.4185303514376997, "no_speech_prob": 0.007103004492819309}, {"id": 262, "seek": 93836, "start": 954.32, "end": 956.92, "text": " Pytanie nie brzmi ju\u017c tylko, czy Mamba mo\u017ce", "tokens": [51162, 430, 4328, 7155, 2838, 738, 89, 3057, 10678, 13219, 11, 6430, 376, 23337, 12034, 51292], "temperature": 0.0, "avg_logprob": -0.15015822003601462, "compression_ratio": 1.4185303514376997, "no_speech_prob": 0.007103004492819309}, {"id": 263, "seek": 93836, "start": 957.12, "end": 959.8000000000001, "text": " dor\u00f3wna\u0107 transformerom w zadaniach, kt\u00f3re ju\u017c znamy.", "tokens": [51302, 26313, 3901, 629, 2162, 4088, 260, 298, 261, 42788, 3782, 608, 11, 8864, 10678, 710, 5378, 88, 13, 51436], "temperature": 0.0, "avg_logprob": -0.15015822003601462, "compression_ratio": 1.4185303514376997, "no_speech_prob": 0.007103004492819309}, {"id": 264, "seek": 93836, "start": 960.0, "end": 960.76, "text": " A jak?", "tokens": [51446, 316, 4207, 30, 51484], "temperature": 0.0, "avg_logprob": -0.15015822003601462, "compression_ratio": 1.4185303514376997, "no_speech_prob": 0.007103004492819309}, {"id": 265, "seek": 93836, "start": 960.96, "end": 965.04, "text": " Czy jej fundamentalnie inny spos\u00f3b przetwarzania informacji oparty na", "tokens": [51494, 19832, 28924, 8088, 2766, 294, 1634, 22904, 6541, 302, 31991, 5609, 1356, 13152, 999, 446, 88, 1667, 51698], "temperature": 0.0, "avg_logprob": -0.15015822003601462, "compression_ratio": 1.4185303514376997, "no_speech_prob": 0.007103004492819309}, {"id": 266, "seek": 96504, "start": 965.04, "end": 969.0799999999999, "text": " inteligentnej kompresji kontekstu do stanu, a nie na przechowywaniu", "tokens": [50364, 24777, 25002, 11794, 5207, 14508, 4013, 14373, 916, 372, 84, 360, 27984, 84, 11, 257, 2838, 1667, 8325, 339, 10089, 86, 25849, 50566], "temperature": 0.0, "avg_logprob": -0.15036971533476418, "compression_ratio": 1.494809688581315, "no_speech_prob": 0.04820204898715019}, {"id": 267, "seek": 96504, "start": 969.28, "end": 973.28, "text": " wszystkiego w pami\u0119ci jak attention, mo\u017ce odblokowa\u0107 zupe\u0142nie nowe,", "tokens": [50576, 14615, 12200, 261, 31088, 537, 4207, 3202, 11, 12034, 3611, 5199, 453, 11445, 49922, 586, 68, 11, 50776], "temperature": 0.0, "avg_logprob": -0.15036971533476418, "compression_ratio": 1.494809688581315, "no_speech_prob": 0.04820204898715019}, {"id": 268, "seek": 96504, "start": 973.48, "end": 975.12, "text": " nieodkryte jeszcze mo\u017cliwo\u015bci?", "tokens": [50786, 2838, 378, 43298, 975, 14168, 30854, 36476, 30, 50868], "temperature": 0.0, "avg_logprob": -0.15036971533476418, "compression_ratio": 1.494809688581315, "no_speech_prob": 0.04820204898715019}, {"id": 269, "seek": 96504, "start": 975.3199999999999, "end": 979.24, "text": " Czyli m\u00f3wisz, \u017ce prawdziwym prze\u0142omem mo\u017ce nie by\u0107 nawet szybko\u015b\u0107,", "tokens": [50878, 37099, 13489, 23848, 11, 3561, 41175, 3992, 86, 4199, 8325, 1221, 423, 76, 12034, 2838, 15069, 22696, 36456, 4093, 7753, 11, 51074], "temperature": 0.0, "avg_logprob": -0.15036971533476418, "compression_ratio": 1.494809688581315, "no_speech_prob": 0.04820204898715019}, {"id": 270, "seek": 96504, "start": 979.4399999999999, "end": 982.92, "text": " ale zupe\u0142nie nowy spos\u00f3b my\u015blenia tych modeli.", "tokens": [51084, 6775, 49922, 586, 88, 22904, 48633, 6698, 654, 15180, 2316, 72, 13, 51258], "temperature": 0.0, "avg_logprob": -0.15036971533476418, "compression_ratio": 1.494809688581315, "no_speech_prob": 0.04820204898715019}, {"id": 271, "seek": 96504, "start": 983.12, "end": 988.52, "text": " Przej\u015bcie od takiej brutalnej, si\u0142owej pami\u0119ci do czego\u015b, co bardziej przypomina", "tokens": [51268, 2114, 16920, 9815, 3611, 38941, 17878, 11794, 11, 1511, 1221, 21091, 31088, 537, 360, 36559, 1788, 11, 598, 27209, 41780, 49217, 51538], "temperature": 0.0, "avg_logprob": -0.15036971533476418, "compression_ratio": 1.494809688581315, "no_speech_prob": 0.04820204898715019}, {"id": 272, "seek": 96504, "start": 988.7199999999999, "end": 991.24, "text": " inteligentne streszczenie i rozumienie.", "tokens": [51548, 24777, 25002, 716, 342, 495, 89, 39043, 741, 48797, 27385, 13, 51674], "temperature": 0.0, "avg_logprob": -0.15036971533476418, "compression_ratio": 1.494809688581315, "no_speech_prob": 0.04820204898715019}, {"id": 273, "seek": 96504, "start": 991.4399999999999, "end": 992.4, "text": " W\u0142a\u015bnie.", "tokens": [51684, 343, 5024, 12221, 13, 51732], "temperature": 0.0, "avg_logprob": -0.15036971533476418, "compression_ratio": 1.494809688581315, "no_speech_prob": 0.04820204898715019}, {"id": 274, "seek": 99240, "start": 992.6, "end": 996.56, "text": " By\u0107 mo\u017ce transformery ze swoj\u0105 metod\u0105 patrzenia na wszystko naraz", "tokens": [50374, 3146, 2162, 12034, 4088, 2109, 5277, 49194, 1131, 378, 1611, 1947, 81, 14320, 1667, 22607, 6714, 921, 50572], "temperature": 0.0, "avg_logprob": -0.12919373313585916, "compression_ratio": 1.3438914027149322, "no_speech_prob": 0.03610972687602043}, {"id": 275, "seek": 99240, "start": 996.76, "end": 999.64, "text": " osi\u0105gn\u0119\u0142y ju\u017c swoje poznawcze granice.", "tokens": [50582, 3003, 11404, 4568, 1274, 6825, 10678, 29489, 21281, 629, 86, 9680, 9370, 573, 13, 50726], "temperature": 0.0, "avg_logprob": -0.12919373313585916, "compression_ratio": 1.3438914027149322, "no_speech_prob": 0.03610972687602043}, {"id": 276, "seek": 99240, "start": 999.84, "end": 1003.4399999999999, "text": " A Mamba ze swoj\u0105 elegancj\u0105 i efektywno\u015bci\u0105", "tokens": [50736, 316, 376, 23337, 5277, 49194, 1118, 1275, 66, 8555, 741, 31482, 916, 874, 20944, 50227, 50916], "temperature": 0.0, "avg_logprob": -0.12919373313585916, "compression_ratio": 1.3438914027149322, "no_speech_prob": 0.03610972687602043}, {"id": 277, "seek": 99240, "start": 1003.64, "end": 1006.8, "text": " dopiero zaczyna nam pokazywa\u0107, co jest mo\u017cliwe.", "tokens": [50926, 21900, 12030, 43811, 629, 8835, 13010, 33235, 25234, 11, 598, 3492, 30854, 826, 13, 51084], "temperature": 0.0, "avg_logprob": -0.12919373313585916, "compression_ratio": 1.3438914027149322, "no_speech_prob": 0.03610972687602043}, {"id": 278, "seek": 99240, "start": 1007.0, "end": 1013.0799999999999, "text": " I to jest my\u015bl, kt\u00f3ra sprawia, \u017ce przysz\u0142o\u015b\u0107 tej dziedziny jest tak ekscytuj\u0105ca.", "tokens": [51094, 286, 281, 3492, 452, 19212, 11, 19456, 22734, 654, 11, 3561, 44018, 44742, 12573, 9758, 15338, 3519, 3492, 991, 30724, 1344, 83, 13263, 496, 13, 51398], "temperature": 0.0, "avg_logprob": -0.12919373313585916, "compression_ratio": 1.3438914027149322, "no_speech_prob": 0.03610972687602043}], "language": "pl"}