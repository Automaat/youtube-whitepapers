{"text": " Wyobra\u017amy sobie taki globalny wy\u015bcig zbroje\u0144. Wszyscy buduj\u0105 coraz wi\u0119ksze, coraz pot\u0119\u017cniejsze maszyny. Kieruj\u0105c si\u0119 tak\u0105 prost\u0105 logik\u0105, kto ma najwi\u0119ksz\u0105, ten wygrywa. Dok\u0142adnie. A\u017c tu nagle, pewnego dnia kto\u015b pokazuje, \u017ce znacznie mniejsza, ale o wiele, wiele lepiej wytrenowana maszyna, jest w stanie pokona\u0107 te wszystkie giganty. I pojawia si\u0119 pytanie. Co je\u015bli ca\u0142a dotychczasowa strategia tego wy\u015bcigo by\u0142a, no, fundamentalnie b\u0142\u0119dna. Dzisiaj bierzemy na warsztat jeden z najwa\u017cniejszych, jak s\u0105dz\u0119, artyku\u0142\u00f3w naukowych w dziedzinie AI ostatnich lat. Prosto z laboratorium DeepMind. I przedstawia model, kt\u00f3ry, no, nie ma co ukrywa\u0107 wstrz\u0105sn\u0105 posadami ca\u0142ej bran\u017cy. Hinchila. Nasz\u0105 misj\u0105 jest zrozumienie, dlaczego te dotychczasowe ogromne modele j\u0119zykowe by\u0142y, jak to uj\u0119li autorzy, znacz\u0105co niedotrenowane. I jak ta jedna praca badawcza w zasadzie zmieni\u0142a regu\u0142y gry. A naszym jedynym \u017ar\u00f3d\u0142em jest w\u0142a\u015bnie ten artyku\u0142. Dobrze, to cofnijmy si\u0119 na chwil\u0119. Zanim pojawi\u0142a si\u0119 Hinchila, w \u015bwiecie AI panowa\u0142a jedna prosta zasada. Wi\u0119ksze znaczy lepsze. Tak. Byli\u015bmy \u015bwiadkami prawdziwej eksplozji. Mieli\u015bmy GPT-3 ze 175 miliardami parametr\u00f3w, potem GoFer od DeepMind, kt\u00f3ry mia\u0142 280 miliard\u00f3w. A nawet Megatron Turing NLG z ponad 500 miliardami. To by\u0142a absolutnie dominuj\u0105ca filozofia. I trzeba przyzna\u0107, \u017ce ta filozofia nie wzi\u0119\u0142a si\u0119 znik\u0105d, prawda? Absolutnie nie. Mia\u0142a solidne, naukowe podstawy. Zw\u0142aszcza te badania opublikowane przez Kaplan et alf 2020 roku. One pokazywa\u0142y, \u017ce istnieje niemal matematyczna zale\u017cno\u015b\u0107. To s\u0142ynne Power Low. W\u0142a\u015bnie. Prawo pot\u0119gowe, kt\u00f3re \u0142\u0105czy\u0142o rozmiar modelu, ilo\u015bci danych i bud\u017cet obliczeniowy z jego wydajno\u015bci\u0105. W wielkim skr\u00f3cie, jak podwaja\u0142a\u015b liczb\u0119 parametr\u00f3w, to uzyskiwa\u0142a\u015b przewidywalny, mierzalny spadek los. Czyli b\u0142\u0119du modelu. Tak. To dawa\u0142o poczucie kontroli, wiesz, przewidywalno\u015bci. Mieli przepis. Prosty przepis. Wsyp wi\u0119cej parametr\u00f3w, a dostanie szlepszy model. Dok\u0142adnie. Ale w tym przepisie by\u0142 kluczowy szczeg\u00f3\u0142, kt\u00f3ry, jak si\u0119 okaza\u0142o, wszyscy zinterpretowali w ten sam spos\u00f3b. Zalecenia z tamtej pracy sugerowa\u0142y, \u017ce jak zwi\u0119kszasz bud\u017cet, to rozmiar modelu powinien rosn\u0105\u0107 znacznie szybciej ni\u017c ilo\u015b\u0107 danych. O ile szybciej. Wed\u0142ug ich szacunk\u00f3w na ka\u017cde 10 okrotne zwi\u0119kszenie bud\u017cetu, model powinien by\u0107 wi\u0119kszy o 5,5 razy, a ilo\u015b\u0107 danych zaledwie o 1,8 razy. Czyli prawie ca\u0142y wysi\u0142ek szed\u0142 w architektur\u0119, w in\u017cenieri\u0119, w budowanie coraz wi\u0119kszych cyfrowych katedr. Tak jest. I w efekcie wi\u0119kszo\u015b\u0107 tych gigant\u00f3w, mimo r\u00f3\u017cnych rozmiar\u00f3w, by\u0142a trenowana na bardzo podobnej ilo\u015bci danych. Jakiej? Oko\u0142o 300 miliardach token\u00f3w. To sta\u0142o si\u0119 takim niepisanym standardem, ka\u017cdy chcia\u0142 mie\u0107 najwi\u0119kszy model, bo to rozmiar by\u0142 g\u0142\u00f3wnym wyznacznikiem post\u0119pu. Czekaj, ale to brzmi tak, tak logicznie. Je\u015bli wszyscy opierali si\u0119 na tych samych badaniach, jak to mo\u017cliwe, \u017ce ca\u0142a bran\u017ca, no przecie\u017c najt\u0119\u017csze umys\u0142y VI, przez lata sz\u0142a w jednym kierunku, kt\u00f3ry okaza\u0142 si\u0119 nieoptymalny? My\u015bl\u0119, \u017ce to mieszanka kilku rzeczy. Pycha, inercja. Po pierwsze, koszt, trenowania tych modeli, poch\u0142ania niewyobra\u017calne zasoby. M\u00f3wimy o dziesi\u0105tkach milion\u00f3w dolar\u00f3w za jeden trening. Jeden strza\u0142. W\u0142a\u015bnie. To jest jednorazowa, niezwykle droga operacja. Wi\u0119c jak masz jeden strza\u0142, to trzymasz si\u0119 przepisu, kt\u00f3ry wydaje si\u0119 dzia\u0142a\u0107. Nikt nie chcia\u0142 ryzykowa\u0107 fortuny. A po drugie? A po drugie by\u0142a pewna inercja my\u015blowa. Skoro wi\u0119ksze modele dawa\u0142y lepsze wyniki, naturalnym odruchem by\u0142o budowanie jeszcze wi\u0119kszych. To by\u0142o widowiskowe, wiesz, \u0142atwe do zakomunikowania. A\u017c w ko\u0144cu kto\u015b w Deep Mind powiedzia\u0142 stop i zada\u0142 to fundamentalne pytanie. Dok\u0142adnie. I to jest sedno tej pracy. Pytanie by\u0142o proste, ale jego konsekwencje rewolucyjne. Jak brzmia\u0142o? Maj\u0105c z g\u00f3r\u0119 okre\u015blony bud\u017cet, czyli sta\u0142\u0105 ilo\u015b\u0107 mocy obliczeniowej, tych flops, jak najlepiej zr\u00f3wnowa\u017cy\u0107 rozmiar modelu i ilo\u015b\u0107 danych, \u017ceby dosta\u0107 najlepszy mo\u017cliwy wynik. Czyli zamiast pyta\u0107, jak du\u017cy model mo\u017cemy zbudowa\u0107, zapytali jaki jest optymalny model, kt\u00f3ry mo\u017cemy zbudowa\u0107. W\u0142a\u015bnie. I jaka by\u0142a ich hipoteza, kt\u00f3ra wywr\u00f3ci\u0142a stolik? Hipoteza by\u0142a taka, \u017ce do tych czasowe modele by\u0142y ra\u017c\u0105co undertrained. Po prostu niedotrenowane. Niedotrenowane. Mimo tych setek miliard\u00f3w parametr\u00f3w. Tak. Twierdzili, \u017ce optymaln\u0105 strategi\u0105 nie jest maksymalizowanie rozmiaru, ale r\u00f3wne proporcjonalne skalowanie obu tych czynnik\u00f3w. Czyli... Hmm... Jak? Na ka\u017cde podwojenie liczby parametr\u00f3w modelu powinno przypada\u0107 podwojenie liczby token\u00f3w treningowych. Proporcja jeden do jednego. Ale czy to nie jest zbyt nieuproszczenie? \u015awiat rzadko jest taki idealny. Czy naprawd\u0119 prosta zasada podw\u00f3j parametry, podw\u00f3j dane? Dzia\u0142a w ka\u017cdej skali? To jest \u015bwietne pytanie i w\u0142a\u015bnie dlatego ich metodologia jest tak imponuj\u0105ca. Oni nie wyszli z t\u0105 tez\u0105 znik\u0105d. To wniosek z... Naprawd\u0119 szeroko zakrojonych eksperyment\u00f3w. Jak szeroko? Wytrenowali ponad 400 modeli o bardzo r\u00f3\u017cnych rozmiarach. Od malutkich, 70 milion\u00f3w parametr\u00f3w, do ponad 16 miliard\u00f3w i na r\u00f3\u017cnej ilo\u015bci danych. Aha. To im pozwoli\u0142o stworzy\u0107 tak\u0105 swoist\u0105 map\u0119, kt\u00f3ra pokazywa\u0142a, jak osi\u0105gn\u0105\u0107 najni\u017cszy los przy danym koszcie. Czyli zamiast jednego wielkiego eksperymentu, zrobili setki mniejszych. \u017beby zrozumie\u0107 sam\u0105 fizyk\u0119 tego procesu. I co wi\u0119cej, zastosowa\u0142y trzy r\u00f3\u017cne metody analityczne, \u017ceby potwierdzi\u0107 swoje ustalenia. Trzy? Tak. Pierwsza to by\u0142o klasyczne badanie krzywych treningowych. Druga to analiza profili Isoflop i to jest bardzo ciekawe. Co to jest ten profil Isoflop? To brzmi do\u015b\u0107 technicznie. Wyobra\u017a sobie, \u017ce masz okre\u015blony bud\u017cet na budow\u0119 samochodu wy\u015bcigowego. Okej. Profil Isoflop to jakby\u015b testowa\u0142a setki kombinacji. Wielki silnik z ma\u0142ymi ko\u0142ami, ma\u0142y silnik z super lekk\u0105 karoseri\u0105 i tak dalej. Wszystkie te kombinacje kosztuj\u0105 dok\u0142adnie tyle samo, a ja szukam tej jednej, idealnej, kt\u00f3ra da mi najlepszy czas na torze. Dok\u0142adnie. I tu by\u0142o tak samo. Przy sta\u0142ym bud\u017cecie flops szukali optymalnej kombinacji parametr\u00f3w i tokan\u00f3w, kt\u00f3ra daje najni\u017cszy los. Rozumiem, czyli to jest poszukiwanie z\u0142otego \u015brodka, a nie pchanie jednej zmiennej do ekstremum. Tak i wreszcie trzecia metoda, najbardziej matematyczna, to dopasowanie parametrycznej funkcji straty do wszystkich wynik\u00f3w. Ale co najwa\u017cniejsze, wszystkie trzy podej\u015bcia doprowadzi\u0142y ich do tego samego wniosku. Czyli, \u017ce dla optymalnej wydajno\u015bci rozmiar modelu i ilo\u015b\u0107 danych treningowych powinny rosn\u0105\u0107 w r\u00f3wnych proporcjach. To by\u0142o solidne, potr\u00f3jnie zweryfikowane odkrycie. W porz\u0105dku tu robi si\u0119 naprawd\u0119 ciekawie. Hipoteza jest mocna, poparta setkami eksperyment\u00f3w, ale teoria to jedno. Postanowili j\u0105 udowodni\u0107 w najbardziej dosadny spos\u00f3b. Stworzyli model, kt\u00f3ry nazwali Chinchilla. To by\u0142 ten decyduj\u0105cy eksperyment, prawda? Takie sprawdzam. Dok\u0142adnie. Wzi\u0119li dok\u0142adnie ten sam bud\u017cet obliczeniowy, jakiego u\u017cyto do wytrenowania ich poprzedniego flagowca, Gofer'a. Gofer mia\u0142 280 miliard\u00f3w parametr\u00f3w. Tak. A z nas stworzy\u0107 korejemnego giganta post\u0105pili zgodnie ze swoj\u0105 now\u0105 hipotez\u0105. Chwila, czyli jak? Zbudowali mniejszy model. Znacznie mniejszy. Zbudowali Chinchilla. Model o zaledwie 70 miliardach parametr\u00f3w, 70. To czterokrotnie mniejsze od Gofer'a. To wbrew wszelkiej intuicji. Jak to w og\u00f3le jest mo\u017cliwe przy tym samym bud\u017cecie? Ot\u00f3\u017c to. Czterokrotnie mniejszy, ale zwytrenowali go na jeden przecinek 4 biliona token\u00f3w. A to ponad czterokrotnie wi\u0119cej danych ni\u017c u\u017cyto dla Gofer'a. Ten sam koszt, zupe\u0142nie inna alokacja zasob\u00f3w. Dok\u0142adnie tak. Ok, czyli mamy scen\u0119 ustawion\u0105. W jednym naro\u017cniku Gigant Gofer, 280 miliard\u00f3w parametr\u00f3w, symbol starej szko\u0142y bigger is better. W drugim czterokrotnie mniejsza Chinchilla. Jak sko\u0144czy\u0142a si\u0119 ta walka? Chinchilla, m\u00f3wi\u0105c wprost, zmiarzy\u0142a Gofer'a. Ale nie tylko jego. Zmiarzy\u0142a te\u017c inne giganty tamtych czas\u00f3w jak GPT-3 czy Megatron Turing NLG. To nie by\u0142a wygrana w jednym specyficznym zadaniu? Nie, to by\u0142a deklasacja w szerokim spektrum test\u00f3w. Od rozumienia j\u0119zyka przez odpowiadanie na pytania, po wiedz\u0119 og\u00f3ln\u0105. Daj mu jakie\u015b przyk\u0142ady, bo to brzmi a\u017c niewiarygodnie. No dobrze, liczby m\u00f3wi\u0105 same za siebie. Na bardzo wymagaj\u0105cym testie MMLU, kt\u00f3ry sprawdza wiedz\u0119 z 57 dziedzin akademickich, Chinchilla osi\u0105gn\u0119\u0142a 67,5% dok\u0142adno\u015bci. A Gofer? Zaledwie 60%. To jest ponad 7% r\u00f3\u017cnicy. W \u015bwiecie modeli j\u0119zykowych to jest przepa\u015b\u0107. Zatrzymajmy si\u0119 tu na chwil\u0119. W artykule autorzy dodaj\u0105 tak\u0105 fascynuj\u0105c\u0105 uwag\u0119, \u017ce ten wynik Chinchilli on przywy\u017cszy\u0142 prognoz\u0119 ekspert\u00f3w dotycz\u0105ce tego, co b\u0119dzie mo\u017cliwe do osi\u0105gni\u0119cia w tej dziedzinie dopiero w czerwcu 2023 roku. A artyku\u0142 jest z marca 2022 roku? W\u0142a\u015bnie. To jest jakby kto\u015b w 2022 roku zbudowa\u0142 co\u015b, czego spodziewali\u015bmy si\u0119 dopiero za p\u00f3\u0142tara roku. To zaburza ca\u0142\u0105 o\u015b czasu post\u0119pu w AI. To jest doskona\u0142e podsumowanie. Oni nie tylko pokazali lepsz\u0105 metod\u0119, oni przyspieszyli post\u0119p. I to wida\u0107 wsz\u0119dzie. Gdzie jeszcze? W zadaniach z czytania ze zrozumieniem jak benchmark race Chinchilla uzyska\u0142a 82.3% w por\u00f3wnaniu do 71.6% Gofera. Wow. Na ogromnym zbiorze test\u00f3w BigBange \u015brednia wydajno\u015b\u0107 Chinchilli by\u0142a wy\u017csza od 10.7% w zadaniach z odpowiadania na pytania w trybie closed book, czyli bez dostabu do zewn\u0119trznych \u017ar\u00f3de\u0142. Gdzie model musi polega\u0107 wy\u0142\u0105cznie na swojej wiedzy? Tak, ustanowi\u0142a nowy rekord. Wygrana by\u0142a konsekwentna, znacz\u0105ca i widoczna praktycznie na ka\u017cdym polu. To brzmi jak fundamentalna zmiana paradygmatu. Jakie s\u0105 praktyczne implikacje tego odkrycia? Co to wszystko oznacza dla kogo\u015b, kto nie buduje modeli za setki milion\u00f3w, ale chce z nich korzysta\u0107? Konsekwencje s\u0105 ogromne na kilku poziomach. Po pierwsze, i to jest najbardziej oczywiste, efektywno\u015b\u0107. Co to znaczy? Mniejszy model taki jak Chinchilla jest znacznie ta\u0144szy i szybszy w u\u017cyciu do tzw. inferencji, czyli generowania odpowiedzi. To samo dotyczy procesu fine tuning, czyli dostosowywania go do konkretnych zada\u0144. Czyli ma\u0142a firma lub startup, kt\u00f3ry wcze\u015bniej m\u00f3g\u0142 tylko poma\u017cy\u0107 o wykorzystaniu modelu klasy GPT-3 nagle dostaje do r\u0119ki narz\u0119dzie o por\u00f3wniwalnej, a nawet lepszej, mocy. Ale znacznie bardziej dost\u0119pne, dok\u0142adnie tak, to demokratyzuje dost\u0119p do pot\u0119\u017cnej AI. A druga, mo\u017ce wa\u017cniejsza konsekwencja? Druga to zmiana priortet\u00f3w w ca\u0142ej bran\u017cy. W\u0105zkim gard\u0142em w rozwoju AI przestaje by\u0107 tylko moc obliczeniowa, a staje si\u0119 nim dost\u0119p do ogromnych, ale co kluczowe, wysokiej jako\u015bci zbior\u00f3w danych. Artyku\u0142 pokazuje prognozy i szacuje, \u017ce optymalny model o rozmiarze biliona parametr\u00f3w wymaga\u0142by treningu na ponad 21 bilion\u00f3w token\u00f3w. 21 bilion\u00f3w to s\u0105 astronomiczne liczby. Tak, wy\u015bcig na parametry zamieni\u0142 si\u0119 w wy\u015bcig na dane. To wszystko brzmi jak ostateczne zamkni\u0119te rozwi\u0105zanie, ale w nauce nigdy tak nie jest. Gdzie s\u0105 dziury w tej teorii? Co\u015b, co sami autorzy przyznali, \u017ce nie do ko\u0144ca im si\u0119 zgadza. I to jest w\u0142a\u015bnie cecha \u015bwietnych prac naukowych. Byli bardzo transparentni, co do ogranicze\u0144. Na przyk\u0142ad? Po pierwsze, ich prawa skalowania opieraj\u0105 si\u0119 na za\u0142o\u017ceniu o idealnie pot\u0119gowym charakterze tej zale\u017cno\u015bci. Ale sami zauwa\u017cyli pewn\u0105 subteln\u0105 krzywizn\u0119 w danych przy najwi\u0119kszych modelach. Co to mo\u017ce oznacza\u0107? To mo\u017ce oznacza\u0107, \u017ce nawet ich, ju\u017c obni\u017cone, szacunki co do optymalnego rozmiaru s\u0105 wci\u0105\u017c nieco zawy\u017cone. Innymi s\u0142owy, w przysz\u0142o\u015bci optymalne modele mog\u0105 by\u0107 jeszcze mniejsze. A potrzebowa\u0107 jeszcze wi\u0119cej danych? Czyli trend mo\u017ce by\u0107 nawet bardziej ekstremalny ni\u017c pokazali? Mo\u017cliwe. Po drugie, i to jest wa\u017cne ograniczenie, ca\u0142a ich analiza zosta\u0142a przeprowadzona na modelach trenowanych przez mniej ni\u017c jedn\u0105 epok\u0119. Czyli ka\u017cdy fragment danych model widzia\u0142 tylko raz. Dok\u0142adnie. A to pozostawia otwarte pytanie o to, jak te zale\u017cno\u015bci wygl\u0105da\u0142yby w re\u017cymie wielokrotnego przetwarzania tych samych danych. Czy to jest efektywne? Czy prowadzi do przeuczenia? Tego ich praca nie rozstrzyga. Ca\u0142a spo\u0142eczno\u015b\u0107 AI by\u0142a zafiksowana na wy\u015bcigu na rozmiar, na budowaniu coraz wi\u0119kszych silnik\u00f3w. Podczas gdy prawdziwa odpowied\u017a le\u017ca\u0142a w r\u00f3wnowadze. W znalezieniu idealnej proporcji mi\u0119dzy rozmiarem silnika, a powiedzmy jako\u015bci\u0105 paliwa. I czasem sp\u0119dzonym na torze. Okaza\u0142o si\u0119, \u017ce mniejsze, ale znacznie lepiej wydukowany model mo\u017ce z \u0142atwo\u015bci\u0105 przewy\u017cszy\u0107 wi\u0119kszego, ale nie do uczonego giganta. To doskona\u0142a metafora. Nie chodzi o to, by mie\u0107 najwi\u0119kszy m\u00f3zg, ale by mie\u0107 najlepiej wytrenowany m\u00f3zg. Ksi\u0119cila to dow\u00f3d, \u017ce w AI liczy si\u0119 nie tylko architektura, ale przede wszystkim do\u015bwiadczenie, czyli dane. Na koniec zostawmy tak\u0105 prowokuj\u0105c\u0105 my\u015bl, kt\u00f3ra wy\u0142ania si\u0119 wprost z tej pracy. Artyku\u0142 dowodzi, \u017ce dane s\u0105 now\u0105 granic\u0105, nowym z\u0142otem, nowym polem wy\u015bcigu. Ale to rodzi fundamentalne, by\u0107 mo\u017ce znacznie trudniejsze pytanie. I wiesz, to jest co\u015b, co sp\u0119dza sens, powiek badaczom. Z jednej strony jeste\u015bmy okrok od niesamowitych prze\u0142om\u00f3w, z drugiej patrzymy na internet, kt\u00f3ry jest g\u0142\u00f3wnym \u017ar\u00f3d\u0142em tych danych i widzimy... C\u00f3\u017c, jako\u015b\u0107 bywa r\u00f3\u017cna. To jest prawdziwe wyzwanie. Dok\u0142adnie. Skoro do trenowania przysz\u0142ych jeszcze pot\u0119\u017cniejszych modeli b\u0119dziemy potrzebowa\u0107 bilion\u00f3w token\u00f3w wysokiej jako\u015bci danych, to sk\u0105d je we\u017amiemy? Jak b\u0119dziemy je pozyskiwa\u0107 i filtrowa\u0107 w spos\u00f3b odpowiedzialny? I jak unikniemy pu\u0142apek uprzedze\u0144, dezinformacji i toksyczno\u015bci, kt\u00f3re ju\u017c teraz s\u0105 ogromnym problemem? Artyku\u0142 o Chinczyli rozwi\u0105za\u0142 jeden, niezwykle wa\u017cny problem, problem optymalizacji obliczeniowej. Ale w zamian, jak na d\u0142oni, pokaza\u0142 nam kolejny, znacznie g\u0142\u0119bszy problem, logistyczny i ostatecznie etyczny, z kt\u00f3rym ca\u0142a dziedzina b\u0119dzie musia\u0142a si\u0119 zmierzy\u0107. Znalezienie tych danych i upewnienie si\u0119, \u017ce s\u0105 one dobre to mo\u017ce by\u0107 najtrudniejsze zadanie dla AI w tej dekadzie.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.3200000000000003, "text": " Wyobra\u017amy sobie taki globalny wy\u015bcig zbroje\u0144.", "tokens": [50364, 14458, 24393, 10659, 2226, 13652, 20065, 4338, 1634, 4628, 1788, 66, 328, 710, 9120, 2884, 5248, 13, 50530], "temperature": 0.0, "avg_logprob": -0.1498026909766259, "compression_ratio": 1.4813559322033898, "no_speech_prob": 0.009344353340566158}, {"id": 1, "seek": 0, "start": 3.3200000000000003, "end": 7.44, "text": " Wszyscy buduj\u0105 coraz wi\u0119ksze, coraz pot\u0119\u017cniejsze maszyny.", "tokens": [50530, 343, 15453, 38966, 3265, 13263, 25899, 29968, 1381, 11, 25899, 1847, 1274, 1427, 44258, 2300, 1229, 1634, 13, 50736], "temperature": 0.0, "avg_logprob": -0.1498026909766259, "compression_ratio": 1.4813559322033898, "no_speech_prob": 0.009344353340566158}, {"id": 2, "seek": 0, "start": 7.44, "end": 12.52, "text": " Kieruj\u0105c si\u0119 tak\u0105 prost\u0105 logik\u0105, kto ma najwi\u0119ksz\u0105, ten wygrywa.", "tokens": [50736, 591, 811, 44733, 3244, 31069, 10293, 1611, 3565, 1035, 1611, 11, 23780, 463, 48636, 1694, 8925, 11, 2064, 4628, 70, 627, 4151, 13, 50990], "temperature": 0.0, "avg_logprob": -0.1498026909766259, "compression_ratio": 1.4813559322033898, "no_speech_prob": 0.009344353340566158}, {"id": 3, "seek": 0, "start": 12.52, "end": 13.88, "text": " Dok\u0142adnie.", "tokens": [50990, 29768, 10358, 2766, 13, 51058], "temperature": 0.0, "avg_logprob": -0.1498026909766259, "compression_ratio": 1.4813559322033898, "no_speech_prob": 0.009344353340566158}, {"id": 4, "seek": 0, "start": 13.88, "end": 17.68, "text": " A\u017c tu nagle, pewnego dnia kto\u015b pokazuje, \u017ce znacznie mniejsza,", "tokens": [51058, 316, 1427, 2604, 297, 15088, 11, 25889, 11858, 274, 12679, 32982, 13010, 43317, 11, 3561, 15397, 14875, 2766, 275, 30295, 2394, 11, 51248], "temperature": 0.0, "avg_logprob": -0.1498026909766259, "compression_ratio": 1.4813559322033898, "no_speech_prob": 0.009344353340566158}, {"id": 5, "seek": 0, "start": 17.68, "end": 20.400000000000002, "text": " ale o wiele, wiele lepiej wytrenowana maszyna,", "tokens": [51248, 6775, 277, 33137, 11, 33137, 476, 39699, 261, 4328, 1095, 40458, 2300, 1229, 629, 11, 51384], "temperature": 0.0, "avg_logprob": -0.1498026909766259, "compression_ratio": 1.4813559322033898, "no_speech_prob": 0.009344353340566158}, {"id": 6, "seek": 0, "start": 20.400000000000002, "end": 23.0, "text": " jest w stanie pokona\u0107 te wszystkie giganty.", "tokens": [51384, 3492, 261, 40013, 13010, 4037, 2162, 535, 31723, 8741, 394, 88, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1498026909766259, "compression_ratio": 1.4813559322033898, "no_speech_prob": 0.009344353340566158}, {"id": 7, "seek": 0, "start": 23.0, "end": 24.88, "text": " I pojawia si\u0119 pytanie.", "tokens": [51514, 286, 30655, 654, 3244, 36610, 13, 51608], "temperature": 0.0, "avg_logprob": -0.1498026909766259, "compression_ratio": 1.4813559322033898, "no_speech_prob": 0.009344353340566158}, {"id": 8, "seek": 0, "start": 24.88, "end": 28.32, "text": " Co je\u015bli ca\u0142a dotychczasowa strategia tego wy\u015bcigo by\u0142a,", "tokens": [51608, 3066, 25630, 1335, 5024, 5893, 16384, 30989, 5528, 5464, 654, 8627, 4628, 1788, 66, 7483, 23936, 11, 51780], "temperature": 0.0, "avg_logprob": -0.1498026909766259, "compression_ratio": 1.4813559322033898, "no_speech_prob": 0.009344353340566158}, {"id": 9, "seek": 2832, "start": 28.32, "end": 31.240000000000002, "text": " no, fundamentalnie b\u0142\u0119dna.", "tokens": [50364, 572, 11, 8088, 2766, 272, 1221, 6298, 629, 13, 50510], "temperature": 0.0, "avg_logprob": -0.14259910583496094, "compression_ratio": 1.3874172185430464, "no_speech_prob": 0.00758730573579669}, {"id": 10, "seek": 2832, "start": 31.240000000000002, "end": 34.44, "text": " Dzisiaj bierzemy na warsztat jeden z najwa\u017cniejszych, jak s\u0105dz\u0119,", "tokens": [50510, 39448, 22356, 272, 34602, 3633, 1667, 13718, 2682, 267, 12906, 710, 11212, 27111, 10402, 45021, 11, 4207, 9015, 67, 11052, 11, 50670], "temperature": 0.0, "avg_logprob": -0.14259910583496094, "compression_ratio": 1.3874172185430464, "no_speech_prob": 0.00758730573579669}, {"id": 11, "seek": 2832, "start": 34.44, "end": 37.6, "text": " artyku\u0142\u00f3w naukowych w dziedzinie AI ostatnich lat.", "tokens": [50670, 594, 874, 5279, 1221, 3901, 35616, 74, 19605, 261, 9758, 15338, 259, 414, 7318, 32686, 77, 480, 4465, 13, 50828], "temperature": 0.0, "avg_logprob": -0.14259910583496094, "compression_ratio": 1.3874172185430464, "no_speech_prob": 0.00758730573579669}, {"id": 12, "seek": 2832, "start": 37.6, "end": 39.96, "text": " Prosto z laboratorium DeepMind.", "tokens": [50828, 2114, 22756, 710, 5938, 41679, 14895, 44, 471, 13, 50946], "temperature": 0.0, "avg_logprob": -0.14259910583496094, "compression_ratio": 1.3874172185430464, "no_speech_prob": 0.00758730573579669}, {"id": 13, "seek": 2832, "start": 39.96, "end": 45.6, "text": " I przedstawia model, kt\u00f3ry, no, nie ma co ukrywa\u0107 wstrz\u0105sn\u0105 posadami ca\u0142ej bran\u017cy.", "tokens": [50946, 286, 45616, 654, 2316, 11, 9913, 11, 572, 11, 2838, 463, 598, 26769, 627, 25234, 261, 9733, 8925, 82, 13113, 1366, 345, 4526, 47631, 73, 12029, 7735, 13, 51228], "temperature": 0.0, "avg_logprob": -0.14259910583496094, "compression_ratio": 1.3874172185430464, "no_speech_prob": 0.00758730573579669}, {"id": 14, "seek": 2832, "start": 45.6, "end": 46.879999999999995, "text": " Hinchila.", "tokens": [51228, 389, 12415, 7371, 13, 51292], "temperature": 0.0, "avg_logprob": -0.14259910583496094, "compression_ratio": 1.3874172185430464, "no_speech_prob": 0.00758730573579669}, {"id": 15, "seek": 2832, "start": 46.879999999999995, "end": 52.72, "text": " Nasz\u0105 misj\u0105 jest zrozumienie, dlaczego te dotychczasowe ogromne modele j\u0119zykowe", "tokens": [51292, 16151, 8925, 3346, 8555, 3492, 710, 27857, 449, 27385, 11, 37873, 39329, 535, 5893, 16384, 30989, 6880, 34416, 298, 716, 4391, 306, 49055, 74, 6880, 51584], "temperature": 0.0, "avg_logprob": -0.14259910583496094, "compression_ratio": 1.3874172185430464, "no_speech_prob": 0.00758730573579669}, {"id": 16, "seek": 2832, "start": 52.72, "end": 57.120000000000005, "text": " by\u0142y, jak to uj\u0119li autorzy, znacz\u0105co niedotrenowane.", "tokens": [51584, 26366, 11, 4207, 281, 344, 11115, 2081, 19510, 1229, 11, 15397, 326, 8925, 1291, 32488, 310, 1095, 23066, 13, 51804], "temperature": 0.0, "avg_logprob": -0.14259910583496094, "compression_ratio": 1.3874172185430464, "no_speech_prob": 0.00758730573579669}, {"id": 17, "seek": 5712, "start": 57.12, "end": 61.919999999999995, "text": " I jak ta jedna praca badawcza w zasadzie zmieni\u0142a regu\u0142y gry.", "tokens": [50364, 286, 4207, 1846, 5232, 629, 582, 6628, 272, 1538, 86, 41524, 261, 44585, 3283, 17020, 35462, 5024, 1121, 84, 6825, 41974, 13, 50604], "temperature": 0.0, "avg_logprob": -0.1218045222294795, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.02648051828145981}, {"id": 18, "seek": 5712, "start": 61.919999999999995, "end": 64.84, "text": " A naszym jedynym \u017ar\u00f3d\u0142em jest w\u0142a\u015bnie ten artyku\u0142.", "tokens": [50604, 316, 48094, 5232, 88, 12996, 50212, 43678, 11126, 3492, 14234, 2064, 594, 874, 5279, 1221, 13, 50750], "temperature": 0.0, "avg_logprob": -0.1218045222294795, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.02648051828145981}, {"id": 19, "seek": 5712, "start": 64.84, "end": 67.47999999999999, "text": " Dobrze, to cofnijmy si\u0119 na chwil\u0119.", "tokens": [50750, 29679, 13503, 11, 281, 598, 69, 77, 1718, 2226, 3244, 1667, 41941, 1274, 13, 50882], "temperature": 0.0, "avg_logprob": -0.1218045222294795, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.02648051828145981}, {"id": 20, "seek": 5712, "start": 67.47999999999999, "end": 73.03999999999999, "text": " Zanim pojawi\u0142a si\u0119 Hinchila, w \u015bwiecie AI panowa\u0142a jedna prosta zasada.", "tokens": [50882, 1176, 17869, 30655, 72, 5024, 3244, 389, 12415, 7371, 11, 261, 40078, 4260, 7318, 2462, 5528, 5024, 5232, 629, 582, 8638, 26530, 1538, 13, 51160], "temperature": 0.0, "avg_logprob": -0.1218045222294795, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.02648051828145981}, {"id": 21, "seek": 5712, "start": 73.03999999999999, "end": 74.56, "text": " Wi\u0119ksze znaczy lepsze.", "tokens": [51160, 30127, 1694, 1381, 36584, 476, 1878, 1381, 13, 51236], "temperature": 0.0, "avg_logprob": -0.1218045222294795, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.02648051828145981}, {"id": 22, "seek": 5712, "start": 74.56, "end": 75.4, "text": " Tak.", "tokens": [51236, 9118, 13, 51278], "temperature": 0.0, "avg_logprob": -0.1218045222294795, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.02648051828145981}, {"id": 23, "seek": 5712, "start": 75.4, "end": 77.84, "text": " Byli\u015bmy \u015bwiadkami prawdziwej eksplozji.", "tokens": [51278, 3146, 38452, 21485, 345, 48737, 41175, 3992, 826, 73, 30724, 564, 15151, 4013, 13, 51400], "temperature": 0.0, "avg_logprob": -0.1218045222294795, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.02648051828145981}, {"id": 24, "seek": 5712, "start": 77.84, "end": 82.84, "text": " Mieli\u015bmy GPT-3 ze 175 miliardami parametr\u00f3w,", "tokens": [51400, 376, 23099, 10513, 26039, 51, 12, 18, 5277, 41165, 1962, 72, 515, 4526, 6220, 27965, 3901, 11, 51650], "temperature": 0.0, "avg_logprob": -0.1218045222294795, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.02648051828145981}, {"id": 25, "seek": 5712, "start": 82.84, "end": 86.92, "text": " potem GoFer od DeepMind, kt\u00f3ry mia\u0142 280 miliard\u00f3w.", "tokens": [51650, 36513, 1037, 37, 260, 3611, 14895, 44, 471, 11, 9913, 27989, 41229, 1962, 72, 515, 3901, 13, 51854], "temperature": 0.0, "avg_logprob": -0.1218045222294795, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.02648051828145981}, {"id": 26, "seek": 8692, "start": 86.92, "end": 92.04, "text": " A nawet Megatron Turing NLG z ponad 500 miliardami.", "tokens": [50364, 316, 22696, 9986, 267, 2044, 314, 1345, 426, 43, 38, 710, 9224, 345, 5923, 1962, 72, 515, 4526, 13, 50620], "temperature": 0.0, "avg_logprob": -0.11497405749648365, "compression_ratio": 1.3680297397769516, "no_speech_prob": 0.0008146983454935253}, {"id": 27, "seek": 8692, "start": 92.04, "end": 94.8, "text": " To by\u0142a absolutnie dominuj\u0105ca filozofia.", "tokens": [50620, 1407, 23936, 18757, 2766, 8859, 13263, 496, 1387, 15151, 2670, 654, 13, 50758], "temperature": 0.0, "avg_logprob": -0.11497405749648365, "compression_ratio": 1.3680297397769516, "no_speech_prob": 0.0008146983454935253}, {"id": 28, "seek": 8692, "start": 94.8, "end": 98.36, "text": " I trzeba przyzna\u0107, \u017ce ta filozofia nie wzi\u0119\u0142a si\u0119 znik\u0105d, prawda?", "tokens": [50758, 286, 25860, 6501, 35458, 2162, 11, 3561, 1846, 1387, 15151, 2670, 654, 2838, 261, 16706, 5024, 3244, 710, 13123, 18962, 11, 43607, 30, 50936], "temperature": 0.0, "avg_logprob": -0.11497405749648365, "compression_ratio": 1.3680297397769516, "no_speech_prob": 0.0008146983454935253}, {"id": 29, "seek": 8692, "start": 98.36, "end": 99.36, "text": " Absolutnie nie.", "tokens": [50936, 5813, 2308, 2766, 2838, 13, 50986], "temperature": 0.0, "avg_logprob": -0.11497405749648365, "compression_ratio": 1.3680297397769516, "no_speech_prob": 0.0008146983454935253}, {"id": 30, "seek": 8692, "start": 99.36, "end": 101.96000000000001, "text": " Mia\u0142a solidne, naukowe podstawy.", "tokens": [50986, 376, 25605, 5100, 716, 11, 35616, 74, 6880, 43443, 88, 13, 51116], "temperature": 0.0, "avg_logprob": -0.11497405749648365, "compression_ratio": 1.3680297397769516, "no_speech_prob": 0.0008146983454935253}, {"id": 31, "seek": 8692, "start": 101.96000000000001, "end": 107.12, "text": " Zw\u0142aszcza te badania opublikowane przez Kaplan et alf 2020 roku.", "tokens": [51116, 29385, 1221, 19601, 41524, 535, 1578, 5609, 999, 48620, 23066, 14064, 10988, 16554, 1030, 419, 69, 4808, 19451, 13, 51374], "temperature": 0.0, "avg_logprob": -0.11497405749648365, "compression_ratio": 1.3680297397769516, "no_speech_prob": 0.0008146983454935253}, {"id": 32, "seek": 8692, "start": 107.12, "end": 111.36, "text": " One pokazywa\u0142y, \u017ce istnieje niemal matematyczna zale\u017cno\u015b\u0107.", "tokens": [51374, 1485, 13010, 33235, 4151, 6825, 11, 3561, 1418, 2766, 2884, 2838, 5579, 3803, 8615, 17466, 629, 710, 45494, 23293, 13, 51586], "temperature": 0.0, "avg_logprob": -0.11497405749648365, "compression_ratio": 1.3680297397769516, "no_speech_prob": 0.0008146983454935253}, {"id": 33, "seek": 8692, "start": 111.36, "end": 112.96000000000001, "text": " To s\u0142ynne Power Low.", "tokens": [51586, 1407, 15116, 2534, 716, 7086, 17078, 13, 51666], "temperature": 0.0, "avg_logprob": -0.11497405749648365, "compression_ratio": 1.3680297397769516, "no_speech_prob": 0.0008146983454935253}, {"id": 34, "seek": 11296, "start": 112.96, "end": 113.96, "text": " W\u0142a\u015bnie.", "tokens": [50364, 343, 5024, 12221, 13, 50414], "temperature": 0.0, "avg_logprob": -0.12853484434240003, "compression_ratio": 1.4767025089605734, "no_speech_prob": 0.15324664115905762}, {"id": 35, "seek": 11296, "start": 113.96, "end": 120.96, "text": " Prawo pot\u0119gowe, kt\u00f3re \u0142\u0105czy\u0142o rozmiar modelu, ilo\u015bci danych i bud\u017cet obliczeniowy z jego wydajno\u015bci\u0105.", "tokens": [50414, 430, 5131, 78, 1847, 1274, 70, 6880, 11, 8864, 220, 15926, 6522, 5249, 9544, 3057, 289, 2316, 84, 11, 1930, 44468, 274, 34644, 741, 3265, 1427, 302, 1111, 1050, 42124, 10089, 710, 26542, 25984, 1805, 16438, 1611, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12853484434240003, "compression_ratio": 1.4767025089605734, "no_speech_prob": 0.15324664115905762}, {"id": 36, "seek": 11296, "start": 120.96, "end": 124.24, "text": " W wielkim skr\u00f3cie, jak podwaja\u0142a\u015b liczb\u0119 parametr\u00f3w,", "tokens": [50764, 343, 20570, 25112, 1110, 11721, 4260, 11, 4207, 2497, 86, 12908, 5024, 1788, 6169, 89, 65, 1274, 6220, 27965, 3901, 11, 50928], "temperature": 0.0, "avg_logprob": -0.12853484434240003, "compression_ratio": 1.4767025089605734, "no_speech_prob": 0.15324664115905762}, {"id": 37, "seek": 11296, "start": 124.24, "end": 128.2, "text": " to uzyskiwa\u0142a\u015b przewidywalny, mierzalny spadek los.", "tokens": [50928, 281, 16851, 749, 2984, 4151, 5024, 1788, 39758, 327, 27112, 304, 1634, 11, 275, 34602, 304, 1634, 637, 762, 74, 1750, 13, 51126], "temperature": 0.0, "avg_logprob": -0.12853484434240003, "compression_ratio": 1.4767025089605734, "no_speech_prob": 0.15324664115905762}, {"id": 38, "seek": 11296, "start": 128.2, "end": 129.6, "text": " Czyli b\u0142\u0119du modelu.", "tokens": [51126, 37099, 272, 46564, 769, 2316, 84, 13, 51196], "temperature": 0.0, "avg_logprob": -0.12853484434240003, "compression_ratio": 1.4767025089605734, "no_speech_prob": 0.15324664115905762}, {"id": 39, "seek": 11296, "start": 129.6, "end": 130.2, "text": " Tak.", "tokens": [51196, 9118, 13, 51226], "temperature": 0.0, "avg_logprob": -0.12853484434240003, "compression_ratio": 1.4767025089605734, "no_speech_prob": 0.15324664115905762}, {"id": 40, "seek": 11296, "start": 130.2, "end": 133.92, "text": " To dawa\u0142o poczucie kontroli, wiesz, przewidywalno\u015bci.", "tokens": [51226, 1407, 1120, 4151, 5249, 26423, 1311, 414, 14373, 340, 2081, 11, 261, 15347, 11, 39758, 327, 27112, 304, 16438, 13, 51412], "temperature": 0.0, "avg_logprob": -0.12853484434240003, "compression_ratio": 1.4767025089605734, "no_speech_prob": 0.15324664115905762}, {"id": 41, "seek": 11296, "start": 133.92, "end": 134.95999999999998, "text": " Mieli przepis.", "tokens": [51412, 376, 23099, 30829, 271, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12853484434240003, "compression_ratio": 1.4767025089605734, "no_speech_prob": 0.15324664115905762}, {"id": 42, "seek": 11296, "start": 134.95999999999998, "end": 136.28, "text": " Prosty przepis.", "tokens": [51464, 2114, 555, 88, 30829, 271, 13, 51530], "temperature": 0.0, "avg_logprob": -0.12853484434240003, "compression_ratio": 1.4767025089605734, "no_speech_prob": 0.15324664115905762}, {"id": 43, "seek": 11296, "start": 136.28, "end": 139.35999999999999, "text": " Wsyp wi\u0119cej parametr\u00f3w, a dostanie szlepszy model.", "tokens": [51530, 343, 3187, 79, 26004, 6220, 27965, 3901, 11, 257, 20568, 7155, 7870, 306, 1878, 1229, 2316, 13, 51684], "temperature": 0.0, "avg_logprob": -0.12853484434240003, "compression_ratio": 1.4767025089605734, "no_speech_prob": 0.15324664115905762}, {"id": 44, "seek": 11296, "start": 139.35999999999999, "end": 140.35999999999999, "text": " Dok\u0142adnie.", "tokens": [51684, 29768, 10358, 2766, 13, 51734], "temperature": 0.0, "avg_logprob": -0.12853484434240003, "compression_ratio": 1.4767025089605734, "no_speech_prob": 0.15324664115905762}, {"id": 45, "seek": 14036, "start": 140.36, "end": 147.84, "text": " Ale w tym przepisie by\u0142 kluczowy szczeg\u00f3\u0142, kt\u00f3ry, jak si\u0119 okaza\u0142o, wszyscy zinterpretowali w ten sam spos\u00f3b.", "tokens": [50364, 9366, 261, 8107, 30829, 271, 414, 16673, 9671, 1311, 89, 10089, 22090, 1146, 16181, 11, 9913, 11, 4207, 3244, 3133, 12257, 5249, 11, 44232, 710, 41935, 305, 5103, 261, 2064, 3247, 22904, 13, 50738], "temperature": 0.0, "avg_logprob": -0.09819069407344644, "compression_ratio": 1.4599303135888502, "no_speech_prob": 0.009743870235979557}, {"id": 46, "seek": 14036, "start": 147.84, "end": 151.72000000000003, "text": " Zalecenia z tamtej pracy sugerowa\u0142y, \u017ce jak zwi\u0119kszasz bud\u017cet,", "tokens": [50738, 1176, 1220, 13037, 654, 710, 7677, 975, 73, 35591, 459, 1321, 5528, 6825, 11, 3561, 4207, 11873, 5034, 1694, 89, 19601, 3265, 1427, 302, 11, 50932], "temperature": 0.0, "avg_logprob": -0.09819069407344644, "compression_ratio": 1.4599303135888502, "no_speech_prob": 0.009743870235979557}, {"id": 47, "seek": 14036, "start": 151.72000000000003, "end": 156.60000000000002, "text": " to rozmiar modelu powinien rosn\u0105\u0107 znacznie szybciej ni\u017c ilo\u015b\u0107 danych.", "tokens": [50932, 281, 9544, 3057, 289, 2316, 84, 27310, 1053, 18953, 13113, 2162, 15397, 14875, 2766, 36456, 4260, 73, 28502, 1930, 78, 7753, 274, 34644, 13, 51176], "temperature": 0.0, "avg_logprob": -0.09819069407344644, "compression_ratio": 1.4599303135888502, "no_speech_prob": 0.009743870235979557}, {"id": 48, "seek": 14036, "start": 156.60000000000002, "end": 157.8, "text": " O ile szybciej.", "tokens": [51176, 422, 15465, 36456, 4260, 73, 13, 51236], "temperature": 0.0, "avg_logprob": -0.09819069407344644, "compression_ratio": 1.4599303135888502, "no_speech_prob": 0.009743870235979557}, {"id": 49, "seek": 14036, "start": 157.8, "end": 162.0, "text": " Wed\u0142ug ich szacunk\u00f3w na ka\u017cde 10 okrotne zwi\u0119kszenie bud\u017cetu,", "tokens": [51236, 9589, 34077, 1893, 7870, 326, 3197, 3901, 1667, 21912, 1479, 1266, 3133, 10536, 716, 11873, 5034, 1694, 16778, 3265, 1427, 41236, 11, 51446], "temperature": 0.0, "avg_logprob": -0.09819069407344644, "compression_ratio": 1.4599303135888502, "no_speech_prob": 0.009743870235979557}, {"id": 50, "seek": 14036, "start": 162.0, "end": 168.44000000000003, "text": " model powinien by\u0107 wi\u0119kszy o 5,5 razy, a ilo\u015b\u0107 danych zaledwie o 1,8 razy.", "tokens": [51446, 2316, 27310, 1053, 15069, 29968, 1229, 277, 1025, 11, 20, 9639, 88, 11, 257, 1930, 78, 7753, 274, 34644, 710, 5573, 8699, 277, 502, 11, 23, 9639, 88, 13, 51768], "temperature": 0.0, "avg_logprob": -0.09819069407344644, "compression_ratio": 1.4599303135888502, "no_speech_prob": 0.009743870235979557}, {"id": 51, "seek": 16844, "start": 168.48, "end": 172.07999999999998, "text": " Czyli prawie ca\u0142y wysi\u0142ek szed\u0142 w architektur\u0119, w in\u017cenieri\u0119,", "tokens": [50366, 37099, 3206, 8699, 35226, 27062, 40622, 916, 7870, 292, 1221, 261, 3912, 642, 2320, 374, 1274, 11, 261, 294, 1427, 268, 811, 5034, 11, 50546], "temperature": 0.0, "avg_logprob": -0.11891389180378742, "compression_ratio": 1.4588607594936709, "no_speech_prob": 0.0050805010832846165}, {"id": 52, "seek": 16844, "start": 172.07999999999998, "end": 174.72, "text": " w budowanie coraz wi\u0119kszych cyfrowych katedr.", "tokens": [50546, 261, 3265, 22028, 25899, 29968, 28051, 3185, 69, 1892, 16384, 350, 770, 81, 13, 50678], "temperature": 0.0, "avg_logprob": -0.11891389180378742, "compression_ratio": 1.4588607594936709, "no_speech_prob": 0.0050805010832846165}, {"id": 53, "seek": 16844, "start": 174.72, "end": 175.88, "text": " Tak jest.", "tokens": [50678, 9118, 3492, 13, 50736], "temperature": 0.0, "avg_logprob": -0.11891389180378742, "compression_ratio": 1.4588607594936709, "no_speech_prob": 0.0050805010832846165}, {"id": 54, "seek": 16844, "start": 175.88, "end": 179.32, "text": " I w efekcie wi\u0119kszo\u015b\u0107 tych gigant\u00f3w, mimo r\u00f3\u017cnych rozmiar\u00f3w,", "tokens": [50736, 286, 261, 31482, 916, 4260, 29968, 4765, 7753, 15180, 8741, 394, 3901, 11, 275, 6934, 42602, 9544, 3057, 289, 3901, 11, 50908], "temperature": 0.0, "avg_logprob": -0.11891389180378742, "compression_ratio": 1.4588607594936709, "no_speech_prob": 0.0050805010832846165}, {"id": 55, "seek": 16844, "start": 179.32, "end": 182.32, "text": " by\u0142a trenowana na bardzo podobnej ilo\u015bci danych.", "tokens": [50908, 23936, 23136, 40458, 1667, 9034, 43024, 11794, 1930, 44468, 274, 34644, 13, 51058], "temperature": 0.0, "avg_logprob": -0.11891389180378742, "compression_ratio": 1.4588607594936709, "no_speech_prob": 0.0050805010832846165}, {"id": 56, "seek": 16844, "start": 182.32, "end": 183.07999999999998, "text": " Jakiej?", "tokens": [51058, 15029, 7764, 30, 51096], "temperature": 0.0, "avg_logprob": -0.11891389180378742, "compression_ratio": 1.4588607594936709, "no_speech_prob": 0.0050805010832846165}, {"id": 57, "seek": 16844, "start": 183.07999999999998, "end": 186.2, "text": " Oko\u0142o 300 miliardach token\u00f3w.", "tokens": [51096, 3477, 78, 5249, 6641, 1962, 72, 515, 608, 14862, 3901, 13, 51252], "temperature": 0.0, "avg_logprob": -0.11891389180378742, "compression_ratio": 1.4588607594936709, "no_speech_prob": 0.0050805010832846165}, {"id": 58, "seek": 16844, "start": 186.2, "end": 190.4, "text": " To sta\u0142o si\u0119 takim niepisanym standardem, ka\u017cdy chcia\u0142 mie\u0107 najwi\u0119kszy model,", "tokens": [51252, 1407, 11135, 5249, 3244, 31732, 2838, 40516, 1325, 76, 3832, 443, 11, 31615, 26497, 1221, 35612, 48636, 1694, 1229, 2316, 11, 51462], "temperature": 0.0, "avg_logprob": -0.11891389180378742, "compression_ratio": 1.4588607594936709, "no_speech_prob": 0.0050805010832846165}, {"id": 59, "seek": 16844, "start": 190.4, "end": 193.52, "text": " bo to rozmiar by\u0142 g\u0142\u00f3wnym wyznacznikiem post\u0119pu.", "tokens": [51462, 748, 281, 9544, 3057, 289, 16673, 18117, 812, 895, 4199, 4628, 22672, 14875, 13123, 4907, 2183, 18085, 84, 13, 51618], "temperature": 0.0, "avg_logprob": -0.11891389180378742, "compression_ratio": 1.4588607594936709, "no_speech_prob": 0.0050805010832846165}, {"id": 60, "seek": 16844, "start": 193.52, "end": 196.36, "text": " Czekaj, ale to brzmi tak, tak logicznie.", "tokens": [51618, 383, 19878, 1805, 11, 6775, 281, 738, 89, 3057, 991, 11, 991, 9952, 89, 2766, 13, 51760], "temperature": 0.0, "avg_logprob": -0.11891389180378742, "compression_ratio": 1.4588607594936709, "no_speech_prob": 0.0050805010832846165}, {"id": 61, "seek": 19636, "start": 196.36, "end": 198.92000000000002, "text": " Je\u015bli wszyscy opierali si\u0119 na tych samych badaniach,", "tokens": [50364, 37086, 44232, 999, 811, 5103, 3244, 1667, 15180, 3247, 16384, 1578, 3782, 608, 11, 50492], "temperature": 0.0, "avg_logprob": -0.1340219666357754, "compression_ratio": 1.4366197183098592, "no_speech_prob": 0.01157187856733799}, {"id": 62, "seek": 19636, "start": 198.92000000000002, "end": 203.16000000000003, "text": " jak to mo\u017cliwe, \u017ce ca\u0142a bran\u017ca, no przecie\u017c najt\u0119\u017csze umys\u0142y VI,", "tokens": [50492, 4207, 281, 30854, 826, 11, 3561, 1335, 5024, 12029, 35075, 11, 572, 8325, 40082, 11212, 83, 1274, 1427, 82, 1381, 1105, 749, 6825, 27619, 11, 50704], "temperature": 0.0, "avg_logprob": -0.1340219666357754, "compression_ratio": 1.4366197183098592, "no_speech_prob": 0.01157187856733799}, {"id": 63, "seek": 19636, "start": 203.16000000000003, "end": 207.0, "text": " przez lata sz\u0142a w jednym kierunku, kt\u00f3ry okaza\u0142 si\u0119 nieoptymalny?", "tokens": [50704, 14064, 46722, 7870, 5024, 261, 5232, 12996, 38767, 49910, 11, 9913, 3133, 12257, 1221, 3244, 2838, 404, 874, 5579, 1634, 30, 50896], "temperature": 0.0, "avg_logprob": -0.1340219666357754, "compression_ratio": 1.4366197183098592, "no_speech_prob": 0.01157187856733799}, {"id": 64, "seek": 19636, "start": 207.0, "end": 209.4, "text": " My\u015bl\u0119, \u017ce to mieszanka kilku rzeczy.", "tokens": [50896, 1222, 28749, 11, 3561, 281, 33039, 21729, 5128, 5279, 26297, 13, 51016], "temperature": 0.0, "avg_logprob": -0.1340219666357754, "compression_ratio": 1.4366197183098592, "no_speech_prob": 0.01157187856733799}, {"id": 65, "seek": 19636, "start": 209.4, "end": 211.04000000000002, "text": " Pycha, inercja.", "tokens": [51016, 9953, 4413, 11, 294, 260, 34056, 13, 51098], "temperature": 0.0, "avg_logprob": -0.1340219666357754, "compression_ratio": 1.4366197183098592, "no_speech_prob": 0.01157187856733799}, {"id": 66, "seek": 19636, "start": 211.04000000000002, "end": 216.96, "text": " Po pierwsze, koszt, trenowania tych modeli, poch\u0142ania niewyobra\u017calne zasoby.", "tokens": [51098, 6165, 45994, 11, 19532, 2682, 11, 23136, 21308, 15180, 2316, 72, 11, 714, 339, 1221, 5609, 43622, 88, 24393, 1427, 304, 716, 26530, 13944, 13, 51394], "temperature": 0.0, "avg_logprob": -0.1340219666357754, "compression_ratio": 1.4366197183098592, "no_speech_prob": 0.01157187856733799}, {"id": 67, "seek": 19636, "start": 216.96, "end": 220.84, "text": " M\u00f3wimy o dziesi\u0105tkach milion\u00f3w dolar\u00f3w za jeden trening.", "tokens": [51394, 376, 3901, 13189, 277, 9758, 530, 11404, 83, 41326, 1962, 313, 3901, 360, 2200, 3901, 7949, 12906, 2192, 773, 13, 51588], "temperature": 0.0, "avg_logprob": -0.1340219666357754, "compression_ratio": 1.4366197183098592, "no_speech_prob": 0.01157187856733799}, {"id": 68, "seek": 19636, "start": 220.84, "end": 222.48000000000002, "text": " Jeden strza\u0142.", "tokens": [51588, 508, 6876, 1056, 2394, 1221, 13, 51670], "temperature": 0.0, "avg_logprob": -0.1340219666357754, "compression_ratio": 1.4366197183098592, "no_speech_prob": 0.01157187856733799}, {"id": 69, "seek": 22248, "start": 222.48, "end": 226.84, "text": " W\u0142a\u015bnie. To jest jednorazowa, niezwykle droga operacja.", "tokens": [50364, 343, 5024, 12221, 13, 1407, 3492, 5232, 19048, 921, 5528, 11, 33511, 9726, 14677, 3789, 3680, 2208, 23395, 13, 50582], "temperature": 0.0, "avg_logprob": -0.1032635588083208, "compression_ratio": 1.482866043613707, "no_speech_prob": 0.011899201199412346}, {"id": 70, "seek": 22248, "start": 226.84, "end": 231.16, "text": " Wi\u0119c jak masz jeden strza\u0142, to trzymasz si\u0119 przepisu, kt\u00f3ry wydaje si\u0119 dzia\u0142a\u0107.", "tokens": [50582, 32508, 4207, 2300, 89, 12906, 1056, 2394, 1221, 11, 281, 34573, 3799, 89, 3244, 30829, 25871, 11, 9913, 49165, 3244, 37903, 2162, 13, 50798], "temperature": 0.0, "avg_logprob": -0.1032635588083208, "compression_ratio": 1.482866043613707, "no_speech_prob": 0.011899201199412346}, {"id": 71, "seek": 22248, "start": 231.16, "end": 233.0, "text": " Nikt nie chcia\u0142 ryzykowa\u0107 fortuny.", "tokens": [50798, 426, 9874, 2838, 26497, 1221, 20791, 1229, 74, 11445, 10506, 88, 13, 50890], "temperature": 0.0, "avg_logprob": -0.1032635588083208, "compression_ratio": 1.482866043613707, "no_speech_prob": 0.011899201199412346}, {"id": 72, "seek": 22248, "start": 233.0, "end": 233.72, "text": " A po drugie?", "tokens": [50890, 316, 714, 4110, 414, 30, 50926], "temperature": 0.0, "avg_logprob": -0.1032635588083208, "compression_ratio": 1.482866043613707, "no_speech_prob": 0.011899201199412346}, {"id": 73, "seek": 22248, "start": 233.72, "end": 236.79999999999998, "text": " A po drugie by\u0142a pewna inercja my\u015blowa.", "tokens": [50926, 316, 714, 4110, 414, 23936, 25889, 629, 294, 260, 34056, 452, 19212, 5528, 13, 51080], "temperature": 0.0, "avg_logprob": -0.1032635588083208, "compression_ratio": 1.482866043613707, "no_speech_prob": 0.011899201199412346}, {"id": 74, "seek": 22248, "start": 236.79999999999998, "end": 239.64, "text": " Skoro wi\u0119ksze modele dawa\u0142y lepsze wyniki,", "tokens": [51080, 7324, 10780, 29968, 1381, 4391, 306, 1120, 4151, 6825, 476, 1878, 1381, 31936, 9850, 11, 51222], "temperature": 0.0, "avg_logprob": -0.1032635588083208, "compression_ratio": 1.482866043613707, "no_speech_prob": 0.011899201199412346}, {"id": 75, "seek": 22248, "start": 239.64, "end": 243.28, "text": " naturalnym odruchem by\u0142o budowanie jeszcze wi\u0119kszych.", "tokens": [51222, 3303, 12996, 3611, 81, 625, 443, 14811, 3265, 22028, 14168, 29968, 28051, 13, 51404], "temperature": 0.0, "avg_logprob": -0.1032635588083208, "compression_ratio": 1.482866043613707, "no_speech_prob": 0.011899201199412346}, {"id": 76, "seek": 22248, "start": 243.28, "end": 246.76, "text": " To by\u0142o widowiskowe, wiesz, \u0142atwe do zakomunikowania.", "tokens": [51404, 1407, 14811, 37207, 7797, 6880, 11, 261, 15347, 11, 47759, 826, 360, 23810, 298, 409, 1035, 21308, 13, 51578], "temperature": 0.0, "avg_logprob": -0.1032635588083208, "compression_ratio": 1.482866043613707, "no_speech_prob": 0.011899201199412346}, {"id": 77, "seek": 22248, "start": 246.76, "end": 250.0, "text": " A\u017c w ko\u0144cu kto\u015b w Deep Mind powiedzia\u0142 stop", "tokens": [51578, 316, 1427, 261, 26470, 12032, 32982, 261, 14895, 13719, 48539, 1590, 51740], "temperature": 0.0, "avg_logprob": -0.1032635588083208, "compression_ratio": 1.482866043613707, "no_speech_prob": 0.011899201199412346}, {"id": 78, "seek": 22248, "start": 250.0, "end": 251.64, "text": " i zada\u0142 to fundamentalne pytanie.", "tokens": [51740, 741, 710, 1538, 1221, 281, 8088, 716, 36610, 13, 51822], "temperature": 0.0, "avg_logprob": -0.1032635588083208, "compression_ratio": 1.482866043613707, "no_speech_prob": 0.011899201199412346}, {"id": 79, "seek": 25164, "start": 251.67999999999998, "end": 254.51999999999998, "text": " Dok\u0142adnie. I to jest sedno tej pracy.", "tokens": [50366, 29768, 10358, 2766, 13, 286, 281, 3492, 9643, 1771, 12573, 35591, 13, 50508], "temperature": 0.0, "avg_logprob": -0.10197865363605861, "compression_ratio": 1.514018691588785, "no_speech_prob": 0.045649174600839615}, {"id": 80, "seek": 25164, "start": 254.51999999999998, "end": 258.52, "text": " Pytanie by\u0142o proste, ale jego konsekwencje rewolucyjne.", "tokens": [50508, 430, 4328, 7155, 14811, 10293, 68, 11, 6775, 26542, 47020, 74, 15615, 44261, 319, 48481, 1311, 88, 73, 716, 13, 50708], "temperature": 0.0, "avg_logprob": -0.10197865363605861, "compression_ratio": 1.514018691588785, "no_speech_prob": 0.045649174600839615}, {"id": 81, "seek": 25164, "start": 258.52, "end": 259.4, "text": " Jak brzmia\u0142o?", "tokens": [50708, 15029, 738, 89, 29958, 5249, 30, 50752], "temperature": 0.0, "avg_logprob": -0.10197865363605861, "compression_ratio": 1.514018691588785, "no_speech_prob": 0.045649174600839615}, {"id": 82, "seek": 25164, "start": 259.4, "end": 265.47999999999996, "text": " Maj\u0105c z g\u00f3r\u0119 okre\u015blony bud\u017cet, czyli sta\u0142\u0105 ilo\u015b\u0107 mocy obliczeniowej, tych flops,", "tokens": [50752, 7048, 1611, 66, 710, 290, 15614, 1274, 3133, 265, 19212, 2526, 3265, 1427, 302, 11, 16591, 11135, 15926, 1930, 78, 7753, 705, 1344, 1111, 1050, 42124, 21091, 11, 15180, 932, 3370, 11, 51056], "temperature": 0.0, "avg_logprob": -0.10197865363605861, "compression_ratio": 1.514018691588785, "no_speech_prob": 0.045649174600839615}, {"id": 83, "seek": 25164, "start": 265.47999999999996, "end": 270.08, "text": " jak najlepiej zr\u00f3wnowa\u017cy\u0107 rozmiar modelu i ilo\u015b\u0107 danych,", "tokens": [51056, 4207, 41903, 39699, 710, 11721, 895, 5528, 39687, 9544, 3057, 289, 2316, 84, 741, 1930, 78, 7753, 274, 34644, 11, 51286], "temperature": 0.0, "avg_logprob": -0.10197865363605861, "compression_ratio": 1.514018691588785, "no_speech_prob": 0.045649174600839615}, {"id": 84, "seek": 25164, "start": 270.08, "end": 272.4, "text": " \u017ceby dosta\u0107 najlepszy mo\u017cliwy wynik.", "tokens": [51286, 11316, 274, 8638, 2162, 41903, 1878, 1229, 30854, 9726, 31936, 1035, 13, 51402], "temperature": 0.0, "avg_logprob": -0.10197865363605861, "compression_ratio": 1.514018691588785, "no_speech_prob": 0.045649174600839615}, {"id": 85, "seek": 25164, "start": 272.4, "end": 275.2, "text": " Czyli zamiast pyta\u0107, jak du\u017cy model mo\u017cemy zbudowa\u0107,", "tokens": [51402, 37099, 710, 4526, 525, 10664, 42931, 11, 4207, 1581, 7735, 2316, 26500, 710, 18281, 11445, 11, 51542], "temperature": 0.0, "avg_logprob": -0.10197865363605861, "compression_ratio": 1.514018691588785, "no_speech_prob": 0.045649174600839615}, {"id": 86, "seek": 25164, "start": 275.2, "end": 278.0, "text": " zapytali jaki jest optymalny model, kt\u00f3ry mo\u017cemy zbudowa\u0107.", "tokens": [51542, 14223, 4328, 5103, 24492, 3492, 2427, 4199, 304, 1634, 2316, 11, 9913, 26500, 710, 18281, 11445, 13, 51682], "temperature": 0.0, "avg_logprob": -0.10197865363605861, "compression_ratio": 1.514018691588785, "no_speech_prob": 0.045649174600839615}, {"id": 87, "seek": 25164, "start": 278.0, "end": 278.8, "text": " W\u0142a\u015bnie.", "tokens": [51682, 343, 5024, 12221, 13, 51722], "temperature": 0.0, "avg_logprob": -0.10197865363605861, "compression_ratio": 1.514018691588785, "no_speech_prob": 0.045649174600839615}, {"id": 88, "seek": 25164, "start": 278.8, "end": 281.59999999999997, "text": " I jaka by\u0142a ich hipoteza, kt\u00f3ra wywr\u00f3ci\u0142a stolik?", "tokens": [51722, 286, 4207, 64, 23936, 1893, 8103, 1370, 2394, 11, 19456, 4628, 7449, 812, 537, 5024, 43553, 1035, 30, 51862], "temperature": 0.0, "avg_logprob": -0.10197865363605861, "compression_ratio": 1.514018691588785, "no_speech_prob": 0.045649174600839615}, {"id": 89, "seek": 28160, "start": 281.6, "end": 287.20000000000005, "text": " Hipoteza by\u0142a taka, \u017ce do tych czasowe modele by\u0142y ra\u017c\u0105co undertrained.", "tokens": [50364, 29596, 1370, 2394, 23936, 28017, 11, 3561, 360, 15180, 13190, 6880, 4391, 306, 26366, 3342, 1427, 1611, 1291, 833, 17227, 2001, 13, 50644], "temperature": 0.0, "avg_logprob": -0.18247285529748716, "compression_ratio": 1.475609756097561, "no_speech_prob": 0.00029826234094798565}, {"id": 90, "seek": 28160, "start": 287.20000000000005, "end": 289.16, "text": " Po prostu niedotrenowane.", "tokens": [50644, 6165, 19518, 32488, 310, 1095, 23066, 13, 50742], "temperature": 0.0, "avg_logprob": -0.18247285529748716, "compression_ratio": 1.475609756097561, "no_speech_prob": 0.00029826234094798565}, {"id": 91, "seek": 28160, "start": 289.16, "end": 292.40000000000003, "text": " Niedotrenowane. Mimo tych setek miliard\u00f3w parametr\u00f3w.", "tokens": [50742, 426, 1091, 310, 1095, 23066, 13, 376, 6934, 15180, 992, 916, 1962, 72, 515, 3901, 6220, 27965, 3901, 13, 50904], "temperature": 0.0, "avg_logprob": -0.18247285529748716, "compression_ratio": 1.475609756097561, "no_speech_prob": 0.00029826234094798565}, {"id": 92, "seek": 28160, "start": 292.40000000000003, "end": 293.36, "text": " Tak.", "tokens": [50904, 9118, 13, 50952], "temperature": 0.0, "avg_logprob": -0.18247285529748716, "compression_ratio": 1.475609756097561, "no_speech_prob": 0.00029826234094798565}, {"id": 93, "seek": 28160, "start": 293.36, "end": 298.24, "text": " Twierdzili, \u017ce optymaln\u0105 strategi\u0105 nie jest maksymalizowanie rozmiaru,", "tokens": [50952, 2574, 811, 28168, 2312, 11, 3561, 2427, 4199, 304, 13113, 5464, 11404, 2838, 3492, 963, 3187, 5579, 590, 22028, 9544, 3057, 16870, 11, 51196], "temperature": 0.0, "avg_logprob": -0.18247285529748716, "compression_ratio": 1.475609756097561, "no_speech_prob": 0.00029826234094798565}, {"id": 94, "seek": 28160, "start": 298.24, "end": 302.20000000000005, "text": " ale r\u00f3wne proporcjonalne skalowanie obu tych czynnik\u00f3w.", "tokens": [51196, 6775, 367, 3901, 716, 2365, 36003, 15735, 304, 716, 16890, 22028, 1111, 84, 15180, 6430, 77, 47447, 13, 51394], "temperature": 0.0, "avg_logprob": -0.18247285529748716, "compression_ratio": 1.475609756097561, "no_speech_prob": 0.00029826234094798565}, {"id": 95, "seek": 28160, "start": 302.20000000000005, "end": 303.28000000000003, "text": " Czyli...", "tokens": [51394, 37099, 485, 51448], "temperature": 0.0, "avg_logprob": -0.18247285529748716, "compression_ratio": 1.475609756097561, "no_speech_prob": 0.00029826234094798565}, {"id": 96, "seek": 28160, "start": 303.28000000000003, "end": 304.96000000000004, "text": " Hmm...", "tokens": [51448, 8239, 485, 51532], "temperature": 0.0, "avg_logprob": -0.18247285529748716, "compression_ratio": 1.475609756097561, "no_speech_prob": 0.00029826234094798565}, {"id": 97, "seek": 28160, "start": 304.96000000000004, "end": 305.92, "text": " Jak?", "tokens": [51532, 15029, 30, 51580], "temperature": 0.0, "avg_logprob": -0.18247285529748716, "compression_ratio": 1.475609756097561, "no_speech_prob": 0.00029826234094798565}, {"id": 98, "seek": 28160, "start": 305.92, "end": 308.96000000000004, "text": " Na ka\u017cde podwojenie liczby parametr\u00f3w modelu", "tokens": [51580, 6056, 21912, 1479, 2497, 6120, 15378, 414, 6169, 89, 2322, 6220, 27965, 3901, 2316, 84, 51732], "temperature": 0.0, "avg_logprob": -0.18247285529748716, "compression_ratio": 1.475609756097561, "no_speech_prob": 0.00029826234094798565}, {"id": 99, "seek": 30896, "start": 309.0, "end": 312.88, "text": " powinno przypada\u0107 podwojenie liczby token\u00f3w treningowych.", "tokens": [50366, 27310, 1771, 41780, 1538, 2162, 2497, 6120, 15378, 414, 6169, 89, 2322, 14862, 3901, 2192, 773, 19605, 13, 50560], "temperature": 0.0, "avg_logprob": -0.11337939603829089, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.04876864701509476}, {"id": 100, "seek": 30896, "start": 312.88, "end": 314.96, "text": " Proporcja jeden do jednego.", "tokens": [50560, 21944, 36003, 2938, 12906, 360, 5232, 11858, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11337939603829089, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.04876864701509476}, {"id": 101, "seek": 30896, "start": 314.96, "end": 317.47999999999996, "text": " Ale czy to nie jest zbyt nieuproszczenie?", "tokens": [50664, 9366, 6430, 281, 2838, 3492, 710, 2322, 83, 2838, 1010, 2635, 89, 39043, 30, 50790], "temperature": 0.0, "avg_logprob": -0.11337939603829089, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.04876864701509476}, {"id": 102, "seek": 30896, "start": 317.47999999999996, "end": 319.96, "text": " \u015awiat rzadko jest taki idealny.", "tokens": [50790, 27933, 6253, 267, 367, 89, 345, 4093, 3492, 20065, 7157, 1634, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11337939603829089, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.04876864701509476}, {"id": 103, "seek": 30896, "start": 319.96, "end": 324.12, "text": " Czy naprawd\u0119 prosta zasada podw\u00f3j parametry, podw\u00f3j dane?", "tokens": [50914, 19832, 20970, 582, 8638, 26530, 1538, 2497, 86, 18999, 6220, 9889, 11, 2497, 86, 18999, 49206, 30, 51122], "temperature": 0.0, "avg_logprob": -0.11337939603829089, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.04876864701509476}, {"id": 104, "seek": 30896, "start": 324.12, "end": 325.71999999999997, "text": " Dzia\u0142a w ka\u017cdej skali?", "tokens": [51122, 39448, 25605, 261, 21912, 1479, 73, 1110, 5103, 30, 51202], "temperature": 0.0, "avg_logprob": -0.11337939603829089, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.04876864701509476}, {"id": 105, "seek": 30896, "start": 325.71999999999997, "end": 330.79999999999995, "text": " To jest \u015bwietne pytanie i w\u0142a\u015bnie dlatego ich metodologia jest tak imponuj\u0105ca.", "tokens": [51202, 1407, 3492, 8299, 39083, 716, 36610, 741, 14234, 32205, 1893, 1131, 378, 24103, 3492, 991, 704, 266, 13263, 496, 13, 51456], "temperature": 0.0, "avg_logprob": -0.11337939603829089, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.04876864701509476}, {"id": 106, "seek": 30896, "start": 330.79999999999995, "end": 333.24, "text": " Oni nie wyszli z t\u0105 tez\u0105 znik\u0105d.", "tokens": [51456, 1282, 72, 2838, 261, 20589, 2081, 710, 32294, 535, 8925, 710, 13123, 18962, 13, 51578], "temperature": 0.0, "avg_logprob": -0.11337939603829089, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.04876864701509476}, {"id": 107, "seek": 30896, "start": 333.24, "end": 335.28, "text": " To wniosek z...", "tokens": [51578, 1407, 261, 3722, 541, 74, 710, 485, 51680], "temperature": 0.0, "avg_logprob": -0.11337939603829089, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.04876864701509476}, {"id": 108, "seek": 30896, "start": 335.28, "end": 337.88, "text": " Naprawd\u0119 szeroko zakrojonych eksperyment\u00f3w.", "tokens": [51680, 18287, 20098, 36160, 13704, 23810, 340, 73, 2526, 339, 30724, 610, 88, 518, 3901, 13, 51810], "temperature": 0.0, "avg_logprob": -0.11337939603829089, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.04876864701509476}, {"id": 109, "seek": 30896, "start": 337.88, "end": 338.91999999999996, "text": " Jak szeroko?", "tokens": [51810, 15029, 36160, 13704, 30, 51862], "temperature": 0.0, "avg_logprob": -0.11337939603829089, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.04876864701509476}, {"id": 110, "seek": 33892, "start": 338.96000000000004, "end": 343.88, "text": " Wytrenowali ponad 400 modeli o bardzo r\u00f3\u017cnych rozmiarach.", "tokens": [50366, 343, 4328, 1095, 305, 5103, 9224, 345, 8423, 2316, 72, 277, 9034, 42602, 9544, 3057, 289, 608, 13, 50612], "temperature": 0.0, "avg_logprob": -0.15485536235652558, "compression_ratio": 1.3586206896551725, "no_speech_prob": 0.003119200700893998}, {"id": 111, "seek": 33892, "start": 343.88, "end": 347.04, "text": " Od malutkich, 70 milion\u00f3w parametr\u00f3w,", "tokens": [50612, 12210, 2806, 325, 48349, 11, 5285, 1962, 313, 3901, 6220, 27965, 3901, 11, 50770], "temperature": 0.0, "avg_logprob": -0.15485536235652558, "compression_ratio": 1.3586206896551725, "no_speech_prob": 0.003119200700893998}, {"id": 112, "seek": 33892, "start": 347.04, "end": 351.52000000000004, "text": " do ponad 16 miliard\u00f3w i na r\u00f3\u017cnej ilo\u015bci danych.", "tokens": [50770, 360, 9224, 345, 3165, 1962, 72, 515, 3901, 741, 1667, 19637, 11794, 1930, 44468, 274, 34644, 13, 50994], "temperature": 0.0, "avg_logprob": -0.15485536235652558, "compression_ratio": 1.3586206896551725, "no_speech_prob": 0.003119200700893998}, {"id": 113, "seek": 33892, "start": 351.52000000000004, "end": 352.52000000000004, "text": " Aha.", "tokens": [50994, 27448, 13, 51044], "temperature": 0.0, "avg_logprob": -0.15485536235652558, "compression_ratio": 1.3586206896551725, "no_speech_prob": 0.003119200700893998}, {"id": 114, "seek": 33892, "start": 352.52000000000004, "end": 355.56, "text": " To im pozwoli\u0142o stworzy\u0107 tak\u0105 swoist\u0105 map\u0119,", "tokens": [51044, 1407, 566, 40557, 9384, 5249, 342, 28321, 27150, 31069, 13291, 468, 1611, 4471, 1274, 11, 51196], "temperature": 0.0, "avg_logprob": -0.15485536235652558, "compression_ratio": 1.3586206896551725, "no_speech_prob": 0.003119200700893998}, {"id": 115, "seek": 33892, "start": 355.56, "end": 359.88, "text": " kt\u00f3ra pokazywa\u0142a, jak osi\u0105gn\u0105\u0107 najni\u017cszy los przy danym koszcie.", "tokens": [51196, 19456, 13010, 33235, 4151, 5024, 11, 4207, 3003, 11404, 4568, 36374, 11212, 3722, 1427, 7706, 1750, 6501, 274, 1325, 76, 19532, 89, 4260, 13, 51412], "temperature": 0.0, "avg_logprob": -0.15485536235652558, "compression_ratio": 1.3586206896551725, "no_speech_prob": 0.003119200700893998}, {"id": 116, "seek": 33892, "start": 359.88, "end": 363.0, "text": " Czyli zamiast jednego wielkiego eksperymentu,", "tokens": [51412, 37099, 710, 4526, 525, 5232, 11858, 20570, 42349, 30724, 610, 88, 518, 84, 11, 51568], "temperature": 0.0, "avg_logprob": -0.15485536235652558, "compression_ratio": 1.3586206896551725, "no_speech_prob": 0.003119200700893998}, {"id": 117, "seek": 33892, "start": 363.0, "end": 365.12, "text": " zrobili setki mniejszych.", "tokens": [51568, 44399, 2312, 992, 2984, 39513, 45021, 13, 51674], "temperature": 0.0, "avg_logprob": -0.15485536235652558, "compression_ratio": 1.3586206896551725, "no_speech_prob": 0.003119200700893998}, {"id": 118, "seek": 33892, "start": 365.12, "end": 368.44, "text": " \u017beby zrozumie\u0107 sam\u0105 fizyk\u0119 tego procesu.", "tokens": [51674, 46864, 2322, 710, 27857, 449, 414, 2162, 3247, 1611, 21000, 88, 15724, 8627, 17565, 84, 13, 51840], "temperature": 0.0, "avg_logprob": -0.15485536235652558, "compression_ratio": 1.3586206896551725, "no_speech_prob": 0.003119200700893998}, {"id": 119, "seek": 36844, "start": 368.52, "end": 372.6, "text": " I co wi\u0119cej, zastosowa\u0142y trzy r\u00f3\u017cne metody analityczne,", "tokens": [50368, 286, 598, 26004, 11, 36746, 329, 5528, 6825, 34573, 47760, 1131, 843, 364, 1860, 38491, 11, 50572], "temperature": 0.0, "avg_logprob": -0.1508204277525557, "compression_ratio": 1.3924528301886792, "no_speech_prob": 0.0019442768534645438}, {"id": 120, "seek": 36844, "start": 372.6, "end": 374.68, "text": " \u017ceby potwierdzi\u0107 swoje ustalenia.", "tokens": [50572, 11316, 1847, 40717, 67, 28496, 29489, 26189, 21745, 654, 13, 50676], "temperature": 0.0, "avg_logprob": -0.1508204277525557, "compression_ratio": 1.3924528301886792, "no_speech_prob": 0.0019442768534645438}, {"id": 121, "seek": 36844, "start": 374.68, "end": 375.52, "text": " Trzy?", "tokens": [50676, 1765, 1229, 30, 50718], "temperature": 0.0, "avg_logprob": -0.1508204277525557, "compression_ratio": 1.3924528301886792, "no_speech_prob": 0.0019442768534645438}, {"id": 122, "seek": 36844, "start": 375.52, "end": 376.16, "text": " Tak.", "tokens": [50718, 9118, 13, 50750], "temperature": 0.0, "avg_logprob": -0.1508204277525557, "compression_ratio": 1.3924528301886792, "no_speech_prob": 0.0019442768534645438}, {"id": 123, "seek": 36844, "start": 376.16, "end": 379.84, "text": " Pierwsza to by\u0142o klasyczne badanie krzywych treningowych.", "tokens": [50750, 16676, 14358, 2394, 281, 14811, 9671, 5871, 38491, 1578, 7155, 350, 13047, 9726, 339, 2192, 773, 19605, 13, 50934], "temperature": 0.0, "avg_logprob": -0.1508204277525557, "compression_ratio": 1.3924528301886792, "no_speech_prob": 0.0019442768534645438}, {"id": 124, "seek": 36844, "start": 379.84, "end": 384.24, "text": " Druga to analiza profili Isoflop i to jest bardzo ciekawe.", "tokens": [50934, 2491, 19364, 281, 2624, 13427, 1740, 2312, 286, 539, 3423, 404, 741, 281, 3492, 9034, 30596, 2330, 826, 13, 51154], "temperature": 0.0, "avg_logprob": -0.1508204277525557, "compression_ratio": 1.3924528301886792, "no_speech_prob": 0.0019442768534645438}, {"id": 125, "seek": 36844, "start": 384.24, "end": 386.44, "text": " Co to jest ten profil Isoflop?", "tokens": [51154, 3066, 281, 3492, 2064, 1740, 388, 286, 539, 3423, 404, 30, 51264], "temperature": 0.0, "avg_logprob": -0.1508204277525557, "compression_ratio": 1.3924528301886792, "no_speech_prob": 0.0019442768534645438}, {"id": 126, "seek": 36844, "start": 386.44, "end": 388.12, "text": " To brzmi do\u015b\u0107 technicznie.", "tokens": [51264, 1407, 738, 89, 3057, 49333, 1537, 17946, 2766, 13, 51348], "temperature": 0.0, "avg_logprob": -0.1508204277525557, "compression_ratio": 1.3924528301886792, "no_speech_prob": 0.0019442768534645438}, {"id": 127, "seek": 36844, "start": 388.12, "end": 393.2, "text": " Wyobra\u017a sobie, \u017ce masz okre\u015blony bud\u017cet na budow\u0119 samochodu wy\u015bcigowego.", "tokens": [51348, 14458, 24393, 10659, 13652, 11, 3561, 2300, 89, 3133, 265, 19212, 2526, 3265, 1427, 302, 1667, 3265, 305, 1274, 3247, 8997, 34873, 4628, 1788, 66, 328, 26576, 13, 51602], "temperature": 0.0, "avg_logprob": -0.1508204277525557, "compression_ratio": 1.3924528301886792, "no_speech_prob": 0.0019442768534645438}, {"id": 128, "seek": 36844, "start": 393.2, "end": 394.2, "text": " Okej.", "tokens": [51602, 29094, 73, 13, 51652], "temperature": 0.0, "avg_logprob": -0.1508204277525557, "compression_ratio": 1.3924528301886792, "no_speech_prob": 0.0019442768534645438}, {"id": 129, "seek": 39420, "start": 394.24, "end": 398.56, "text": " Profil Isoflop to jakby\u015b testowa\u0142a setki kombinacji.", "tokens": [50366, 6039, 388, 286, 539, 3423, 404, 281, 28976, 1788, 1500, 5528, 5024, 992, 2984, 42925, 259, 13152, 13, 50582], "temperature": 0.0, "avg_logprob": -0.13550639152526855, "compression_ratio": 1.5627376425855513, "no_speech_prob": 0.028041081503033638}, {"id": 130, "seek": 39420, "start": 398.56, "end": 400.68, "text": " Wielki silnik z ma\u0142ymi ko\u0142ami,", "tokens": [50582, 343, 1187, 2984, 3425, 13123, 710, 463, 6825, 3057, 8384, 1221, 4526, 11, 50688], "temperature": 0.0, "avg_logprob": -0.13550639152526855, "compression_ratio": 1.5627376425855513, "no_speech_prob": 0.028041081503033638}, {"id": 131, "seek": 39420, "start": 400.68, "end": 404.59999999999997, "text": " ma\u0142y silnik z super lekk\u0105 karoseri\u0105 i tak dalej.", "tokens": [50688, 463, 6825, 3425, 13123, 710, 1687, 30863, 26304, 7917, 22150, 11404, 741, 991, 34257, 13, 50884], "temperature": 0.0, "avg_logprob": -0.13550639152526855, "compression_ratio": 1.5627376425855513, "no_speech_prob": 0.028041081503033638}, {"id": 132, "seek": 39420, "start": 404.59999999999997, "end": 408.56, "text": " Wszystkie te kombinacje kosztuj\u0105 dok\u0142adnie tyle samo,", "tokens": [50884, 343, 10424, 22872, 535, 42925, 259, 29293, 19532, 2682, 13263, 45864, 2766, 39293, 36422, 11, 51082], "temperature": 0.0, "avg_logprob": -0.13550639152526855, "compression_ratio": 1.5627376425855513, "no_speech_prob": 0.028041081503033638}, {"id": 133, "seek": 39420, "start": 408.56, "end": 410.88, "text": " a ja szukam tej jednej, idealnej,", "tokens": [51082, 257, 2784, 7870, 2034, 335, 12573, 5232, 11794, 11, 7157, 11794, 11, 51198], "temperature": 0.0, "avg_logprob": -0.13550639152526855, "compression_ratio": 1.5627376425855513, "no_speech_prob": 0.028041081503033638}, {"id": 134, "seek": 39420, "start": 410.88, "end": 412.91999999999996, "text": " kt\u00f3ra da mi najlepszy czas na torze.", "tokens": [51198, 19456, 1120, 2752, 41903, 1878, 1229, 13190, 1667, 3930, 1381, 13, 51300], "temperature": 0.0, "avg_logprob": -0.13550639152526855, "compression_ratio": 1.5627376425855513, "no_speech_prob": 0.028041081503033638}, {"id": 135, "seek": 39420, "start": 412.91999999999996, "end": 413.91999999999996, "text": " Dok\u0142adnie.", "tokens": [51300, 29768, 10358, 2766, 13, 51350], "temperature": 0.0, "avg_logprob": -0.13550639152526855, "compression_ratio": 1.5627376425855513, "no_speech_prob": 0.028041081503033638}, {"id": 136, "seek": 39420, "start": 413.91999999999996, "end": 415.2, "text": " I tu by\u0142o tak samo.", "tokens": [51350, 286, 2604, 14811, 991, 36422, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13550639152526855, "compression_ratio": 1.5627376425855513, "no_speech_prob": 0.028041081503033638}, {"id": 137, "seek": 39420, "start": 415.2, "end": 420.56, "text": " Przy sta\u0142ym bud\u017cecie flops szukali optymalnej kombinacji parametr\u00f3w i tokan\u00f3w,", "tokens": [51414, 39590, 11135, 1221, 4199, 3265, 2875, 4260, 932, 3370, 7870, 2034, 5103, 2427, 4199, 304, 11794, 42925, 259, 13152, 6220, 27965, 3901, 741, 281, 5225, 3901, 11, 51682], "temperature": 0.0, "avg_logprob": -0.13550639152526855, "compression_ratio": 1.5627376425855513, "no_speech_prob": 0.028041081503033638}, {"id": 138, "seek": 39420, "start": 420.56, "end": 422.56, "text": " kt\u00f3ra daje najni\u017cszy los.", "tokens": [51682, 19456, 1120, 2884, 11212, 3722, 1427, 7706, 1750, 13, 51782], "temperature": 0.0, "avg_logprob": -0.13550639152526855, "compression_ratio": 1.5627376425855513, "no_speech_prob": 0.028041081503033638}, {"id": 139, "seek": 42256, "start": 422.56, "end": 425.44, "text": " Rozumiem, czyli to jest poszukiwanie z\u0142otego \u015brodka,", "tokens": [50364, 43313, 449, 4907, 11, 16591, 281, 3492, 1366, 89, 11788, 86, 7155, 31614, 310, 6308, 28580, 2330, 11, 50508], "temperature": 0.0, "avg_logprob": -0.1121843786800609, "compression_ratio": 1.5, "no_speech_prob": 0.0035933847539126873}, {"id": 140, "seek": 42256, "start": 425.44, "end": 427.84, "text": " a nie pchanie jednej zmiennej do ekstremum.", "tokens": [50508, 257, 2838, 280, 3484, 414, 5232, 11794, 17020, 1053, 11794, 360, 13359, 372, 2579, 449, 13, 50628], "temperature": 0.0, "avg_logprob": -0.1121843786800609, "compression_ratio": 1.5, "no_speech_prob": 0.0035933847539126873}, {"id": 141, "seek": 42256, "start": 427.84, "end": 431.36, "text": " Tak i wreszcie trzecia metoda, najbardziej matematyczna,", "tokens": [50628, 9118, 741, 261, 495, 89, 4260, 22266, 2755, 1131, 13449, 11, 41857, 3803, 8615, 17466, 629, 11, 50804], "temperature": 0.0, "avg_logprob": -0.1121843786800609, "compression_ratio": 1.5, "no_speech_prob": 0.0035933847539126873}, {"id": 142, "seek": 42256, "start": 431.36, "end": 435.44, "text": " to dopasowanie parametrycznej funkcji straty do wszystkich wynik\u00f3w.", "tokens": [50804, 281, 360, 20990, 22028, 6220, 9889, 3689, 11794, 26476, 19649, 1056, 21398, 360, 34234, 31936, 1035, 3901, 13, 51008], "temperature": 0.0, "avg_logprob": -0.1121843786800609, "compression_ratio": 1.5, "no_speech_prob": 0.0035933847539126873}, {"id": 143, "seek": 42256, "start": 435.44, "end": 440.16, "text": " Ale co najwa\u017cniejsze, wszystkie trzy podej\u015bcia doprowadzi\u0142y ich do tego samego wniosku.", "tokens": [51008, 9366, 598, 11212, 27111, 44258, 11, 31723, 34573, 7468, 73, 1788, 2755, 360, 35019, 3992, 6825, 1893, 360, 8627, 912, 1571, 45368, 2717, 5279, 13, 51244], "temperature": 0.0, "avg_logprob": -0.1121843786800609, "compression_ratio": 1.5, "no_speech_prob": 0.0035933847539126873}, {"id": 144, "seek": 42256, "start": 440.16, "end": 442.56, "text": " Czyli, \u017ce dla optymalnej wydajno\u015bci", "tokens": [51244, 37099, 11, 3561, 12285, 2427, 4199, 304, 11794, 25984, 1805, 16438, 51364], "temperature": 0.0, "avg_logprob": -0.1121843786800609, "compression_ratio": 1.5, "no_speech_prob": 0.0035933847539126873}, {"id": 145, "seek": 42256, "start": 442.56, "end": 445.32, "text": " rozmiar modelu i ilo\u015b\u0107 danych treningowych", "tokens": [51364, 9544, 3057, 289, 2316, 84, 741, 1930, 78, 7753, 274, 34644, 2192, 773, 19605, 51502], "temperature": 0.0, "avg_logprob": -0.1121843786800609, "compression_ratio": 1.5, "no_speech_prob": 0.0035933847539126873}, {"id": 146, "seek": 42256, "start": 445.32, "end": 447.68, "text": " powinny rosn\u0105\u0107 w r\u00f3wnych proporcjach.", "tokens": [51502, 27310, 1634, 18953, 13113, 2162, 261, 11416, 895, 16384, 2365, 36003, 45059, 13, 51620], "temperature": 0.0, "avg_logprob": -0.1121843786800609, "compression_ratio": 1.5, "no_speech_prob": 0.0035933847539126873}, {"id": 147, "seek": 42256, "start": 447.68, "end": 450.88, "text": " To by\u0142o solidne, potr\u00f3jnie zweryfikowane odkrycie.", "tokens": [51620, 1407, 14811, 5100, 716, 11, 1847, 11721, 73, 2766, 710, 1554, 88, 31230, 23066, 3611, 43298, 4260, 13, 51780], "temperature": 0.0, "avg_logprob": -0.1121843786800609, "compression_ratio": 1.5, "no_speech_prob": 0.0035933847539126873}, {"id": 148, "seek": 45088, "start": 450.88, "end": 453.12, "text": " W porz\u0105dku tu robi si\u0119 naprawd\u0119 ciekawie.", "tokens": [50364, 343, 1515, 23876, 5279, 2604, 47380, 3244, 20970, 46419, 1607, 414, 13, 50476], "temperature": 0.0, "avg_logprob": -0.13579315546701645, "compression_ratio": 1.421875, "no_speech_prob": 0.04625573009252548}, {"id": 149, "seek": 45088, "start": 453.12, "end": 456.6, "text": " Hipoteza jest mocna, poparta setkami eksperyment\u00f3w,", "tokens": [50476, 29596, 1370, 2394, 3492, 34962, 629, 11, 1665, 19061, 992, 48737, 30724, 610, 88, 518, 3901, 11, 50650], "temperature": 0.0, "avg_logprob": -0.13579315546701645, "compression_ratio": 1.421875, "no_speech_prob": 0.04625573009252548}, {"id": 150, "seek": 45088, "start": 456.6, "end": 458.08, "text": " ale teoria to jedno.", "tokens": [50650, 6775, 535, 8172, 281, 5232, 1771, 13, 50724], "temperature": 0.0, "avg_logprob": -0.13579315546701645, "compression_ratio": 1.421875, "no_speech_prob": 0.04625573009252548}, {"id": 151, "seek": 45088, "start": 458.08, "end": 461.71999999999997, "text": " Postanowili j\u0105 udowodni\u0107 w najbardziej dosadny spos\u00f3b.", "tokens": [50724, 10223, 282, 305, 2312, 35692, 11727, 305, 378, 3722, 2162, 261, 41857, 4491, 345, 1634, 22904, 13, 50906], "temperature": 0.0, "avg_logprob": -0.13579315546701645, "compression_ratio": 1.421875, "no_speech_prob": 0.04625573009252548}, {"id": 152, "seek": 45088, "start": 461.71999999999997, "end": 464.24, "text": " Stworzyli model, kt\u00f3ry nazwali Chinchilla.", "tokens": [50906, 745, 28321, 1229, 2081, 2316, 11, 9913, 20151, 40054, 4430, 339, 5291, 13, 51032], "temperature": 0.0, "avg_logprob": -0.13579315546701645, "compression_ratio": 1.421875, "no_speech_prob": 0.04625573009252548}, {"id": 153, "seek": 45088, "start": 464.24, "end": 466.48, "text": " To by\u0142 ten decyduj\u0105cy eksperyment, prawda?", "tokens": [51032, 1407, 16673, 2064, 979, 88, 769, 8555, 1344, 30724, 610, 88, 518, 11, 43607, 30, 51144], "temperature": 0.0, "avg_logprob": -0.13579315546701645, "compression_ratio": 1.421875, "no_speech_prob": 0.04625573009252548}, {"id": 154, "seek": 45088, "start": 466.48, "end": 467.48, "text": " Takie sprawdzam.", "tokens": [51144, 9118, 414, 46192, 28915, 13, 51194], "temperature": 0.0, "avg_logprob": -0.13579315546701645, "compression_ratio": 1.421875, "no_speech_prob": 0.04625573009252548}, {"id": 155, "seek": 45088, "start": 467.48, "end": 468.24, "text": " Dok\u0142adnie.", "tokens": [51194, 29768, 10358, 2766, 13, 51232], "temperature": 0.0, "avg_logprob": -0.13579315546701645, "compression_ratio": 1.421875, "no_speech_prob": 0.04625573009252548}, {"id": 156, "seek": 45088, "start": 468.24, "end": 470.88, "text": " Wzi\u0119li dok\u0142adnie ten sam bud\u017cet obliczeniowy,", "tokens": [51232, 343, 16706, 2081, 45864, 2766, 2064, 3247, 3265, 1427, 302, 1111, 1050, 42124, 10089, 11, 51364], "temperature": 0.0, "avg_logprob": -0.13579315546701645, "compression_ratio": 1.421875, "no_speech_prob": 0.04625573009252548}, {"id": 157, "seek": 45088, "start": 470.88, "end": 474.76, "text": " jakiego u\u017cyto do wytrenowania ich poprzedniego flagowca, Gofer'a.", "tokens": [51364, 4207, 12200, 34097, 1353, 360, 261, 4328, 1095, 21308, 1893, 1665, 81, 11312, 2766, 1571, 7166, 305, 496, 11, 1037, 612, 6, 64, 13, 51558], "temperature": 0.0, "avg_logprob": -0.13579315546701645, "compression_ratio": 1.421875, "no_speech_prob": 0.04625573009252548}, {"id": 158, "seek": 45088, "start": 474.76, "end": 477.6, "text": " Gofer mia\u0142 280 miliard\u00f3w parametr\u00f3w.", "tokens": [51558, 1037, 612, 27989, 41229, 1962, 72, 515, 3901, 6220, 27965, 3901, 13, 51700], "temperature": 0.0, "avg_logprob": -0.13579315546701645, "compression_ratio": 1.421875, "no_speech_prob": 0.04625573009252548}, {"id": 159, "seek": 45088, "start": 477.6, "end": 478.36, "text": " Tak.", "tokens": [51700, 9118, 13, 51738], "temperature": 0.0, "avg_logprob": -0.13579315546701645, "compression_ratio": 1.421875, "no_speech_prob": 0.04625573009252548}, {"id": 160, "seek": 47836, "start": 478.36, "end": 481.04, "text": " A z nas stworzy\u0107 korejemnego giganta", "tokens": [50364, 316, 710, 5382, 342, 28321, 27150, 350, 418, 73, 443, 11858, 8741, 5983, 50498], "temperature": 0.0, "avg_logprob": -0.15948722097608778, "compression_ratio": 1.3657587548638133, "no_speech_prob": 0.0025330856442451477}, {"id": 161, "seek": 47836, "start": 481.04, "end": 483.88, "text": " post\u0105pili zgodnie ze swoj\u0105 now\u0105 hipotez\u0105.", "tokens": [50498, 2183, 1611, 79, 2312, 710, 21787, 2766, 5277, 49194, 586, 1611, 8103, 1370, 8925, 13, 50640], "temperature": 0.0, "avg_logprob": -0.15948722097608778, "compression_ratio": 1.3657587548638133, "no_speech_prob": 0.0025330856442451477}, {"id": 162, "seek": 47836, "start": 483.88, "end": 485.52000000000004, "text": " Chwila, czyli jak?", "tokens": [50640, 761, 86, 7371, 11, 16591, 4207, 30, 50722], "temperature": 0.0, "avg_logprob": -0.15948722097608778, "compression_ratio": 1.3657587548638133, "no_speech_prob": 0.0025330856442451477}, {"id": 163, "seek": 47836, "start": 485.52000000000004, "end": 487.40000000000003, "text": " Zbudowali mniejszy model.", "tokens": [50722, 1176, 18281, 305, 5103, 39513, 7706, 2316, 13, 50816], "temperature": 0.0, "avg_logprob": -0.15948722097608778, "compression_ratio": 1.3657587548638133, "no_speech_prob": 0.0025330856442451477}, {"id": 164, "seek": 47836, "start": 487.40000000000003, "end": 488.64, "text": " Znacznie mniejszy.", "tokens": [50816, 1176, 77, 14875, 2766, 39513, 7706, 13, 50878], "temperature": 0.0, "avg_logprob": -0.15948722097608778, "compression_ratio": 1.3657587548638133, "no_speech_prob": 0.0025330856442451477}, {"id": 165, "seek": 47836, "start": 488.64, "end": 490.48, "text": " Zbudowali Chinchilla.", "tokens": [50878, 1176, 18281, 305, 5103, 4430, 339, 5291, 13, 50970], "temperature": 0.0, "avg_logprob": -0.15948722097608778, "compression_ratio": 1.3657587548638133, "no_speech_prob": 0.0025330856442451477}, {"id": 166, "seek": 47836, "start": 490.48, "end": 495.44, "text": " Model o zaledwie 70 miliardach parametr\u00f3w, 70.", "tokens": [50970, 17105, 277, 710, 5573, 8699, 5285, 1962, 72, 515, 608, 6220, 27965, 3901, 11, 5285, 13, 51218], "temperature": 0.0, "avg_logprob": -0.15948722097608778, "compression_ratio": 1.3657587548638133, "no_speech_prob": 0.0025330856442451477}, {"id": 167, "seek": 47836, "start": 495.44, "end": 498.0, "text": " To czterokrotnie mniejsze od Gofer'a.", "tokens": [51218, 1407, 6472, 391, 453, 10536, 2766, 275, 44258, 3611, 1037, 612, 6, 64, 13, 51346], "temperature": 0.0, "avg_logprob": -0.15948722097608778, "compression_ratio": 1.3657587548638133, "no_speech_prob": 0.0025330856442451477}, {"id": 168, "seek": 47836, "start": 498.0, "end": 499.8, "text": " To wbrew wszelkiej intuicji.", "tokens": [51346, 1407, 261, 65, 2236, 37647, 12971, 45145, 560, 84, 299, 4013, 13, 51436], "temperature": 0.0, "avg_logprob": -0.15948722097608778, "compression_ratio": 1.3657587548638133, "no_speech_prob": 0.0025330856442451477}, {"id": 169, "seek": 47836, "start": 499.8, "end": 502.40000000000003, "text": " Jak to w og\u00f3le jest mo\u017cliwe przy tym samym bud\u017cecie?", "tokens": [51436, 15029, 281, 261, 29229, 3492, 30854, 826, 6501, 8107, 3247, 4199, 3265, 2875, 4260, 30, 51566], "temperature": 0.0, "avg_logprob": -0.15948722097608778, "compression_ratio": 1.3657587548638133, "no_speech_prob": 0.0025330856442451477}, {"id": 170, "seek": 47836, "start": 502.40000000000003, "end": 503.44, "text": " Ot\u00f3\u017c to.", "tokens": [51566, 422, 4547, 1427, 281, 13, 51618], "temperature": 0.0, "avg_logprob": -0.15948722097608778, "compression_ratio": 1.3657587548638133, "no_speech_prob": 0.0025330856442451477}, {"id": 171, "seek": 50344, "start": 503.44, "end": 509.76, "text": " Czterokrotnie mniejszy, ale zwytrenowali go na jeden przecinek 4 biliona token\u00f3w.", "tokens": [50364, 383, 89, 391, 453, 10536, 2766, 39513, 7706, 11, 6775, 710, 86, 4328, 1095, 305, 5103, 352, 1667, 12906, 8325, 66, 48421, 1017, 8588, 21758, 281, 2653, 3901, 13, 50680], "temperature": 0.0, "avg_logprob": -0.16747125860762924, "compression_ratio": 1.4, "no_speech_prob": 0.08947828412055969}, {"id": 172, "seek": 50344, "start": 509.76, "end": 514.12, "text": " A to ponad czterokrotnie wi\u0119cej danych ni\u017c u\u017cyto dla Gofer'a.", "tokens": [50680, 316, 281, 9224, 345, 6472, 391, 453, 10536, 2766, 26004, 274, 34644, 28502, 34097, 1353, 12285, 1037, 612, 6, 64, 13, 50898], "temperature": 0.0, "avg_logprob": -0.16747125860762924, "compression_ratio": 1.4, "no_speech_prob": 0.08947828412055969}, {"id": 173, "seek": 50344, "start": 514.12, "end": 517.28, "text": " Ten sam koszt, zupe\u0142nie inna alokacja zasob\u00f3w.", "tokens": [50898, 9380, 3247, 19532, 2682, 11, 49922, 294, 629, 419, 453, 23395, 26530, 996, 3901, 13, 51056], "temperature": 0.0, "avg_logprob": -0.16747125860762924, "compression_ratio": 1.4, "no_speech_prob": 0.08947828412055969}, {"id": 174, "seek": 50344, "start": 517.28, "end": 518.04, "text": " Dok\u0142adnie tak.", "tokens": [51056, 29768, 10358, 2766, 991, 13, 51094], "temperature": 0.0, "avg_logprob": -0.16747125860762924, "compression_ratio": 1.4, "no_speech_prob": 0.08947828412055969}, {"id": 175, "seek": 50344, "start": 518.04, "end": 520.6, "text": " Ok, czyli mamy scen\u0119 ustawion\u0105.", "tokens": [51094, 3477, 11, 16591, 17335, 4191, 1274, 26189, 1607, 313, 1611, 13, 51222], "temperature": 0.0, "avg_logprob": -0.16747125860762924, "compression_ratio": 1.4, "no_speech_prob": 0.08947828412055969}, {"id": 176, "seek": 50344, "start": 520.6, "end": 526.0, "text": " W jednym naro\u017cniku Gigant Gofer, 280 miliard\u00f3w parametr\u00f3w,", "tokens": [51222, 343, 5232, 12996, 6714, 78, 1427, 13123, 84, 40489, 394, 1037, 612, 11, 41229, 1962, 72, 515, 3901, 6220, 27965, 3901, 11, 51492], "temperature": 0.0, "avg_logprob": -0.16747125860762924, "compression_ratio": 1.4, "no_speech_prob": 0.08947828412055969}, {"id": 177, "seek": 50344, "start": 526.0, "end": 529.4, "text": " symbol starej szko\u0142y bigger is better.", "tokens": [51492, 5986, 22432, 73, 7870, 4093, 6825, 3801, 307, 1101, 13, 51662], "temperature": 0.0, "avg_logprob": -0.16747125860762924, "compression_ratio": 1.4, "no_speech_prob": 0.08947828412055969}, {"id": 178, "seek": 50344, "start": 529.4, "end": 532.96, "text": " W drugim czterokrotnie mniejsza Chinchilla.", "tokens": [51662, 343, 4110, 332, 6472, 391, 453, 10536, 2766, 275, 30295, 2394, 4430, 339, 5291, 13, 51840], "temperature": 0.0, "avg_logprob": -0.16747125860762924, "compression_ratio": 1.4, "no_speech_prob": 0.08947828412055969}, {"id": 179, "seek": 53296, "start": 533.0400000000001, "end": 534.44, "text": " Jak sko\u0144czy\u0142a si\u0119 ta walka?", "tokens": [50368, 15029, 1110, 78, 5248, 6522, 5024, 3244, 1846, 1792, 64, 30, 50438], "temperature": 0.0, "avg_logprob": -0.12641042886778367, "compression_ratio": 1.3897280966767371, "no_speech_prob": 0.021715927869081497}, {"id": 180, "seek": 53296, "start": 534.44, "end": 538.1600000000001, "text": " Chinchilla, m\u00f3wi\u0105c wprost, zmiarzy\u0142a Gofer'a.", "tokens": [50438, 4430, 339, 5291, 11, 46591, 66, 261, 1424, 555, 11, 710, 3057, 289, 1229, 5024, 1037, 612, 6, 64, 13, 50624], "temperature": 0.0, "avg_logprob": -0.12641042886778367, "compression_ratio": 1.3897280966767371, "no_speech_prob": 0.021715927869081497}, {"id": 181, "seek": 53296, "start": 538.1600000000001, "end": 539.4000000000001, "text": " Ale nie tylko jego.", "tokens": [50624, 9366, 2838, 13219, 26542, 13, 50686], "temperature": 0.0, "avg_logprob": -0.12641042886778367, "compression_ratio": 1.3897280966767371, "no_speech_prob": 0.021715927869081497}, {"id": 182, "seek": 53296, "start": 539.4000000000001, "end": 545.72, "text": " Zmiarzy\u0142a te\u017c inne giganty tamtych czas\u00f3w jak GPT-3 czy Megatron Turing NLG.", "tokens": [50686, 1176, 3057, 289, 1229, 5024, 9516, 24170, 8741, 394, 88, 7677, 874, 339, 13190, 3901, 4207, 26039, 51, 12, 18, 6430, 9986, 267, 2044, 314, 1345, 426, 43, 38, 13, 51002], "temperature": 0.0, "avg_logprob": -0.12641042886778367, "compression_ratio": 1.3897280966767371, "no_speech_prob": 0.021715927869081497}, {"id": 183, "seek": 53296, "start": 545.72, "end": 548.84, "text": " To nie by\u0142a wygrana w jednym specyficznym zadaniu?", "tokens": [51002, 1407, 2838, 23936, 4628, 861, 2095, 261, 5232, 12996, 768, 1344, 1786, 89, 12996, 42788, 25849, 30, 51158], "temperature": 0.0, "avg_logprob": -0.12641042886778367, "compression_ratio": 1.3897280966767371, "no_speech_prob": 0.021715927869081497}, {"id": 184, "seek": 53296, "start": 548.84, "end": 552.5600000000001, "text": " Nie, to by\u0142a deklasacja w szerokim spektrum test\u00f3w.", "tokens": [51158, 12016, 11, 281, 23936, 368, 74, 7743, 23395, 261, 36160, 453, 332, 768, 2320, 6247, 1500, 3901, 13, 51344], "temperature": 0.0, "avg_logprob": -0.12641042886778367, "compression_ratio": 1.3897280966767371, "no_speech_prob": 0.021715927869081497}, {"id": 185, "seek": 53296, "start": 552.5600000000001, "end": 557.0400000000001, "text": " Od rozumienia j\u0119zyka przez odpowiadanie na pytania, po wiedz\u0119 og\u00f3ln\u0105.", "tokens": [51344, 12210, 48797, 18811, 42309, 40940, 14064, 24314, 38069, 7155, 1667, 25878, 5609, 11, 714, 46894, 11052, 5360, 15741, 13113, 13, 51568], "temperature": 0.0, "avg_logprob": -0.12641042886778367, "compression_ratio": 1.3897280966767371, "no_speech_prob": 0.021715927869081497}, {"id": 186, "seek": 53296, "start": 557.0400000000001, "end": 560.2800000000001, "text": " Daj mu jakie\u015b przyk\u0142ady, bo to brzmi a\u017c niewiarygodnie.", "tokens": [51568, 413, 1805, 2992, 31163, 6501, 74, 1221, 880, 11, 748, 281, 738, 89, 3057, 48134, 43622, 29104, 21787, 2766, 13, 51730], "temperature": 0.0, "avg_logprob": -0.12641042886778367, "compression_ratio": 1.3897280966767371, "no_speech_prob": 0.021715927869081497}, {"id": 187, "seek": 53296, "start": 560.2800000000001, "end": 562.9200000000001, "text": " No dobrze, liczby m\u00f3wi\u0105 same za siebie.", "tokens": [51730, 883, 28335, 11, 6169, 89, 2322, 46591, 912, 7949, 39137, 13, 51862], "temperature": 0.0, "avg_logprob": -0.12641042886778367, "compression_ratio": 1.3897280966767371, "no_speech_prob": 0.021715927869081497}, {"id": 188, "seek": 56292, "start": 562.92, "end": 569.0799999999999, "text": " Na bardzo wymagaj\u0105cym testie MMLU, kt\u00f3ry sprawdza wiedz\u0119 z 57 dziedzin akademickich,", "tokens": [50364, 6056, 9034, 29764, 559, 11133, 1344, 76, 1500, 414, 376, 12683, 52, 11, 9913, 46192, 2394, 46894, 11052, 710, 21423, 9758, 15338, 259, 9308, 49290, 618, 480, 11, 50672], "temperature": 0.0, "avg_logprob": -0.14108499380258413, "compression_ratio": 1.2730627306273063, "no_speech_prob": 0.004578105174005032}, {"id": 189, "seek": 56292, "start": 569.0799999999999, "end": 573.0799999999999, "text": " Chinchilla osi\u0105gn\u0119\u0142a 67,5% dok\u0142adno\u015bci.", "tokens": [50672, 4430, 339, 5291, 3003, 11404, 4568, 1274, 5024, 23879, 11, 20, 4, 45864, 16438, 13, 50872], "temperature": 0.0, "avg_logprob": -0.14108499380258413, "compression_ratio": 1.2730627306273063, "no_speech_prob": 0.004578105174005032}, {"id": 190, "seek": 56292, "start": 573.0799999999999, "end": 573.92, "text": " A Gofer?", "tokens": [50872, 316, 1037, 612, 30, 50914], "temperature": 0.0, "avg_logprob": -0.14108499380258413, "compression_ratio": 1.2730627306273063, "no_speech_prob": 0.004578105174005032}, {"id": 191, "seek": 56292, "start": 573.92, "end": 575.68, "text": " Zaledwie 60%.", "tokens": [50914, 1176, 5573, 8699, 4060, 6856, 51002], "temperature": 0.0, "avg_logprob": -0.14108499380258413, "compression_ratio": 1.2730627306273063, "no_speech_prob": 0.004578105174005032}, {"id": 192, "seek": 56292, "start": 575.68, "end": 578.68, "text": " To jest ponad 7% r\u00f3\u017cnicy.", "tokens": [51002, 1407, 3492, 9224, 345, 1614, 4, 19637, 77, 2632, 13, 51152], "temperature": 0.0, "avg_logprob": -0.14108499380258413, "compression_ratio": 1.2730627306273063, "no_speech_prob": 0.004578105174005032}, {"id": 193, "seek": 56292, "start": 578.68, "end": 581.76, "text": " W \u015bwiecie modeli j\u0119zykowych to jest przepa\u015b\u0107.", "tokens": [51152, 343, 40078, 4260, 2316, 72, 49055, 74, 19605, 281, 3492, 30829, 64, 7753, 13, 51306], "temperature": 0.0, "avg_logprob": -0.14108499380258413, "compression_ratio": 1.2730627306273063, "no_speech_prob": 0.004578105174005032}, {"id": 194, "seek": 56292, "start": 581.76, "end": 583.5999999999999, "text": " Zatrzymajmy si\u0119 tu na chwil\u0119.", "tokens": [51306, 1176, 267, 13047, 1696, 73, 2226, 3244, 2604, 1667, 41941, 1274, 13, 51398], "temperature": 0.0, "avg_logprob": -0.14108499380258413, "compression_ratio": 1.2730627306273063, "no_speech_prob": 0.004578105174005032}, {"id": 195, "seek": 56292, "start": 583.5999999999999, "end": 589.0799999999999, "text": " W artykule autorzy dodaj\u0105 tak\u0105 fascynuj\u0105c\u0105 uwag\u0119, \u017ce ten wynik Chinchilli", "tokens": [51398, 343, 594, 874, 74, 2271, 19510, 1229, 13886, 11133, 31069, 30632, 1344, 77, 13263, 32557, 43696, 11, 3561, 2064, 31936, 1035, 4430, 339, 31922, 51672], "temperature": 0.0, "avg_logprob": -0.14108499380258413, "compression_ratio": 1.2730627306273063, "no_speech_prob": 0.004578105174005032}, {"id": 196, "seek": 58908, "start": 589.08, "end": 594.12, "text": " on przywy\u017cszy\u0142 prognoz\u0119 ekspert\u00f3w dotycz\u0105ce tego, co b\u0119dzie mo\u017cliwe do osi\u0105gni\u0119cia", "tokens": [50364, 322, 6501, 9726, 1427, 7706, 1221, 447, 70, 1771, 11052, 30724, 15346, 3901, 5893, 17466, 1611, 384, 8627, 11, 598, 10562, 30854, 826, 360, 3003, 11404, 70, 35938, 2755, 50616], "temperature": 0.0, "avg_logprob": -0.11320820119645861, "compression_ratio": 1.4321428571428572, "no_speech_prob": 0.16030645370483398}, {"id": 197, "seek": 58908, "start": 594.12, "end": 597.72, "text": " w tej dziedzinie dopiero w czerwcu 2023 roku.", "tokens": [50616, 261, 12573, 9758, 15338, 259, 414, 21900, 12030, 261, 269, 4527, 86, 12032, 44377, 19451, 13, 50796], "temperature": 0.0, "avg_logprob": -0.11320820119645861, "compression_ratio": 1.4321428571428572, "no_speech_prob": 0.16030645370483398}, {"id": 198, "seek": 58908, "start": 597.72, "end": 600.76, "text": " A artyku\u0142 jest z marca 2022 roku?", "tokens": [50796, 316, 594, 874, 5279, 1221, 3492, 710, 30582, 20229, 19451, 30, 50948], "temperature": 0.0, "avg_logprob": -0.11320820119645861, "compression_ratio": 1.4321428571428572, "no_speech_prob": 0.16030645370483398}, {"id": 199, "seek": 58908, "start": 600.76, "end": 601.8000000000001, "text": " W\u0142a\u015bnie.", "tokens": [50948, 343, 5024, 12221, 13, 51000], "temperature": 0.0, "avg_logprob": -0.11320820119645861, "compression_ratio": 1.4321428571428572, "no_speech_prob": 0.16030645370483398}, {"id": 200, "seek": 58908, "start": 601.8000000000001, "end": 608.24, "text": " To jest jakby kto\u015b w 2022 roku zbudowa\u0142 co\u015b, czego spodziewali\u015bmy si\u0119 dopiero za p\u00f3\u0142tara roku.", "tokens": [51000, 1407, 3492, 28976, 32982, 261, 20229, 19451, 710, 18281, 30105, 19241, 11, 36559, 637, 378, 89, 1093, 33955, 3244, 21900, 12030, 7949, 47907, 83, 2419, 19451, 13, 51322], "temperature": 0.0, "avg_logprob": -0.11320820119645861, "compression_ratio": 1.4321428571428572, "no_speech_prob": 0.16030645370483398}, {"id": 201, "seek": 58908, "start": 608.24, "end": 611.36, "text": " To zaburza ca\u0142\u0105 o\u015b czasu post\u0119pu w AI.", "tokens": [51322, 1407, 24838, 374, 2394, 1335, 15926, 277, 1788, 40860, 2183, 18085, 84, 261, 7318, 13, 51478], "temperature": 0.0, "avg_logprob": -0.11320820119645861, "compression_ratio": 1.4321428571428572, "no_speech_prob": 0.16030645370483398}, {"id": 202, "seek": 58908, "start": 611.36, "end": 613.48, "text": " To jest doskona\u0142e podsumowanie.", "tokens": [51478, 1407, 3492, 4491, 74, 4037, 19827, 31925, 449, 22028, 13, 51584], "temperature": 0.0, "avg_logprob": -0.11320820119645861, "compression_ratio": 1.4321428571428572, "no_speech_prob": 0.16030645370483398}, {"id": 203, "seek": 58908, "start": 613.48, "end": 615.72, "text": " Oni nie tylko pokazali lepsz\u0105 metod\u0119,", "tokens": [51584, 1282, 72, 2838, 13219, 13010, 921, 5103, 476, 1878, 8925, 1131, 378, 1274, 11, 51696], "temperature": 0.0, "avg_logprob": -0.11320820119645861, "compression_ratio": 1.4321428571428572, "no_speech_prob": 0.16030645370483398}, {"id": 204, "seek": 61572, "start": 615.72, "end": 618.0400000000001, "text": " oni przyspieszyli post\u0119p.", "tokens": [50364, 36317, 6541, 749, 79, 530, 1229, 2081, 2183, 18085, 13, 50480], "temperature": 0.0, "avg_logprob": -0.20223787668589, "compression_ratio": 1.2366071428571428, "no_speech_prob": 0.20692306756973267}, {"id": 205, "seek": 61572, "start": 618.0400000000001, "end": 619.64, "text": " I to wida\u0107 wsz\u0119dzie.", "tokens": [50480, 286, 281, 261, 46898, 38322, 42643, 13, 50560], "temperature": 0.0, "avg_logprob": -0.20223787668589, "compression_ratio": 1.2366071428571428, "no_speech_prob": 0.20692306756973267}, {"id": 206, "seek": 61572, "start": 619.64, "end": 620.76, "text": " Gdzie jeszcze?", "tokens": [50560, 460, 13096, 14168, 30, 50616], "temperature": 0.0, "avg_logprob": -0.20223787668589, "compression_ratio": 1.2366071428571428, "no_speech_prob": 0.20692306756973267}, {"id": 207, "seek": 61572, "start": 620.76, "end": 624.6800000000001, "text": " W zadaniach z czytania ze zrozumieniem jak benchmark race", "tokens": [50616, 343, 42788, 3782, 608, 710, 6430, 83, 5609, 5277, 710, 27857, 449, 1053, 4907, 4207, 18927, 4569, 50812], "temperature": 0.0, "avg_logprob": -0.20223787668589, "compression_ratio": 1.2366071428571428, "no_speech_prob": 0.20692306756973267}, {"id": 208, "seek": 61572, "start": 624.6800000000001, "end": 631.8000000000001, "text": " Chinchilla uzyska\u0142a 82.3% w por\u00f3wnaniu do 71.6% Gofera.", "tokens": [50812, 4430, 339, 5291, 16851, 749, 2330, 5024, 29097, 13, 18, 4, 261, 1515, 812, 895, 25849, 360, 30942, 13, 21, 4, 1037, 45635, 13, 51168], "temperature": 0.0, "avg_logprob": -0.20223787668589, "compression_ratio": 1.2366071428571428, "no_speech_prob": 0.20692306756973267}, {"id": 209, "seek": 61572, "start": 631.8000000000001, "end": 632.44, "text": " Wow.", "tokens": [51168, 3153, 13, 51200], "temperature": 0.0, "avg_logprob": -0.20223787668589, "compression_ratio": 1.2366071428571428, "no_speech_prob": 0.20692306756973267}, {"id": 210, "seek": 61572, "start": 632.44, "end": 639.88, "text": " Na ogromnym zbiorze test\u00f3w BigBange \u015brednia wydajno\u015b\u0107 Chinchilli by\u0142a wy\u017csza od 10.7%", "tokens": [51200, 6056, 34416, 298, 12996, 710, 33362, 1381, 1500, 3901, 5429, 33, 933, 8299, 986, 12679, 25984, 1805, 23293, 4430, 339, 31922, 23936, 4628, 1427, 82, 2394, 3611, 1266, 13, 22, 4, 51572], "temperature": 0.0, "avg_logprob": -0.20223787668589, "compression_ratio": 1.2366071428571428, "no_speech_prob": 0.20692306756973267}, {"id": 211, "seek": 63988, "start": 639.88, "end": 645.72, "text": " w zadaniach z odpowiadania na pytania w trybie closed book, czyli bez dostabu do zewn\u0119trznych \u017ar\u00f3de\u0142.", "tokens": [50364, 261, 42788, 3782, 608, 710, 24314, 38069, 5609, 1667, 25878, 5609, 261, 853, 7392, 5395, 1446, 11, 16591, 10782, 20568, 455, 84, 360, 5277, 895, 1274, 6903, 89, 9399, 50212, 11721, 1479, 1221, 13, 50656], "temperature": 0.0, "avg_logprob": -0.12130435596812855, "compression_ratio": 1.44, "no_speech_prob": 0.514574408531189}, {"id": 212, "seek": 63988, "start": 645.72, "end": 649.0, "text": " Gdzie model musi polega\u0107 wy\u0142\u0105cznie na swojej wiedzy?", "tokens": [50656, 460, 13096, 2316, 37587, 13208, 3680, 2162, 4628, 15926, 19923, 1667, 29489, 73, 46894, 1229, 30, 50820], "temperature": 0.0, "avg_logprob": -0.12130435596812855, "compression_ratio": 1.44, "no_speech_prob": 0.514574408531189}, {"id": 213, "seek": 63988, "start": 649.0, "end": 651.16, "text": " Tak, ustanowi\u0142a nowy rekord.", "tokens": [50820, 9118, 11, 26189, 282, 24503, 5024, 586, 88, 33881, 765, 13, 50928], "temperature": 0.0, "avg_logprob": -0.12130435596812855, "compression_ratio": 1.44, "no_speech_prob": 0.514574408531189}, {"id": 214, "seek": 63988, "start": 651.16, "end": 656.92, "text": " Wygrana by\u0142a konsekwentna, znacz\u0105ca i widoczna praktycznie na ka\u017cdym polu.", "tokens": [50928, 14458, 861, 2095, 23936, 47020, 74, 34798, 629, 11, 15397, 326, 8925, 496, 741, 5274, 905, 35458, 3206, 74, 45586, 1667, 31615, 76, 1180, 84, 13, 51216], "temperature": 0.0, "avg_logprob": -0.12130435596812855, "compression_ratio": 1.44, "no_speech_prob": 0.514574408531189}, {"id": 215, "seek": 63988, "start": 656.92, "end": 659.8, "text": " To brzmi jak fundamentalna zmiana paradygmatu.", "tokens": [51216, 1407, 738, 89, 3057, 4207, 8088, 629, 17020, 8497, 13480, 18103, 15677, 84, 13, 51360], "temperature": 0.0, "avg_logprob": -0.12130435596812855, "compression_ratio": 1.44, "no_speech_prob": 0.514574408531189}, {"id": 216, "seek": 63988, "start": 659.8, "end": 663.56, "text": " Jakie s\u0105 praktyczne implikacje tego odkrycia?", "tokens": [51360, 15029, 414, 9015, 3206, 74, 874, 38491, 8484, 1035, 29293, 8627, 3611, 43298, 2755, 30, 51548], "temperature": 0.0, "avg_logprob": -0.12130435596812855, "compression_ratio": 1.44, "no_speech_prob": 0.514574408531189}, {"id": 217, "seek": 63988, "start": 663.56, "end": 669.0, "text": " Co to wszystko oznacza dla kogo\u015b, kto nie buduje modeli za setki milion\u00f3w, ale chce z nich korzysta\u0107?", "tokens": [51548, 3066, 281, 22607, 277, 22672, 326, 2394, 12285, 350, 23515, 1788, 11, 23780, 2838, 3265, 13008, 2316, 72, 7949, 992, 2984, 1962, 313, 3901, 11, 6775, 28928, 710, 25570, 14784, 49590, 2162, 30, 51820], "temperature": 0.0, "avg_logprob": -0.12130435596812855, "compression_ratio": 1.44, "no_speech_prob": 0.514574408531189}, {"id": 218, "seek": 66900, "start": 669.08, "end": 672.2, "text": " Konsekwencje s\u0105 ogromne na kilku poziomach.", "tokens": [50368, 591, 3739, 74, 15615, 44261, 9015, 34416, 298, 716, 1667, 5128, 5279, 38503, 298, 608, 13, 50524], "temperature": 0.0, "avg_logprob": -0.10201495130297164, "compression_ratio": 1.4096774193548387, "no_speech_prob": 0.002591417171061039}, {"id": 219, "seek": 66900, "start": 672.2, "end": 676.44, "text": " Po pierwsze, i to jest najbardziej oczywiste, efektywno\u015b\u0107.", "tokens": [50524, 6165, 45994, 11, 741, 281, 3492, 41857, 277, 6522, 86, 8375, 11, 31482, 916, 874, 20944, 7753, 13, 50736], "temperature": 0.0, "avg_logprob": -0.10201495130297164, "compression_ratio": 1.4096774193548387, "no_speech_prob": 0.002591417171061039}, {"id": 220, "seek": 66900, "start": 676.44, "end": 677.24, "text": " Co to znaczy?", "tokens": [50736, 3066, 281, 36584, 30, 50776], "temperature": 0.0, "avg_logprob": -0.10201495130297164, "compression_ratio": 1.4096774193548387, "no_speech_prob": 0.002591417171061039}, {"id": 221, "seek": 66900, "start": 677.24, "end": 685.32, "text": " Mniejszy model taki jak Chinchilla jest znacznie ta\u0144szy i szybszy w u\u017cyciu do tzw. inferencji, czyli generowania odpowiedzi.", "tokens": [50776, 376, 10402, 7706, 2316, 20065, 4207, 4430, 339, 5291, 3492, 15397, 14875, 2766, 1846, 5248, 7706, 741, 30526, 929, 1229, 261, 34097, 30795, 360, 256, 14406, 13, 13596, 268, 19649, 11, 16591, 1337, 21308, 36574, 3992, 13, 51180], "temperature": 0.0, "avg_logprob": -0.10201495130297164, "compression_ratio": 1.4096774193548387, "no_speech_prob": 0.002591417171061039}, {"id": 222, "seek": 66900, "start": 685.32, "end": 690.12, "text": " To samo dotyczy procesu fine tuning, czyli dostosowywania go do konkretnych zada\u0144.", "tokens": [51180, 1407, 36422, 5893, 88, 6522, 17565, 84, 2489, 15164, 11, 16591, 20568, 329, 10089, 86, 5609, 352, 360, 36500, 9399, 710, 1538, 5248, 13, 51420], "temperature": 0.0, "avg_logprob": -0.10201495130297164, "compression_ratio": 1.4096774193548387, "no_speech_prob": 0.002591417171061039}, {"id": 223, "seek": 66900, "start": 690.12, "end": 696.36, "text": " Czyli ma\u0142a firma lub startup, kt\u00f3ry wcze\u015bniej m\u00f3g\u0142 tylko poma\u017cy\u0107 o wykorzystaniu modelu klasy GPT-3", "tokens": [51420, 37099, 463, 5024, 12159, 1696, 15980, 18578, 11, 9913, 40785, 275, 14047, 1221, 13219, 280, 6440, 39687, 277, 43606, 36049, 25849, 2316, 84, 9671, 5871, 26039, 51, 12, 18, 51732], "temperature": 0.0, "avg_logprob": -0.10201495130297164, "compression_ratio": 1.4096774193548387, "no_speech_prob": 0.002591417171061039}, {"id": 224, "seek": 69636, "start": 696.36, "end": 701.0, "text": " nagle dostaje do r\u0119ki narz\u0119dzie o por\u00f3wniwalnej, a nawet lepszej, mocy.", "tokens": [50364, 297, 15088, 20568, 11153, 360, 41197, 2984, 6714, 89, 42643, 277, 1515, 812, 895, 72, 29530, 11794, 11, 257, 22696, 476, 1878, 16920, 11, 705, 1344, 13, 50596], "temperature": 0.0, "avg_logprob": -0.12391749295321378, "compression_ratio": 1.4645390070921986, "no_speech_prob": 0.024852558970451355}, {"id": 225, "seek": 69636, "start": 701.0, "end": 706.52, "text": " Ale znacznie bardziej dost\u0119pne, dok\u0142adnie tak, to demokratyzuje dost\u0119p do pot\u0119\u017cnej AI.", "tokens": [50596, 9366, 15397, 14875, 2766, 27209, 48209, 716, 11, 45864, 2766, 991, 11, 281, 49432, 37433, 13008, 48209, 360, 1847, 1274, 1427, 11794, 7318, 13, 50872], "temperature": 0.0, "avg_logprob": -0.12391749295321378, "compression_ratio": 1.4645390070921986, "no_speech_prob": 0.024852558970451355}, {"id": 226, "seek": 69636, "start": 706.52, "end": 708.92, "text": " A druga, mo\u017ce wa\u017cniejsza konsekwencja?", "tokens": [50872, 316, 4110, 64, 11, 12034, 27777, 30295, 2394, 47020, 74, 15615, 34056, 30, 50992], "temperature": 0.0, "avg_logprob": -0.12391749295321378, "compression_ratio": 1.4645390070921986, "no_speech_prob": 0.024852558970451355}, {"id": 227, "seek": 69636, "start": 708.92, "end": 711.64, "text": " Druga to zmiana priortet\u00f3w w ca\u0142ej bran\u017cy.", "tokens": [50992, 2491, 19364, 281, 17020, 8497, 1790, 477, 302, 3901, 261, 47631, 73, 12029, 7735, 13, 51128], "temperature": 0.0, "avg_logprob": -0.12391749295321378, "compression_ratio": 1.4645390070921986, "no_speech_prob": 0.024852558970451355}, {"id": 228, "seek": 69636, "start": 711.64, "end": 717.16, "text": " W\u0105zkim gard\u0142em w rozwoju AI przestaje by\u0107 tylko moc obliczeniowa, a staje si\u0119 nim", "tokens": [51128, 343, 1611, 89, 25112, 5628, 11126, 261, 9544, 6120, 8954, 7318, 44264, 11153, 15069, 13219, 34962, 1111, 1050, 42124, 5528, 11, 257, 342, 11153, 3244, 24887, 51404], "temperature": 0.0, "avg_logprob": -0.12391749295321378, "compression_ratio": 1.4645390070921986, "no_speech_prob": 0.024852558970451355}, {"id": 229, "seek": 69636, "start": 717.16, "end": 722.12, "text": " dost\u0119p do ogromnych, ale co kluczowe, wysokiej jako\u015bci zbior\u00f3w danych.", "tokens": [51404, 48209, 360, 34416, 298, 9399, 11, 6775, 598, 9671, 1311, 89, 6880, 11, 27062, 453, 7764, 17123, 6199, 710, 33362, 3901, 274, 34644, 13, 51652], "temperature": 0.0, "avg_logprob": -0.12391749295321378, "compression_ratio": 1.4645390070921986, "no_speech_prob": 0.024852558970451355}, {"id": 230, "seek": 72212, "start": 722.2, "end": 727.08, "text": " Artyku\u0142 pokazuje prognozy i szacuje, \u017ce optymalny model o rozmiarze biliona parametr\u00f3w", "tokens": [50368, 1587, 874, 5279, 1221, 13010, 43317, 447, 70, 1771, 1229, 741, 7870, 326, 13008, 11, 3561, 2427, 4199, 304, 1634, 2316, 277, 9544, 3057, 289, 1381, 8588, 21758, 6220, 27965, 3901, 50612], "temperature": 0.0, "avg_logprob": -0.13653417067094284, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.03872233256697655}, {"id": 231, "seek": 72212, "start": 727.08, "end": 730.76, "text": " wymaga\u0142by treningu na ponad 21 bilion\u00f3w token\u00f3w.", "tokens": [50612, 29764, 9286, 34635, 2192, 773, 84, 1667, 9224, 345, 5080, 8588, 313, 3901, 14862, 3901, 13, 50796], "temperature": 0.0, "avg_logprob": -0.13653417067094284, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.03872233256697655}, {"id": 232, "seek": 72212, "start": 730.76, "end": 734.44, "text": " 21 bilion\u00f3w to s\u0105 astronomiczne liczby.", "tokens": [50796, 5080, 8588, 313, 3901, 281, 9015, 26302, 17946, 716, 6169, 89, 2322, 13, 50980], "temperature": 0.0, "avg_logprob": -0.13653417067094284, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.03872233256697655}, {"id": 233, "seek": 72212, "start": 734.44, "end": 739.16, "text": " Tak, wy\u015bcig na parametry zamieni\u0142 si\u0119 w wy\u015bcig na dane.", "tokens": [50980, 9118, 11, 4628, 1788, 66, 328, 1667, 6220, 9889, 19876, 35462, 1221, 3244, 261, 4628, 1788, 66, 328, 1667, 49206, 13, 51216], "temperature": 0.0, "avg_logprob": -0.13653417067094284, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.03872233256697655}, {"id": 234, "seek": 72212, "start": 739.16, "end": 746.36, "text": " To wszystko brzmi jak ostateczne zamkni\u0119te rozwi\u0105zanie, ale w nauce nigdy tak nie jest.", "tokens": [51216, 1407, 22607, 738, 89, 3057, 4207, 277, 15406, 38491, 19876, 74, 35938, 975, 9544, 22620, 7155, 11, 6775, 261, 35616, 384, 26996, 3173, 991, 2838, 3492, 13, 51576], "temperature": 0.0, "avg_logprob": -0.13653417067094284, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.03872233256697655}, {"id": 235, "seek": 72212, "start": 746.36, "end": 748.36, "text": " Gdzie s\u0105 dziury w tej teorii?", "tokens": [51576, 460, 13096, 9015, 31981, 2598, 261, 12573, 40238, 5597, 30, 51676], "temperature": 0.0, "avg_logprob": -0.13653417067094284, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.03872233256697655}, {"id": 236, "seek": 74836, "start": 748.36, "end": 752.36, "text": " Co\u015b, co sami autorzy przyznali, \u017ce nie do ko\u0144ca im si\u0119 zgadza.", "tokens": [50364, 3066, 1788, 11, 598, 3247, 72, 19510, 1229, 6501, 89, 4660, 72, 11, 3561, 2838, 360, 26470, 496, 566, 3244, 40948, 345, 2394, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12834772856339163, "compression_ratio": 1.4326241134751774, "no_speech_prob": 0.1725996434688568}, {"id": 237, "seek": 74836, "start": 752.36, "end": 755.64, "text": " I to jest w\u0142a\u015bnie cecha \u015bwietnych prac naukowych.", "tokens": [50564, 286, 281, 3492, 14234, 1769, 4413, 8299, 39083, 9399, 22404, 35616, 74, 19605, 13, 50728], "temperature": 0.0, "avg_logprob": -0.12834772856339163, "compression_ratio": 1.4326241134751774, "no_speech_prob": 0.1725996434688568}, {"id": 238, "seek": 74836, "start": 755.64, "end": 758.6800000000001, "text": " Byli bardzo transparentni, co do ogranicze\u0144.", "tokens": [50728, 3146, 2081, 9034, 12737, 3722, 11, 598, 360, 34416, 30732, 49689, 13, 50880], "temperature": 0.0, "avg_logprob": -0.12834772856339163, "compression_ratio": 1.4326241134751774, "no_speech_prob": 0.1725996434688568}, {"id": 239, "seek": 74836, "start": 758.6800000000001, "end": 759.72, "text": " Na przyk\u0142ad?", "tokens": [50880, 6056, 23144, 30, 50932], "temperature": 0.0, "avg_logprob": -0.12834772856339163, "compression_ratio": 1.4326241134751774, "no_speech_prob": 0.1725996434688568}, {"id": 240, "seek": 74836, "start": 759.72, "end": 766.6800000000001, "text": " Po pierwsze, ich prawa skalowania opieraj\u0105 si\u0119 na za\u0142o\u017ceniu o idealnie pot\u0119gowym charakterze tej zale\u017cno\u015bci.", "tokens": [50932, 6165, 45994, 11, 1893, 3206, 4151, 16890, 21308, 999, 811, 11133, 3244, 1667, 7949, 5249, 24930, 5951, 277, 7157, 2766, 1847, 1274, 70, 31691, 1290, 33557, 1381, 12573, 710, 45494, 16438, 13, 51280], "temperature": 0.0, "avg_logprob": -0.12834772856339163, "compression_ratio": 1.4326241134751774, "no_speech_prob": 0.1725996434688568}, {"id": 241, "seek": 74836, "start": 766.6800000000001, "end": 771.8000000000001, "text": " Ale sami zauwa\u017cyli pewn\u0105 subteln\u0105 krzywizn\u0119 w danych przy najwi\u0119kszych modelach.", "tokens": [51280, 9366, 3247, 72, 710, 1459, 4151, 7735, 2081, 47160, 1611, 7257, 9878, 1611, 350, 13047, 86, 590, 77, 1274, 261, 274, 34644, 6501, 48636, 1694, 28051, 2316, 608, 13, 51536], "temperature": 0.0, "avg_logprob": -0.12834772856339163, "compression_ratio": 1.4326241134751774, "no_speech_prob": 0.1725996434688568}, {"id": 242, "seek": 74836, "start": 771.8000000000001, "end": 773.24, "text": " Co to mo\u017ce oznacza\u0107?", "tokens": [51536, 3066, 281, 12034, 277, 22672, 326, 35873, 30, 51608], "temperature": 0.0, "avg_logprob": -0.12834772856339163, "compression_ratio": 1.4326241134751774, "no_speech_prob": 0.1725996434688568}, {"id": 243, "seek": 77324, "start": 773.24, "end": 780.92, "text": " To mo\u017ce oznacza\u0107, \u017ce nawet ich, ju\u017c obni\u017cone, szacunki co do optymalnego rozmiaru s\u0105 wci\u0105\u017c nieco zawy\u017cone.", "tokens": [50364, 1407, 12034, 277, 22672, 326, 35873, 11, 3561, 22696, 1893, 11, 10678, 1111, 3722, 1427, 546, 11, 7870, 326, 409, 2984, 598, 360, 2427, 4199, 304, 11858, 9544, 3057, 16870, 9015, 261, 537, 27242, 2838, 1291, 28165, 88, 1427, 546, 13, 50748], "temperature": 0.0, "avg_logprob": -0.11733744694636418, "compression_ratio": 1.5067567567567568, "no_speech_prob": 0.5401729941368103}, {"id": 244, "seek": 77324, "start": 780.92, "end": 785.24, "text": " Innymi s\u0142owy, w przysz\u0142o\u015bci optymalne modele mog\u0105 by\u0107 jeszcze mniejsze.", "tokens": [50748, 682, 31813, 15116, 10089, 11, 261, 44018, 35059, 2427, 4199, 304, 716, 4391, 306, 34123, 15069, 14168, 275, 44258, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11733744694636418, "compression_ratio": 1.5067567567567568, "no_speech_prob": 0.5401729941368103}, {"id": 245, "seek": 77324, "start": 785.24, "end": 787.5600000000001, "text": " A potrzebowa\u0107 jeszcze wi\u0119cej danych?", "tokens": [50964, 316, 37595, 11445, 14168, 26004, 274, 34644, 30, 51080], "temperature": 0.0, "avg_logprob": -0.11733744694636418, "compression_ratio": 1.5067567567567568, "no_speech_prob": 0.5401729941368103}, {"id": 246, "seek": 77324, "start": 787.5600000000001, "end": 790.92, "text": " Czyli trend mo\u017ce by\u0107 nawet bardziej ekstremalny ni\u017c pokazali?", "tokens": [51080, 37099, 6028, 12034, 15069, 22696, 27209, 13359, 372, 2579, 304, 1634, 28502, 13010, 921, 5103, 30, 51248], "temperature": 0.0, "avg_logprob": -0.11733744694636418, "compression_ratio": 1.5067567567567568, "no_speech_prob": 0.5401729941368103}, {"id": 247, "seek": 77324, "start": 790.92, "end": 792.2, "text": " Mo\u017cliwe.", "tokens": [51248, 44736, 2081, 826, 13, 51312], "temperature": 0.0, "avg_logprob": -0.11733744694636418, "compression_ratio": 1.5067567567567568, "no_speech_prob": 0.5401729941368103}, {"id": 248, "seek": 77324, "start": 792.2, "end": 795.64, "text": " Po drugie, i to jest wa\u017cne ograniczenie,", "tokens": [51312, 6165, 4110, 414, 11, 741, 281, 3492, 46110, 34416, 30732, 16778, 11, 51484], "temperature": 0.0, "avg_logprob": -0.11733744694636418, "compression_ratio": 1.5067567567567568, "no_speech_prob": 0.5401729941368103}, {"id": 249, "seek": 77324, "start": 795.64, "end": 802.28, "text": " ca\u0142a ich analiza zosta\u0142a przeprowadzona na modelach trenowanych przez mniej ni\u017c jedn\u0105 epok\u0119.", "tokens": [51484, 1335, 5024, 1893, 2624, 13427, 23154, 5024, 30829, 1892, 345, 13383, 1667, 2316, 608, 23136, 23341, 339, 14064, 39513, 28502, 5232, 13113, 2388, 453, 1274, 13, 51816], "temperature": 0.0, "avg_logprob": -0.11733744694636418, "compression_ratio": 1.5067567567567568, "no_speech_prob": 0.5401729941368103}, {"id": 250, "seek": 80228, "start": 802.28, "end": 805.9599999999999, "text": " Czyli ka\u017cdy fragment danych model widzia\u0142 tylko raz.", "tokens": [50364, 37099, 31615, 26424, 274, 34644, 2316, 27486, 8908, 13219, 9639, 13, 50548], "temperature": 0.0, "avg_logprob": -0.1048858379495555, "compression_ratio": 1.4462540716612378, "no_speech_prob": 0.0015421229181811213}, {"id": 251, "seek": 80228, "start": 805.9599999999999, "end": 807.24, "text": " Dok\u0142adnie.", "tokens": [50548, 29768, 10358, 2766, 13, 50612], "temperature": 0.0, "avg_logprob": -0.1048858379495555, "compression_ratio": 1.4462540716612378, "no_speech_prob": 0.0015421229181811213}, {"id": 252, "seek": 80228, "start": 807.24, "end": 814.8399999999999, "text": " A to pozostawia otwarte pytanie o to, jak te zale\u017cno\u015bci wygl\u0105da\u0142yby w re\u017cymie wielokrotnego przetwarzania tych samych danych.", "tokens": [50612, 316, 281, 21281, 555, 34953, 4337, 86, 11026, 36610, 277, 281, 11, 4207, 535, 710, 45494, 16438, 32015, 6825, 2322, 261, 319, 1427, 4199, 414, 20570, 453, 10536, 11858, 6541, 302, 31991, 5609, 15180, 3247, 16384, 274, 34644, 13, 50992], "temperature": 0.0, "avg_logprob": -0.1048858379495555, "compression_ratio": 1.4462540716612378, "no_speech_prob": 0.0015421229181811213}, {"id": 253, "seek": 80228, "start": 814.8399999999999, "end": 820.1999999999999, "text": " Czy to jest efektywne? Czy prowadzi do przeuczenia? Tego ich praca nie rozstrzyga.", "tokens": [50992, 19832, 281, 3492, 31482, 916, 874, 86, 716, 30, 19832, 36590, 3992, 360, 8325, 1311, 14320, 30, 314, 6308, 1893, 582, 6628, 2838, 9544, 9733, 1229, 3680, 13, 51260], "temperature": 0.0, "avg_logprob": -0.1048858379495555, "compression_ratio": 1.4462540716612378, "no_speech_prob": 0.0015421229181811213}, {"id": 254, "seek": 80228, "start": 820.1999999999999, "end": 827.64, "text": " Ca\u0142a spo\u0142eczno\u015b\u0107 AI by\u0142a zafiksowana na wy\u015bcigu na rozmiar, na budowaniu coraz wi\u0119kszych silnik\u00f3w.", "tokens": [51260, 7544, 5024, 36851, 89, 23293, 7318, 23936, 710, 2792, 23292, 40458, 1667, 4628, 1788, 66, 16397, 1667, 9544, 3057, 289, 11, 1667, 3265, 305, 25849, 25899, 29968, 28051, 3425, 47447, 13, 51632], "temperature": 0.0, "avg_logprob": -0.1048858379495555, "compression_ratio": 1.4462540716612378, "no_speech_prob": 0.0015421229181811213}, {"id": 255, "seek": 80228, "start": 827.64, "end": 830.8399999999999, "text": " Podczas gdy prawdziwa odpowied\u017a le\u017ca\u0142a w r\u00f3wnowadze.", "tokens": [51632, 12646, 30989, 28405, 41175, 3992, 4151, 36574, 10659, 476, 35075, 5024, 261, 11416, 895, 305, 345, 1381, 13, 51792], "temperature": 0.0, "avg_logprob": -0.1048858379495555, "compression_ratio": 1.4462540716612378, "no_speech_prob": 0.0015421229181811213}, {"id": 256, "seek": 83084, "start": 830.9200000000001, "end": 836.84, "text": " W znalezieniu idealnej proporcji mi\u0119dzy rozmiarem silnika, a powiedzmy jako\u015bci\u0105 paliwa.", "tokens": [50368, 343, 15397, 37646, 1053, 5951, 7157, 11794, 2365, 36003, 4013, 33964, 9544, 3057, 19183, 3425, 77, 5439, 11, 257, 27617, 2226, 17123, 50227, 3984, 72, 4151, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11508578062057495, "compression_ratio": 1.4860557768924303, "no_speech_prob": 0.004972819238901138}, {"id": 257, "seek": 83084, "start": 836.84, "end": 839.1600000000001, "text": " I czasem sp\u0119dzonym na torze.", "tokens": [50664, 286, 13190, 443, 637, 6298, 89, 12732, 1667, 3930, 1381, 13, 50780], "temperature": 0.0, "avg_logprob": -0.11508578062057495, "compression_ratio": 1.4860557768924303, "no_speech_prob": 0.004972819238901138}, {"id": 258, "seek": 83084, "start": 839.1600000000001, "end": 847.96, "text": " Okaza\u0142o si\u0119, \u017ce mniejsze, ale znacznie lepiej wydukowany model mo\u017ce z \u0142atwo\u015bci\u0105 przewy\u017cszy\u0107 wi\u0119kszego, ale nie do uczonego giganta.", "tokens": [50780, 3477, 12257, 5249, 3244, 11, 3561, 275, 44258, 11, 6775, 15397, 14875, 2766, 476, 39699, 4628, 769, 74, 23341, 2316, 12034, 710, 47759, 36476, 1611, 39758, 88, 1427, 7706, 2162, 29968, 27725, 11, 6775, 2838, 360, 35403, 546, 1571, 8741, 5983, 13, 51220], "temperature": 0.0, "avg_logprob": -0.11508578062057495, "compression_ratio": 1.4860557768924303, "no_speech_prob": 0.004972819238901138}, {"id": 259, "seek": 83084, "start": 847.96, "end": 849.72, "text": " To doskona\u0142a metafora.", "tokens": [51220, 1407, 4491, 74, 4037, 5024, 1131, 2792, 3252, 13, 51308], "temperature": 0.0, "avg_logprob": -0.11508578062057495, "compression_ratio": 1.4860557768924303, "no_speech_prob": 0.004972819238901138}, {"id": 260, "seek": 83084, "start": 849.72, "end": 856.0400000000001, "text": " Nie chodzi o to, by mie\u0107 najwi\u0119kszy m\u00f3zg, ale by mie\u0107 najlepiej wytrenowany m\u00f3zg.", "tokens": [51308, 12016, 23998, 277, 281, 11, 538, 35612, 48636, 1694, 1229, 32515, 89, 70, 11, 6775, 538, 35612, 41903, 39699, 261, 4328, 1095, 23341, 32515, 89, 70, 13, 51624], "temperature": 0.0, "avg_logprob": -0.11508578062057495, "compression_ratio": 1.4860557768924303, "no_speech_prob": 0.004972819238901138}, {"id": 261, "seek": 85604, "start": 856.04, "end": 863.56, "text": " Ksi\u0119cila to dow\u00f3d, \u017ce w AI liczy si\u0119 nie tylko architektura, ale przede wszystkim do\u015bwiadczenie, czyli dane.", "tokens": [50364, 591, 82, 5034, 66, 7371, 281, 9459, 17081, 11, 3561, 261, 7318, 6169, 1229, 3244, 2838, 13219, 3912, 642, 2320, 2991, 11, 6775, 44786, 30481, 46661, 39043, 11, 16591, 49206, 13, 50740], "temperature": 0.0, "avg_logprob": -0.1561959538146527, "compression_ratio": 1.4236111111111112, "no_speech_prob": 0.40473151206970215}, {"id": 262, "seek": 85604, "start": 863.56, "end": 868.5999999999999, "text": " Na koniec zostawmy tak\u0105 prowokuj\u0105c\u0105 my\u015bl, kt\u00f3ra wy\u0142ania si\u0119 wprost z tej pracy.", "tokens": [50740, 6056, 5897, 35733, 31873, 1607, 2226, 31069, 45553, 453, 13263, 32557, 452, 19212, 11, 19456, 4628, 1221, 5609, 3244, 261, 1424, 555, 710, 12573, 35591, 13, 50992], "temperature": 0.0, "avg_logprob": -0.1561959538146527, "compression_ratio": 1.4236111111111112, "no_speech_prob": 0.40473151206970215}, {"id": 263, "seek": 85604, "start": 868.5999999999999, "end": 874.36, "text": " Artyku\u0142 dowodzi, \u017ce dane s\u0105 now\u0105 granic\u0105, nowym z\u0142otem, nowym polem wy\u015bcigu.", "tokens": [50992, 1587, 874, 5279, 1221, 9459, 14543, 11, 3561, 49206, 9015, 586, 1611, 9370, 299, 1611, 11, 586, 4199, 31614, 310, 443, 11, 586, 4199, 714, 10386, 4628, 1788, 66, 16397, 13, 51280], "temperature": 0.0, "avg_logprob": -0.1561959538146527, "compression_ratio": 1.4236111111111112, "no_speech_prob": 0.40473151206970215}, {"id": 264, "seek": 85604, "start": 874.36, "end": 878.12, "text": " Ale to rodzi fundamentalne, by\u0107 mo\u017ce znacznie trudniejsze pytanie.", "tokens": [51280, 9366, 281, 8685, 3992, 8088, 716, 11, 15069, 12034, 15397, 14875, 2766, 32007, 44258, 36610, 13, 51468], "temperature": 0.0, "avg_logprob": -0.1561959538146527, "compression_ratio": 1.4236111111111112, "no_speech_prob": 0.40473151206970215}, {"id": 265, "seek": 85604, "start": 878.12, "end": 881.24, "text": " I wiesz, to jest co\u015b, co sp\u0119dza sens, powiek badaczom.", "tokens": [51468, 286, 261, 15347, 11, 281, 3492, 19241, 11, 598, 637, 6298, 2394, 2923, 11, 3388, 414, 74, 1578, 14875, 298, 13, 51624], "temperature": 0.0, "avg_logprob": -0.1561959538146527, "compression_ratio": 1.4236111111111112, "no_speech_prob": 0.40473151206970215}, {"id": 266, "seek": 88124, "start": 881.24, "end": 889.64, "text": " Z jednej strony jeste\u015bmy okrok od niesamowitych prze\u0142om\u00f3w, z drugiej patrzymy na internet, kt\u00f3ry jest g\u0142\u00f3wnym \u017ar\u00f3d\u0142em tych danych i widzimy...", "tokens": [50364, 1176, 5232, 11794, 32406, 35928, 3133, 31621, 3611, 48100, 335, 305, 507, 339, 8325, 1221, 298, 3901, 11, 710, 47373, 1947, 13047, 2226, 1667, 4705, 11, 9913, 3492, 18117, 812, 895, 4199, 50212, 43678, 11126, 15180, 274, 34644, 741, 27486, 13189, 485, 50784], "temperature": 0.0, "avg_logprob": -0.1274247133642211, "compression_ratio": 1.4605263157894737, "no_speech_prob": 0.06495840102434158}, {"id": 267, "seek": 88124, "start": 889.64, "end": 893.4, "text": " C\u00f3\u017c, jako\u015b\u0107 bywa r\u00f3\u017cna. To jest prawdziwe wyzwanie.", "tokens": [50784, 41306, 1427, 11, 17123, 7753, 538, 4151, 19637, 629, 13, 1407, 3492, 41175, 3992, 826, 4628, 14406, 7155, 13, 50972], "temperature": 0.0, "avg_logprob": -0.1274247133642211, "compression_ratio": 1.4605263157894737, "no_speech_prob": 0.06495840102434158}, {"id": 268, "seek": 88124, "start": 893.4, "end": 903.24, "text": " Dok\u0142adnie. Skoro do trenowania przysz\u0142ych jeszcze pot\u0119\u017cniejszych modeli b\u0119dziemy potrzebowa\u0107 bilion\u00f3w token\u00f3w wysokiej jako\u015bci danych, to sk\u0105d je we\u017amiemy?", "tokens": [50972, 29768, 10358, 2766, 13, 7324, 10780, 360, 23136, 21308, 44018, 47655, 14168, 1847, 1274, 1427, 10402, 45021, 2316, 72, 31966, 37595, 11445, 8588, 313, 3901, 14862, 3901, 27062, 453, 7764, 17123, 6199, 274, 34644, 11, 281, 1110, 18962, 1506, 321, 10659, 25210, 2226, 30, 51464], "temperature": 0.0, "avg_logprob": -0.1274247133642211, "compression_ratio": 1.4605263157894737, "no_speech_prob": 0.06495840102434158}, {"id": 269, "seek": 88124, "start": 903.24, "end": 907.8, "text": " Jak b\u0119dziemy je pozyskiwa\u0107 i filtrowa\u0107 w spos\u00f3b odpowiedzialny?", "tokens": [51464, 15029, 31966, 1506, 21281, 749, 2984, 25234, 741, 1387, 6903, 11445, 261, 22904, 24314, 15338, 831, 1634, 30, 51692], "temperature": 0.0, "avg_logprob": -0.1274247133642211, "compression_ratio": 1.4605263157894737, "no_speech_prob": 0.06495840102434158}, {"id": 270, "seek": 90780, "start": 907.88, "end": 915.4, "text": " I jak unikniemy pu\u0142apek uprzedze\u0144, dezinformacji i toksyczno\u015bci, kt\u00f3re ju\u017c teraz s\u0105 ogromnym problemem?", "tokens": [50368, 286, 4207, 517, 1035, 2766, 2226, 2362, 5024, 32659, 493, 81, 11312, 49689, 11, 368, 23584, 837, 13152, 741, 281, 1694, 17466, 16438, 11, 8864, 10678, 16854, 9015, 34416, 298, 12996, 1154, 443, 30, 50744], "temperature": 0.0, "avg_logprob": -0.09154441412978286, "compression_ratio": 1.4365671641791045, "no_speech_prob": 0.09496276825666428}, {"id": 271, "seek": 90780, "start": 915.4, "end": 922.68, "text": " Artyku\u0142 o Chinczyli rozwi\u0105za\u0142 jeden, niezwykle wa\u017cny problem, problem optymalizacji obliczeniowej.", "tokens": [50744, 1587, 874, 5279, 1221, 277, 4430, 6522, 2081, 9544, 18234, 2394, 1221, 12906, 11, 33511, 9726, 14677, 27777, 1634, 1154, 11, 1154, 2427, 4199, 304, 590, 13152, 1111, 1050, 42124, 21091, 13, 51108], "temperature": 0.0, "avg_logprob": -0.09154441412978286, "compression_ratio": 1.4365671641791045, "no_speech_prob": 0.09496276825666428}, {"id": 272, "seek": 90780, "start": 922.68, "end": 933.16, "text": " Ale w zamian, jak na d\u0142oni, pokaza\u0142 nam kolejny, znacznie g\u0142\u0119bszy problem, logistyczny i ostatecznie etyczny, z kt\u00f3rym ca\u0142a dziedzina b\u0119dzie musia\u0142a si\u0119 zmierzy\u0107.", "tokens": [51108, 9366, 261, 19876, 952, 11, 4207, 1667, 44042, 17049, 11, 13010, 12257, 1221, 8835, 23749, 1634, 11, 15397, 14875, 2766, 18117, 1274, 929, 1229, 1154, 11, 3565, 468, 17466, 1634, 741, 277, 15406, 19923, 1030, 17466, 1634, 11, 710, 30120, 1335, 5024, 9758, 15338, 1426, 10562, 1038, 25605, 3244, 17020, 811, 27150, 13, 51632], "temperature": 0.0, "avg_logprob": -0.09154441412978286, "compression_ratio": 1.4365671641791045, "no_speech_prob": 0.09496276825666428}, {"id": 273, "seek": 93316, "start": 933.16, "end": 940.36, "text": " Znalezienie tych danych i upewnienie si\u0119, \u017ce s\u0105 one dobre to mo\u017ce by\u0107 najtrudniejsze zadanie dla AI w tej dekadzie.", "tokens": [50364, 1176, 77, 37646, 27385, 15180, 274, 34644, 741, 493, 68, 895, 27385, 3244, 11, 3561, 9015, 472, 41959, 281, 12034, 15069, 11212, 6903, 532, 44258, 42788, 7155, 12285, 7318, 261, 12573, 368, 34985, 3283, 13, 50724], "temperature": 0.0, "avg_logprob": -0.1342838814384059, "compression_ratio": 1.0619469026548674, "no_speech_prob": 0.4452976882457733}], "language": "pl"}