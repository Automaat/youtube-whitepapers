{"text": " Jak w\u0142a\u015bciwie sprawdzi\u0107, kt\u00f3ry model j\u0119zykowy jest, no, lepszy od innego. Przed prac\u0105, kt\u00f3rej si\u0119 dzisiaj przyjrzymy, to by\u0142o troch\u0119 jak pr\u00f3ba por\u00f3wnania dw\u00f3ch, no wiesz, wybitnych sportowc\u00f3w, kt\u00f3rzy nigdy nie startowali w tej samej dyscyplinie. Jeden to mistrz tenisa, drugi p\u0142ywania. Oba\u0144 \u015bwietni, ale kto jest lepszy? No w\u0142a\u015bnie, w \u015bwiecie AI panowa\u0142 dok\u0142adnie taki chaos. To jest idealne por\u00f3wnanie, naprawd\u0119. \u0179r\u00f3d\u0142a podaj\u0105, \u017ce w tych oryginalnych publikacjach, kt\u00f3re opisywa\u0142y dwa czo\u0142owe modele. Na przyk\u0142ad. No powiedzmy, T5 od Google'a i model od Entropic. Nie by\u0142o tam ani jednego wsp\u00f3lnego testu. Ani jednego. Ani jednego. To tak, jakby producenci samochod\u00f3w chwalili si\u0119 osi\u0105gami, ale jeden mierzy\u0142 przyspieszenie do setki, a drugi zu\u017cycie paliwa. Por\u00f3wnanie by\u0142o po prostu niemo\u017cliwe. Dlatego dzisiaj zanurzymy si\u0119 w temat monumentalnej pr\u00f3by uporz\u0105dkowania tego ba\u0142aganu. To projekt z Stanford'a o nazwie Helm. Czyli holistyczna ocena modeli j\u0119zykowych. W zasadzie to jest pr\u00f3ba stworzenia takich pierwszych, prawdziwie ustandaryzowanych i grzysk olimpijskich dla sztucznej inteligencji. A naszym celem jest zrozumienie, co sprawia, \u017ce Helm jest tak prze\u0142omowy. Jak on dzia\u0142a i co chyba najciekawsze, jakie absolutnie zaskakuj\u0105ce prawdy o 30 wiod\u0105cych modelach na \u015bwiecie ujawni\u0142. A podobno niekt\u00f3re z nich wywracaj\u0105 do g\u00f3ry nogami to, co my\u015bleli\u015bmy, \u017ce wiemy o AI. Oj tak. Dobrze, to zacznijmy od podstaw. Czym w\u0142a\u015bciwie jest Helm? Bo to z tego, co czyta\u0142em, nie jest po prostu kolejny, nudny benchmark z list\u0105 wynik\u00f3w. Absolutnie nie. W materia\u0142ach autorzy ci\u0105gle podkre\u015blaj\u0105, \u017ce to ca\u0142a filozofia ewaluacji. Filozofia. Dok\u0142adnie. S\u0142owo holistyczna w nazwie jest tutaj kluczowe. Zamiast skupia\u0107 si\u0119 na jednym w\u0105skim zadaniu, jak, no nie wiem, t\u0142umaczenie zda\u0144, postanowili spojrze\u0107 na problem z lotoptaka. I ta wizja opiera si\u0119 na trzech chwilarach. Pierwszy to szeroki zakres i co r\u00f3wnie wa\u017cne, \u015bwiadomo\u015b\u0107 w\u0142asnych brak\u00f3w. O, to mi si\u0119 bardzo spodoba\u0142o. Nie udaj\u0105, \u017ce zbadali wszystko. Prawda? Zmapowali\u015bmy ogromny obszar od odpowiadania na pytania, postrzeszczanie, ale o to, gdzie wci\u0105\u017c s\u0105 bia\u0142y plamy. Np. przyznaj\u0105, \u017ce brakuje im test\u00f3w dla rzadziej u\u017cywanych dialekt\u00f3w angielskiego. Nie m\u00f3wi\u0105c ju\u017c o innych j\u0119zykach, to buduje wiarygodno\u015b\u0107. Niezwykle. A drugi filar to podej\u015bcie, kt\u00f3re nazywaj\u0105 wielomiarowym. I to jest wiesz, fundamentalna zmiana. Wielometryczne, czyli do tej pory w AI kr\u00f3lowa\u0142a jedna metryka. Accuracy, dok\u0142adno\u015b\u0107. No tak, autorze Helm stwierdzili, to nie wystarczy. Model, kt\u00f3ry jest dok\u0142adny, ale jednocze\u015bnie, no nie wiem, toksyczny, albo skrajnie stroniczy jest bezu\u017cyteczny. A nawet szkowliwy. Czyli dla ka\u017cdego zadania mierzyli nie jedn\u0105, a ca\u0142y zestaw cech. Tak, a\u017c siedem metryk. I to dla ka\u017cdego z 16 g\u0142\u00f3wnych scenariuszy. Obok Accuracy pojawi\u0142y si\u0119 Calibration, Robustness, Fairness, Bias. Zatrzymajmy si\u0119 tu na chwila, bo to wa\u017cne. Robustness to w zasadzie test na to, czy model spanikuje, jak zrobili te r\u00f3wk\u0119 w pytaniu. Dok\u0142adnie tak. Albo czy kompletnie zmieni odpowied\u017a, je\u015bli przestawisz szyk zdania? Z kolei Calibration to jest fascynuj\u0105ce. Mierzy jak dobrze model zna samego siebie. Czyli czy wie, kiedy nie wie? W\u0142a\u015bnie. Czy kiedy m\u00f3wi, \u017ce jest czego\u015b pewien na 99% to faktycznie ma racj\u0119 w 99% przypadk\u00f3w, czy mo\u017ce jest nadmiernie pewne siebie? To kluczowe w zastosowaniach medycznych czy prawnych. Wole model, kt\u00f3ry m\u00f3wi nie wiem ni\u017c taki, co z pe\u0142nym przekonaniem podaje b\u0142\u0105dn\u0105 diagnoz\u0119. A trzeci filar? To podobno ten, kt\u00f3ry wywo\u0142a\u0142 rewolucj\u0119. Standardyzacja. To jest serce ca\u0142ego projektu. Wzi\u0119li 30 prominentnych modeli od 12 r\u00f3\u017cnych firm AI21 Labs, Google, Meta, OpenAI, Microsoft. Wszystkich du\u017cych graczy. Tak. I wsadzili je do tego samego laboratorium. Wszystkie musia\u0142y rozwi\u0105zywa\u0107 te same zadania w tych samych warunkach. U\u017cyto te\u017c tej samej metody adaptacji, czyli few shot prompting. Czyli pokazano ka\u017cdemu modelowi kilka przyk\u0142ad\u00f3w, \u017ceby zrozumia\u0142, o co chodzi. Tele oceniano \u015brednio na zaledwie 17 i 9 procenta tych kluczowych scenariuszy. A po ich pracy. Pokrycie wzros\u0142o do 96 procenta. Wow. Z chaosu przeszli\u015bmy do uporz\u0105dkowanej tabeli wynik\u00f3w. Nagle okaza\u0142o si\u0119, \u017ce wszyscy ci sportowcy nie tylko stan\u0119li na starcie tej samej olimpiady, ale wzi\u0119li udzia\u0142 w pe\u0142nym siedmioboju. Kt\u00f3ry testuje nie tylko szybko\u015b\u0107, ale i si\u0142\u0119, zwinno\u015b\u0107 i uczciw\u0105 gr\u0119. W\u0142a\u015bnie. I kiedy ju\u017c ustawiono te same regu\u0142y gry dla wszystkich, wyniki okaza\u0142y si\u0119, c\u00f3\u017c, pe\u0142ne niespodziadek. Przejd\u017amy do tych najbardziej zaskakuj\u0105cych odkry\u0107. Co jako pierwsze rzuca si\u0119 w oczy? Pierwsze i by\u0107 mo\u017ce najwa\u017cniejsze z perspektywy ca\u0142ego ekosystemu AI jest odkrycie dotycz\u0105ce dost\u0119pu. Dost\u0119p. Tak. Istnieje wyra\u017cne i konsekwentna przepa\u015b\u0107 w wydajno\u015bci mi\u0119dzy modelami open source, kt\u00f3re ka\u017cdy mo\u017ce pobra\u0107, a modelami o ograniczonym dost\u0119pie, tymi zamkni\u0119cymi, jak Text Davinci 002 od OpenAI czy TNLGV2 od Microsoftu. Czyli najlepsze modele wci\u0105\u017c s\u0105 za zamkni\u0119tymi d\u017awiami. Dost\u0119pno\u015b\u0107 ma swoj\u0105 cen\u0119 pod wzgl\u0119dem surowej wydajno\u015bci, ale czy ta wydajno\u015b\u0107 to prosta liniowa skala, \u017ce model X jest po prostu lepszy od modelu Y we wszystkim? Absolutnie nie. I to jest drugie wielkie odkrycie. Relacje mi\u0119dzy r\u00f3\u017cnymi metrykami s\u0105 niezwykle skomplikowane i cz\u0119sto sprzeczne z intuicj\u0105. Aha. We\u017amy na przyk\u0142ad ten zwi\u0105zek mi\u0119dzy dok\u0142adno\u015bci\u0105, czyli accuracy, a kalibracj\u0105, czyli calibration, o kt\u00f3rej m\u00f3wili\u015bmy. Czyli mi\u0119dzy poprawno\u015bci\u0105 odpowiedzi, a samo ocen\u0105 modelu. Dok\u0142adnie. W scenariuszu Hellaswek, kt\u00f3ry testuje takie zdroworoszcz\u0105tkowe my\u015blenie, okaza\u0142o si\u0119, \u017ce im dok\u0142adniejszy stawa\u0142 si\u0119 model, tym jego kalibracja si\u0119 pogarsza\u0142a. Pogarsza\u0142a? Tak. Stawa\u0142 si\u0119 bardziej pewny siebie, ale jednocze\u015bnie cz\u0119\u015bciej myli\u0142 si\u0119 w ocenie tej pewno\u015bci. To brzmi jak cyfrowa wersja efektu Daninga Krugera. Im model by\u0142 lepszy, tym gorsze w ocenie w\u0142asnych mo\u017cliwo\u015bci. To troch\u0119 niepokoj\u0105ce. Jest. Ale \u017ceby by\u0142o jeszcze ciekawiej, w innym scenariuszu OpenBook QA, opartym na faktach, zaobserwowano dok\u0142adnie odwrotn\u0105 zale\u017cno\u015b\u0107. Czyli? Tam poprawa dok\u0142adno\u015bci polepsza\u0142a kalibracj\u0119. Aha. To pokazuje, \u017ce nie ma jednej prostej regu\u0142y. Zachowanie modelu jest silnie uzale\u017cnione od kontekstu zadania. Nie mo\u017cna powiedzie\u0107, ten model ma dobr\u0105 kalibracj\u0119. Trzeba zapyta\u0107, dobr\u0105 kalibracj\u0119? W jakim zadaniu? To ju\u017c jest ciekawe, ale jest jeszcze jedno odkrycie, kt\u00f3re wydaje mi si\u0119 kompletnie wbrew intuicji. Chodzi o modele trenowane na kodzie programistycznym. Tak, na pierwszy rzod oka mo\u017cna by pomy\u015ble\u0107, \u017ce b\u0119d\u0105 dobre w, no c\u00f3\u017c, w pisaniu kodu. I tyle. Ale okazuje si\u0119, \u017ce ta umiej\u0119tno\u015b\u0107 przenosi si\u0119 na co\u015b znacznie szerszego. Na co\u015b, co z pozoru nie ma z kodem nic wsp\u00f3lnego. Na przyk\u0142ad na rozumowanie w j\u0119zyku naturalnym? Dok\u0142adnie. Wzi\u0119li model kod Da Vinci 002 zoptymalizowany pod k\u0105tem kodu i postawili go przed testem matematycznym GSM 8K. To s\u0105 te zadania tekstowe, jak ze szko\u0142y typu jasio ma pi\u0119\u0107 jab\u0142ek? Tak, dok\u0142adnie. I ten model, trenowany na Pythonie i Javascriptie, po prostu zmiarzy\u0142 najlepszy model czysto tekstowy, czyli tekst Da Vinci 002. Zmiarzy\u0142, jak du\u017ca by\u0142a ta r\u00f3\u017cnica. Ogromna. Kod Da Vinci 002 osi\u0105gn\u0105\u0142 52,1% dok\u0142adno\u015bci, podczas gdy jego tekstowy odpowiednik z tej samej rodziny tylko 35%. To nie jest b\u0142\u0105d statystyczny, to jest przepa\u015b\u0107. Zdecydowanie. Wygl\u0105da na to, \u017ce nauka tej \u015bcis\u0142ej, logicznej struktury kodu uczy model pewnego rodzaju abstrakcyjnego rozumowania. Kt\u00f3re mo\u017ce potem zastosowa\u0107 w zupe\u0142nie innej domenie. Niesamowite. To tak, jakby nauka \u0142aciny, czyni\u0142a ci\u0119 lepszym w rozwi\u0105zywaniu zagadek logicznych. Co\u015b w tym jest. Ale jest jeszcze jedno odkrecie, kt\u00f3re w materia\u0142ach \u017ar\u00f3d\u0142owych jest opisane jako fundamentalne wyzwanie dla ca\u0142ej dziedziny. To, jak niewiarygodnie wra\u017cliwe s\u0105 te modele na sam spos\u00f3b, w jaki zadajemy im pytania. Och, to jest chyba najbardziej szokuj\u0105cy w wyniku ca\u0142ego badania. Podwa\u017ca wiele naszych podstawowych za\u0142o\u017ce\u0144 o tym, co my w og\u00f3le mierzymy. Daj jaki\u015b przyk\u0142ad. Dobrze, test wielokrotnego wyboru Hellaswag i model OPTI-175B od mety. Kiedy przedstawiono mu zadanie w jednym formacie, daj\u0105c pocz\u0105tek zdania i ka\u017cd\u0105 z czterech mo\u017cliwych odpowiedzi osobno. Tak. Prosz\u0105c o ocen\u0119 prawdopodobie\u0144stwa ka\u017cdej z nich? Tak. Model osi\u0105gn\u0105\u0142 imponuj\u0105ce 79,1% dok\u0142adno\u015bci. Czyli radzi\u0142 sobie \u015bwietnie. Ale co si\u0119 sta\u0142o, gdy zmieniono tylko format pytania, nie jego tre\u015b\u0107? Gdy te same odpowiedzi po\u0142\u0105czono w jeden prompt, tak jest to wygl\u0105da na klasycznym egzaminie w szkole. Oto pytanie, a oto opcj\u0119 A, B, C, D. Wybierz poprawn\u0105. Dok\u0142adnie tak. Jego dok\u0142adno\u015b\u0107 spad\u0142a na \u0142eb na szyj\u0105. Do 30 i 2%. Czekaj, czekaj. Sprawie 80% do 30. Przecie\u017c to jest poni\u017cej progu losowego zgadywania. Przy czterech opcjach to 25%. W\u0142a\u015bnie. Ten sam model, ta sama wiedza, to samo zadanie. Drobna zmiana w formatowaniu pytania spowodowa\u0142a katastrofalny, kompletny kolaps wydajno\u015bci. To tak jakby Einstein nie potrafi\u0142 rozwi\u0105za\u0107 prostego zadania z fizyki, bo kto\u015b zapisa\u0142 je inn\u0105 czcionk\u0105. Ta analogia jest w\u0142a\u015bciwie przera\u017caj\u0105ca. Bo to by oznacza\u0142o, \u017ce ca\u0142a nasza metoda testowania jest fundamentalnie wadliwa. Mo\u017ce nie mierzymy wiedzy czy rozumowania, tylko zdolno\u015b\u0107 modelu do dopasowania si\u0119 do specyficznego szablonu. I to jest w\u0142a\u015bnie pytanie za milion dolar\u00f3w, kt\u00f3re stawia helm, co prowadzi nas prosto do szerszych implikacji. Co te wszystkie odkrycia oznaczaj\u0105 w praktyce? No w\u0142a\u015bnie, co to wszystko oznacza? Pierwszy wniosek, jaki mi si\u0119 nasuwa to, \u017ce ta obsesja na punkcie wielko\u015bci modeli, liczby parametr\u00f3w, jest chyba troch\u0119 przysadzona. Jest. A przynajmniej nie jest to jedyny czynnik. Helm pokazuje, \u017ce cho\u0107 w ramach jednej rodziny modeli, na przyk\u0142ad GPT-3 wi\u0119kszy zazwyczaj znaczy lepszy, to w por\u00f3wnaniach mi\u0119dzy r\u00f3\u017cnymi rodzinami ta zasada ju\u017c nie obowi\u0105zuje. Dok\u0142adnie. Wszystkie najskuteczniejsze modele mia\u0142y co najmniej 50 miliard\u00f3w parametr\u00f3w, to prawda. Ale niekt\u00f3re z absolutnie najlepszych jak Anthropix, LMV4S3 z 52 miliardami regularnie pokonywa\u0142y znacznie wi\u0119kszych konkurent\u00f3w. Czyli musz\u0105 istnie\u0107 inne, kluczowe sk\u0142adniki tej tajemniczej receptury? Jakie? Dwa wydaj\u0105 si\u0119 najwa\u017cniejsze. Po pierwsze, dane treningowe. Ale po drugie i mo\u017ce nawet wa\u017cniejsze, metody dostrajania po g\u0142\u00f3wnym treningu. Modele, kt\u00f3re przesz\u0142y przez procesy takie jak Instruction Tuning czy RLHF. Czyli Reinforcement Learning from Human Feedback. Tak. Czyli w uproszczeniu. By\u0142y wychowywane przez ludzi, kt\u00f3rzy oceniali ich odpowiedzi i uczyli je co jest dobre, a co z\u0142e. Dok\u0142adnie. I te modele jak w\u0142a\u015bnie ten od Anthropix czy tekst DaVinci 002 cz\u0119sto wykazuj\u0105 znacznie lepsze zdolno\u015bci. Nawet je\u015bli s\u0105 mniejsze okazuje si\u0119, \u017ce jako\u015b\u0107 tego wychowania mo\u017ce by\u0107 wa\u017cniejsza ni\u017c surowa wielko\u015b\u0107 m\u00f3zgu. A wracaj\u0105c do tego problemu z formatowaniem pyta\u0144. Jakie to ma implikacje dla nas jako u\u017cytkownik\u00f3w czy developer\u00f3w? Ogromne. To prowadzi do wniosku, \u017ce najlepszy model na rynku mo\u017ce by\u0107 po prostu tym, kt\u00f3ry najlepiej pasuje do naszego konkretnego stylu zadawania pyta\u0144. Czyli u\u017cyteczno\u015b\u0107 modelu w realnym \u015bwiecie mo\u017ce zale\u017cy\u0107 nie od jego miejsca w rankingu. Ale od tego jak intuicyjnie potrafi on interpretowa\u0107 ludzkie intencje. By\u0107 mo\u017ce w przysz\u0142o\u015bci nie b\u0119dziemy wybiera\u0107 modeli na podstawie ich mocy, ale na podstawie ich charakteru. To stawia ca\u0142\u0105 dziedzin\u0119 prompt engineeringu w zupe\u0142nie nowym \u015bwietle. To nie jest tylko technika, ale niemal sztuka komunikacji z nieludzk\u0105 inteligencj\u0105. Mhm. Ale sam Helm te\u017c nie jest idealny. Autorzy otwarcie m\u00f3wi\u0105 o jego ograniczeniach. Tak i to jest bardzo uczciwe z ich strony. Podkre\u015blaj\u0105, \u017ce Helm to \u017cywy benchmark. Projekt, kt\u00f3ry b\u0119dzie stale rozwijany. Sami wskazuj\u0105 na jego braki. Najwi\u0119ksze to oczywi\u015bcie dominacja j\u0119zyka angielskiego. Jasne. Ale podnosz\u0105 te\u017c inne, znacznie powa\u017cniejsze problem. Co\u015b co jest jak tykaj\u0105ca bomba pod ca\u0142\u0105 polembada\u0144 nad EI. M\u00f3wisz o data contamination. Tak. Czyli o ryzyku, \u017ce modele, kt\u00f3re testujemy mog\u0142y by\u0107 trenowane na danych testowych. Innymi s\u0142owy, \u017ce model widzia\u0142 pytania z egzaminu, zanim do niego przyst\u0105pi\u0142. Je\u015bli tak, to ca\u0142y test jest niewa\u017cne. Dok\u0142adnie. Jak du\u017cy jest to problem? Da si\u0119 to w og\u00f3le sprawdzi\u0107? I tu jest pies pogrzebany. Poniewa\u017c dane treningowe wielu czo\u0142owych modeli s\u0105 \u015bci\u015ble strze\u017con\u0105 tajemnic\u0105, pe\u0142na weryfikacja jest niemo\u017cliwa. Aha. Autorzy Helm przeprowadzili pewne analizy, ale nie s\u0105 w stanie da\u0107 gwarancji. To jest fundamentalne wyzwanie, kt\u00f3re wymaga od tw\u00f3rc\u00f3w modeli znacznie wi\u0119kszej transparentno\u015bci. Bez tego nasza ewaluacja opiera si\u0119 na wierze, a nie na faktach. Podsumowuj\u0105c, projekt Helm to bez w\u0105tpienia krok milowy. Wprowadzi\u0142 bardzo potrzebny porz\u0105dek w tym chaotycznym \u015bwiecie oceny EI, ujawni\u0142, \u017ce ecures i to tylko wierzcho\u0142ek g\u00f3ry lodowej. \u017be takie detale, jak w formatowanie pyta\u0144, maj\u0105 gigantyczny, wcze\u015bniej niedoceniany wp\u0142yw na wyniki. Ale ta praca pozostawia nas te\u017c z jednym fundamentalnym i troch\u0119 niepokoj\u0105cym pytaniem. Skoro wydajno\u015b\u0107 pot\u0119\u017cnego modelu mo\u017ce si\u0119 kompletnie za\u0142ama\u0107 z powodu drobnej zmiany w formacie pytania, to co my tak naprawd\u0119 mierzymy? Czy jest to autentyczna zdolno\u015b\u0107 modelu do rozumowania i jego wiedz\u0119 o \u015bwiecie? Czy tylko jego niesamowicie rozwini\u0119ta, ale powierzchowna umiej\u0119tno\u015b\u0107 dopasowania si\u0119 do naszego specyficznego? Mo\u017ce nawet arbitralnego sposobu zadawania pyta\u0144? W\u0142a\u015bnie. To, \u017ce odpowiada poprawnie tylko wtedy, gdy pytanie jest sformu\u0142owane w jeden konkretny spos\u00f3b naprawd\u0119 rozumie problem, a mo\u017ce po prostu nauczy\u0142 si\u0119 niezwykle zaawansowanego, ale wci\u0105\u017c \u015blepego dopasowywania wzorc\u00f3w? Helm nie daje na to ostatecznej odpowiedzi. Ale? Pytania w precyzyjny i systematyczny spos\u00f3b i to mo\u017ce by\u0107 jego najwi\u0119kszym wk\u0142adem.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.9, "text": " Jak w\u0142a\u015bciwie sprawdzi\u0107, kt\u00f3ry model j\u0119zykowy jest, no, lepszy od innego.", "tokens": [50364, 15029, 50108, 46192, 28496, 11, 9913, 2316, 49055, 74, 10089, 3492, 11, 572, 11, 476, 1878, 1229, 3611, 294, 11858, 13, 50609], "temperature": 0.0, "avg_logprob": -0.17025355635018186, "compression_ratio": 1.4648829431438126, "no_speech_prob": 0.01888909563422203}, {"id": 1, "seek": 0, "start": 5.1000000000000005, "end": 9.3, "text": " Przed prac\u0105, kt\u00f3rej si\u0119 dzisiaj przyjrzymy, to by\u0142o troch\u0119 jak pr\u00f3ba por\u00f3wnania dw\u00f3ch,", "tokens": [50619, 2114, 11312, 22404, 1611, 11, 36023, 3244, 25772, 6501, 73, 13047, 2226, 11, 281, 14811, 24926, 4207, 8565, 4231, 1515, 812, 895, 5609, 27379, 812, 339, 11, 50829], "temperature": 0.0, "avg_logprob": -0.17025355635018186, "compression_ratio": 1.4648829431438126, "no_speech_prob": 0.01888909563422203}, {"id": 2, "seek": 0, "start": 9.5, "end": 14.1, "text": " no wiesz, wybitnych sportowc\u00f3w, kt\u00f3rzy nigdy nie startowali w tej samej dyscyplinie.", "tokens": [50839, 572, 261, 15347, 11, 4628, 5260, 9399, 7282, 305, 29268, 11, 25382, 26996, 3173, 2838, 722, 305, 5103, 261, 12573, 912, 73, 15243, 1344, 48102, 414, 13, 51069], "temperature": 0.0, "avg_logprob": -0.17025355635018186, "compression_ratio": 1.4648829431438126, "no_speech_prob": 0.01888909563422203}, {"id": 3, "seek": 0, "start": 14.9, "end": 17.3, "text": " Jeden to mistrz tenisa, drugi p\u0142ywania.", "tokens": [51109, 508, 6876, 281, 3544, 19390, 2064, 3837, 11, 4110, 72, 280, 6825, 86, 5609, 13, 51229], "temperature": 0.0, "avg_logprob": -0.17025355635018186, "compression_ratio": 1.4648829431438126, "no_speech_prob": 0.01888909563422203}, {"id": 4, "seek": 0, "start": 17.7, "end": 20.1, "text": " Oba\u0144 \u015bwietni, ale kto jest lepszy?", "tokens": [51249, 422, 4231, 5248, 8299, 39083, 3722, 11, 6775, 23780, 3492, 476, 1878, 1229, 30, 51369], "temperature": 0.0, "avg_logprob": -0.17025355635018186, "compression_ratio": 1.4648829431438126, "no_speech_prob": 0.01888909563422203}, {"id": 5, "seek": 0, "start": 20.3, "end": 24.900000000000002, "text": " No w\u0142a\u015bnie, w \u015bwiecie AI panowa\u0142 dok\u0142adnie taki chaos.", "tokens": [51379, 883, 14234, 11, 261, 40078, 4260, 7318, 2462, 30105, 45864, 2766, 20065, 14158, 13, 51609], "temperature": 0.0, "avg_logprob": -0.17025355635018186, "compression_ratio": 1.4648829431438126, "no_speech_prob": 0.01888909563422203}, {"id": 6, "seek": 0, "start": 25.1, "end": 27.6, "text": " To jest idealne por\u00f3wnanie, naprawd\u0119.", "tokens": [51619, 1407, 3492, 7157, 716, 1515, 812, 895, 7155, 11, 20970, 13, 51744], "temperature": 0.0, "avg_logprob": -0.17025355635018186, "compression_ratio": 1.4648829431438126, "no_speech_prob": 0.01888909563422203}, {"id": 7, "seek": 2760, "start": 27.8, "end": 33.2, "text": " \u0179r\u00f3d\u0142a podaj\u0105, \u017ce w tych oryginalnych publikacjach, kt\u00f3re opisywa\u0142y dwa czo\u0142owe modele.", "tokens": [50374, 4423, 117, 43678, 5024, 2497, 11133, 11, 3561, 261, 15180, 420, 88, 1494, 304, 9399, 11227, 1035, 326, 45059, 11, 8864, 999, 14169, 4151, 6825, 35045, 269, 4765, 1221, 6880, 4391, 306, 13, 50644], "temperature": 0.0, "avg_logprob": -0.137430899852031, "compression_ratio": 1.4270462633451957, "no_speech_prob": 0.0017498889937996864}, {"id": 8, "seek": 2760, "start": 33.4, "end": 34.2, "text": " Na przyk\u0142ad.", "tokens": [50654, 6056, 23144, 13, 50694], "temperature": 0.0, "avg_logprob": -0.137430899852031, "compression_ratio": 1.4270462633451957, "no_speech_prob": 0.0017498889937996864}, {"id": 9, "seek": 2760, "start": 34.4, "end": 38.5, "text": " No powiedzmy, T5 od Google'a i model od Entropic.", "tokens": [50704, 883, 27617, 2226, 11, 314, 20, 3611, 3329, 6, 64, 741, 2316, 3611, 3951, 39173, 13, 50909], "temperature": 0.0, "avg_logprob": -0.137430899852031, "compression_ratio": 1.4270462633451957, "no_speech_prob": 0.0017498889937996864}, {"id": 10, "seek": 2760, "start": 38.7, "end": 41.2, "text": " Nie by\u0142o tam ani jednego wsp\u00f3lnego testu.", "tokens": [50919, 12016, 14811, 7677, 40477, 5232, 11858, 47148, 11858, 1500, 84, 13, 51044], "temperature": 0.0, "avg_logprob": -0.137430899852031, "compression_ratio": 1.4270462633451957, "no_speech_prob": 0.0017498889937996864}, {"id": 11, "seek": 2760, "start": 41.400000000000006, "end": 42.300000000000004, "text": " Ani jednego.", "tokens": [51054, 1107, 72, 5232, 11858, 13, 51099], "temperature": 0.0, "avg_logprob": -0.137430899852031, "compression_ratio": 1.4270462633451957, "no_speech_prob": 0.0017498889937996864}, {"id": 12, "seek": 2760, "start": 42.5, "end": 43.300000000000004, "text": " Ani jednego.", "tokens": [51109, 1107, 72, 5232, 11858, 13, 51149], "temperature": 0.0, "avg_logprob": -0.137430899852031, "compression_ratio": 1.4270462633451957, "no_speech_prob": 0.0017498889937996864}, {"id": 13, "seek": 2760, "start": 43.5, "end": 50.3, "text": " To tak, jakby producenci samochod\u00f3w chwalili si\u0119 osi\u0105gami, ale jeden mierzy\u0142 przyspieszenie do setki, a drugi zu\u017cycie paliwa.", "tokens": [51159, 1407, 991, 11, 28976, 1082, 13037, 537, 3247, 8997, 378, 3901, 26237, 304, 2312, 3244, 3003, 11404, 70, 4526, 11, 6775, 12906, 47448, 1229, 1221, 6541, 749, 79, 530, 16778, 360, 992, 2984, 11, 257, 4110, 72, 2164, 7735, 4260, 3984, 72, 4151, 13, 51499], "temperature": 0.0, "avg_logprob": -0.137430899852031, "compression_ratio": 1.4270462633451957, "no_speech_prob": 0.0017498889937996864}, {"id": 14, "seek": 2760, "start": 50.5, "end": 52.5, "text": " Por\u00f3wnanie by\u0142o po prostu niemo\u017cliwe.", "tokens": [51509, 5269, 812, 895, 7155, 14811, 714, 19518, 2838, 3280, 1427, 2081, 826, 13, 51609], "temperature": 0.0, "avg_logprob": -0.137430899852031, "compression_ratio": 1.4270462633451957, "no_speech_prob": 0.0017498889937996864}, {"id": 15, "seek": 5250, "start": 52.6, "end": 58.4, "text": " Dlatego dzisiaj zanurzymy si\u0119 w temat monumentalnej pr\u00f3by uporz\u0105dkowania tego ba\u0142aganu.", "tokens": [50369, 47184, 25772, 710, 282, 374, 1229, 2226, 3244, 261, 32954, 43105, 11794, 8565, 2322, 493, 284, 23876, 74, 21308, 8627, 4773, 1221, 14167, 84, 13, 50659], "temperature": 0.0, "avg_logprob": -0.12235807078753332, "compression_ratio": 1.4259259259259258, "no_speech_prob": 0.010681337676942348}, {"id": 16, "seek": 5250, "start": 58.6, "end": 60.9, "text": " To projekt z Stanford'a o nazwie Helm.", "tokens": [50669, 1407, 26261, 710, 20374, 6, 64, 277, 20151, 8699, 6128, 76, 13, 50784], "temperature": 0.0, "avg_logprob": -0.12235807078753332, "compression_ratio": 1.4259259259259258, "no_speech_prob": 0.010681337676942348}, {"id": 17, "seek": 5250, "start": 61.1, "end": 63.9, "text": " Czyli holistyczna ocena modeli j\u0119zykowych.", "tokens": [50794, 37099, 4091, 468, 17466, 629, 10409, 4118, 2316, 72, 49055, 74, 19605, 13, 50934], "temperature": 0.0, "avg_logprob": -0.12235807078753332, "compression_ratio": 1.4259259259259258, "no_speech_prob": 0.010681337676942348}, {"id": 18, "seek": 5250, "start": 64.1, "end": 71.6, "text": " W zasadzie to jest pr\u00f3ba stworzenia takich pierwszych, prawdziwie ustandaryzowanych i grzysk olimpijskich dla sztucznej inteligencji.", "tokens": [50944, 343, 44585, 3283, 281, 3492, 8565, 4231, 342, 28321, 14320, 29607, 34016, 339, 11, 41175, 3992, 8699, 26189, 474, 822, 89, 23341, 339, 741, 677, 89, 749, 74, 2545, 8814, 1718, 5161, 480, 12285, 262, 2682, 1311, 89, 11794, 24777, 3213, 19649, 13, 51319], "temperature": 0.0, "avg_logprob": -0.12235807078753332, "compression_ratio": 1.4259259259259258, "no_speech_prob": 0.010681337676942348}, {"id": 19, "seek": 5250, "start": 71.8, "end": 76.1, "text": " A naszym celem jest zrozumienie, co sprawia, \u017ce Helm jest tak prze\u0142omowy.", "tokens": [51329, 316, 48094, 1769, 10386, 3492, 710, 27857, 449, 27385, 11, 598, 22734, 654, 11, 3561, 6128, 76, 3492, 991, 8325, 1221, 298, 10089, 13, 51544], "temperature": 0.0, "avg_logprob": -0.12235807078753332, "compression_ratio": 1.4259259259259258, "no_speech_prob": 0.010681337676942348}, {"id": 20, "seek": 7610, "start": 76.19999999999999, "end": 84.89999999999999, "text": " Jak on dzia\u0142a i co chyba najciekawsze, jakie absolutnie zaskakuj\u0105ce prawdy o 30 wiod\u0105cych modelach na \u015bwiecie ujawni\u0142.", "tokens": [50369, 15029, 322, 37903, 741, 598, 31532, 11212, 4260, 74, 28354, 11, 22124, 18757, 2766, 710, 3863, 514, 13263, 384, 22508, 3173, 277, 2217, 261, 2695, 1611, 31306, 2316, 608, 1667, 40078, 4260, 344, 2938, 895, 40622, 13, 50804], "temperature": 0.0, "avg_logprob": -0.10251262422241916, "compression_ratio": 1.4358208955223881, "no_speech_prob": 0.5922190546989441}, {"id": 21, "seek": 7610, "start": 85.1, "end": 89.5, "text": " A podobno niekt\u00f3re z nich wywracaj\u0105 do g\u00f3ry nogami to, co my\u015bleli\u015bmy, \u017ce wiemy o AI.", "tokens": [50814, 316, 43024, 1771, 2838, 43073, 265, 710, 25570, 4628, 7449, 326, 11133, 360, 290, 812, 627, 9638, 4526, 281, 11, 598, 48633, 306, 38452, 11, 3561, 3355, 2226, 277, 7318, 13, 51034], "temperature": 0.0, "avg_logprob": -0.10251262422241916, "compression_ratio": 1.4358208955223881, "no_speech_prob": 0.5922190546989441}, {"id": 22, "seek": 7610, "start": 89.69999999999999, "end": 90.19999999999999, "text": " Oj tak.", "tokens": [51044, 47100, 991, 13, 51069], "temperature": 0.0, "avg_logprob": -0.10251262422241916, "compression_ratio": 1.4358208955223881, "no_speech_prob": 0.5922190546989441}, {"id": 23, "seek": 7610, "start": 90.39999999999999, "end": 92.19999999999999, "text": " Dobrze, to zacznijmy od podstaw.", "tokens": [51079, 29679, 13503, 11, 281, 710, 14875, 77, 1718, 2226, 3611, 43443, 13, 51169], "temperature": 0.0, "avg_logprob": -0.10251262422241916, "compression_ratio": 1.4358208955223881, "no_speech_prob": 0.5922190546989441}, {"id": 24, "seek": 7610, "start": 92.39999999999999, "end": 93.8, "text": " Czym w\u0142a\u015bciwie jest Helm?", "tokens": [51179, 19832, 76, 50108, 3492, 6128, 76, 30, 51249], "temperature": 0.0, "avg_logprob": -0.10251262422241916, "compression_ratio": 1.4358208955223881, "no_speech_prob": 0.5922190546989441}, {"id": 25, "seek": 7610, "start": 94.19999999999999, "end": 99.19999999999999, "text": " Bo to z tego, co czyta\u0142em, nie jest po prostu kolejny, nudny benchmark z list\u0105 wynik\u00f3w.", "tokens": [51269, 3286, 281, 710, 8627, 11, 598, 6430, 1328, 11126, 11, 2838, 3492, 714, 19518, 23749, 1634, 11, 40045, 1634, 18927, 710, 1329, 1611, 31936, 1035, 3901, 13, 51519], "temperature": 0.0, "avg_logprob": -0.10251262422241916, "compression_ratio": 1.4358208955223881, "no_speech_prob": 0.5922190546989441}, {"id": 26, "seek": 7610, "start": 99.39999999999999, "end": 100.3, "text": " Absolutnie nie.", "tokens": [51529, 5813, 2308, 2766, 2838, 13, 51574], "temperature": 0.0, "avg_logprob": -0.10251262422241916, "compression_ratio": 1.4358208955223881, "no_speech_prob": 0.5922190546989441}, {"id": 27, "seek": 7610, "start": 100.5, "end": 104.89999999999999, "text": " W materia\u0142ach autorzy ci\u0105gle podkre\u015blaj\u0105, \u017ce to ca\u0142a filozofia ewaluacji.", "tokens": [51584, 343, 2389, 8908, 608, 19510, 1229, 42398, 22631, 2497, 27885, 1788, 875, 8555, 11, 3561, 281, 1335, 5024, 1387, 15151, 2670, 654, 43364, 4929, 13152, 13, 51804], "temperature": 0.0, "avg_logprob": -0.10251262422241916, "compression_ratio": 1.4358208955223881, "no_speech_prob": 0.5922190546989441}, {"id": 28, "seek": 7610, "start": 105.1, "end": 105.8, "text": " Filozofia.", "tokens": [51814, 7905, 15151, 2670, 654, 13, 51849], "temperature": 0.0, "avg_logprob": -0.10251262422241916, "compression_ratio": 1.4358208955223881, "no_speech_prob": 0.5922190546989441}, {"id": 29, "seek": 10580, "start": 106.1, "end": 106.89999999999999, "text": " Dok\u0142adnie.", "tokens": [50379, 29768, 10358, 2766, 13, 50419], "temperature": 0.0, "avg_logprob": -0.1434748024776064, "compression_ratio": 1.3873239436619718, "no_speech_prob": 0.0027252878062427044}, {"id": 30, "seek": 10580, "start": 107.3, "end": 110.7, "text": " S\u0142owo holistyczna w nazwie jest tutaj kluczowe.", "tokens": [50439, 318, 1221, 19941, 4091, 468, 17466, 629, 261, 20151, 8699, 3492, 12749, 9671, 1311, 89, 6880, 13, 50609], "temperature": 0.0, "avg_logprob": -0.1434748024776064, "compression_ratio": 1.3873239436619718, "no_speech_prob": 0.0027252878062427044}, {"id": 31, "seek": 10580, "start": 111.3, "end": 120.7, "text": " Zamiast skupia\u0107 si\u0119 na jednym w\u0105skim zadaniu, jak, no nie wiem, t\u0142umaczenie zda\u0144, postanowili spojrze\u0107 na problem z lotoptaka.", "tokens": [50639, 1176, 4526, 525, 1110, 1010, 654, 2162, 3244, 1667, 5232, 12996, 261, 1611, 5161, 332, 42788, 25849, 11, 4207, 11, 572, 2838, 26522, 11, 256, 49166, 326, 16778, 710, 2675, 5248, 11, 2183, 282, 305, 2312, 8243, 73, 13503, 2162, 1667, 1154, 710, 688, 404, 83, 7849, 13, 51109], "temperature": 0.0, "avg_logprob": -0.1434748024776064, "compression_ratio": 1.3873239436619718, "no_speech_prob": 0.0027252878062427044}, {"id": 32, "seek": 10580, "start": 121.39999999999999, "end": 123.9, "text": " I ta wizja opiera si\u0119 na trzech chwilarach.", "tokens": [51144, 286, 1846, 40808, 2938, 999, 10609, 3244, 1667, 504, 19439, 41941, 289, 608, 13, 51269], "temperature": 0.0, "avg_logprob": -0.1434748024776064, "compression_ratio": 1.3873239436619718, "no_speech_prob": 0.0027252878062427044}, {"id": 33, "seek": 10580, "start": 124.2, "end": 130.1, "text": " Pierwszy to szeroki zakres i co r\u00f3wnie wa\u017cne, \u015bwiadomo\u015b\u0107 w\u0142asnych brak\u00f3w.", "tokens": [51284, 16676, 30012, 281, 36160, 17056, 23810, 495, 741, 598, 11416, 14215, 46110, 11, 21485, 40633, 7753, 43572, 9399, 1548, 23849, 13, 51579], "temperature": 0.0, "avg_logprob": -0.1434748024776064, "compression_ratio": 1.3873239436619718, "no_speech_prob": 0.0027252878062427044}, {"id": 34, "seek": 10580, "start": 130.5, "end": 134.4, "text": " O, to mi si\u0119 bardzo spodoba\u0142o. Nie udaj\u0105, \u017ce zbadali wszystko.", "tokens": [51599, 422, 11, 281, 2752, 3244, 9034, 637, 378, 19481, 5249, 13, 12016, 11727, 11133, 11, 3561, 710, 27580, 5103, 22607, 13, 51794], "temperature": 0.0, "avg_logprob": -0.1434748024776064, "compression_ratio": 1.3873239436619718, "no_speech_prob": 0.0027252878062427044}, {"id": 35, "seek": 10580, "start": 134.7, "end": 135.2, "text": " Prawda?", "tokens": [51809, 430, 5131, 2675, 30, 51834], "temperature": 0.0, "avg_logprob": -0.1434748024776064, "compression_ratio": 1.3873239436619718, "no_speech_prob": 0.0027252878062427044}, {"id": 36, "seek": 13520, "start": 135.39999999999998, "end": 141.89999999999998, "text": " Zmapowali\u015bmy ogromny obszar od odpowiadania na pytania, postrzeszczanie, ale o to, gdzie wci\u0105\u017c s\u0105 bia\u0142y plamy.", "tokens": [50374, 1176, 24223, 305, 33955, 34416, 298, 1634, 3181, 26236, 3611, 24314, 38069, 5609, 1667, 25878, 5609, 11, 2183, 19390, 10430, 3689, 7155, 11, 6775, 277, 281, 11, 18922, 261, 537, 27242, 9015, 272, 654, 6825, 499, 7804, 13, 50699], "temperature": 0.0, "avg_logprob": -0.126834442366415, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.002750140381976962}, {"id": 37, "seek": 13520, "start": 142.79999999999998, "end": 147.7, "text": " Np. przyznaj\u0105, \u017ce brakuje im test\u00f3w dla rzadziej u\u017cywanych dialekt\u00f3w angielskiego.", "tokens": [50744, 426, 79, 13, 6501, 35458, 8555, 11, 3561, 1548, 5279, 2884, 566, 1500, 3901, 12285, 367, 89, 345, 19554, 34097, 86, 34644, 5502, 8192, 3901, 2562, 1187, 5161, 12200, 13, 50989], "temperature": 0.0, "avg_logprob": -0.126834442366415, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.002750140381976962}, {"id": 38, "seek": 13520, "start": 147.89999999999998, "end": 151.29999999999998, "text": " Nie m\u00f3wi\u0105c ju\u017c o innych j\u0119zykach, to buduje wiarygodno\u015b\u0107.", "tokens": [50999, 12016, 46591, 66, 10678, 277, 36286, 49055, 41326, 11, 281, 3265, 13008, 26393, 822, 21787, 23293, 13, 51169], "temperature": 0.0, "avg_logprob": -0.126834442366415, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.002750140381976962}, {"id": 39, "seek": 13520, "start": 151.5, "end": 152.29999999999998, "text": " Niezwykle.", "tokens": [51179, 12016, 89, 9726, 14677, 13, 51219], "temperature": 0.0, "avg_logprob": -0.126834442366415, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.002750140381976962}, {"id": 40, "seek": 13520, "start": 152.7, "end": 156.89999999999998, "text": " A drugi filar to podej\u015bcie, kt\u00f3re nazywaj\u0105 wielomiarowym.", "tokens": [51239, 316, 4110, 72, 1387, 289, 281, 7468, 73, 9815, 11, 8864, 20151, 27112, 11133, 20570, 9220, 289, 31691, 13, 51449], "temperature": 0.0, "avg_logprob": -0.126834442366415, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.002750140381976962}, {"id": 41, "seek": 13520, "start": 157.29999999999998, "end": 159.79999999999998, "text": " I to jest wiesz, fundamentalna zmiana.", "tokens": [51469, 286, 281, 3492, 261, 15347, 11, 8088, 629, 17020, 8497, 13, 51594], "temperature": 0.0, "avg_logprob": -0.126834442366415, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.002750140381976962}, {"id": 42, "seek": 15980, "start": 159.9, "end": 165.4, "text": " Wielometryczne, czyli do tej pory w AI kr\u00f3lowa\u0142a jedna metryka.", "tokens": [50369, 343, 1187, 34730, 38491, 11, 16591, 360, 12573, 280, 827, 261, 7318, 42366, 75, 5528, 5024, 5232, 629, 1131, 627, 2330, 13, 50644], "temperature": 0.0, "avg_logprob": -0.1698321845587783, "compression_ratio": 1.416326530612245, "no_speech_prob": 0.008964670822024345}, {"id": 43, "seek": 15980, "start": 165.8, "end": 167.8, "text": " Accuracy, dok\u0142adno\u015b\u0107.", "tokens": [50664, 5725, 374, 2551, 11, 45864, 23293, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1698321845587783, "compression_ratio": 1.416326530612245, "no_speech_prob": 0.008964670822024345}, {"id": 44, "seek": 15980, "start": 168.0, "end": 171.70000000000002, "text": " No tak, autorze Helm stwierdzili, to nie wystarczy.", "tokens": [50774, 883, 991, 11, 19510, 1381, 6128, 76, 342, 40717, 28168, 2312, 11, 281, 2838, 4628, 9710, 6522, 13, 50959], "temperature": 0.0, "avg_logprob": -0.1698321845587783, "compression_ratio": 1.416326530612245, "no_speech_prob": 0.008964670822024345}, {"id": 45, "seek": 15980, "start": 171.9, "end": 178.9, "text": " Model, kt\u00f3ry jest dok\u0142adny, ale jednocze\u015bnie, no nie wiem, toksyczny, albo skrajnie stroniczy jest bezu\u017cyteczny.", "tokens": [50969, 17105, 11, 9913, 3492, 45864, 1634, 11, 6775, 5232, 26694, 1381, 12221, 11, 572, 2838, 26522, 11, 281, 1694, 17466, 1634, 11, 22622, 1110, 48690, 2766, 1056, 11630, 1229, 3492, 10782, 84, 7735, 975, 3689, 1634, 13, 51319], "temperature": 0.0, "avg_logprob": -0.1698321845587783, "compression_ratio": 1.416326530612245, "no_speech_prob": 0.008964670822024345}, {"id": 46, "seek": 15980, "start": 179.5, "end": 180.4, "text": " A nawet szkowliwy.", "tokens": [51349, 316, 22696, 7870, 74, 305, 2081, 9726, 13, 51394], "temperature": 0.0, "avg_logprob": -0.1698321845587783, "compression_ratio": 1.416326530612245, "no_speech_prob": 0.008964670822024345}, {"id": 47, "seek": 15980, "start": 180.9, "end": 185.10000000000002, "text": " Czyli dla ka\u017cdego zadania mierzyli nie jedn\u0105, a ca\u0142y zestaw cech.", "tokens": [51419, 37099, 12285, 21912, 67, 6308, 42788, 5609, 47448, 1229, 2081, 2838, 5232, 13113, 11, 257, 35226, 37889, 1607, 1769, 339, 13, 51629], "temperature": 0.0, "avg_logprob": -0.1698321845587783, "compression_ratio": 1.416326530612245, "no_speech_prob": 0.008964670822024345}, {"id": 48, "seek": 18510, "start": 185.6, "end": 187.4, "text": " Tak, a\u017c siedem metryk.", "tokens": [50389, 9118, 11, 48134, 262, 1091, 443, 1131, 627, 74, 13, 50479], "temperature": 0.0, "avg_logprob": -0.151397578763646, "compression_ratio": 1.3904109589041096, "no_speech_prob": 0.01042591966688633}, {"id": 49, "seek": 18510, "start": 187.6, "end": 190.0, "text": " I to dla ka\u017cdego z 16 g\u0142\u00f3wnych scenariuszy.", "tokens": [50489, 286, 281, 12285, 21912, 67, 6308, 710, 3165, 18117, 812, 895, 16384, 4191, 27440, 1229, 13, 50609], "temperature": 0.0, "avg_logprob": -0.151397578763646, "compression_ratio": 1.3904109589041096, "no_speech_prob": 0.01042591966688633}, {"id": 50, "seek": 18510, "start": 190.4, "end": 196.0, "text": " Obok Accuracy pojawi\u0142y si\u0119 Calibration, Robustness, Fairness, Bias.", "tokens": [50629, 4075, 453, 5725, 374, 2551, 30655, 72, 6825, 3244, 3511, 897, 2405, 11, 5424, 381, 1287, 11, 12157, 1287, 11, 363, 4609, 13, 50909], "temperature": 0.0, "avg_logprob": -0.151397578763646, "compression_ratio": 1.3904109589041096, "no_speech_prob": 0.01042591966688633}, {"id": 51, "seek": 18510, "start": 196.2, "end": 198.2, "text": " Zatrzymajmy si\u0119 tu na chwila, bo to wa\u017cne.", "tokens": [50919, 1176, 267, 13047, 1696, 73, 2226, 3244, 2604, 1667, 26237, 7371, 11, 748, 281, 46110, 13, 51019], "temperature": 0.0, "avg_logprob": -0.151397578763646, "compression_ratio": 1.3904109589041096, "no_speech_prob": 0.01042591966688633}, {"id": 52, "seek": 18510, "start": 198.6, "end": 204.4, "text": " Robustness to w zasadzie test na to, czy model spanikuje, jak zrobili te r\u00f3wk\u0119 w pytaniu.", "tokens": [51039, 5424, 381, 1287, 281, 261, 44585, 3283, 1500, 1667, 281, 11, 6430, 2316, 637, 282, 1035, 13008, 11, 4207, 44399, 2312, 535, 367, 3901, 15724, 261, 25878, 25849, 13, 51329], "temperature": 0.0, "avg_logprob": -0.151397578763646, "compression_ratio": 1.3904109589041096, "no_speech_prob": 0.01042591966688633}, {"id": 53, "seek": 18510, "start": 204.6, "end": 205.4, "text": " Dok\u0142adnie tak.", "tokens": [51339, 29768, 10358, 2766, 991, 13, 51379], "temperature": 0.0, "avg_logprob": -0.151397578763646, "compression_ratio": 1.3904109589041096, "no_speech_prob": 0.01042591966688633}, {"id": 54, "seek": 18510, "start": 205.79999999999998, "end": 209.6, "text": " Albo czy kompletnie zmieni odpowied\u017a, je\u015bli przestawisz szyk zdania?", "tokens": [51399, 967, 1763, 6430, 5207, 14657, 2766, 17020, 35462, 36574, 10659, 11, 25630, 44264, 1607, 23848, 30526, 74, 16221, 5609, 30, 51589], "temperature": 0.0, "avg_logprob": -0.151397578763646, "compression_ratio": 1.3904109589041096, "no_speech_prob": 0.01042591966688633}, {"id": 55, "seek": 18510, "start": 210.0, "end": 213.0, "text": " Z kolei Calibration to jest fascynuj\u0105ce.", "tokens": [51609, 1176, 18303, 72, 3511, 897, 2405, 281, 3492, 30632, 1344, 77, 13263, 384, 13, 51759], "temperature": 0.0, "avg_logprob": -0.151397578763646, "compression_ratio": 1.3904109589041096, "no_speech_prob": 0.01042591966688633}, {"id": 56, "seek": 21300, "start": 213.4, "end": 216.1, "text": " Mierzy jak dobrze model zna samego siebie.", "tokens": [50384, 376, 811, 1229, 4207, 28335, 2316, 710, 629, 912, 1571, 39137, 13, 50519], "temperature": 0.0, "avg_logprob": -0.1278511229015532, "compression_ratio": 1.5123674911660778, "no_speech_prob": 0.007639599032700062}, {"id": 57, "seek": 21300, "start": 216.3, "end": 217.9, "text": " Czyli czy wie, kiedy nie wie?", "tokens": [50529, 37099, 6430, 3355, 11, 18777, 2838, 3355, 30, 50609], "temperature": 0.0, "avg_logprob": -0.1278511229015532, "compression_ratio": 1.5123674911660778, "no_speech_prob": 0.007639599032700062}, {"id": 58, "seek": 21300, "start": 218.1, "end": 218.7, "text": " W\u0142a\u015bnie.", "tokens": [50619, 343, 5024, 12221, 13, 50649], "temperature": 0.0, "avg_logprob": -0.1278511229015532, "compression_ratio": 1.5123674911660778, "no_speech_prob": 0.007639599032700062}, {"id": 59, "seek": 21300, "start": 219.3, "end": 230.3, "text": " Czy kiedy m\u00f3wi, \u017ce jest czego\u015b pewien na 99% to faktycznie ma racj\u0119 w 99% przypadk\u00f3w, czy mo\u017ce jest nadmiernie pewne siebie?", "tokens": [50679, 19832, 18777, 24592, 11, 3561, 3492, 36559, 1788, 25889, 1053, 1667, 11803, 4, 281, 33647, 45586, 463, 4129, 11115, 261, 11803, 4, 33100, 23849, 11, 6430, 12034, 3492, 12617, 76, 811, 2766, 25889, 716, 39137, 30, 51229], "temperature": 0.0, "avg_logprob": -0.1278511229015532, "compression_ratio": 1.5123674911660778, "no_speech_prob": 0.007639599032700062}, {"id": 60, "seek": 21300, "start": 230.5, "end": 233.3, "text": " To kluczowe w zastosowaniach medycznych czy prawnych.", "tokens": [51239, 1407, 9671, 1311, 89, 6880, 261, 36746, 329, 305, 3782, 608, 1205, 17466, 9399, 6430, 37047, 16384, 13, 51379], "temperature": 0.0, "avg_logprob": -0.1278511229015532, "compression_ratio": 1.5123674911660778, "no_speech_prob": 0.007639599032700062}, {"id": 61, "seek": 21300, "start": 233.7, "end": 238.5, "text": " Wole model, kt\u00f3ry m\u00f3wi nie wiem ni\u017c taki, co z pe\u0142nym przekonaniem podaje b\u0142\u0105dn\u0105 diagnoz\u0119.", "tokens": [51399, 343, 4812, 2316, 11, 9913, 24592, 2838, 26522, 28502, 20065, 11, 598, 710, 43205, 12996, 29785, 266, 282, 4907, 2497, 11153, 272, 15926, 67, 13113, 1026, 559, 1771, 11052, 13, 51639], "temperature": 0.0, "avg_logprob": -0.1278511229015532, "compression_ratio": 1.5123674911660778, "no_speech_prob": 0.007639599032700062}, {"id": 62, "seek": 21300, "start": 238.7, "end": 239.7, "text": " A trzeci filar?", "tokens": [51649, 316, 22266, 537, 1387, 289, 30, 51699], "temperature": 0.0, "avg_logprob": -0.1278511229015532, "compression_ratio": 1.5123674911660778, "no_speech_prob": 0.007639599032700062}, {"id": 63, "seek": 21300, "start": 240.2, "end": 242.5, "text": " To podobno ten, kt\u00f3ry wywo\u0142a\u0142 rewolucj\u0119.", "tokens": [51724, 1407, 43024, 1771, 2064, 11, 9913, 4628, 6120, 5024, 1221, 319, 48481, 1311, 11115, 13, 51839], "temperature": 0.0, "avg_logprob": -0.1278511229015532, "compression_ratio": 1.5123674911660778, "no_speech_prob": 0.007639599032700062}, {"id": 64, "seek": 24250, "start": 242.7, "end": 243.8, "text": " Standardyzacja.", "tokens": [50374, 21298, 37433, 23395, 13, 50429], "temperature": 0.0, "avg_logprob": -0.15043244296557282, "compression_ratio": 1.3930817610062893, "no_speech_prob": 0.007450148928910494}, {"id": 65, "seek": 24250, "start": 244.3, "end": 246.2, "text": " To jest serce ca\u0142ego projektu.", "tokens": [50454, 1407, 3492, 816, 384, 35224, 6308, 26261, 84, 13, 50549], "temperature": 0.0, "avg_logprob": -0.15043244296557282, "compression_ratio": 1.3930817610062893, "no_speech_prob": 0.007450148928910494}, {"id": 66, "seek": 24250, "start": 246.5, "end": 255.0, "text": " Wzi\u0119li 30 prominentnych modeli od 12 r\u00f3\u017cnych firm AI21 Labs, Google, Meta, OpenAI, Microsoft.", "tokens": [50564, 343, 16706, 2081, 2217, 17034, 9399, 2316, 72, 3611, 2272, 42602, 6174, 7318, 4436, 40047, 11, 3329, 11, 6377, 64, 11, 7238, 48698, 11, 8116, 13, 50989], "temperature": 0.0, "avg_logprob": -0.15043244296557282, "compression_ratio": 1.3930817610062893, "no_speech_prob": 0.007450148928910494}, {"id": 67, "seek": 24250, "start": 255.4, "end": 256.7, "text": " Wszystkich du\u017cych graczy.", "tokens": [51009, 343, 10424, 48349, 1581, 7735, 339, 11625, 1229, 13, 51074], "temperature": 0.0, "avg_logprob": -0.15043244296557282, "compression_ratio": 1.3930817610062893, "no_speech_prob": 0.007450148928910494}, {"id": 68, "seek": 24250, "start": 256.8, "end": 257.0, "text": " Tak.", "tokens": [51079, 9118, 13, 51089], "temperature": 0.0, "avg_logprob": -0.15043244296557282, "compression_ratio": 1.3930817610062893, "no_speech_prob": 0.007450148928910494}, {"id": 69, "seek": 24250, "start": 257.4, "end": 259.6, "text": " I wsadzili je do tego samego laboratorium.", "tokens": [51109, 286, 37647, 345, 89, 2312, 1506, 360, 8627, 912, 1571, 5938, 41679, 13, 51219], "temperature": 0.0, "avg_logprob": -0.15043244296557282, "compression_ratio": 1.3930817610062893, "no_speech_prob": 0.007450148928910494}, {"id": 70, "seek": 24250, "start": 259.9, "end": 263.6, "text": " Wszystkie musia\u0142y rozwi\u0105zywa\u0107 te same zadania w tych samych warunkach.", "tokens": [51234, 343, 10424, 22872, 1038, 654, 6825, 9544, 18234, 1229, 25234, 535, 912, 42788, 5609, 261, 15180, 3247, 16384, 1516, 3197, 608, 13, 51419], "temperature": 0.0, "avg_logprob": -0.15043244296557282, "compression_ratio": 1.3930817610062893, "no_speech_prob": 0.007450148928910494}, {"id": 71, "seek": 24250, "start": 264.0, "end": 267.8, "text": " U\u017cyto te\u017c tej samej metody adaptacji, czyli few shot prompting.", "tokens": [51439, 624, 7735, 1353, 9516, 12573, 912, 73, 1131, 843, 6231, 13152, 11, 16591, 1326, 3347, 12391, 278, 13, 51629], "temperature": 0.0, "avg_logprob": -0.15043244296557282, "compression_ratio": 1.3930817610062893, "no_speech_prob": 0.007450148928910494}, {"id": 72, "seek": 24250, "start": 267.9, "end": 271.8, "text": " Czyli pokazano ka\u017cdemu modelowi kilka przyk\u0142ad\u00f3w, \u017ceby zrozumia\u0142, o co chodzi.", "tokens": [51634, 37099, 13010, 921, 3730, 21912, 10730, 84, 2316, 24503, 36466, 23144, 3901, 11, 11316, 710, 27857, 449, 8908, 11, 277, 598, 23998, 13, 51829], "temperature": 0.0, "avg_logprob": -0.15043244296557282, "compression_ratio": 1.3930817610062893, "no_speech_prob": 0.007450148928910494}, {"id": 73, "seek": 27180, "start": 271.8, "end": 277.5, "text": " Tele oceniano \u015brednio na zaledwie 17 i 9 procenta tych kluczowych scenariuszy.", "tokens": [50364, 1989, 306, 10409, 268, 6254, 8299, 986, 41084, 1667, 710, 5573, 8699, 3282, 741, 1722, 38826, 64, 15180, 9671, 1311, 89, 19605, 4191, 27440, 1229, 13, 50649], "temperature": 0.0, "avg_logprob": -0.14660963669321894, "compression_ratio": 1.3979591836734695, "no_speech_prob": 0.003619719296693802}, {"id": 74, "seek": 27180, "start": 277.90000000000003, "end": 279.1, "text": " A po ich pracy.", "tokens": [50669, 316, 714, 1893, 35591, 13, 50729], "temperature": 0.0, "avg_logprob": -0.14660963669321894, "compression_ratio": 1.3979591836734695, "no_speech_prob": 0.003619719296693802}, {"id": 75, "seek": 27180, "start": 279.2, "end": 282.7, "text": " Pokrycie wzros\u0142o do 96 procenta.", "tokens": [50734, 14958, 627, 4260, 24809, 2635, 5249, 360, 24124, 38826, 64, 13, 50909], "temperature": 0.0, "avg_logprob": -0.14660963669321894, "compression_ratio": 1.3979591836734695, "no_speech_prob": 0.003619719296693802}, {"id": 76, "seek": 27180, "start": 282.8, "end": 283.3, "text": " Wow.", "tokens": [50914, 3153, 13, 50939], "temperature": 0.0, "avg_logprob": -0.14660963669321894, "compression_ratio": 1.3979591836734695, "no_speech_prob": 0.003619719296693802}, {"id": 77, "seek": 27180, "start": 283.40000000000003, "end": 286.8, "text": " Z chaosu przeszli\u015bmy do uporz\u0105dkowanej tabeli wynik\u00f3w.", "tokens": [50944, 1176, 14158, 84, 6541, 10430, 38452, 360, 493, 284, 23876, 74, 23066, 73, 4421, 10148, 31936, 1035, 3901, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14660963669321894, "compression_ratio": 1.3979591836734695, "no_speech_prob": 0.003619719296693802}, {"id": 78, "seek": 27180, "start": 287.3, "end": 292.5, "text": " Nagle okaza\u0142o si\u0119, \u017ce wszyscy ci sportowcy nie tylko stan\u0119li na starcie tej samej olimpiady,", "tokens": [51139, 426, 15088, 3133, 12257, 5249, 3244, 11, 3561, 44232, 6983, 7282, 305, 1344, 2838, 13219, 27984, 1274, 2081, 1667, 3543, 4260, 12573, 912, 73, 2545, 8814, 72, 880, 11, 51399], "temperature": 0.0, "avg_logprob": -0.14660963669321894, "compression_ratio": 1.3979591836734695, "no_speech_prob": 0.003619719296693802}, {"id": 79, "seek": 27180, "start": 293.0, "end": 295.6, "text": " ale wzi\u0119li udzia\u0142 w pe\u0142nym siedmioboju.", "tokens": [51424, 6775, 261, 16706, 2081, 11727, 43070, 261, 43205, 12996, 262, 1091, 3057, 996, 78, 8954, 13, 51554], "temperature": 0.0, "avg_logprob": -0.14660963669321894, "compression_ratio": 1.3979591836734695, "no_speech_prob": 0.003619719296693802}, {"id": 80, "seek": 27180, "start": 296.0, "end": 301.5, "text": " Kt\u00f3ry testuje nie tylko szybko\u015b\u0107, ale i si\u0142\u0119, zwinno\u015b\u0107 i uczciw\u0105 gr\u0119.", "tokens": [51574, 591, 4547, 627, 1500, 13008, 2838, 13219, 36456, 4093, 7753, 11, 6775, 741, 1511, 46564, 11, 710, 9136, 23293, 741, 35403, 537, 86, 1611, 677, 1274, 13, 51849], "temperature": 0.0, "avg_logprob": -0.14660963669321894, "compression_ratio": 1.3979591836734695, "no_speech_prob": 0.003619719296693802}, {"id": 81, "seek": 30150, "start": 301.5, "end": 302.2, "text": " W\u0142a\u015bnie.", "tokens": [50364, 343, 5024, 12221, 13, 50399], "temperature": 0.0, "avg_logprob": -0.13301637013753256, "compression_ratio": 1.398671096345515, "no_speech_prob": 0.0057572913356125355}, {"id": 82, "seek": 30150, "start": 302.3, "end": 307.5, "text": " I kiedy ju\u017c ustawiono te same regu\u0142y gry dla wszystkich, wyniki okaza\u0142y si\u0119,", "tokens": [50404, 286, 18777, 10678, 26189, 1607, 49020, 535, 912, 1121, 84, 6825, 41974, 12285, 34234, 11, 31936, 9850, 3133, 12257, 6825, 3244, 11, 50664], "temperature": 0.0, "avg_logprob": -0.13301637013753256, "compression_ratio": 1.398671096345515, "no_speech_prob": 0.0057572913356125355}, {"id": 83, "seek": 30150, "start": 307.7, "end": 309.4, "text": " c\u00f3\u017c, pe\u0142ne niespodziadek.", "tokens": [50674, 6333, 1427, 11, 43205, 716, 48100, 79, 14543, 762, 74, 13, 50759], "temperature": 0.0, "avg_logprob": -0.13301637013753256, "compression_ratio": 1.398671096345515, "no_speech_prob": 0.0057572913356125355}, {"id": 84, "seek": 30150, "start": 309.7, "end": 312.5, "text": " Przejd\u017amy do tych najbardziej zaskakuj\u0105cych odkry\u0107.", "tokens": [50774, 2114, 16920, 67, 10659, 2226, 360, 15180, 41857, 710, 3863, 514, 13263, 31306, 3611, 43298, 2162, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13301637013753256, "compression_ratio": 1.398671096345515, "no_speech_prob": 0.0057572913356125355}, {"id": 85, "seek": 30150, "start": 312.6, "end": 314.4, "text": " Co jako pierwsze rzuca si\u0119 w oczy?", "tokens": [50919, 3066, 17123, 45994, 367, 11728, 496, 3244, 261, 277, 6522, 30, 51009], "temperature": 0.0, "avg_logprob": -0.13301637013753256, "compression_ratio": 1.398671096345515, "no_speech_prob": 0.0057572913356125355}, {"id": 86, "seek": 30150, "start": 314.5, "end": 318.7, "text": " Pierwsze i by\u0107 mo\u017ce najwa\u017cniejsze z perspektywy ca\u0142ego ekosystemu AI", "tokens": [51014, 16676, 14358, 1381, 741, 15069, 12034, 11212, 27111, 44258, 710, 868, 32659, 874, 9726, 35224, 6308, 13359, 329, 9321, 84, 7318, 51224], "temperature": 0.0, "avg_logprob": -0.13301637013753256, "compression_ratio": 1.398671096345515, "no_speech_prob": 0.0057572913356125355}, {"id": 87, "seek": 30150, "start": 318.8, "end": 321.2, "text": " jest odkrycie dotycz\u0105ce dost\u0119pu.", "tokens": [51229, 3492, 3611, 43298, 4260, 5893, 17466, 1611, 384, 48209, 84, 13, 51349], "temperature": 0.0, "avg_logprob": -0.13301637013753256, "compression_ratio": 1.398671096345515, "no_speech_prob": 0.0057572913356125355}, {"id": 88, "seek": 30150, "start": 321.3, "end": 321.9, "text": " Dost\u0119p.", "tokens": [51354, 413, 555, 18085, 13, 51384], "temperature": 0.0, "avg_logprob": -0.13301637013753256, "compression_ratio": 1.398671096345515, "no_speech_prob": 0.0057572913356125355}, {"id": 89, "seek": 30150, "start": 322.0, "end": 322.5, "text": " Tak.", "tokens": [51389, 9118, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13301637013753256, "compression_ratio": 1.398671096345515, "no_speech_prob": 0.0057572913356125355}, {"id": 90, "seek": 30150, "start": 322.6, "end": 327.8, "text": " Istnieje wyra\u017cne i konsekwentna przepa\u015b\u0107 w wydajno\u015bci mi\u0119dzy modelami open source,", "tokens": [51419, 12810, 2766, 2884, 4628, 424, 1427, 716, 741, 47020, 74, 34798, 629, 30829, 64, 7753, 261, 25984, 1805, 16438, 33964, 2316, 4526, 1269, 4009, 11, 51679], "temperature": 0.0, "avg_logprob": -0.13301637013753256, "compression_ratio": 1.398671096345515, "no_speech_prob": 0.0057572913356125355}, {"id": 91, "seek": 32780, "start": 327.8, "end": 331.6, "text": " kt\u00f3re ka\u017cdy mo\u017ce pobra\u0107, a modelami o ograniczonym dost\u0119pie,", "tokens": [50364, 8864, 31615, 12034, 714, 6198, 2162, 11, 257, 2316, 4526, 277, 34416, 282, 17946, 12732, 20568, 1274, 9144, 11, 50554], "temperature": 0.0, "avg_logprob": -0.18322430541183773, "compression_ratio": 1.4013157894736843, "no_speech_prob": 0.01954217627644539}, {"id": 92, "seek": 32780, "start": 331.7, "end": 337.6, "text": " tymi zamkni\u0119cymi, jak Text Davinci 002 od OpenAI czy TNLGV2 od Microsoftu.", "tokens": [50559, 1104, 3057, 19876, 74, 35938, 1344, 3057, 11, 4207, 18643, 3724, 21961, 7143, 17, 3611, 7238, 48698, 6430, 314, 45, 43, 38, 53, 17, 3611, 8116, 84, 13, 50854], "temperature": 0.0, "avg_logprob": -0.18322430541183773, "compression_ratio": 1.4013157894736843, "no_speech_prob": 0.01954217627644539}, {"id": 93, "seek": 32780, "start": 337.7, "end": 341.40000000000003, "text": " Czyli najlepsze modele wci\u0105\u017c s\u0105 za zamkni\u0119tymi d\u017awiami.", "tokens": [50859, 37099, 41903, 1878, 1381, 4391, 306, 261, 537, 27242, 9015, 7949, 19876, 74, 35938, 874, 3057, 274, 10659, 86, 15568, 13, 51044], "temperature": 0.0, "avg_logprob": -0.18322430541183773, "compression_ratio": 1.4013157894736843, "no_speech_prob": 0.01954217627644539}, {"id": 94, "seek": 32780, "start": 341.90000000000003, "end": 345.3, "text": " Dost\u0119pno\u015b\u0107 ma swoj\u0105 cen\u0119 pod wzgl\u0119dem surowej wydajno\u015bci,", "tokens": [51069, 413, 555, 18085, 23293, 463, 49194, 27900, 1274, 2497, 48538, 6298, 443, 1022, 21091, 25984, 1805, 16438, 11, 51239], "temperature": 0.0, "avg_logprob": -0.18322430541183773, "compression_ratio": 1.4013157894736843, "no_speech_prob": 0.01954217627644539}, {"id": 95, "seek": 32780, "start": 345.6, "end": 349.0, "text": " ale czy ta wydajno\u015b\u0107 to prosta liniowa skala,", "tokens": [51254, 6775, 6430, 1846, 25984, 1805, 23293, 281, 582, 8638, 287, 3812, 5528, 1110, 5159, 11, 51424], "temperature": 0.0, "avg_logprob": -0.18322430541183773, "compression_ratio": 1.4013157894736843, "no_speech_prob": 0.01954217627644539}, {"id": 96, "seek": 32780, "start": 349.40000000000003, "end": 353.5, "text": " \u017ce model X jest po prostu lepszy od modelu Y we wszystkim?", "tokens": [51444, 3561, 2316, 1783, 3492, 714, 19518, 476, 1878, 1229, 3611, 2316, 84, 398, 321, 30481, 30, 51649], "temperature": 0.0, "avg_logprob": -0.18322430541183773, "compression_ratio": 1.4013157894736843, "no_speech_prob": 0.01954217627644539}, {"id": 97, "seek": 32780, "start": 353.8, "end": 354.7, "text": " Absolutnie nie.", "tokens": [51664, 5813, 2308, 2766, 2838, 13, 51709], "temperature": 0.0, "avg_logprob": -0.18322430541183773, "compression_ratio": 1.4013157894736843, "no_speech_prob": 0.01954217627644539}, {"id": 98, "seek": 32780, "start": 354.90000000000003, "end": 356.90000000000003, "text": " I to jest drugie wielkie odkrycie.", "tokens": [51719, 286, 281, 3492, 4110, 414, 20570, 22872, 3611, 43298, 4260, 13, 51819], "temperature": 0.0, "avg_logprob": -0.18322430541183773, "compression_ratio": 1.4013157894736843, "no_speech_prob": 0.01954217627644539}, {"id": 99, "seek": 35690, "start": 357.09999999999997, "end": 361.0, "text": " Relacje mi\u0119dzy r\u00f3\u017cnymi metrykami s\u0105 niezwykle skomplikowane", "tokens": [50374, 8738, 29293, 33964, 19637, 31813, 1131, 627, 48737, 9015, 33511, 9726, 14677, 1110, 298, 564, 1035, 23066, 50569], "temperature": 0.0, "avg_logprob": -0.15457339345672985, "compression_ratio": 1.542319749216301, "no_speech_prob": 0.002742771990597248}, {"id": 100, "seek": 35690, "start": 361.2, "end": 362.9, "text": " i cz\u0119sto sprzeczne z intuicj\u0105.", "tokens": [50579, 741, 34369, 6103, 1381, 38491, 710, 560, 84, 299, 8555, 13, 50664], "temperature": 0.0, "avg_logprob": -0.15457339345672985, "compression_ratio": 1.542319749216301, "no_speech_prob": 0.002742771990597248}, {"id": 101, "seek": 35690, "start": 363.0, "end": 363.5, "text": " Aha.", "tokens": [50669, 27448, 13, 50694], "temperature": 0.0, "avg_logprob": -0.15457339345672985, "compression_ratio": 1.542319749216301, "no_speech_prob": 0.002742771990597248}, {"id": 102, "seek": 35690, "start": 363.79999999999995, "end": 367.29999999999995, "text": " We\u017amy na przyk\u0142ad ten zwi\u0105zek mi\u0119dzy dok\u0142adno\u015bci\u0105, czyli accuracy,", "tokens": [50709, 492, 10659, 2226, 1667, 23144, 2064, 710, 18234, 19878, 33964, 45864, 16438, 1611, 11, 16591, 14170, 11, 50884], "temperature": 0.0, "avg_logprob": -0.15457339345672985, "compression_ratio": 1.542319749216301, "no_speech_prob": 0.002742771990597248}, {"id": 103, "seek": 35690, "start": 367.4, "end": 370.29999999999995, "text": " a kalibracj\u0105, czyli calibration, o kt\u00f3rej m\u00f3wili\u015bmy.", "tokens": [50889, 257, 7788, 6414, 326, 8555, 11, 16591, 38732, 11, 277, 36023, 13489, 43912, 13, 51034], "temperature": 0.0, "avg_logprob": -0.15457339345672985, "compression_ratio": 1.542319749216301, "no_speech_prob": 0.002742771990597248}, {"id": 104, "seek": 35690, "start": 370.4, "end": 373.79999999999995, "text": " Czyli mi\u0119dzy poprawno\u015bci\u0105 odpowiedzi, a samo ocen\u0105 modelu.", "tokens": [51039, 37099, 33964, 1665, 424, 20944, 50227, 36574, 3992, 11, 257, 36422, 10409, 268, 1611, 2316, 84, 13, 51209], "temperature": 0.0, "avg_logprob": -0.15457339345672985, "compression_ratio": 1.542319749216301, "no_speech_prob": 0.002742771990597248}, {"id": 105, "seek": 35690, "start": 373.9, "end": 374.59999999999997, "text": " Dok\u0142adnie.", "tokens": [51214, 29768, 10358, 2766, 13, 51249], "temperature": 0.0, "avg_logprob": -0.15457339345672985, "compression_ratio": 1.542319749216301, "no_speech_prob": 0.002742771990597248}, {"id": 106, "seek": 35690, "start": 375.0, "end": 379.0, "text": " W scenariuszu Hellaswek, kt\u00f3ry testuje takie zdroworoszcz\u0105tkowe my\u015blenie,", "tokens": [51269, 343, 4191, 3504, 301, 11728, 12090, 296, 826, 74, 11, 9913, 1500, 13008, 15963, 49745, 284, 329, 43771, 23430, 74, 6880, 48633, 6698, 414, 11, 51469], "temperature": 0.0, "avg_logprob": -0.15457339345672985, "compression_ratio": 1.542319749216301, "no_speech_prob": 0.002742771990597248}, {"id": 107, "seek": 35690, "start": 379.09999999999997, "end": 385.2, "text": " okaza\u0142o si\u0119, \u017ce im dok\u0142adniejszy stawa\u0142 si\u0119 model, tym jego kalibracja si\u0119 pogarsza\u0142a.", "tokens": [51474, 3133, 12257, 5249, 3244, 11, 3561, 566, 45864, 10402, 7706, 342, 10449, 1221, 3244, 2316, 11, 8107, 26542, 7788, 6414, 23395, 3244, 32037, 685, 2394, 5024, 13, 51779], "temperature": 0.0, "avg_logprob": -0.15457339345672985, "compression_ratio": 1.542319749216301, "no_speech_prob": 0.002742771990597248}, {"id": 108, "seek": 35690, "start": 385.29999999999995, "end": 386.4, "text": " Pogarsza\u0142a?", "tokens": [51784, 430, 664, 685, 2394, 5024, 30, 51839], "temperature": 0.0, "avg_logprob": -0.15457339345672985, "compression_ratio": 1.542319749216301, "no_speech_prob": 0.002742771990597248}, {"id": 109, "seek": 38640, "start": 386.4, "end": 386.9, "text": " Tak.", "tokens": [50364, 9118, 13, 50389], "temperature": 0.0, "avg_logprob": -0.1689979397520727, "compression_ratio": 1.4014084507042253, "no_speech_prob": 0.0004094210744369775}, {"id": 110, "seek": 38640, "start": 387.2, "end": 393.2, "text": " Stawa\u0142 si\u0119 bardziej pewny siebie, ale jednocze\u015bnie cz\u0119\u015bciej myli\u0142 si\u0119 w ocenie tej pewno\u015bci.", "tokens": [50404, 745, 10449, 1221, 3244, 27209, 520, 43682, 39137, 11, 6775, 5232, 26694, 1381, 12221, 18544, 9815, 73, 452, 2081, 1221, 3244, 261, 10409, 268, 414, 12573, 33002, 6199, 13, 50704], "temperature": 0.0, "avg_logprob": -0.1689979397520727, "compression_ratio": 1.4014084507042253, "no_speech_prob": 0.0004094210744369775}, {"id": 111, "seek": 38640, "start": 393.4, "end": 396.4, "text": " To brzmi jak cyfrowa wersja efektu Daninga Krugera.", "tokens": [50714, 1407, 738, 89, 3057, 4207, 3185, 69, 1892, 64, 261, 433, 2938, 31482, 8192, 84, 413, 8415, 64, 6332, 697, 1663, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1689979397520727, "compression_ratio": 1.4014084507042253, "no_speech_prob": 0.0004094210744369775}, {"id": 112, "seek": 38640, "start": 396.7, "end": 401.0, "text": " Im model by\u0142 lepszy, tym gorsze w ocenie w\u0142asnych mo\u017cliwo\u015bci.", "tokens": [50879, 4331, 2316, 16673, 476, 1878, 1229, 11, 8107, 290, 830, 1381, 261, 10409, 268, 414, 43572, 9399, 30854, 36476, 13, 51094], "temperature": 0.0, "avg_logprob": -0.1689979397520727, "compression_ratio": 1.4014084507042253, "no_speech_prob": 0.0004094210744369775}, {"id": 113, "seek": 38640, "start": 401.5, "end": 402.9, "text": " To troch\u0119 niepokoj\u0105ce.", "tokens": [51119, 1407, 24926, 2838, 79, 13704, 8555, 384, 13, 51189], "temperature": 0.0, "avg_logprob": -0.1689979397520727, "compression_ratio": 1.4014084507042253, "no_speech_prob": 0.0004094210744369775}, {"id": 114, "seek": 38640, "start": 403.0, "end": 403.5, "text": " Jest.", "tokens": [51194, 24918, 13, 51219], "temperature": 0.0, "avg_logprob": -0.1689979397520727, "compression_ratio": 1.4014084507042253, "no_speech_prob": 0.0004094210744369775}, {"id": 115, "seek": 38640, "start": 403.9, "end": 408.79999999999995, "text": " Ale \u017ceby by\u0142o jeszcze ciekawiej, w innym scenariuszu OpenBook QA,", "tokens": [51239, 9366, 11316, 14811, 14168, 46419, 1607, 7764, 11, 261, 294, 12996, 4191, 3504, 301, 11728, 7238, 19203, 1249, 32, 11, 51484], "temperature": 0.0, "avg_logprob": -0.1689979397520727, "compression_ratio": 1.4014084507042253, "no_speech_prob": 0.0004094210744369775}, {"id": 116, "seek": 38640, "start": 409.0, "end": 413.79999999999995, "text": " opartym na faktach, zaobserwowano dok\u0142adnie odwrotn\u0105 zale\u017cno\u015b\u0107.", "tokens": [51494, 999, 446, 4199, 1667, 21310, 608, 11, 7949, 16537, 260, 34354, 3730, 45864, 2766, 3611, 7449, 310, 13113, 710, 45494, 23293, 13, 51734], "temperature": 0.0, "avg_logprob": -0.1689979397520727, "compression_ratio": 1.4014084507042253, "no_speech_prob": 0.0004094210744369775}, {"id": 117, "seek": 38640, "start": 413.9, "end": 414.4, "text": " Czyli?", "tokens": [51739, 37099, 30, 51764], "temperature": 0.0, "avg_logprob": -0.1689979397520727, "compression_ratio": 1.4014084507042253, "no_speech_prob": 0.0004094210744369775}, {"id": 118, "seek": 41440, "start": 414.4, "end": 417.4, "text": " Tam poprawa dok\u0142adno\u015bci polepsza\u0142a kalibracj\u0119.", "tokens": [50364, 8540, 1665, 424, 4151, 45864, 16438, 13208, 1878, 2394, 5024, 7788, 6414, 29924, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11620765118985563, "compression_ratio": 1.4965753424657535, "no_speech_prob": 0.001158737693913281}, {"id": 119, "seek": 41440, "start": 417.4, "end": 417.9, "text": " Aha.", "tokens": [50514, 27448, 13, 50539], "temperature": 0.0, "avg_logprob": -0.11620765118985563, "compression_ratio": 1.4965753424657535, "no_speech_prob": 0.001158737693913281}, {"id": 120, "seek": 41440, "start": 418.59999999999997, "end": 421.0, "text": " To pokazuje, \u017ce nie ma jednej prostej regu\u0142y.", "tokens": [50574, 1407, 13010, 43317, 11, 3561, 2838, 463, 5232, 11794, 10293, 40779, 1121, 84, 6825, 13, 50694], "temperature": 0.0, "avg_logprob": -0.11620765118985563, "compression_ratio": 1.4965753424657535, "no_speech_prob": 0.001158737693913281}, {"id": 121, "seek": 41440, "start": 421.09999999999997, "end": 424.7, "text": " Zachowanie modelu jest silnie uzale\u017cnione od kontekstu zadania.", "tokens": [50699, 21028, 22028, 2316, 84, 3492, 3425, 2766, 16851, 45494, 77, 5328, 3611, 14373, 916, 372, 84, 42788, 5609, 13, 50879], "temperature": 0.0, "avg_logprob": -0.11620765118985563, "compression_ratio": 1.4965753424657535, "no_speech_prob": 0.001158737693913281}, {"id": 122, "seek": 41440, "start": 425.2, "end": 428.09999999999997, "text": " Nie mo\u017cna powiedzie\u0107, ten model ma dobr\u0105 kalibracj\u0119.", "tokens": [50904, 12016, 17790, 27886, 11, 2064, 2316, 463, 23067, 1611, 7788, 6414, 29924, 13, 51049], "temperature": 0.0, "avg_logprob": -0.11620765118985563, "compression_ratio": 1.4965753424657535, "no_speech_prob": 0.001158737693913281}, {"id": 123, "seek": 41440, "start": 428.4, "end": 430.59999999999997, "text": " Trzeba zapyta\u0107, dobr\u0105 kalibracj\u0119?", "tokens": [51064, 1765, 1381, 4231, 14223, 88, 42931, 11, 23067, 1611, 7788, 6414, 29924, 30, 51174], "temperature": 0.0, "avg_logprob": -0.11620765118985563, "compression_ratio": 1.4965753424657535, "no_speech_prob": 0.001158737693913281}, {"id": 124, "seek": 41440, "start": 430.79999999999995, "end": 431.79999999999995, "text": " W jakim zadaniu?", "tokens": [51184, 343, 49410, 42788, 25849, 30, 51234], "temperature": 0.0, "avg_logprob": -0.11620765118985563, "compression_ratio": 1.4965753424657535, "no_speech_prob": 0.001158737693913281}, {"id": 125, "seek": 41440, "start": 432.59999999999997, "end": 437.9, "text": " To ju\u017c jest ciekawe, ale jest jeszcze jedno odkrycie, kt\u00f3re wydaje mi si\u0119 kompletnie wbrew intuicji.", "tokens": [51274, 1407, 10678, 3492, 30596, 2330, 826, 11, 6775, 3492, 14168, 5232, 1771, 3611, 43298, 4260, 11, 8864, 49165, 2752, 3244, 5207, 14657, 2766, 261, 65, 2236, 560, 84, 299, 4013, 13, 51539], "temperature": 0.0, "avg_logprob": -0.11620765118985563, "compression_ratio": 1.4965753424657535, "no_speech_prob": 0.001158737693913281}, {"id": 126, "seek": 41440, "start": 438.4, "end": 441.5, "text": " Chodzi o modele trenowane na kodzie programistycznym.", "tokens": [51564, 761, 14543, 277, 4391, 306, 23136, 23066, 1667, 350, 378, 3283, 1461, 468, 17466, 12996, 13, 51719], "temperature": 0.0, "avg_logprob": -0.11620765118985563, "compression_ratio": 1.4965753424657535, "no_speech_prob": 0.001158737693913281}, {"id": 127, "seek": 44150, "start": 441.5, "end": 447.4, "text": " Tak, na pierwszy rzod oka mo\u017cna by pomy\u015ble\u0107, \u017ce b\u0119d\u0105 dobre w, no c\u00f3\u017c, w pisaniu kodu.", "tokens": [50364, 9118, 11, 1667, 34016, 367, 89, 378, 277, 2330, 17790, 538, 280, 8488, 1788, 306, 2162, 11, 3561, 26239, 41959, 261, 11, 572, 6333, 1427, 11, 261, 26584, 25849, 350, 34873, 13, 50659], "temperature": 0.0, "avg_logprob": -0.13052067140332696, "compression_ratio": 1.3945578231292517, "no_speech_prob": 0.004763077013194561}, {"id": 128, "seek": 44150, "start": 447.6, "end": 448.1, "text": " I tyle.", "tokens": [50669, 286, 39293, 13, 50694], "temperature": 0.0, "avg_logprob": -0.13052067140332696, "compression_ratio": 1.3945578231292517, "no_speech_prob": 0.004763077013194561}, {"id": 129, "seek": 44150, "start": 448.2, "end": 453.6, "text": " Ale okazuje si\u0119, \u017ce ta umiej\u0119tno\u015b\u0107 przenosi si\u0119 na co\u015b znacznie szerszego.", "tokens": [50699, 9366, 3133, 43317, 3244, 11, 3561, 1846, 1105, 7764, 46788, 23293, 582, 2904, 21521, 3244, 1667, 19241, 15397, 14875, 2766, 7870, 433, 27725, 13, 50969], "temperature": 0.0, "avg_logprob": -0.13052067140332696, "compression_ratio": 1.3945578231292517, "no_speech_prob": 0.004763077013194561}, {"id": 130, "seek": 44150, "start": 453.7, "end": 457.2, "text": " Na co\u015b, co z pozoru nie ma z kodem nic wsp\u00f3lnego.", "tokens": [50974, 6056, 19241, 11, 598, 710, 21281, 32963, 2838, 463, 710, 350, 378, 443, 6201, 47148, 11858, 13, 51149], "temperature": 0.0, "avg_logprob": -0.13052067140332696, "compression_ratio": 1.3945578231292517, "no_speech_prob": 0.004763077013194561}, {"id": 131, "seek": 44150, "start": 457.4, "end": 460.5, "text": " Na przyk\u0142ad na rozumowanie w j\u0119zyku naturalnym?", "tokens": [51159, 6056, 23144, 1667, 48797, 22028, 261, 49055, 5279, 3303, 12996, 30, 51314], "temperature": 0.0, "avg_logprob": -0.13052067140332696, "compression_ratio": 1.3945578231292517, "no_speech_prob": 0.004763077013194561}, {"id": 132, "seek": 44150, "start": 460.6, "end": 461.3, "text": " Dok\u0142adnie.", "tokens": [51319, 29768, 10358, 2766, 13, 51354], "temperature": 0.0, "avg_logprob": -0.13052067140332696, "compression_ratio": 1.3945578231292517, "no_speech_prob": 0.004763077013194561}, {"id": 133, "seek": 44150, "start": 461.4, "end": 469.4, "text": " Wzi\u0119li model kod Da Vinci 002 zoptymalizowany pod k\u0105tem kodu i postawili go przed testem matematycznym GSM 8K.", "tokens": [51359, 343, 16706, 2081, 2316, 350, 378, 3933, 15011, 537, 7143, 17, 710, 404, 874, 5579, 590, 23341, 2497, 350, 1611, 18275, 350, 34873, 741, 2183, 1607, 2312, 352, 18334, 1500, 443, 3803, 8615, 17466, 12996, 460, 26693, 1649, 42, 13, 51759], "temperature": 0.0, "avg_logprob": -0.13052067140332696, "compression_ratio": 1.3945578231292517, "no_speech_prob": 0.004763077013194561}, {"id": 134, "seek": 46940, "start": 469.5, "end": 473.29999999999995, "text": " To s\u0105 te zadania tekstowe, jak ze szko\u0142y typu jasio ma pi\u0119\u0107 jab\u0142ek?", "tokens": [50369, 1407, 9015, 535, 42788, 5609, 16624, 372, 6880, 11, 4207, 5277, 7870, 4093, 6825, 2125, 84, 361, 296, 1004, 463, 32677, 2162, 33475, 1221, 916, 30, 50559], "temperature": 0.0, "avg_logprob": -0.1439932592166877, "compression_ratio": 1.4271844660194175, "no_speech_prob": 0.005680213216692209}, {"id": 135, "seek": 46940, "start": 473.4, "end": 474.29999999999995, "text": " Tak, dok\u0142adnie.", "tokens": [50564, 9118, 11, 45864, 2766, 13, 50609], "temperature": 0.0, "avg_logprob": -0.1439932592166877, "compression_ratio": 1.4271844660194175, "no_speech_prob": 0.005680213216692209}, {"id": 136, "seek": 46940, "start": 474.4, "end": 482.59999999999997, "text": " I ten model, trenowany na Pythonie i Javascriptie, po prostu zmiarzy\u0142 najlepszy model czysto tekstowy, czyli tekst Da Vinci 002.", "tokens": [50614, 286, 2064, 2316, 11, 23136, 23341, 1667, 15329, 414, 741, 508, 37331, 5944, 414, 11, 714, 19518, 710, 3057, 289, 1229, 1221, 41903, 1878, 1229, 2316, 6430, 20875, 16624, 372, 10089, 11, 16591, 16624, 372, 3933, 15011, 537, 7143, 17, 13, 51024], "temperature": 0.0, "avg_logprob": -0.1439932592166877, "compression_ratio": 1.4271844660194175, "no_speech_prob": 0.005680213216692209}, {"id": 137, "seek": 46940, "start": 482.7, "end": 484.9, "text": " Zmiarzy\u0142, jak du\u017ca by\u0142a ta r\u00f3\u017cnica.", "tokens": [51029, 1176, 3057, 289, 1229, 1221, 11, 4207, 21783, 64, 23936, 1846, 19637, 32687, 13, 51139], "temperature": 0.0, "avg_logprob": -0.1439932592166877, "compression_ratio": 1.4271844660194175, "no_speech_prob": 0.005680213216692209}, {"id": 138, "seek": 46940, "start": 485.0, "end": 485.7, "text": " Ogromna.", "tokens": [51144, 422, 861, 298, 629, 13, 51179], "temperature": 0.0, "avg_logprob": -0.1439932592166877, "compression_ratio": 1.4271844660194175, "no_speech_prob": 0.005680213216692209}, {"id": 139, "seek": 46940, "start": 485.79999999999995, "end": 495.9, "text": " Kod Da Vinci 002 osi\u0105gn\u0105\u0142 52,1% dok\u0142adno\u015bci, podczas gdy jego tekstowy odpowiednik z tej samej rodziny tylko 35%.", "tokens": [51184, 591, 378, 3933, 15011, 537, 7143, 17, 3003, 11404, 4568, 1611, 1221, 18079, 11, 16, 4, 45864, 16438, 11, 2497, 30989, 28405, 26542, 16624, 372, 10089, 36574, 13123, 710, 12573, 912, 73, 28607, 3519, 13219, 6976, 6856, 51689], "temperature": 0.0, "avg_logprob": -0.1439932592166877, "compression_ratio": 1.4271844660194175, "no_speech_prob": 0.005680213216692209}, {"id": 140, "seek": 46940, "start": 496.0, "end": 498.4, "text": " To nie jest b\u0142\u0105d statystyczny, to jest przepa\u015b\u0107.", "tokens": [51694, 1407, 2838, 3492, 272, 15926, 67, 2219, 38593, 17466, 1634, 11, 281, 3492, 30829, 64, 7753, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1439932592166877, "compression_ratio": 1.4271844660194175, "no_speech_prob": 0.005680213216692209}, {"id": 141, "seek": 49840, "start": 498.4, "end": 506.5, "text": " Zdecydowanie. Wygl\u0105da na to, \u017ce nauka tej \u015bcis\u0142ej, logicznej struktury kodu uczy model pewnego rodzaju abstrakcyjnego rozumowania.", "tokens": [50364, 1176, 1479, 1344, 67, 22028, 13, 14458, 7191, 26398, 1667, 281, 11, 3561, 35616, 2330, 12573, 8299, 26720, 19827, 73, 11, 9952, 89, 11794, 342, 19977, 2598, 350, 34873, 344, 6522, 2316, 25889, 11858, 28607, 33166, 10823, 11272, 42949, 11858, 48797, 21308, 13, 50769], "temperature": 0.0, "avg_logprob": -0.1106146229993577, "compression_ratio": 1.4397394136807817, "no_speech_prob": 0.003519519232213497}, {"id": 142, "seek": 49840, "start": 506.59999999999997, "end": 509.9, "text": " Kt\u00f3re mo\u017ce potem zastosowa\u0107 w zupe\u0142nie innej domenie.", "tokens": [50774, 591, 4547, 265, 12034, 36513, 36746, 329, 11445, 261, 49922, 294, 11794, 3285, 268, 414, 13, 50939], "temperature": 0.0, "avg_logprob": -0.1106146229993577, "compression_ratio": 1.4397394136807817, "no_speech_prob": 0.003519519232213497}, {"id": 143, "seek": 49840, "start": 510.0, "end": 511.09999999999997, "text": " Niesamowite.", "tokens": [50944, 426, 530, 335, 305, 642, 13, 50999], "temperature": 0.0, "avg_logprob": -0.1106146229993577, "compression_ratio": 1.4397394136807817, "no_speech_prob": 0.003519519232213497}, {"id": 144, "seek": 49840, "start": 511.2, "end": 515.9, "text": " To tak, jakby nauka \u0142aciny, czyni\u0142a ci\u0119 lepszym w rozwi\u0105zywaniu zagadek logicznych.", "tokens": [51004, 1407, 991, 11, 28976, 35616, 2330, 25387, 326, 3519, 11, 6430, 3722, 5024, 35484, 476, 1878, 26681, 261, 9544, 18234, 1229, 86, 25849, 27001, 762, 74, 9952, 89, 9399, 13, 51239], "temperature": 0.0, "avg_logprob": -0.1106146229993577, "compression_ratio": 1.4397394136807817, "no_speech_prob": 0.003519519232213497}, {"id": 145, "seek": 49840, "start": 516.0, "end": 516.8, "text": " Co\u015b w tym jest.", "tokens": [51244, 3066, 1788, 261, 8107, 3492, 13, 51284], "temperature": 0.0, "avg_logprob": -0.1106146229993577, "compression_ratio": 1.4397394136807817, "no_speech_prob": 0.003519519232213497}, {"id": 146, "seek": 49840, "start": 516.9, "end": 523.6, "text": " Ale jest jeszcze jedno odkrecie, kt\u00f3re w materia\u0142ach \u017ar\u00f3d\u0142owych jest opisane jako fundamentalne wyzwanie dla ca\u0142ej dziedziny.", "tokens": [51289, 9366, 3492, 14168, 5232, 1771, 3611, 27885, 4260, 11, 8864, 261, 2389, 8908, 608, 50212, 43678, 1221, 19605, 3492, 45477, 1929, 17123, 8088, 716, 4628, 14406, 7155, 12285, 47631, 73, 9758, 15338, 3519, 13, 51624], "temperature": 0.0, "avg_logprob": -0.1106146229993577, "compression_ratio": 1.4397394136807817, "no_speech_prob": 0.003519519232213497}, {"id": 147, "seek": 52360, "start": 524.0, "end": 529.1, "text": " To, jak niewiarygodnie wra\u017cliwe s\u0105 te modele na sam spos\u00f3b, w jaki zadajemy im pytania.", "tokens": [50384, 1407, 11, 4207, 43622, 29104, 21787, 2766, 7843, 1427, 2081, 826, 9015, 535, 4391, 306, 1667, 3247, 22904, 11, 261, 24492, 710, 1538, 73, 3633, 566, 25878, 5609, 13, 50639], "temperature": 0.0, "avg_logprob": -0.15340004211817032, "compression_ratio": 1.4241486068111455, "no_speech_prob": 0.01808786578476429}, {"id": 148, "seek": 52360, "start": 529.2, "end": 533.3000000000001, "text": " Och, to jest chyba najbardziej szokuj\u0105cy w wyniku ca\u0142ego badania.", "tokens": [50644, 13128, 11, 281, 3492, 31532, 41857, 7870, 453, 13263, 1344, 261, 31936, 24320, 35224, 6308, 1578, 5609, 13, 50849], "temperature": 0.0, "avg_logprob": -0.15340004211817032, "compression_ratio": 1.4241486068111455, "no_speech_prob": 0.01808786578476429}, {"id": 149, "seek": 52360, "start": 533.4, "end": 537.6, "text": " Podwa\u017ca wiele naszych podstawowych za\u0142o\u017ce\u0144 o tym, co my w og\u00f3le mierzymy.", "tokens": [50854, 12646, 27111, 64, 33137, 45002, 43443, 19605, 7949, 5249, 2875, 5248, 277, 8107, 11, 598, 452, 261, 29229, 47448, 1229, 2226, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15340004211817032, "compression_ratio": 1.4241486068111455, "no_speech_prob": 0.01808786578476429}, {"id": 150, "seek": 52360, "start": 537.7, "end": 538.7, "text": " Daj jaki\u015b przyk\u0142ad.", "tokens": [51069, 413, 1805, 34721, 23144, 13, 51119], "temperature": 0.0, "avg_logprob": -0.15340004211817032, "compression_ratio": 1.4241486068111455, "no_speech_prob": 0.01808786578476429}, {"id": 151, "seek": 52360, "start": 538.8000000000001, "end": 544.6, "text": " Dobrze, test wielokrotnego wyboru Hellaswag i model OPTI-175B od mety.", "tokens": [51124, 29679, 13503, 11, 1500, 20570, 453, 10536, 11858, 4628, 3918, 84, 12090, 296, 86, 559, 741, 2316, 23324, 5422, 12, 7773, 20, 33, 3611, 1131, 88, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15340004211817032, "compression_ratio": 1.4241486068111455, "no_speech_prob": 0.01808786578476429}, {"id": 152, "seek": 52360, "start": 544.7, "end": 551.9, "text": " Kiedy przedstawiono mu zadanie w jednym formacie, daj\u0105c pocz\u0105tek zdania i ka\u017cd\u0105 z czterech mo\u017cliwych odpowiedzi osobno.", "tokens": [51419, 591, 16446, 45616, 49020, 2992, 42788, 7155, 261, 5232, 12996, 1254, 30805, 11, 1120, 8555, 66, 34397, 916, 16221, 5609, 741, 21912, 67, 1611, 710, 269, 2682, 323, 339, 30854, 9726, 339, 36574, 3992, 41518, 1771, 13, 51779], "temperature": 0.0, "avg_logprob": -0.15340004211817032, "compression_ratio": 1.4241486068111455, "no_speech_prob": 0.01808786578476429}, {"id": 153, "seek": 52360, "start": 552.0, "end": 552.4, "text": " Tak.", "tokens": [51784, 9118, 13, 51804], "temperature": 0.0, "avg_logprob": -0.15340004211817032, "compression_ratio": 1.4241486068111455, "no_speech_prob": 0.01808786578476429}, {"id": 154, "seek": 55240, "start": 552.4, "end": 555.1, "text": " Prosz\u0105c o ocen\u0119 prawdopodobie\u0144stwa ka\u017cdej z nich?", "tokens": [50364, 26024, 8925, 66, 277, 10409, 268, 1274, 41175, 46684, 996, 414, 12229, 4151, 21912, 1479, 73, 710, 25570, 30, 50499], "temperature": 0.0, "avg_logprob": -0.14983143041163316, "compression_ratio": 1.3936507936507936, "no_speech_prob": 0.040336258709430695}, {"id": 155, "seek": 55240, "start": 555.1999999999999, "end": 555.6999999999999, "text": " Tak.", "tokens": [50504, 9118, 13, 50529], "temperature": 0.0, "avg_logprob": -0.14983143041163316, "compression_ratio": 1.3936507936507936, "no_speech_prob": 0.040336258709430695}, {"id": 156, "seek": 55240, "start": 555.8, "end": 560.0, "text": " Model osi\u0105gn\u0105\u0142 imponuj\u0105ce 79,1% dok\u0142adno\u015bci.", "tokens": [50534, 17105, 3003, 11404, 4568, 1611, 1221, 704, 266, 13263, 384, 32803, 11, 16, 4, 45864, 16438, 13, 50744], "temperature": 0.0, "avg_logprob": -0.14983143041163316, "compression_ratio": 1.3936507936507936, "no_speech_prob": 0.040336258709430695}, {"id": 157, "seek": 55240, "start": 560.1, "end": 561.6, "text": " Czyli radzi\u0142 sobie \u015bwietnie.", "tokens": [50749, 37099, 2843, 3992, 1221, 13652, 8299, 39083, 2766, 13, 50824], "temperature": 0.0, "avg_logprob": -0.14983143041163316, "compression_ratio": 1.3936507936507936, "no_speech_prob": 0.040336258709430695}, {"id": 158, "seek": 55240, "start": 561.6999999999999, "end": 565.3, "text": " Ale co si\u0119 sta\u0142o, gdy zmieniono tylko format pytania, nie jego tre\u015b\u0107?", "tokens": [50829, 9366, 598, 3244, 11135, 5249, 11, 28405, 17020, 1053, 49020, 13219, 7877, 25878, 5609, 11, 2838, 26542, 2192, 7753, 30, 51009], "temperature": 0.0, "avg_logprob": -0.14983143041163316, "compression_ratio": 1.3936507936507936, "no_speech_prob": 0.040336258709430695}, {"id": 159, "seek": 55240, "start": 565.4, "end": 571.6, "text": " Gdy te same odpowiedzi po\u0142\u0105czono w jeden prompt, tak jest to wygl\u0105da na klasycznym egzaminie w szkole.", "tokens": [51014, 460, 3173, 535, 912, 36574, 3992, 714, 43558, 8957, 261, 12906, 12391, 11, 991, 3492, 281, 32015, 1667, 9671, 5871, 3689, 12996, 24263, 89, 7428, 414, 261, 7870, 4093, 306, 13, 51324], "temperature": 0.0, "avg_logprob": -0.14983143041163316, "compression_ratio": 1.3936507936507936, "no_speech_prob": 0.040336258709430695}, {"id": 160, "seek": 55240, "start": 571.6999999999999, "end": 576.1999999999999, "text": " Oto pytanie, a oto opcj\u0119 A, B, C, D. Wybierz poprawn\u0105.", "tokens": [51329, 422, 1353, 36610, 11, 257, 277, 1353, 999, 41960, 316, 11, 363, 11, 383, 11, 413, 13, 14458, 65, 34602, 1665, 29603, 1611, 13, 51554], "temperature": 0.0, "avg_logprob": -0.14983143041163316, "compression_ratio": 1.3936507936507936, "no_speech_prob": 0.040336258709430695}, {"id": 161, "seek": 55240, "start": 576.3, "end": 577.3, "text": " Dok\u0142adnie tak.", "tokens": [51559, 29768, 10358, 2766, 991, 13, 51609], "temperature": 0.0, "avg_logprob": -0.14983143041163316, "compression_ratio": 1.3936507936507936, "no_speech_prob": 0.040336258709430695}, {"id": 162, "seek": 55240, "start": 577.4, "end": 580.5, "text": " Jego dok\u0142adno\u015b\u0107 spad\u0142a na \u0142eb na szyj\u0105.", "tokens": [51614, 508, 6308, 45864, 23293, 637, 345, 5024, 1667, 220, 19827, 65, 1667, 30526, 8555, 13, 51769], "temperature": 0.0, "avg_logprob": -0.14983143041163316, "compression_ratio": 1.3936507936507936, "no_speech_prob": 0.040336258709430695}, {"id": 163, "seek": 58050, "start": 580.7, "end": 582.9, "text": " Do 30 i 2%.", "tokens": [50374, 1144, 2217, 741, 568, 6856, 50484], "temperature": 0.0, "avg_logprob": -0.12319438934326171, "compression_ratio": 1.3881118881118881, "no_speech_prob": 0.05111346393823624}, {"id": 164, "seek": 58050, "start": 583.0, "end": 583.7, "text": " Czekaj, czekaj.", "tokens": [50489, 383, 19878, 1805, 11, 6472, 916, 1805, 13, 50524], "temperature": 0.0, "avg_logprob": -0.12319438934326171, "compression_ratio": 1.3881118881118881, "no_speech_prob": 0.05111346393823624}, {"id": 165, "seek": 58050, "start": 583.8, "end": 586.5, "text": " Sprawie 80% do 30.", "tokens": [50529, 1738, 5131, 414, 4688, 4, 360, 2217, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12319438934326171, "compression_ratio": 1.3881118881118881, "no_speech_prob": 0.05111346393823624}, {"id": 166, "seek": 58050, "start": 586.6, "end": 589.5, "text": " Przecie\u017c to jest poni\u017cej progu losowego zgadywania.", "tokens": [50669, 2114, 1381, 40082, 281, 3492, 9224, 72, 38493, 447, 2794, 1750, 26576, 40948, 880, 86, 5609, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12319438934326171, "compression_ratio": 1.3881118881118881, "no_speech_prob": 0.05111346393823624}, {"id": 167, "seek": 58050, "start": 589.6, "end": 591.8, "text": " Przy czterech opcjach to 25%.", "tokens": [50819, 39590, 269, 2682, 323, 339, 999, 66, 45059, 281, 3552, 6856, 50929], "temperature": 0.0, "avg_logprob": -0.12319438934326171, "compression_ratio": 1.3881118881118881, "no_speech_prob": 0.05111346393823624}, {"id": 168, "seek": 58050, "start": 591.9, "end": 593.1, "text": " W\u0142a\u015bnie.", "tokens": [50934, 343, 5024, 12221, 13, 50994], "temperature": 0.0, "avg_logprob": -0.12319438934326171, "compression_ratio": 1.3881118881118881, "no_speech_prob": 0.05111346393823624}, {"id": 169, "seek": 58050, "start": 593.2, "end": 596.1, "text": " Ten sam model, ta sama wiedza, to samo zadanie.", "tokens": [50999, 9380, 3247, 2316, 11, 1846, 17768, 46894, 2394, 11, 281, 36422, 42788, 7155, 13, 51144], "temperature": 0.0, "avg_logprob": -0.12319438934326171, "compression_ratio": 1.3881118881118881, "no_speech_prob": 0.05111346393823624}, {"id": 170, "seek": 58050, "start": 596.2, "end": 603.1, "text": " Drobna zmiana w formatowaniu pytania spowodowa\u0142a katastrofalny, kompletny kolaps wydajno\u015bci.", "tokens": [51149, 413, 16614, 629, 17020, 8497, 261, 7877, 305, 25849, 25878, 5609, 637, 305, 378, 5528, 5024, 16536, 525, 340, 36474, 1634, 11, 5207, 14657, 1634, 17818, 2382, 25984, 1805, 16438, 13, 51494], "temperature": 0.0, "avg_logprob": -0.12319438934326171, "compression_ratio": 1.3881118881118881, "no_speech_prob": 0.05111346393823624}, {"id": 171, "seek": 58050, "start": 603.2, "end": 609.0, "text": " To tak jakby Einstein nie potrafi\u0142 rozwi\u0105za\u0107 prostego zadania z fizyki, bo kto\u015b zapisa\u0142 je inn\u0105 czcionk\u0105.", "tokens": [51499, 1407, 991, 28976, 23486, 2838, 1847, 10437, 40622, 9544, 18234, 35873, 10293, 6308, 42788, 5609, 710, 21000, 88, 2984, 11, 748, 32982, 14223, 3837, 1221, 1506, 7714, 1611, 6472, 10015, 26304, 13, 51789], "temperature": 0.0, "avg_logprob": -0.12319438934326171, "compression_ratio": 1.3881118881118881, "no_speech_prob": 0.05111346393823624}, {"id": 172, "seek": 60900, "start": 609.1, "end": 611.4, "text": " Ta analogia jest w\u0142a\u015bciwie przera\u017caj\u0105ca.", "tokens": [50369, 6551, 16660, 654, 3492, 50108, 6541, 1663, 1427, 11133, 496, 13, 50484], "temperature": 0.0, "avg_logprob": -0.08969824806937958, "compression_ratio": 1.552112676056338, "no_speech_prob": 0.005045959260314703}, {"id": 173, "seek": 60900, "start": 611.5, "end": 615.8, "text": " Bo to by oznacza\u0142o, \u017ce ca\u0142a nasza metoda testowania jest fundamentalnie wadliwa.", "tokens": [50489, 3286, 281, 538, 277, 22672, 326, 2394, 5249, 11, 3561, 1335, 5024, 5382, 2394, 1131, 13449, 1500, 21308, 3492, 8088, 2766, 261, 345, 2081, 4151, 13, 50704], "temperature": 0.0, "avg_logprob": -0.08969824806937958, "compression_ratio": 1.552112676056338, "no_speech_prob": 0.005045959260314703}, {"id": 174, "seek": 60900, "start": 615.9, "end": 622.3, "text": " Mo\u017ce nie mierzymy wiedzy czy rozumowania, tylko zdolno\u015b\u0107 modelu do dopasowania si\u0119 do specyficznego szablonu.", "tokens": [50709, 43774, 2838, 47448, 1229, 2226, 46894, 1229, 6430, 48797, 21308, 11, 13219, 16221, 401, 23293, 2316, 84, 360, 360, 20990, 21308, 3244, 360, 768, 1344, 1786, 89, 11858, 7870, 455, 14864, 84, 13, 51029], "temperature": 0.0, "avg_logprob": -0.08969824806937958, "compression_ratio": 1.552112676056338, "no_speech_prob": 0.005045959260314703}, {"id": 175, "seek": 60900, "start": 622.4, "end": 628.7, "text": " I to jest w\u0142a\u015bnie pytanie za milion dolar\u00f3w, kt\u00f3re stawia helm, co prowadzi nas prosto do szerszych implikacji.", "tokens": [51034, 286, 281, 3492, 14234, 36610, 7949, 1962, 313, 360, 2200, 3901, 11, 8864, 342, 34953, 29554, 11, 598, 36590, 3992, 5382, 10293, 78, 360, 7870, 433, 28051, 8484, 1035, 13152, 13, 51349], "temperature": 0.0, "avg_logprob": -0.08969824806937958, "compression_ratio": 1.552112676056338, "no_speech_prob": 0.005045959260314703}, {"id": 176, "seek": 60900, "start": 628.8, "end": 631.2, "text": " Co te wszystkie odkrycia oznaczaj\u0105 w praktyce?", "tokens": [51354, 3066, 535, 31723, 3611, 43298, 2755, 277, 22672, 14875, 11133, 261, 3206, 74, 874, 384, 30, 51474], "temperature": 0.0, "avg_logprob": -0.08969824806937958, "compression_ratio": 1.552112676056338, "no_speech_prob": 0.005045959260314703}, {"id": 177, "seek": 60900, "start": 631.3, "end": 633.1, "text": " No w\u0142a\u015bnie, co to wszystko oznacza?", "tokens": [51479, 883, 14234, 11, 598, 281, 22607, 277, 22672, 326, 2394, 30, 51569], "temperature": 0.0, "avg_logprob": -0.08969824806937958, "compression_ratio": 1.552112676056338, "no_speech_prob": 0.005045959260314703}, {"id": 178, "seek": 60900, "start": 633.2, "end": 638.9, "text": " Pierwszy wniosek, jaki mi si\u0119 nasuwa to, \u017ce ta obsesja na punkcie wielko\u015bci modeli, liczby parametr\u00f3w,", "tokens": [51574, 16676, 30012, 261, 3722, 541, 74, 11, 24492, 2752, 3244, 5382, 84, 4151, 281, 11, 3561, 1846, 3181, 279, 2938, 1667, 25188, 4260, 20570, 4093, 6199, 2316, 72, 11, 6169, 89, 2322, 6220, 27965, 3901, 11, 51859], "temperature": 0.0, "avg_logprob": -0.08969824806937958, "compression_ratio": 1.552112676056338, "no_speech_prob": 0.005045959260314703}, {"id": 179, "seek": 63900, "start": 639.0, "end": 640.6, "text": " jest chyba troch\u0119 przysadzona.", "tokens": [50364, 3492, 31532, 24926, 6541, 749, 345, 13383, 13, 50444], "temperature": 0.0, "avg_logprob": -0.11667634295178698, "compression_ratio": 1.4469453376205788, "no_speech_prob": 0.0015508461510762572}, {"id": 180, "seek": 63900, "start": 640.7, "end": 644.1, "text": " Jest. A przynajmniej nie jest to jedyny czynnik.", "tokens": [50449, 24918, 13, 316, 6501, 20981, 47658, 2838, 3492, 281, 5232, 88, 1634, 6430, 77, 13123, 13, 50619], "temperature": 0.0, "avg_logprob": -0.11667634295178698, "compression_ratio": 1.4469453376205788, "no_speech_prob": 0.0015508461510762572}, {"id": 181, "seek": 63900, "start": 644.2, "end": 650.8, "text": " Helm pokazuje, \u017ce cho\u0107 w ramach jednej rodziny modeli, na przyk\u0142ad GPT-3 wi\u0119kszy zazwyczaj znaczy lepszy,", "tokens": [50624, 6128, 76, 13010, 43317, 11, 3561, 1586, 2162, 261, 10211, 608, 5232, 11794, 28607, 3519, 2316, 72, 11, 1667, 23144, 26039, 51, 12, 18, 29968, 1229, 710, 921, 9726, 3689, 1805, 36584, 476, 1878, 1229, 11, 50954], "temperature": 0.0, "avg_logprob": -0.11667634295178698, "compression_ratio": 1.4469453376205788, "no_speech_prob": 0.0015508461510762572}, {"id": 182, "seek": 63900, "start": 650.9, "end": 654.3, "text": " to w por\u00f3wnaniach mi\u0119dzy r\u00f3\u017cnymi rodzinami ta zasada ju\u017c nie obowi\u0105zuje.", "tokens": [50959, 281, 261, 1515, 812, 895, 3782, 608, 33964, 19637, 31813, 8685, 23584, 4526, 1846, 26530, 1538, 10678, 2838, 1111, 47886, 11728, 2884, 13, 51129], "temperature": 0.0, "avg_logprob": -0.11667634295178698, "compression_ratio": 1.4469453376205788, "no_speech_prob": 0.0015508461510762572}, {"id": 183, "seek": 63900, "start": 654.4, "end": 660.3, "text": " Dok\u0142adnie. Wszystkie najskuteczniejsze modele mia\u0142y co najmniej 50 miliard\u00f3w parametr\u00f3w, to prawda.", "tokens": [51134, 29768, 10358, 2766, 13, 343, 10424, 22872, 11212, 5161, 1169, 3689, 44258, 4391, 306, 21290, 6825, 598, 11212, 47658, 2625, 1962, 72, 515, 3901, 6220, 27965, 3901, 11, 281, 43607, 13, 51429], "temperature": 0.0, "avg_logprob": -0.11667634295178698, "compression_ratio": 1.4469453376205788, "no_speech_prob": 0.0015508461510762572}, {"id": 184, "seek": 63900, "start": 660.4, "end": 668.2, "text": " Ale niekt\u00f3re z absolutnie najlepszych jak Anthropix, LMV4S3 z 52 miliardami", "tokens": [51434, 9366, 2838, 43073, 265, 710, 18757, 2766, 41903, 1878, 28051, 4207, 12727, 1513, 970, 11, 46529, 53, 19, 50, 18, 710, 18079, 1962, 72, 515, 4526, 51824], "temperature": 0.0, "avg_logprob": -0.11667634295178698, "compression_ratio": 1.4469453376205788, "no_speech_prob": 0.0015508461510762572}, {"id": 185, "seek": 66820, "start": 668.2, "end": 670.9000000000001, "text": " regularnie pokonywa\u0142y znacznie wi\u0119kszych konkurent\u00f3w.", "tokens": [50364, 3890, 2766, 13010, 2526, 4151, 6825, 15397, 14875, 2766, 29968, 28051, 21428, 540, 580, 3901, 13, 50499], "temperature": 0.0, "avg_logprob": -0.11327263134628979, "compression_ratio": 1.394648829431438, "no_speech_prob": 0.012690288946032524}, {"id": 186, "seek": 66820, "start": 671.0, "end": 675.5, "text": " Czyli musz\u0105 istnie\u0107 inne, kluczowe sk\u0142adniki tej tajemniczej receptury? Jakie?", "tokens": [50504, 37099, 1038, 8925, 1418, 2766, 2162, 24170, 11, 9671, 1311, 89, 6880, 1110, 10358, 77, 9850, 12573, 256, 1805, 443, 7692, 16920, 15263, 2598, 30, 15029, 414, 30, 50729], "temperature": 0.0, "avg_logprob": -0.11327263134628979, "compression_ratio": 1.394648829431438, "no_speech_prob": 0.012690288946032524}, {"id": 187, "seek": 66820, "start": 675.6, "end": 680.1, "text": " Dwa wydaj\u0105 si\u0119 najwa\u017cniejsze. Po pierwsze, dane treningowe.", "tokens": [50734, 413, 4151, 25984, 11133, 3244, 11212, 27111, 44258, 13, 6165, 45994, 11, 49206, 2192, 773, 6880, 13, 50959], "temperature": 0.0, "avg_logprob": -0.11327263134628979, "compression_ratio": 1.394648829431438, "no_speech_prob": 0.012690288946032524}, {"id": 188, "seek": 66820, "start": 680.2, "end": 685.9000000000001, "text": " Ale po drugie i mo\u017ce nawet wa\u017cniejsze, metody dostrajania po g\u0142\u00f3wnym treningu.", "tokens": [50964, 9366, 714, 4110, 414, 741, 12034, 22696, 27777, 44258, 11, 1131, 843, 20568, 48690, 5609, 714, 18117, 812, 895, 4199, 2192, 773, 84, 13, 51249], "temperature": 0.0, "avg_logprob": -0.11327263134628979, "compression_ratio": 1.394648829431438, "no_speech_prob": 0.012690288946032524}, {"id": 189, "seek": 66820, "start": 686.0, "end": 691.3000000000001, "text": " Modele, kt\u00f3re przesz\u0142y przez procesy takie jak Instruction Tuning czy RLHF.", "tokens": [51254, 20500, 306, 11, 8864, 6541, 10430, 6825, 14064, 17565, 88, 15963, 4207, 2730, 3826, 21363, 278, 6430, 497, 43, 39, 37, 13, 51519], "temperature": 0.0, "avg_logprob": -0.11327263134628979, "compression_ratio": 1.394648829431438, "no_speech_prob": 0.012690288946032524}, {"id": 190, "seek": 66820, "start": 691.4000000000001, "end": 694.5, "text": " Czyli Reinforcement Learning from Human Feedback.", "tokens": [51524, 37099, 42116, 9382, 15205, 490, 10294, 33720, 3207, 13, 51679], "temperature": 0.0, "avg_logprob": -0.11327263134628979, "compression_ratio": 1.394648829431438, "no_speech_prob": 0.012690288946032524}, {"id": 191, "seek": 66820, "start": 694.6, "end": 695.1, "text": " Tak.", "tokens": [51684, 9118, 13, 51709], "temperature": 0.0, "avg_logprob": -0.11327263134628979, "compression_ratio": 1.394648829431438, "no_speech_prob": 0.012690288946032524}, {"id": 192, "seek": 69510, "start": 695.1, "end": 700.0, "text": " Czyli w uproszczeniu. By\u0142y wychowywane przez ludzi, kt\u00f3rzy oceniali ich odpowiedzi", "tokens": [50364, 37099, 261, 493, 2635, 89, 66, 39651, 13, 3146, 6825, 4628, 339, 10089, 86, 1929, 14064, 29586, 11, 25382, 10409, 268, 831, 72, 1893, 36574, 3992, 50609], "temperature": 0.0, "avg_logprob": -0.12592955115887758, "compression_ratio": 1.4222972972972974, "no_speech_prob": 0.0005960788112133741}, {"id": 193, "seek": 69510, "start": 700.1, "end": 702.5, "text": " i uczyli je co jest dobre, a co z\u0142e.", "tokens": [50614, 741, 344, 6522, 2081, 1506, 598, 3492, 41959, 11, 257, 598, 710, 19827, 13, 50734], "temperature": 0.0, "avg_logprob": -0.12592955115887758, "compression_ratio": 1.4222972972972974, "no_speech_prob": 0.0005960788112133741}, {"id": 194, "seek": 69510, "start": 702.6, "end": 709.0, "text": " Dok\u0142adnie. I te modele jak w\u0142a\u015bnie ten od Anthropix czy tekst DaVinci 002", "tokens": [50739, 29768, 10358, 2766, 13, 286, 535, 4391, 306, 4207, 14234, 2064, 3611, 12727, 1513, 970, 6430, 16624, 372, 3933, 53, 21961, 7143, 17, 51059], "temperature": 0.0, "avg_logprob": -0.12592955115887758, "compression_ratio": 1.4222972972972974, "no_speech_prob": 0.0005960788112133741}, {"id": 195, "seek": 69510, "start": 709.1, "end": 712.0, "text": " cz\u0119sto wykazuj\u0105 znacznie lepsze zdolno\u015bci.", "tokens": [51064, 34369, 39287, 921, 13263, 15397, 14875, 2766, 476, 1878, 1381, 16221, 401, 16438, 13, 51209], "temperature": 0.0, "avg_logprob": -0.12592955115887758, "compression_ratio": 1.4222972972972974, "no_speech_prob": 0.0005960788112133741}, {"id": 196, "seek": 69510, "start": 712.1, "end": 720.0, "text": " Nawet je\u015bli s\u0105 mniejsze okazuje si\u0119, \u017ce jako\u015b\u0107 tego wychowania mo\u017ce by\u0107 wa\u017cniejsza ni\u017c surowa wielko\u015b\u0107 m\u00f3zgu.", "tokens": [51214, 40315, 302, 25630, 9015, 275, 44258, 3133, 43317, 3244, 11, 3561, 17123, 7753, 8627, 4628, 339, 21308, 12034, 15069, 27777, 30295, 2394, 28502, 1022, 5528, 20570, 4093, 7753, 32515, 89, 2794, 13, 51609], "temperature": 0.0, "avg_logprob": -0.12592955115887758, "compression_ratio": 1.4222972972972974, "no_speech_prob": 0.0005960788112133741}, {"id": 197, "seek": 69510, "start": 720.1, "end": 723.0, "text": " A wracaj\u0105c do tego problemu z formatowaniem pyta\u0144.", "tokens": [51614, 316, 928, 326, 38757, 360, 8627, 1154, 84, 710, 7877, 37345, 4907, 10664, 1328, 5248, 13, 51759], "temperature": 0.0, "avg_logprob": -0.12592955115887758, "compression_ratio": 1.4222972972972974, "no_speech_prob": 0.0005960788112133741}, {"id": 198, "seek": 72300, "start": 723.0, "end": 726.9, "text": " Jakie to ma implikacje dla nas jako u\u017cytkownik\u00f3w czy developer\u00f3w?", "tokens": [50364, 15029, 414, 281, 463, 8484, 1035, 29293, 12285, 5382, 17123, 344, 1427, 4328, 74, 44895, 3901, 6430, 10754, 3901, 30, 50559], "temperature": 0.0, "avg_logprob": -0.0924180778297218, "compression_ratio": 1.4797507788161994, "no_speech_prob": 0.007037583738565445}, {"id": 199, "seek": 72300, "start": 727.0, "end": 731.9, "text": " Ogromne. To prowadzi do wniosku, \u017ce najlepszy model na rynku mo\u017ce by\u0107 po prostu tym,", "tokens": [50564, 422, 861, 298, 716, 13, 1407, 36590, 3992, 360, 45368, 2717, 5279, 11, 3561, 41903, 1878, 1229, 2316, 1667, 367, 2534, 5279, 12034, 15069, 714, 19518, 8107, 11, 50809], "temperature": 0.0, "avg_logprob": -0.0924180778297218, "compression_ratio": 1.4797507788161994, "no_speech_prob": 0.007037583738565445}, {"id": 200, "seek": 72300, "start": 732.0, "end": 735.9, "text": " kt\u00f3ry najlepiej pasuje do naszego konkretnego stylu zadawania pyta\u0144.", "tokens": [50814, 9913, 41903, 39699, 1736, 13008, 360, 44517, 36500, 11858, 7952, 2781, 710, 1538, 86, 5609, 10664, 1328, 5248, 13, 51009], "temperature": 0.0, "avg_logprob": -0.0924180778297218, "compression_ratio": 1.4797507788161994, "no_speech_prob": 0.007037583738565445}, {"id": 201, "seek": 72300, "start": 736.0, "end": 741.9, "text": " Czyli u\u017cyteczno\u015b\u0107 modelu w realnym \u015bwiecie mo\u017ce zale\u017cy\u0107 nie od jego miejsca w rankingu.", "tokens": [51014, 37099, 34097, 975, 3689, 23293, 2316, 84, 261, 957, 12996, 40078, 4260, 12034, 710, 1220, 39687, 2838, 3611, 26542, 18522, 44239, 261, 17833, 84, 13, 51309], "temperature": 0.0, "avg_logprob": -0.0924180778297218, "compression_ratio": 1.4797507788161994, "no_speech_prob": 0.007037583738565445}, {"id": 202, "seek": 72300, "start": 742.0, "end": 746.9, "text": " Ale od tego jak intuicyjnie potrafi on interpretowa\u0107 ludzkie intencje.", "tokens": [51314, 9366, 3611, 8627, 4207, 560, 84, 2632, 73, 2766, 1847, 10437, 72, 322, 7302, 11445, 15946, 89, 22872, 560, 22660, 2884, 13, 51559], "temperature": 0.0, "avg_logprob": -0.0924180778297218, "compression_ratio": 1.4797507788161994, "no_speech_prob": 0.007037583738565445}, {"id": 203, "seek": 72300, "start": 747.0, "end": 751.9, "text": " By\u0107 mo\u017ce w przysz\u0142o\u015bci nie b\u0119dziemy wybiera\u0107 modeli na podstawie ich mocy,", "tokens": [51564, 3146, 2162, 12034, 261, 44018, 35059, 2838, 31966, 45780, 10609, 2162, 2316, 72, 1667, 43443, 414, 1893, 705, 1344, 11, 51809], "temperature": 0.0, "avg_logprob": -0.0924180778297218, "compression_ratio": 1.4797507788161994, "no_speech_prob": 0.007037583738565445}, {"id": 204, "seek": 75190, "start": 751.9, "end": 754.8, "text": " ale na podstawie ich charakteru.", "tokens": [50364, 6775, 1667, 43443, 414, 1893, 1290, 33557, 84, 13, 50509], "temperature": 0.0, "avg_logprob": -0.0682135695841775, "compression_ratio": 1.4270833333333333, "no_speech_prob": 0.013910086825489998}, {"id": 205, "seek": 75190, "start": 754.9, "end": 758.8, "text": " To stawia ca\u0142\u0105 dziedzin\u0119 prompt engineeringu w zupe\u0142nie nowym \u015bwietle.", "tokens": [50514, 1407, 342, 34953, 1335, 15926, 9758, 15338, 259, 1274, 12391, 7043, 84, 261, 49922, 586, 4199, 8299, 39083, 306, 13, 50709], "temperature": 0.0, "avg_logprob": -0.0682135695841775, "compression_ratio": 1.4270833333333333, "no_speech_prob": 0.013910086825489998}, {"id": 206, "seek": 75190, "start": 758.9, "end": 764.8, "text": " To nie jest tylko technika, ale niemal sztuka komunikacji z nieludzk\u0105 inteligencj\u0105.", "tokens": [50714, 1407, 2838, 3492, 13219, 1537, 5439, 11, 6775, 2838, 5579, 262, 2682, 13599, 45359, 1035, 13152, 710, 297, 1187, 532, 89, 26304, 24777, 3213, 66, 8555, 13, 51009], "temperature": 0.0, "avg_logprob": -0.0682135695841775, "compression_ratio": 1.4270833333333333, "no_speech_prob": 0.013910086825489998}, {"id": 207, "seek": 75190, "start": 764.9, "end": 765.8, "text": " Mhm.", "tokens": [51014, 26272, 13, 51059], "temperature": 0.0, "avg_logprob": -0.0682135695841775, "compression_ratio": 1.4270833333333333, "no_speech_prob": 0.013910086825489998}, {"id": 208, "seek": 75190, "start": 765.9, "end": 770.8, "text": " Ale sam Helm te\u017c nie jest idealny. Autorzy otwarcie m\u00f3wi\u0105 o jego ograniczeniach.", "tokens": [51064, 9366, 3247, 6128, 76, 9516, 2838, 3492, 7157, 1634, 13, 6049, 284, 1229, 4337, 6925, 4260, 46591, 277, 26542, 34416, 30732, 42124, 608, 13, 51309], "temperature": 0.0, "avg_logprob": -0.0682135695841775, "compression_ratio": 1.4270833333333333, "no_speech_prob": 0.013910086825489998}, {"id": 209, "seek": 75190, "start": 770.9, "end": 776.8, "text": " Tak i to jest bardzo uczciwe z ich strony. Podkre\u015blaj\u0105, \u017ce Helm to \u017cywy benchmark.", "tokens": [51314, 9118, 741, 281, 3492, 9034, 35403, 537, 826, 710, 1893, 32406, 13, 12646, 27885, 1788, 875, 8555, 11, 3561, 6128, 76, 281, 16136, 9726, 18927, 13, 51609], "temperature": 0.0, "avg_logprob": -0.0682135695841775, "compression_ratio": 1.4270833333333333, "no_speech_prob": 0.013910086825489998}, {"id": 210, "seek": 75190, "start": 776.9, "end": 779.8, "text": " Projekt, kt\u00f3ry b\u0119dzie stale rozwijany.", "tokens": [51614, 34804, 11, 9913, 10562, 342, 1220, 9544, 36652, 1325, 13, 51759], "temperature": 0.0, "avg_logprob": -0.0682135695841775, "compression_ratio": 1.4270833333333333, "no_speech_prob": 0.013910086825489998}, {"id": 211, "seek": 77980, "start": 779.8, "end": 785.6999999999999, "text": " Sami wskazuj\u0105 na jego braki. Najwi\u0119ksze to oczywi\u015bcie dominacja j\u0119zyka angielskiego.", "tokens": [50364, 44029, 261, 5161, 921, 13263, 1667, 26542, 1548, 2984, 13, 31576, 22423, 1694, 1381, 281, 23862, 8859, 23395, 42309, 40940, 2562, 1187, 5161, 12200, 13, 50659], "temperature": 0.0, "avg_logprob": -0.0967472698671598, "compression_ratio": 1.3927392739273927, "no_speech_prob": 0.012620128691196442}, {"id": 212, "seek": 77980, "start": 785.8, "end": 786.6999999999999, "text": " Jasne.", "tokens": [50664, 34023, 716, 13, 50709], "temperature": 0.0, "avg_logprob": -0.0967472698671598, "compression_ratio": 1.3927392739273927, "no_speech_prob": 0.012620128691196442}, {"id": 213, "seek": 77980, "start": 786.8, "end": 789.6999999999999, "text": " Ale podnosz\u0105 te\u017c inne, znacznie powa\u017cniejsze problem.", "tokens": [50714, 9366, 2497, 16751, 8925, 9516, 24170, 11, 15397, 14875, 2766, 3388, 18264, 44258, 1154, 13, 50859], "temperature": 0.0, "avg_logprob": -0.0967472698671598, "compression_ratio": 1.3927392739273927, "no_speech_prob": 0.012620128691196442}, {"id": 214, "seek": 77980, "start": 789.8, "end": 794.6999999999999, "text": " Co\u015b co jest jak tykaj\u0105ca bomba pod ca\u0142\u0105 polembada\u0144 nad EI.", "tokens": [50864, 3066, 1788, 598, 3492, 4207, 1104, 74, 11133, 496, 7851, 64, 2497, 1335, 15926, 13208, 2504, 1538, 5248, 12617, 462, 40, 13, 51109], "temperature": 0.0, "avg_logprob": -0.0967472698671598, "compression_ratio": 1.3927392739273927, "no_speech_prob": 0.012620128691196442}, {"id": 215, "seek": 77980, "start": 794.8, "end": 796.6999999999999, "text": " M\u00f3wisz o data contamination.", "tokens": [51114, 376, 3901, 23848, 277, 1412, 33012, 13, 51209], "temperature": 0.0, "avg_logprob": -0.0967472698671598, "compression_ratio": 1.3927392739273927, "no_speech_prob": 0.012620128691196442}, {"id": 216, "seek": 77980, "start": 796.8, "end": 803.6999999999999, "text": " Tak. Czyli o ryzyku, \u017ce modele, kt\u00f3re testujemy mog\u0142y by\u0107 trenowane na danych testowych.", "tokens": [51214, 9118, 13, 37099, 277, 20791, 1229, 5279, 11, 3561, 4391, 306, 11, 8864, 1500, 21767, 13172, 6825, 15069, 23136, 23066, 1667, 274, 34644, 1500, 19605, 13, 51559], "temperature": 0.0, "avg_logprob": -0.0967472698671598, "compression_ratio": 1.3927392739273927, "no_speech_prob": 0.012620128691196442}, {"id": 217, "seek": 77980, "start": 803.8, "end": 808.6999999999999, "text": " Innymi s\u0142owy, \u017ce model widzia\u0142 pytania z egzaminu, zanim do niego przyst\u0105pi\u0142.", "tokens": [51564, 682, 31813, 15116, 10089, 11, 3561, 2316, 27486, 8908, 25878, 5609, 710, 24263, 89, 7428, 84, 11, 710, 17869, 360, 49615, 6501, 372, 1611, 22630, 1221, 13, 51809], "temperature": 0.0, "avg_logprob": -0.0967472698671598, "compression_ratio": 1.3927392739273927, "no_speech_prob": 0.012620128691196442}, {"id": 218, "seek": 80870, "start": 808.7, "end": 810.6, "text": " Je\u015bli tak, to ca\u0142y test jest niewa\u017cne.", "tokens": [50364, 37086, 991, 11, 281, 35226, 1500, 3492, 297, 27806, 716, 13, 50459], "temperature": 0.0, "avg_logprob": -0.060763098976828835, "compression_ratio": 1.434920634920635, "no_speech_prob": 0.0008726319065317512}, {"id": 219, "seek": 80870, "start": 810.7, "end": 811.6, "text": " Dok\u0142adnie.", "tokens": [50464, 29768, 10358, 2766, 13, 50509], "temperature": 0.0, "avg_logprob": -0.060763098976828835, "compression_ratio": 1.434920634920635, "no_speech_prob": 0.0008726319065317512}, {"id": 220, "seek": 80870, "start": 811.7, "end": 815.6, "text": " Jak du\u017cy jest to problem? Da si\u0119 to w og\u00f3le sprawdzi\u0107?", "tokens": [50514, 15029, 1581, 7735, 3492, 281, 1154, 30, 3933, 3244, 281, 261, 29229, 46192, 28496, 30, 50709], "temperature": 0.0, "avg_logprob": -0.060763098976828835, "compression_ratio": 1.434920634920635, "no_speech_prob": 0.0008726319065317512}, {"id": 221, "seek": 80870, "start": 815.7, "end": 822.6, "text": " I tu jest pies pogrzebany. Poniewa\u017c dane treningowe wielu czo\u0142owych modeli s\u0105 \u015bci\u015ble strze\u017con\u0105 tajemnic\u0105,", "tokens": [50714, 286, 2604, 3492, 29640, 32037, 13503, 65, 1325, 13, 31756, 27806, 49206, 2192, 773, 6880, 40437, 269, 4765, 1221, 19605, 2316, 72, 9015, 220, 6199, 1788, 306, 1056, 1381, 1427, 266, 1611, 256, 1805, 443, 7692, 1611, 11, 51059], "temperature": 0.0, "avg_logprob": -0.060763098976828835, "compression_ratio": 1.434920634920635, "no_speech_prob": 0.0008726319065317512}, {"id": 222, "seek": 80870, "start": 822.7, "end": 824.6, "text": " pe\u0142na weryfikacja jest niemo\u017cliwa.", "tokens": [51064, 43205, 629, 261, 2109, 31230, 23395, 3492, 2838, 3280, 1427, 2081, 4151, 13, 51159], "temperature": 0.0, "avg_logprob": -0.060763098976828835, "compression_ratio": 1.434920634920635, "no_speech_prob": 0.0008726319065317512}, {"id": 223, "seek": 80870, "start": 824.7, "end": 825.6, "text": " Aha.", "tokens": [51164, 27448, 13, 51209], "temperature": 0.0, "avg_logprob": -0.060763098976828835, "compression_ratio": 1.434920634920635, "no_speech_prob": 0.0008726319065317512}, {"id": 224, "seek": 80870, "start": 825.7, "end": 830.6, "text": " Autorzy Helm przeprowadzili pewne analizy, ale nie s\u0105 w stanie da\u0107 gwarancji.", "tokens": [51214, 6049, 284, 1229, 6128, 76, 30829, 1892, 345, 89, 2312, 25889, 716, 2624, 590, 88, 11, 6775, 2838, 9015, 261, 40013, 1120, 2162, 290, 6925, 4463, 4013, 13, 51459], "temperature": 0.0, "avg_logprob": -0.060763098976828835, "compression_ratio": 1.434920634920635, "no_speech_prob": 0.0008726319065317512}, {"id": 225, "seek": 80870, "start": 830.7, "end": 837.6, "text": " To jest fundamentalne wyzwanie, kt\u00f3re wymaga od tw\u00f3rc\u00f3w modeli znacznie wi\u0119kszej transparentno\u015bci.", "tokens": [51464, 1407, 3492, 8088, 716, 4628, 14406, 7155, 11, 8864, 29764, 9286, 3611, 683, 15614, 29268, 2316, 72, 15397, 14875, 2766, 29968, 16920, 12737, 16438, 13, 51809], "temperature": 0.0, "avg_logprob": -0.060763098976828835, "compression_ratio": 1.434920634920635, "no_speech_prob": 0.0008726319065317512}, {"id": 226, "seek": 83760, "start": 837.6, "end": 841.5, "text": " Bez tego nasza ewaluacja opiera si\u0119 na wierze, a nie na faktach.", "tokens": [50364, 879, 89, 8627, 5382, 2394, 43364, 4929, 23395, 999, 10609, 3244, 1667, 261, 811, 1381, 11, 257, 2838, 1667, 21310, 608, 13, 50559], "temperature": 0.0, "avg_logprob": -0.07441967538317794, "compression_ratio": 1.4236760124610592, "no_speech_prob": 0.007178109139204025}, {"id": 227, "seek": 83760, "start": 841.6, "end": 845.5, "text": " Podsumowuj\u0105c, projekt Helm to bez w\u0105tpienia krok milowy.", "tokens": [50564, 12646, 82, 449, 305, 44733, 11, 26261, 6128, 76, 281, 10782, 261, 23430, 79, 18811, 350, 31621, 1962, 10089, 13, 50759], "temperature": 0.0, "avg_logprob": -0.07441967538317794, "compression_ratio": 1.4236760124610592, "no_speech_prob": 0.007178109139204025}, {"id": 228, "seek": 83760, "start": 845.6, "end": 849.5, "text": " Wprowadzi\u0142 bardzo potrzebny porz\u0105dek w tym chaotycznym \u015bwiecie oceny EI,", "tokens": [50764, 343, 35019, 3992, 1221, 9034, 37595, 1634, 1515, 23876, 916, 261, 8107, 6294, 6737, 3689, 12996, 40078, 4260, 10409, 43100, 462, 40, 11, 50959], "temperature": 0.0, "avg_logprob": -0.07441967538317794, "compression_ratio": 1.4236760124610592, "no_speech_prob": 0.007178109139204025}, {"id": 229, "seek": 83760, "start": 849.6, "end": 853.5, "text": " ujawni\u0142, \u017ce ecures i to tylko wierzcho\u0142ek g\u00f3ry lodowej.", "tokens": [50964, 344, 2938, 895, 40622, 11, 3561, 11437, 1303, 741, 281, 13219, 261, 34602, 5738, 1221, 916, 290, 812, 627, 33311, 21091, 13, 51159], "temperature": 0.0, "avg_logprob": -0.07441967538317794, "compression_ratio": 1.4236760124610592, "no_speech_prob": 0.007178109139204025}, {"id": 230, "seek": 83760, "start": 853.6, "end": 860.5, "text": " \u017be takie detale, jak w formatowanie pyta\u0144, maj\u0105 gigantyczny, wcze\u015bniej niedoceniany wp\u0142yw na wyniki.", "tokens": [51164, 46864, 15963, 1141, 1220, 11, 4207, 261, 7877, 22028, 10664, 1328, 5248, 11, 26064, 8741, 394, 17466, 1634, 11, 40785, 32488, 905, 268, 952, 88, 32444, 6825, 86, 1667, 31936, 9850, 13, 51509], "temperature": 0.0, "avg_logprob": -0.07441967538317794, "compression_ratio": 1.4236760124610592, "no_speech_prob": 0.007178109139204025}, {"id": 231, "seek": 83760, "start": 860.6, "end": 866.5, "text": " Ale ta praca pozostawia nas te\u017c z jednym fundamentalnym i troch\u0119 niepokoj\u0105cym pytaniem.", "tokens": [51514, 9366, 1846, 582, 6628, 21281, 555, 34953, 5382, 9516, 710, 5232, 12996, 8088, 12996, 741, 24926, 2838, 79, 13704, 8555, 1344, 76, 25878, 282, 4907, 13, 51809], "temperature": 0.0, "avg_logprob": -0.07441967538317794, "compression_ratio": 1.4236760124610592, "no_speech_prob": 0.007178109139204025}, {"id": 232, "seek": 86650, "start": 866.5, "end": 873.4, "text": " Skoro wydajno\u015b\u0107 pot\u0119\u017cnego modelu mo\u017ce si\u0119 kompletnie za\u0142ama\u0107 z powodu drobnej zmiany w formacie pytania,", "tokens": [50364, 7324, 10780, 25984, 1805, 23293, 1847, 1274, 1427, 11858, 2316, 84, 12034, 3244, 5207, 14657, 2766, 7949, 1221, 2404, 2162, 710, 3388, 34873, 3789, 65, 11794, 43591, 88, 261, 1254, 30805, 25878, 5609, 11, 50709], "temperature": 0.0, "avg_logprob": -0.05377729203965929, "compression_ratio": 1.4945054945054945, "no_speech_prob": 0.0009249859722331166}, {"id": 233, "seek": 86650, "start": 873.5, "end": 876.4, "text": " to co my tak naprawd\u0119 mierzymy?", "tokens": [50714, 281, 598, 452, 991, 20970, 47448, 1229, 2226, 30, 50859], "temperature": 0.0, "avg_logprob": -0.05377729203965929, "compression_ratio": 1.4945054945054945, "no_speech_prob": 0.0009249859722331166}, {"id": 234, "seek": 86650, "start": 876.5, "end": 881.4, "text": " Czy jest to autentyczna zdolno\u015b\u0107 modelu do rozumowania i jego wiedz\u0119 o \u015bwiecie?", "tokens": [50864, 19832, 3492, 281, 1476, 4179, 3689, 629, 16221, 401, 23293, 2316, 84, 360, 48797, 21308, 741, 26542, 46894, 11052, 277, 40078, 4260, 30, 51109], "temperature": 0.0, "avg_logprob": -0.05377729203965929, "compression_ratio": 1.4945054945054945, "no_speech_prob": 0.0009249859722331166}, {"id": 235, "seek": 86650, "start": 881.5, "end": 888.4, "text": " Czy tylko jego niesamowicie rozwini\u0119ta, ale powierzchowna umiej\u0119tno\u015b\u0107 dopasowania si\u0119 do naszego specyficznego?", "tokens": [51114, 19832, 13219, 26542, 48100, 335, 305, 28434, 9544, 86, 3812, 1274, 1328, 11, 6775, 3388, 34602, 339, 305, 629, 1105, 7764, 46788, 23293, 360, 20990, 21308, 3244, 360, 44517, 768, 1344, 1786, 89, 11858, 30, 51459], "temperature": 0.0, "avg_logprob": -0.05377729203965929, "compression_ratio": 1.4945054945054945, "no_speech_prob": 0.0009249859722331166}, {"id": 236, "seek": 86650, "start": 888.5, "end": 891.4, "text": " Mo\u017ce nawet arbitralnego sposobu zadawania pyta\u0144?", "tokens": [51464, 43774, 22696, 14931, 2155, 11858, 20443, 996, 84, 710, 1538, 86, 5609, 10664, 1328, 5248, 30, 51609], "temperature": 0.0, "avg_logprob": -0.05377729203965929, "compression_ratio": 1.4945054945054945, "no_speech_prob": 0.0009249859722331166}, {"id": 237, "seek": 86650, "start": 891.5, "end": 892.4, "text": " W\u0142a\u015bnie.", "tokens": [51614, 343, 5024, 12221, 13, 51659], "temperature": 0.0, "avg_logprob": -0.05377729203965929, "compression_ratio": 1.4945054945054945, "no_speech_prob": 0.0009249859722331166}, {"id": 238, "seek": 89240, "start": 892.4, "end": 899.3, "text": " To, \u017ce odpowiada poprawnie tylko wtedy, gdy pytanie jest sformu\u0142owane w jeden konkretny spos\u00f3b naprawd\u0119 rozumie problem,", "tokens": [50364, 1407, 11, 3561, 24314, 39018, 1665, 424, 14215, 13219, 26959, 11, 28405, 36610, 3492, 262, 837, 84, 1221, 23066, 261, 12906, 36500, 1634, 22904, 20970, 48797, 414, 1154, 11, 50709], "temperature": 0.0, "avg_logprob": -0.10535060034857856, "compression_ratio": 1.4038461538461537, "no_speech_prob": 0.18177807331085205}, {"id": 239, "seek": 89240, "start": 899.4, "end": 906.3, "text": " a mo\u017ce po prostu nauczy\u0142 si\u0119 niezwykle zaawansowanego, ale wci\u0105\u017c \u015blepego dopasowywania wzorc\u00f3w?", "tokens": [50714, 257, 12034, 714, 19518, 49103, 1229, 1221, 3244, 33511, 9726, 14677, 7949, 1607, 599, 37345, 6308, 11, 6775, 261, 537, 27242, 8299, 306, 494, 1571, 360, 20990, 10089, 86, 5609, 24809, 284, 29268, 30, 51059], "temperature": 0.0, "avg_logprob": -0.10535060034857856, "compression_ratio": 1.4038461538461537, "no_speech_prob": 0.18177807331085205}, {"id": 240, "seek": 89240, "start": 906.4, "end": 909.3, "text": " Helm nie daje na to ostatecznej odpowiedzi.", "tokens": [51064, 6128, 76, 2838, 1120, 2884, 1667, 281, 277, 15406, 3689, 11794, 36574, 3992, 13, 51209], "temperature": 0.0, "avg_logprob": -0.10535060034857856, "compression_ratio": 1.4038461538461537, "no_speech_prob": 0.18177807331085205}, {"id": 241, "seek": 89240, "start": 909.4, "end": 910.3, "text": " Ale?", "tokens": [51214, 9366, 30, 51259], "temperature": 0.0, "avg_logprob": -0.10535060034857856, "compression_ratio": 1.4038461538461537, "no_speech_prob": 0.18177807331085205}, {"id": 242, "seek": 89240, "start": 910.4, "end": 915.3, "text": " Pytania w precyzyjny i systematyczny spos\u00f3b i to mo\u017ce by\u0107 jego najwi\u0119kszym wk\u0142adem.", "tokens": [51264, 430, 4328, 5609, 261, 659, 1344, 1229, 73, 1634, 741, 1185, 267, 17466, 1634, 22904, 741, 281, 12034, 15069, 26542, 48636, 1694, 26681, 261, 15317, 443, 13, 51509], "temperature": 0.0, "avg_logprob": -0.10535060034857856, "compression_ratio": 1.4038461538461537, "no_speech_prob": 0.18177807331085205}], "language": "pl"}