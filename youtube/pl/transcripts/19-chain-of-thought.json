{"text": " Du\u017ce modele j\u0119zykowe to s\u0105, no, prawdziwi wirtuosi, pisz\u0105 wiersze, tworz\u0105 dzia\u0142aj\u0105cy kod, zdaj\u0105 egzaminy prawnicze. Ale czasem, wiesz, wystarczy proste zadanie z podstaw\u00f3wki, \u017ceby ten ca\u0142y cyfrowy geniusz kompletnie si\u0119 pogubi\u0142. Mam tu przyk\u0142ad prosto z materia\u0142\u00f3w, kt\u00f3re dzi\u015b analizujemy. W sto\u0142\u00f3wce by\u0142y 23 jab\u0142ka, zu\u017cyto 20, a potem dokupiono 6. Ile jab\u0142ek jest teraz? Proste, prawda? Tymczasem pot\u0119\u017cny model j\u0119zykowy zapytany w taki standardowy spos\u00f3b z pe\u0142n\u0105 pewno\u015bci\u0105 siebie odpowiada 27. I to jest w\u0142a\u015bnie ten fascynuj\u0105cy paradoks. Model widzi liczby 23, 26. Widzi pytanie ile jest teraz i dochodzi do wniosku, \u017ce najbezpieczniej b\u0119dzie je po prostu zesumowa\u0107. Gubi ca\u0142\u0105 logik\u0119 tej opowie\u015bci, \u017ce co\u015b uby\u0142o, a co\u015b przyby\u0142o. To pokazuje, \u017ce pod t\u0105 ca\u0142\u0105 elokwencj\u0105, no wiesz, kryje si\u0119 mechanizm, kt\u00f3ry jest genialny w dopasowywaniu wzorc\u00f3w, ale niekoniecznie w wieloetapowym sekwencyjnym rozumowaniu. W\u0142a\u015bnie. I to fundamentalne p\u0119kni\u0119cie w logice AI jest w centrum naszej dzisiejszej analizy. Zanurzamy si\u0119 w prze\u0142omowej pracy badaczy z Google Research, zatytu\u0142owanej Chain of Thought Prompting Elisids Reasoning in Large Language Models. A nasza misja jest prosta. Dowiedzie\u0107 si\u0119, jak pozornie drobna zmiana w sposobie zadawania pyta\u0144 mo\u017ce nagle obudzi\u0107 w AI u\u015bpion\u0105 zdolno\u015b\u0107 do logicznego my\u015blenia. To troch\u0119 jak odkrycie, \u017ce tw\u00f3j kalkulator potrafi filozofowa\u0107, tylko nikt nie wiedzia\u0142, kt\u00f3ry guzik nacisn\u0105\u0107. OK, rozpakujmy to. Na czym polega ta rewolucja w zadawaniu pyta\u0144? Wr\u00f3\u0107my do naszych jab\u0142ek. Standardowe podej\u015bcie, czyli Standard Prompting, to prosta transakcja. My dajemy pytanie, model daje odpowied\u017a. Pytanie. W sto\u0142\u00f3wce by\u0142y 23 jab\u0142ka. Zu\u017cyto 20 i dokupiono 6. Ile jest teraz? Odpowied\u017a modelu. 27. Koniec. I b\u0142\u0119dna. I b\u0142\u0119dna. To jest podej\u015bcie zerojedynkowe. Model musi w jednym kroku przeskoczy\u0107 odz\u0142o\u017conego problemu do ostatecznej odpowiedzi. I da\u0107 ten skok jest cz\u0119sto po prostu zbyt du\u017ce, zw\u0142aszcza gdy po drodze s\u0105 pu\u0142apki logiczne. I tu na scen\u0119 wchodzi nowe podej\u015bcie. Chain of Thought Prompting. I teraz uwaga, bo to wydaje si\u0119 absurdalnie proste. Zamiast od razu pyta\u0107 o wynik, pokazujemy modelowi jeden lub kilka przyk\u0142ad\u00f3w, w kt\u00f3rych problem jest roz\u0142o\u017cony na czynniki pierwsze. Dajemy mu taki wzorzec. Pytanie o jab\u0142ka. Odpowied\u017a. W sto\u0142\u00f3wce by\u0142y 23 jab\u0142ka. U\u017cyto 20, wi\u0119c zosta\u0142o 23 minus 20 r\u00f3wna si\u0119 3. Dokupiono 6, wi\u0119c teraz jest 3 plus 6 r\u00f3wna si\u0119 9. Ostateczna odpowied\u017a to 9. I to wszystko. Naprawd\u0119 wszystko. I nagle, kiedy model dostaje zupe\u0142nie nowe zadanie, sam zaczyna generowa\u0107 podobny, logiczny ci\u0105g my\u015blowy. Chwila. To naprawd\u0119 tylko tyle? Wystarczy pokaza\u0107 mu, jak si\u0119 my\u015bli? W\u0142a\u015bnie na tym polega pi\u0119kno i si\u0142a tego odkrycia. To jest fundamentalna zmiana paradygmatu. Zamiast traktowa\u0107 model jak czarn\u0105 skrzynk\u0119, od kt\u00f3rej rz\u0105damy w wyniku, zaczynamy go traktowa\u0107 jak ucznia. Zamiast pokazywa\u0107 mu tysi\u0105ce par pytanie-odpowied\u017a, pokazujemy mu kilka przyk\u0142ad\u00f3w pytanie-kroki my\u015blowe-odpowied\u017a. To tak, jakby\u015bmy w szkole zamiast wkuwania na pami\u0119\u0107 wynik\u00f3w, w ko\u0144cu zacz\u0119li pokazywa\u0107, jak dochodzi si\u0119 do rozwi\u0105zania krok po kroku. Czyli to nie jest kwestia do uczania modelu nowej wiedzy, tylko\u2026 Odblokowania zdolno\u015bci, kt\u00f3r\u0105 on ju\u017c gdzie\u015b w sobie ma. Zdolno\u015bci do dekompozycji problemu. Dok\u0142adnie. I co najwa\u017cniejsze, nie wymaga to ponownego, niezwykle kosztownego trenowania ca\u0142ego modelu. To kluczowe. To tylko zmiana w sposobie formu\u0142owania zapyta\u0144, czyli w promptingu. W badaniu u\u017cyto zaledwie o\u015bmiu takich przyk\u0142ad\u00f3w, \u017ceby model oskali setek miliard\u00f3w parametr\u00f3w, no wiesz, za\u0142apa\u0142 now\u0105 metod\u0119. To idea Fuse Shot Learning, ale przeniesiona na zupe\u0142nie nowy, niemal koncepcyjny poziom. Nie uczymy go fakt\u00f3w, uczymy go metody. No dobrze. Metoda wydaje si\u0119 prosta. Ale jak dobrze ona dzia\u0142a? M\u00f3wimy tu o kosmetycznej poprawce, czy o czym\u015b, co naprawd\u0119 zmienia zasady gry. M\u00f3wimy o trz\u0119sieniu ziemi. Wyniki, kt\u00f3re uzyskali s\u0105, no, absolutnie spektakularne. Najbardziej uderzaj\u0105cy wniosek z badania, model Palm, maj\u0105cy 540 miliard\u00f3w parametr\u00f3w, po zobaczeniu zaledwie o\u015bmiu przyk\u0142ad\u00f3w z Chain of Tot, osi\u0105gn\u0105\u0142 absolutnie najnowocze\u015bniejsz\u0105 dok\u0142adno\u015b\u0107 na benchmarku zada\u0144 matematyczny DSM-8K. A co to oznacza w praktyce? Czym jest ten benchmark? To zbi\u00f3r trudnych, wieloetapowych zada\u0144 tekstowych z matematyki na poziomie szko\u0142y podstawowej. Takich, kt\u00f3re wymagaj\u0105 nie tylko liczenia, ale przede wszystkim, wiesz, zrozumienia historii i rozplanowania krok\u00f3w. Rozumiem. I tu dochodzimy do Sedna. Jak pokazuje figur tu w tej pracy? Palm z t\u0105 prost\u0105 technik\u0105, na g\u0142ow\u0119 nawet specjalnie fine-tunowany model GPT-3, a ten GPT-3 by\u0142 trenowany w\u0142a\u015bnie do rozwi\u0105zywania problem\u00f3w matematycznych i dodatkowo wspierane przez zewn\u0119trzny program weryfikuj\u0105ce poprawno\u015b\u0107 oblicze\u0144. Zaraz, zaraz. Czyli model og\u00f3lnego przeznaczenia, kt\u00f3remu po prostu inaczej zadan pytanie? Tak. Pokona\u0142 wyspecjalizowanego zawodnika na jego w\u0142asnym boisku? To nie jest znalezienie lepszego algorytmu. To jak odkrycie, \u017ce w silniku, kt\u00f3ry mieli\u015bmy od dawno, jest ukryty dodatkowy bieg? To jest idealna analogia. A co wi\u0119cej, badacze odkryli, \u017ce ten dodatkowy bieg nie istnieje w mniejszych silnikach. Nazwali to emergent ability z dolno\u015bci\u0105 wy\u0142aniaj\u0105c\u0105 si\u0119. Ona po prostu nie pojawia si\u0119 dop\u00f3ki model nie osi\u0105gnie pewnej krytycznej masy. Chwila, zatrzymajmy si\u0119 tutaj. Czyli je\u015bli spr\u00f3buj\u0119 tej samej techniki na mniejszym modelu, to to nie zadzia\u0142a. Co wi\u0119cej, mo\u017ce nawet pogorszy\u0107 jego wyniki. Jak spojrzymy na figur 4, wida\u0107 to doskonale. Poni\u017cej progu oko\u0142o 100 miliard\u00f3w parametr\u00f3w, Chain of Thought Prompting prowadzi do gorszych odpowiedzi ni\u017c standardowe pytanie. To jest kompletnie wbrew intuicji. Dlaczego mniejszy model, pr\u00f3buj\u0105c na\u015bladowa\u0107 ten proces, staje si\u0119 gorszy? Brzmi to tak, jakby pr\u00f3ba nauczenia dziecka algebryzbyt wcze\u015bniej sprawia\u0142a, \u017ce zapomina jak dodawa\u0107. W\u0142a\u015bnie tak. Mniejszy model potrafi wygenerowa\u0107 tekst, kt\u00f3ry wygl\u0105da jak \u0142a\u0144cuch my\u015bli, jest p\u0142ynny, gramatyczny, ma pozory logiki. Ale w rzeczywisto\u015bci jest be\u0142kotem. Generuje niepoprawne kroki po\u015brednie, kt\u00f3re prowadz\u0105 go na manowce. Zdolno\u015b\u0107 do generowania faktycznie sp\u00f3jnego, logicznego rozumowania pojawia si\u0119 skokowo. Dopiero po przekroczeniu pewnego progu skali. To sugeruje, \u017ce w tych najwi\u0119kszych modelach drzemie ukryty potencja\u0142, kt\u00f3ry trzeba po prostu umie\u0107 aktywowa\u0107. Nie wystarczy mie\u0107 pot\u0119\u017cny silnik. Trzeba te\u017c wiedzie\u0107, jak go uruchomi\u0107. Okej, ale to brzmi prawie zbyt prosto. Ka\u017cdy z ceptyk zapyta\u0142 by, a mo\u017ce to jaka\u015b statystyczna sztuczka. Mo\u017ce model wcale nie my\u015bli, tylko, no wiesz, oszukuje. Na szczu\u015bcie badaczy wcielili si\u0119 w rol\u0119 detektyw\u00f3w i postanowili obali\u0107 w\u0142asn\u0105 tez\u0119 na kilka sprytnych sposob\u00f3w. Dok\u0142adnie. Przeprowadzili seri\u0119 genialnych eksperyment\u00f3w kontrolnych, tak zwane ablation study, \u017ceby wyizorowa\u0107 ten kluczowy czyn n\u00f3g. Chcieli sprawdzi\u0107, czy to na pewno ten logiczny, j\u0119zykowy proces my\u015blowy jest odpowiedzialny za sukces. Figure 5 w pracy \u015bwietnie to ilustruje. Pierwsza hipoteza na celowniku detektyw\u00f3w. A mo\u017ce nie trzeba tej ca\u0142ej opowie\u015bci. Mo\u017ce wystaraczy poda\u0107 modelowi samor\u00f3wnanie matematyczne bez tego ca\u0142ego opisu s\u0142ownego. Sprawdzili to. Nazwali ten variant equation only. I okaza\u0142o si\u0119, \u017ce to nie wystarcza. Model, widz\u0105c tylko suche 20 plus 6 r\u00f3wna si\u0119 9, nie potrafi\u0142 przenie\u015b\u0107 tej logiki na nowe, z\u0142o\u017cone zadania tekstrowe. Gubi\u0142 si\u0119 w semantyce, nie wiedzia\u0142, kt\u00f3r\u0105 liczb\u0119 odj\u0105\u0107, a kt\u00f3r\u0105 doda\u0107. To w\u0142a\u015bnie kroki opisanej j\u0119zykiem naturalnym zosta\u0142o dokupiono, okaza\u0142y si\u0119 kluczowe, \u017ceby zrozumia\u0142 kontekst problemu. Dobrze, czyli nie chodzi o sam\u0105 matematyk\u0119. To prowadzi do drugiej hipotezy. A mo\u017ce model po prostu potrzebuje wi\u0119cej czasu na my\u015blenie. Wi\u0119cej przestrzeni obliczeniowej. Mo\u017ce samo generowanie d\u0142u\u017cszego tekstu, nie wa\u017cne jakiego, pomaga mu doj\u015b\u0107 do poprawnego wyniku. Nazwali to variable compute only. To by\u0142 naprawd\u0119 sprytny eksperyment. Zamiast kaza\u0107 modelowi generowa\u0107 logiczne kroki, poleci mu wygenerowa\u0107 seri\u0105 kropek. Kropek. Tak, kropek. Tyle kropek ile znak\u00f3w mia\u0142oby poprawne rozumowanie. Chcieli sprawdzi\u0107, czy samo wysilenie si\u0119 co\u015b da. Czyli w zasadzie kazali mu pomedytowa\u0107 nad problemem, generuj\u0105c kropki. I okaza\u0142o si\u0119, \u017ce to nic nie da\u0142o. To chyba z\u0142a wiadomo\u015b\u0107 dla fan\u00f3w pustego patrzenia w \u015bcian\u0119 w poszukiwaniu natchnienia. Niestety tak. Wynik \u017cadnej poprawy w stosunku do standardowego podej\u015bcia. To by\u0142 nokautuj\u0105cy dow\u00f3d, \u017ce nie chodzi o sam\u0105 ilo\u015b\u0107 oblicze\u0144 czy wygenerowanych token\u00f3w, ale o semantyczn\u0105 i logiczn\u0105 struktur\u0119 tego, co jest generowane. Nie wystarczy my\u015ble\u0107 d\u0142u\u017cej. Trzeba my\u015ble\u0107 w w\u0142a\u015bciwy spos\u00f3b. I ostatni cios, chyba najbardziej podchwytli\u0142a hipoteza. A co, je\u015bli ten ca\u0142y proces my\u015blowy wcale nie s\u0142u\u017cy do rozwi\u0105zania problemu, a jedynie do aktywowania odpowiedniej wiedzy w modelu? Taka rozgrzewka przed odpowiedzi\u0105. Sprawdzili to, karz\u0105c modelowi najpierw poda\u0107 odpowied\u017a, a dopiero potem wygenerowa\u0107 wyja\u015bnienie. Nazwali to chain of thought after answer. I ta metoda r\u00f3wnie\u017c spektakularnie zawiod\u0142a. Serio? Tak. To chyba najmocniejszy dow\u00f3d z ca\u0142ego badania. Pokazuje, \u017ce kolejno\u015b\u0107 ma fundamentalne znaczenie. Model faktycznie wykorzystuje wygenerowane przez siebie kolejne kroki, aby doj\u015b\u0107 do poprawnego wyniku na ko\u0144cu. To nie jest postracjonalizacja, to nie jest uzasadnianie z g\u00f3ry podj\u0119tej decyzji. To jest autentyczny krok po kroku, proces dochodzenia do odpowiedzi. Czyli podsumowuj\u0105c t\u0119 detektywistyczn\u0105 prac\u0119, wygl\u0105da na to, \u017ce model rzeczywi\u015bcie, w pewnym sensie, my\u015bli na g\u0142os. Generuje po\u015bredni stan, analizuje go i na jego podstawie generuje kolejny, a\u017c dojdzie to fina\u0142u. To co\u015b znacznie wi\u0119cej ni\u017c tylko odnajdywanie statystycznych wzorc\u00f3w w tak\u015bcie. I tu dochodzimy do sedna pot\u0119gi tej metody. Poniewa\u017c ten \u0142a\u0144cuch my\u015bli jest wyra\u017cony w j\u0119zyku naturalnym, a nie w j\u0119zyku formalnym, matematycznym czy programistycznym, mo\u017cna go zastosowa\u0107 do niemal ka\u017cdego problemu, kt\u00f3ry wymaga rozumowania. Nie ogranicza nas to do zada\u0144 arytmetycznych. W\u0142a\u015bnie badanie pokazuje niesamowit\u0105 skuteczno\u015b\u0107 w zadaniach na tak zwany zdrowy rozs\u0105dek, czyli common sense reasoning, a tak\u017ce w rozumowaniu symbolicznym. Figure 3 w pracy jest pe\u0142ne \u015bwietnych przyk\u0142ad\u00f3w. Jeden z nich mnie zachwyci\u0142. Pytanie. Czy zdanie, Jua\u0142o Moutinho z\u0142apa\u0142 podanie w mistrzostwach NFC jest prawdopodobne? Standardowy model m\u00f3g\u0142by si\u0119 pogubi\u0107. Kojarzy sportowiec, mistrzostwa, podanie i odpowiedzie\u0107, \u017ce tak. Ale model u\u017cywaj\u0105cy Chain of Sold rozumuje tak. Jua\u0142o Moutinho to portugalski pi\u0142karz. Pi\u0142ka no\u017cna to soker. Mistrzostwa NFAC to futbol ameryka\u0144ski. W futbolu ameryka\u0144skim u\u0142apie si\u0119 podania, ale Moutinho nie jest futbolist\u0105. Zatem odpowied\u017a brzmi? Nie. To jest niesamowite. To nie jest proste kojarzenie fakt\u00f3w. To jest \u0142\u0105czenie wiedzy z dw\u00f3ch zupe\u0142nie r\u00f3\u017cnych domen. Europejskiej pi\u0142ki no\u017cnej i ameryka\u0144skiego futbolu, \u017ceby wyci\u0105gn\u0105\u0107 logiczny wniosek. To jest zupe\u0142nie inny poziom rozumienia \u015bwiata. Albo inny przyk\u0142ad z rozmowania symbolicznego. Zadanie po\u0142\u0105cz ostatnie litery imienia i nazwiska. Na przyk\u0142ad dla Lady Gaga. Model t\u0142umaczy. Ostatnia litera s\u0142owa Lady to Y. Ostatnia litera s\u0142owa Gaga to A. Po\u0142\u0105czenie ich daje ja. To zadanie, kt\u00f3re dla cz\u0142owieka jest banalne. Dla modelu my\u015bl\u0105cego w kategoriach statystyki j\u0119zykowej jest abstrakcyjn\u0105 operacj\u0105 na symbolach. Chain of Sold pozwala moj\u0105 przeprowadzi\u0107. Ok, ale czy on naprawd\u0119 rozumie t\u0119 regu\u0142\u0119? Czy po prostu nauczy\u0142 si\u0119 na\u015bladowa\u0107 ten jeden konkretny trik, kt\u00f3ry mu pokazano w przyk\u0142adach? Jak G\u0142\u0119boka jest ta zdolno\u015b\u0107 generalizacji? To jest kluczowe pytanie i badacze ja zadali. Sp\u00f3jrzmy na figure 8. Pokazali modelowi przyk\u0142ady \u0142\u0105czenia liter w imionach dwucz\u0142onowych, jak Lady Gaga. A nast\u0119pnie przetestowali go na imionach czterocz\u0142onowych, czyli na problemie o strukturze, kt\u00f3rej nigdy wcze\u015bniej nie widzia\u0142. Jak posz\u0142o? Z przypadku standard prompting model kompletnie sobie z tym nie radzi. Jego skuteczno\u015b\u0107 spada praktycznie do zera. A z Chain of Thought. Z Chain of Thought, model, kt\u00f3ry nauczy\u0142 si\u0119 procedury dla dw\u00f3ch s\u0142\u00f3w, potrafi j\u0105 bezb\u0142\u0119dnie zastosowa\u0107 do czterech. Wow. To jest dow\u00f3d na to, \u017ce on nie uczy si\u0119 na pami\u0119\u0107 konkretnego wzorca odpowiedzi, ale internalizuje og\u00f3ln\u0105, abstrakcyjn\u0105 procedur\u0119 i potrafi j\u0105 ekstrapolowa\u0107 na nowe, bardziej z\u0142o\u017cone przypadki. To jest prawdziwa si\u0142a tej metody. Brzmi to niemal jak \u015bwi\u0119ty gra\u0142 AI, ale musz\u0119 zapyta\u0107, jakie s\u0105 wady? Gdzie jest haczyk? Bo zawsze jest jaki\u015b haczyk. Oczywi\u015bcie. I autorzy badania s\u0105 w tej kwestii bardzo transparentni. Wskazuj\u0105 na co najmniej trzy kluczowe ograniczenia. Pierwsze i najwa\u017cniejsze to skala. Jak ju\u017c m\u00f3wi\u0142y\u015bmy, to jest emergent ability. Chwila. Czyli wracamy do tego, \u017ce to dzia\u0142a tylko na gigantycznych, niezwykle kosztownych modelach. Czy to nie oznacza, \u017ce to odkrycie jest w zasadzie bezu\u017cyteczne dla 99% developer\u00f3w i firm, kt\u00f3re nie maj\u0105 dost\u0119pu do infrastruktury Google'a? Czy to nie jest po prostu ciekawostka ze \u015bwiata gigant\u00f3w technologicznych? To bardzo s\u0142uszna uwaga. Na ten moment tak, jest to w du\u017cej mierze narz\u0119dzie dla najwi\u0119kszych graczy. Ale to odkrycie wyznacza kierunek. Pokazuje, \u017ce celem nie musi by\u0107 tylko budowanie jeszcze wi\u0119kszych modeli, ale te\u017c szukanie sposob\u00f3w na wywo\u0142anie takich zdolno\u015bci w mniejszych, bardziej efektywnych architekturach. To jest teraz otwarte pole do bada\u0144. Ok. Czyli to drogowska zna przysz\u0142o\u015b\u0107. A drugie ograniczenie? Tak, gwarancji poprawno\u015bci samego rozumowania. Wygenerowany \u0142a\u0144cuch my\u015bli nie zawsze jest logiczny, nawet je\u015bli ostateczna odpowied\u017a si\u0119 zgadza. Zw\u0142aszcza w zadaniach wielokrotnego wyboru model mo\u017ce doj\u015b\u0107 do prawid\u0142owego wyniku przez przypadek, a jego wyja\u015bnienie mo\u017ce by\u0107 pe\u0142ne b\u0142\u0119d\u00f3w i konfabulacji. Czyli model mo\u017ce uzyska\u0107 dobr\u0105 odpowied\u017a z zupe\u0142nie z\u0142ych powod\u00f3w, a potem z pe\u0142n\u0105 pewno\u015bci\u0105 siebie ok\u0142ama\u0107 mnie, jak do niej doszed\u0142. To jest troch\u0119 niepokoj\u0105ce. Ta interpretowalno\u015b\u0107, kt\u00f3ra mia\u0142a by\u0107 zalet\u0105, okazuje si\u0119 mieczem obosiecznym. Dok\u0142adnie. To, \u017ce model pokazuje swoj\u0105 prac\u0119 nie znaczy, \u017ce ta praca jest zawsze dobrze wykonana. Musimy podchodzi\u0107 do tych wyja\u015bni\u0107 z du\u017c\u0105 doz\u0105 krytyczyzmu. I wreszcie trzecia kwestia. Wp\u0142yw przyk\u0142ad\u00f3w. Mimo, \u017ce metoda jest do\u015b\u0107 odporna na styl pisania tych przyk\u0142ad\u00f3w przez r\u00f3\u017cne osoby, to jako\u015b\u0107 i dob\u00f3r tych kilku wzorc\u00f3w wci\u0105\u017c maj\u0105 ogromne znaczenie. Jeden z\u0142y lub myl\u0105cy przyk\u0142ad mo\u017ce skierowa\u0107 ca\u0142y proces rozumowania modelu na manowce. Wci\u0105\u017c istnieje tu du\u017cy element tego, co nazywamy prompt engineering. Dobrze. Zbierzmy to wszystko do kupy. Co z tego wynika w praktyce? Jaka jest ta jedna najwa\u017cniejsza lekcja, kt\u00f3r\u0105 powinni\u015bmy z tego wyci\u0105gn\u0105\u0107? Najwa\u017cniejsza lekcja jest taka, \u017ce chain of thought prompting to prosta w za\u0142o\u017ceniach, ale rewolucyjna metoda na odblokowanie ukrytych zdolno\u015bci rozumowania w du\u017cych modelach j\u0119zykowych. Kluczo\u0142e jest to, \u017ce jest to emergent ability, co\u015b, co pojawia si\u0119 dopiero przy odpowiedniej skali, pozwalaj\u0105c modelom na dekompozycj\u0119 problem\u00f3w i m\u00f3wi\u0105c kolokwialnie pokazanie swojej pracy. A patrz\u0105c szerzej, co to zmienia w naszym my\u015bleniu o sztucznej inteligencji? To prowadzi do rewolucyjnego wniosku. By\u0107 mo\u017ce problemem nie jest to, \u017ce nasze modele s\u0105 za ma\u0142o inteligentne, ale to, \u017ce my jeste\u015bmy za ma\u0142o sprytni w rozmowie z nimi. Okazuje si\u0119, \u017ce w tych cyfrowych umys\u0142ach drzemi\u0105 ukryte zdolno\u015bci, a naszym zadaniem jest sta\u0107 si\u0119 swego rodzaju zaklinaczami AI, kt\u00f3rzy potrafi\u0105 je wydoby\u0107 na powierzchni\u0119 za pomoc\u0105 odpowiednich s\u0142\u00f3w. To zmienia perspektyw\u0119 z samego budowania coraz wi\u0119kszych modeli na szukanie lepszych, bardziej inteligentnych sposob\u00f3w interakcji z tymi, kt\u00f3re ju\u017c mamy. I to zostawia nas z jedn\u0105, niezwykle prowokuj\u0105c\u0105 my\u015bl\u0105 na koniec. Skoro tak prosta zmiana w sposobie zadawania pytania mo\u017ce uwolni\u0107 tak zaawansowane zdolno\u015bci logicznego my\u015blenia, to jakie inne, by\u0107 mo\u017ce znacznie pot\u0119\u017cniejsze ukryte talenty drzemi\u0105 w tych ogromnych sieciach neuronowych, czekaj\u0105c jedynie na w\u0142a\u015bciwy klucz, na odpowiedni pr\u0105d, kt\u00f3ry otworzy kolejne drzwi.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.5, "text": " Du\u017ce modele j\u0119zykowe to s\u0105, no, prawdziwi wirtuosi, pisz\u0105 wiersze, tworz\u0105 dzia\u0142aj\u0105cy kod, zdaj\u0105 egzaminy prawnicze.", "tokens": [50364, 5153, 2875, 4391, 306, 49055, 74, 6880, 281, 9015, 11, 572, 11, 41175, 3992, 6253, 1987, 9179, 21521, 11, 26584, 8925, 261, 4890, 1381, 11, 46288, 8925, 27121, 11133, 1344, 350, 378, 11, 16221, 11133, 24263, 28915, 3519, 37047, 299, 1381, 13, 50839], "temperature": 0.0, "avg_logprob": -0.1350316195420816, "compression_ratio": 1.3932203389830509, "no_speech_prob": 0.01826644316315651}, {"id": 1, "seek": 0, "start": 9.5, "end": 17.0, "text": " Ale czasem, wiesz, wystarczy proste zadanie z podstaw\u00f3wki, \u017ceby ten ca\u0142y cyfrowy geniusz kompletnie si\u0119 pogubi\u0142.", "tokens": [50839, 9366, 13190, 443, 11, 261, 15347, 11, 4628, 9710, 6522, 10293, 68, 42788, 7155, 710, 43443, 3901, 2984, 11, 11316, 2064, 35226, 3185, 69, 1892, 88, 14017, 89, 5207, 14657, 2766, 3244, 32037, 836, 40622, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1350316195420816, "compression_ratio": 1.3932203389830509, "no_speech_prob": 0.01826644316315651}, {"id": 2, "seek": 0, "start": 17.0, "end": 20.5, "text": " Mam tu przyk\u0142ad prosto z materia\u0142\u00f3w, kt\u00f3re dzi\u015b analizujemy.", "tokens": [51214, 19899, 2604, 23144, 10293, 78, 710, 2389, 8908, 3901, 11, 8864, 31981, 1788, 2624, 590, 21767, 13, 51389], "temperature": 0.0, "avg_logprob": -0.1350316195420816, "compression_ratio": 1.3932203389830509, "no_speech_prob": 0.01826644316315651}, {"id": 3, "seek": 0, "start": 20.5, "end": 28.5, "text": " W sto\u0142\u00f3wce by\u0142y 23 jab\u0142ka, zu\u017cyto 20, a potem dokupiono 6. Ile jab\u0142ek jest teraz? Proste, prawda?", "tokens": [51389, 343, 22784, 1221, 3901, 384, 26366, 6673, 33475, 1221, 2330, 11, 2164, 7735, 1353, 945, 11, 257, 36513, 25037, 1010, 49020, 1386, 13, 286, 306, 33475, 1221, 916, 3492, 16854, 30, 2114, 555, 68, 11, 43607, 30, 51789], "temperature": 0.0, "avg_logprob": -0.1350316195420816, "compression_ratio": 1.3932203389830509, "no_speech_prob": 0.01826644316315651}, {"id": 4, "seek": 2850, "start": 28.5, "end": 36.5, "text": " Tymczasem pot\u0119\u017cny model j\u0119zykowy zapytany w taki standardowy spos\u00f3b z pe\u0142n\u0105 pewno\u015bci\u0105 siebie odpowiada 27.", "tokens": [50364, 314, 4199, 30989, 443, 1847, 1274, 1427, 1634, 2316, 49055, 74, 10089, 14223, 4328, 1325, 261, 20065, 3832, 10089, 22904, 710, 43205, 13113, 33002, 50227, 39137, 24314, 39018, 7634, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08060627148069184, "compression_ratio": 1.3582089552238805, "no_speech_prob": 0.023505980148911476}, {"id": 5, "seek": 2850, "start": 36.5, "end": 47.5, "text": " I to jest w\u0142a\u015bnie ten fascynuj\u0105cy paradoks. Model widzi liczby 23, 26. Widzi pytanie ile jest teraz i dochodzi do wniosku,", "tokens": [50764, 286, 281, 3492, 14234, 2064, 30632, 1344, 77, 13263, 1344, 13480, 25500, 13, 17105, 5274, 3992, 6169, 89, 2322, 6673, 11, 7551, 13, 28331, 3992, 36610, 15465, 3492, 16854, 741, 9243, 14543, 360, 45368, 2717, 5279, 11, 51314], "temperature": 0.0, "avg_logprob": -0.08060627148069184, "compression_ratio": 1.3582089552238805, "no_speech_prob": 0.023505980148911476}, {"id": 6, "seek": 2850, "start": 47.5, "end": 56.5, "text": " \u017ce najbezpieczniej b\u0119dzie je po prostu zesumowa\u0107. Gubi ca\u0142\u0105 logik\u0119 tej opowie\u015bci, \u017ce co\u015b uby\u0142o, a co\u015b przyby\u0142o.", "tokens": [51314, 3561, 11212, 650, 89, 9144, 3689, 10402, 10562, 1506, 714, 19518, 710, 279, 449, 11445, 13, 460, 836, 72, 1335, 15926, 3565, 1035, 1274, 12573, 999, 13998, 6199, 11, 3561, 19241, 344, 2322, 5249, 11, 257, 19241, 6501, 2322, 5249, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08060627148069184, "compression_ratio": 1.3582089552238805, "no_speech_prob": 0.023505980148911476}, {"id": 7, "seek": 5650, "start": 56.5, "end": 64.5, "text": " To pokazuje, \u017ce pod t\u0105 ca\u0142\u0105 elokwencj\u0105, no wiesz, kryje si\u0119 mechanizm, kt\u00f3ry jest genialny w dopasowywaniu wzorc\u00f3w,", "tokens": [50364, 1407, 13010, 43317, 11, 3561, 2497, 32294, 1335, 15926, 806, 453, 15615, 66, 8555, 11, 572, 261, 15347, 11, 34847, 2884, 3244, 4236, 590, 76, 11, 9913, 3492, 48228, 1634, 261, 360, 20990, 10089, 86, 25849, 24809, 284, 29268, 11, 50764], "temperature": 0.0, "avg_logprob": -0.10475690705435617, "compression_ratio": 1.4019607843137254, "no_speech_prob": 0.11106578260660172}, {"id": 8, "seek": 5650, "start": 64.5, "end": 68.5, "text": " ale niekoniecznie w wieloetapowym sekwencyjnym rozumowaniu.", "tokens": [50764, 6775, 2838, 18295, 414, 19923, 261, 20570, 78, 302, 569, 31691, 17215, 15615, 42949, 12996, 48797, 305, 25849, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10475690705435617, "compression_ratio": 1.4019607843137254, "no_speech_prob": 0.11106578260660172}, {"id": 9, "seek": 5650, "start": 68.5, "end": 76.5, "text": " W\u0142a\u015bnie. I to fundamentalne p\u0119kni\u0119cie w logice AI jest w centrum naszej dzisiejszej analizy.", "tokens": [50964, 343, 5024, 12221, 13, 286, 281, 8088, 716, 280, 1274, 74, 35938, 4260, 261, 3565, 573, 7318, 3492, 261, 1489, 6247, 42946, 9758, 50117, 82, 16920, 2624, 590, 88, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10475690705435617, "compression_ratio": 1.4019607843137254, "no_speech_prob": 0.11106578260660172}, {"id": 10, "seek": 5650, "start": 76.5, "end": 85.5, "text": " Zanurzamy si\u0119 w prze\u0142omowej pracy badaczy z Google Research, zatytu\u0142owanej Chain of Thought Prompting Elisids Reasoning in Large Language Models.", "tokens": [51364, 1176, 282, 374, 89, 7804, 3244, 261, 8325, 1221, 298, 21091, 35591, 1578, 14691, 710, 3329, 10303, 11, 35802, 4328, 84, 1221, 23066, 73, 33252, 295, 23058, 15833, 662, 278, 2699, 271, 3742, 39693, 278, 294, 33092, 24445, 6583, 1625, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10475690705435617, "compression_ratio": 1.4019607843137254, "no_speech_prob": 0.11106578260660172}, {"id": 11, "seek": 8550, "start": 85.5, "end": 92.5, "text": " A nasza misja jest prosta. Dowiedzie\u0107 si\u0119, jak pozornie drobna zmiana w sposobie zadawania pyta\u0144", "tokens": [50364, 316, 5382, 2394, 3346, 2938, 3492, 582, 8638, 13, 20947, 22078, 3244, 11, 4207, 21281, 1865, 414, 3789, 65, 629, 17020, 8497, 261, 20443, 996, 414, 710, 1538, 86, 5609, 10664, 1328, 5248, 50714], "temperature": 0.0, "avg_logprob": -0.06030641283307757, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.023505497723817825}, {"id": 12, "seek": 8550, "start": 92.5, "end": 97.5, "text": " mo\u017ce nagle obudzi\u0107 w AI u\u015bpion\u0105 zdolno\u015b\u0107 do logicznego my\u015blenia.", "tokens": [50714, 12034, 297, 15088, 1111, 532, 28496, 261, 7318, 344, 1788, 79, 313, 1611, 16221, 401, 23293, 360, 9952, 89, 11858, 48633, 6698, 654, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06030641283307757, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.023505497723817825}, {"id": 13, "seek": 8550, "start": 97.5, "end": 103.5, "text": " To troch\u0119 jak odkrycie, \u017ce tw\u00f3j kalkulator potrafi filozofowa\u0107, tylko nikt nie wiedzia\u0142, kt\u00f3ry guzik nacisn\u0105\u0107.", "tokens": [50964, 1407, 24926, 4207, 3611, 43298, 4260, 11, 3561, 683, 18999, 34960, 16381, 1847, 10437, 72, 1387, 15151, 2670, 11445, 11, 13219, 297, 9874, 2838, 261, 15338, 8908, 11, 9913, 695, 89, 1035, 42071, 271, 13113, 2162, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06030641283307757, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.023505497723817825}, {"id": 14, "seek": 8550, "start": 103.5, "end": 110.5, "text": " OK, rozpakujmy to. Na czym polega ta rewolucja w zadawaniu pyta\u0144? Wr\u00f3\u0107my do naszych jab\u0142ek.", "tokens": [51264, 2264, 11, 9544, 45944, 4579, 2226, 281, 13, 6056, 31466, 13208, 3680, 1846, 319, 48481, 1311, 2938, 261, 710, 1538, 86, 25849, 10664, 1328, 5248, 30, 10159, 812, 2162, 2226, 360, 45002, 33475, 1221, 916, 13, 51614], "temperature": 0.0, "avg_logprob": -0.06030641283307757, "compression_ratio": 1.3402777777777777, "no_speech_prob": 0.023505497723817825}, {"id": 15, "seek": 11050, "start": 111.5, "end": 118.5, "text": " Standardowe podej\u015bcie, czyli Standard Prompting, to prosta transakcja. My dajemy pytanie, model daje odpowied\u017a.", "tokens": [50414, 21298, 6880, 7468, 73, 9815, 11, 16591, 21298, 15833, 662, 278, 11, 281, 582, 8638, 1145, 514, 34056, 13, 1222, 1120, 73, 3633, 36610, 11, 2316, 1120, 2884, 36574, 10659, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10413734894946106, "compression_ratio": 1.4128787878787878, "no_speech_prob": 0.11036312580108643}, {"id": 16, "seek": 11050, "start": 118.5, "end": 128.5, "text": " Pytanie. W sto\u0142\u00f3wce by\u0142y 23 jab\u0142ka. Zu\u017cyto 20 i dokupiono 6. Ile jest teraz? Odpowied\u017a modelu. 27. Koniec.", "tokens": [50764, 430, 4328, 7155, 13, 343, 22784, 1221, 3901, 384, 26366, 6673, 33475, 1221, 2330, 13, 1176, 84, 7735, 1353, 945, 741, 25037, 1010, 49020, 1386, 13, 286, 306, 3492, 16854, 30, 12210, 14701, 1091, 10659, 2316, 84, 13, 7634, 13, 12718, 35733, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10413734894946106, "compression_ratio": 1.4128787878787878, "no_speech_prob": 0.11036312580108643}, {"id": 17, "seek": 11050, "start": 128.5, "end": 137.5, "text": " I b\u0142\u0119dna. I b\u0142\u0119dna. To jest podej\u015bcie zerojedynkowe. Model musi w jednym kroku przeskoczy\u0107 odz\u0142o\u017conego problemu do ostatecznej odpowiedzi.", "tokens": [51264, 286, 272, 1221, 6298, 629, 13, 286, 272, 1221, 6298, 629, 13, 1407, 3492, 7468, 73, 9815, 4018, 40543, 2534, 74, 6880, 13, 17105, 37587, 261, 5232, 12996, 45909, 5279, 6541, 279, 74, 905, 27150, 3611, 89, 5249, 1427, 546, 1571, 1154, 84, 360, 277, 15406, 3689, 11794, 36574, 3992, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10413734894946106, "compression_ratio": 1.4128787878787878, "no_speech_prob": 0.11036312580108643}, {"id": 18, "seek": 13750, "start": 137.5, "end": 143.5, "text": " I da\u0107 ten skok jest cz\u0119sto po prostu zbyt du\u017ce, zw\u0142aszcza gdy po drodze s\u0105 pu\u0142apki logiczne.", "tokens": [50364, 286, 1120, 2162, 2064, 1110, 453, 3492, 34369, 714, 19518, 710, 2322, 83, 1581, 2875, 11, 11873, 1221, 19601, 41524, 28405, 714, 3789, 67, 1381, 9015, 2362, 1221, 569, 2984, 9952, 43077, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08765737989307505, "compression_ratio": 1.3650190114068441, "no_speech_prob": 0.026223493739962578}, {"id": 19, "seek": 13750, "start": 143.5, "end": 152.5, "text": " I tu na scen\u0119 wchodzi nowe podej\u015bcie. Chain of Thought Prompting. I teraz uwaga, bo to wydaje si\u0119 absurdalnie proste.", "tokens": [50664, 286, 2604, 1667, 4191, 1274, 261, 34616, 586, 68, 7468, 73, 9815, 13, 33252, 295, 23058, 15833, 662, 278, 13, 286, 16854, 23147, 9286, 11, 748, 281, 49165, 3244, 19774, 304, 2766, 10293, 68, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08765737989307505, "compression_ratio": 1.3650190114068441, "no_speech_prob": 0.026223493739962578}, {"id": 20, "seek": 13750, "start": 152.5, "end": 160.5, "text": " Zamiast od razu pyta\u0107 o wynik, pokazujemy modelowi jeden lub kilka przyk\u0142ad\u00f3w, w kt\u00f3rych problem jest roz\u0142o\u017cony na czynniki pierwsze.", "tokens": [51114, 1176, 4526, 525, 3611, 367, 8813, 10664, 42931, 277, 31936, 1035, 11, 13010, 921, 21767, 2316, 24503, 12906, 15980, 36466, 23144, 3901, 11, 261, 30382, 1154, 3492, 9544, 5249, 1427, 2526, 1667, 6430, 26384, 9850, 45994, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08765737989307505, "compression_ratio": 1.3650190114068441, "no_speech_prob": 0.026223493739962578}, {"id": 21, "seek": 16050, "start": 160.5, "end": 166.5, "text": " Dajemy mu taki wzorzec. Pytanie o jab\u0142ka. Odpowied\u017a. W sto\u0142\u00f3wce by\u0142y 23 jab\u0142ka.", "tokens": [50364, 413, 1805, 3633, 2992, 20065, 24809, 284, 1381, 66, 13, 430, 4328, 7155, 277, 33475, 1221, 2330, 13, 12210, 14701, 1091, 10659, 13, 343, 22784, 1221, 3901, 384, 26366, 6673, 33475, 1221, 2330, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08109777973544213, "compression_ratio": 1.3984674329501916, "no_speech_prob": 0.7201647162437439}, {"id": 22, "seek": 16050, "start": 166.5, "end": 178.5, "text": " U\u017cyto 20, wi\u0119c zosta\u0142o 23 minus 20 r\u00f3wna si\u0119 3. Dokupiono 6, wi\u0119c teraz jest 3 plus 6 r\u00f3wna si\u0119 9. Ostateczna odpowied\u017a to 9. I to wszystko.", "tokens": [50664, 624, 7735, 1353, 945, 11, 16677, 23154, 5249, 6673, 3175, 945, 367, 3901, 629, 3244, 805, 13, 29768, 1010, 49020, 1386, 11, 16677, 16854, 3492, 805, 1804, 1386, 367, 3901, 629, 3244, 1722, 13, 422, 15406, 3689, 629, 36574, 10659, 281, 1722, 13, 286, 281, 22607, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08109777973544213, "compression_ratio": 1.3984674329501916, "no_speech_prob": 0.7201647162437439}, {"id": 23, "seek": 16050, "start": 178.5, "end": 186.5, "text": " Naprawd\u0119 wszystko. I nagle, kiedy model dostaje zupe\u0142nie nowe zadanie, sam zaczyna generowa\u0107 podobny, logiczny ci\u0105g my\u015blowy.", "tokens": [51264, 18287, 20098, 22607, 13, 286, 297, 15088, 11, 18777, 2316, 20568, 11153, 49922, 586, 68, 42788, 7155, 11, 3247, 43811, 629, 1337, 11445, 43024, 1634, 11, 9952, 89, 1634, 42398, 70, 452, 19212, 10089, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08109777973544213, "compression_ratio": 1.3984674329501916, "no_speech_prob": 0.7201647162437439}, {"id": 24, "seek": 18650, "start": 186.5, "end": 190.5, "text": " Chwila. To naprawd\u0119 tylko tyle? Wystarczy pokaza\u0107 mu, jak si\u0119 my\u015bli?", "tokens": [50364, 761, 86, 7371, 13, 1407, 20970, 13219, 39293, 30, 14458, 9710, 6522, 13010, 12257, 2162, 2992, 11, 4207, 3244, 452, 15350, 30, 50564], "temperature": 0.0, "avg_logprob": -0.08135118383042356, "compression_ratio": 1.3286384976525822, "no_speech_prob": 0.05065052583813667}, {"id": 25, "seek": 18650, "start": 190.5, "end": 197.5, "text": " W\u0142a\u015bnie na tym polega pi\u0119kno i si\u0142a tego odkrycia. To jest fundamentalna zmiana paradygmatu.", "tokens": [50564, 343, 5024, 12221, 1667, 8107, 13208, 3680, 48085, 1771, 741, 1511, 5024, 8627, 3611, 43298, 2755, 13, 1407, 3492, 8088, 629, 17020, 8497, 13480, 18103, 15677, 84, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08135118383042356, "compression_ratio": 1.3286384976525822, "no_speech_prob": 0.05065052583813667}, {"id": 26, "seek": 18650, "start": 197.5, "end": 204.5, "text": " Zamiast traktowa\u0107 model jak czarn\u0105 skrzynk\u0119, od kt\u00f3rej rz\u0105damy w wyniku, zaczynamy go traktowa\u0107 jak ucznia.", "tokens": [50914, 1176, 4526, 525, 944, 2320, 11445, 2316, 4207, 6472, 1083, 1611, 1110, 13047, 77, 15724, 11, 3611, 36023, 367, 23876, 7804, 261, 31936, 24320, 11, 43811, 5378, 88, 352, 944, 2320, 11445, 4207, 35403, 12679, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08135118383042356, "compression_ratio": 1.3286384976525822, "no_speech_prob": 0.05065052583813667}, {"id": 27, "seek": 20450, "start": 205.5, "end": 212.5, "text": " Zamiast pokazywa\u0107 mu tysi\u0105ce par pytanie-odpowied\u017a, pokazujemy mu kilka przyk\u0142ad\u00f3w pytanie-kroki my\u015blowe-odpowied\u017a.", "tokens": [50414, 1176, 4526, 525, 13010, 33235, 25234, 2992, 38156, 11404, 384, 971, 36610, 12, 378, 14701, 1091, 10659, 11, 13010, 921, 21767, 2992, 36466, 23144, 3901, 36610, 12, 74, 340, 2984, 452, 19212, 6880, 12, 378, 14701, 1091, 10659, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10765713244884997, "compression_ratio": 1.4744027303754266, "no_speech_prob": 0.6539145112037659}, {"id": 28, "seek": 20450, "start": 212.5, "end": 221.5, "text": " To tak, jakby\u015bmy w szkole zamiast wkuwania na pami\u0119\u0107 wynik\u00f3w, w ko\u0144cu zacz\u0119li pokazywa\u0107, jak dochodzi si\u0119 do rozwi\u0105zania krok po kroku.", "tokens": [50764, 1407, 991, 11, 28976, 10513, 261, 7870, 4093, 306, 710, 4526, 525, 261, 5279, 86, 5609, 1667, 31088, 2162, 31936, 1035, 3901, 11, 261, 26470, 12032, 34430, 11052, 2081, 13010, 33235, 25234, 11, 4207, 9243, 14543, 3244, 360, 9544, 22620, 5609, 350, 31621, 714, 45909, 5279, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10765713244884997, "compression_ratio": 1.4744027303754266, "no_speech_prob": 0.6539145112037659}, {"id": 29, "seek": 20450, "start": 221.5, "end": 225.5, "text": " Czyli to nie jest kwestia do uczania modelu nowej wiedzy, tylko\u2026", "tokens": [51214, 37099, 281, 2838, 3492, 42035, 654, 360, 35403, 5609, 2316, 84, 586, 40779, 46894, 1229, 11, 13219, 1260, 51414], "temperature": 0.0, "avg_logprob": -0.10765713244884997, "compression_ratio": 1.4744027303754266, "no_speech_prob": 0.6539145112037659}, {"id": 30, "seek": 20450, "start": 225.5, "end": 231.5, "text": " Odblokowania zdolno\u015bci, kt\u00f3r\u0105 on ju\u017c gdzie\u015b w sobie ma. Zdolno\u015bci do dekompozycji problemu.", "tokens": [51414, 12210, 5199, 453, 21308, 16221, 401, 16438, 11, 37415, 322, 10678, 41359, 261, 13652, 463, 13, 1176, 67, 401, 16438, 360, 368, 20557, 2259, 1229, 19649, 1154, 84, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10765713244884997, "compression_ratio": 1.4744027303754266, "no_speech_prob": 0.6539145112037659}, {"id": 31, "seek": 23150, "start": 232.5, "end": 238.5, "text": " Dok\u0142adnie. I co najwa\u017cniejsze, nie wymaga to ponownego, niezwykle kosztownego trenowania ca\u0142ego modelu.", "tokens": [50414, 29768, 10358, 2766, 13, 286, 598, 11212, 27111, 44258, 11, 2838, 29764, 9286, 281, 9224, 648, 6308, 11, 33511, 9726, 14677, 19532, 2682, 648, 6308, 23136, 21308, 35224, 6308, 2316, 84, 13, 50714], "temperature": 0.0, "avg_logprob": -0.06489797187062492, "compression_ratio": 1.3898305084745763, "no_speech_prob": 0.0426948219537735}, {"id": 32, "seek": 23150, "start": 238.5, "end": 244.5, "text": " To kluczowe. To tylko zmiana w sposobie formu\u0142owania zapyta\u0144, czyli w promptingu.", "tokens": [50714, 1407, 9671, 1311, 89, 6880, 13, 1407, 13219, 17020, 8497, 261, 20443, 996, 414, 1254, 84, 1221, 21308, 14223, 88, 1328, 5248, 11, 16591, 261, 12391, 7050, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06489797187062492, "compression_ratio": 1.3898305084745763, "no_speech_prob": 0.0426948219537735}, {"id": 33, "seek": 23150, "start": 244.5, "end": 253.5, "text": " W badaniu u\u017cyto zaledwie o\u015bmiu takich przyk\u0142ad\u00f3w, \u017ceby model oskali setek miliard\u00f3w parametr\u00f3w, no wiesz, za\u0142apa\u0142 now\u0105 metod\u0119.", "tokens": [51014, 343, 1578, 25849, 34097, 1353, 710, 5573, 8699, 277, 1788, 3057, 84, 29607, 23144, 3901, 11, 11316, 2316, 3003, 74, 5103, 992, 916, 1962, 72, 515, 3901, 6220, 27965, 3901, 11, 572, 261, 15347, 11, 7949, 1221, 7961, 1221, 586, 1611, 1131, 378, 1274, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06489797187062492, "compression_ratio": 1.3898305084745763, "no_speech_prob": 0.0426948219537735}, {"id": 34, "seek": 25350, "start": 254.5, "end": 263.5, "text": " To idea Fuse Shot Learning, ale przeniesiona na zupe\u0142nie nowy, niemal koncepcyjny poziom. Nie uczymy go fakt\u00f3w, uczymy go metody.", "tokens": [50414, 1407, 1558, 479, 438, 28845, 15205, 11, 6775, 582, 2904, 530, 21758, 1667, 49922, 586, 88, 11, 2838, 5579, 5897, 27493, 42949, 1634, 38503, 298, 13, 12016, 344, 6522, 2226, 352, 21310, 3901, 11, 344, 6522, 2226, 352, 1131, 843, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10647346708509657, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.6980538964271545}, {"id": 35, "seek": 25350, "start": 263.5, "end": 274.5, "text": " No dobrze. Metoda wydaje si\u0119 prosta. Ale jak dobrze ona dzia\u0142a? M\u00f3wimy tu o kosmetycznej poprawce, czy o czym\u015b, co naprawd\u0119 zmienia zasady gry.", "tokens": [50864, 883, 28335, 13, 6377, 13449, 49165, 3244, 582, 8638, 13, 9366, 4207, 28335, 20325, 37903, 30, 376, 3901, 13189, 2604, 277, 19532, 76, 2210, 3689, 11794, 1665, 5131, 384, 11, 6430, 277, 31466, 1788, 11, 598, 20970, 17020, 18811, 26530, 880, 41974, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10647346708509657, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.6980538964271545}, {"id": 36, "seek": 27450, "start": 275.5, "end": 281.5, "text": " M\u00f3wimy o trz\u0119sieniu ziemi. Wyniki, kt\u00f3re uzyskali s\u0105, no, absolutnie spektakularne.", "tokens": [50414, 376, 3901, 13189, 277, 504, 11052, 82, 1053, 5951, 16503, 3057, 13, 343, 2534, 9850, 11, 8864, 16851, 749, 74, 5103, 9015, 11, 572, 11, 18757, 2766, 768, 2320, 514, 1040, 716, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1095182929240482, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.4040232300758362}, {"id": 37, "seek": 27450, "start": 281.5, "end": 288.5, "text": " Najbardziej uderzaj\u0105cy wniosek z badania, model Palm, maj\u0105cy 540 miliard\u00f3w parametr\u00f3w,", "tokens": [50714, 31576, 40392, 344, 1068, 89, 11133, 1344, 261, 3722, 541, 74, 710, 1578, 5609, 11, 2316, 32668, 11, 26064, 1344, 1025, 5254, 1962, 72, 515, 3901, 6220, 27965, 3901, 11, 51064], "temperature": 0.0, "avg_logprob": -0.1095182929240482, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.4040232300758362}, {"id": 38, "seek": 27450, "start": 288.5, "end": 298.5, "text": " po zobaczeniu zaledwie o\u015bmiu przyk\u0142ad\u00f3w z Chain of Tot, osi\u0105gn\u0105\u0142 absolutnie najnowocze\u015bniejsz\u0105 dok\u0142adno\u015b\u0107 na benchmarku zada\u0144 matematyczny DSM-8K.", "tokens": [51064, 714, 25100, 326, 39651, 710, 5573, 8699, 277, 1788, 3057, 84, 23144, 3901, 710, 33252, 295, 11236, 11, 3003, 11404, 4568, 1611, 1221, 18757, 2766, 11212, 3785, 905, 1381, 37511, 82, 8925, 45864, 23293, 1667, 18927, 84, 710, 1538, 5248, 3803, 8615, 17466, 1634, 15816, 44, 12, 23, 42, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1095182929240482, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.4040232300758362}, {"id": 39, "seek": 27450, "start": 298.5, "end": 302.5, "text": " A co to oznacza w praktyce? Czym jest ten benchmark?", "tokens": [51564, 316, 598, 281, 277, 22672, 326, 2394, 261, 3206, 74, 874, 384, 30, 19832, 76, 3492, 2064, 18927, 30, 51764], "temperature": 0.0, "avg_logprob": -0.1095182929240482, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.4040232300758362}, {"id": 40, "seek": 30250, "start": 302.5, "end": 308.5, "text": " To zbi\u00f3r trudnych, wieloetapowych zada\u0144 tekstowych z matematyki na poziomie szko\u0142y podstawowej.", "tokens": [50364, 1407, 710, 5614, 15614, 32007, 9399, 11, 20570, 78, 302, 569, 19605, 710, 1538, 5248, 16624, 372, 19605, 710, 3803, 8615, 88, 2984, 1667, 38503, 40120, 7870, 4093, 6825, 43443, 21091, 13, 50664], "temperature": 0.0, "avg_logprob": -0.0967132640334795, "compression_ratio": 1.3632478632478633, "no_speech_prob": 0.003537427168339491}, {"id": 41, "seek": 30250, "start": 308.5, "end": 315.5, "text": " Takich, kt\u00f3re wymagaj\u0105 nie tylko liczenia, ale przede wszystkim, wiesz, zrozumienia historii i rozplanowania krok\u00f3w.", "tokens": [50664, 9118, 480, 11, 8864, 29764, 559, 11133, 2838, 13219, 6169, 14320, 11, 6775, 44786, 30481, 11, 261, 15347, 11, 710, 27857, 449, 18811, 4058, 5597, 741, 9544, 16554, 21308, 45909, 23849, 13, 51014], "temperature": 0.0, "avg_logprob": -0.0967132640334795, "compression_ratio": 1.3632478632478633, "no_speech_prob": 0.003537427168339491}, {"id": 42, "seek": 30250, "start": 315.5, "end": 316.5, "text": " Rozumiem.", "tokens": [51014, 43313, 449, 4907, 13, 51064], "temperature": 0.0, "avg_logprob": -0.0967132640334795, "compression_ratio": 1.3632478632478633, "no_speech_prob": 0.003537427168339491}, {"id": 43, "seek": 30250, "start": 316.5, "end": 322.5, "text": " I tu dochodzimy do Sedna. Jak pokazuje figur tu w tej pracy? Palm z t\u0105 prost\u0105 technik\u0105,", "tokens": [51064, 286, 2604, 9243, 378, 89, 13189, 360, 31213, 629, 13, 15029, 13010, 43317, 31094, 2604, 261, 12573, 35591, 30, 32668, 710, 32294, 10293, 1611, 1537, 1035, 1611, 11, 51364], "temperature": 0.0, "avg_logprob": -0.0967132640334795, "compression_ratio": 1.3632478632478633, "no_speech_prob": 0.003537427168339491}, {"id": 44, "seek": 32250, "start": 322.5, "end": 330.5, "text": " na g\u0142ow\u0119 nawet specjalnie fine-tunowany model GPT-3, a ten GPT-3 by\u0142 trenowany w\u0142a\u015bnie do rozwi\u0105zywania problem\u00f3w matematycznych", "tokens": [50364, 1667, 18117, 305, 1274, 22696, 46433, 2766, 2489, 12, 83, 409, 23341, 2316, 26039, 51, 12, 18, 11, 257, 2064, 26039, 51, 12, 18, 16673, 23136, 23341, 14234, 360, 9544, 18234, 1229, 86, 5609, 1154, 3901, 3803, 8615, 17466, 9399, 50764], "temperature": 0.0, "avg_logprob": -0.09562987623543574, "compression_ratio": 1.4247491638795986, "no_speech_prob": 0.8414344787597656}, {"id": 45, "seek": 32250, "start": 330.5, "end": 335.5, "text": " i dodatkowo wspierane przez zewn\u0119trzny program weryfikuj\u0105ce poprawno\u015b\u0107 oblicze\u0144.", "tokens": [50764, 741, 13886, 33525, 19941, 17757, 811, 1929, 14064, 5277, 895, 1274, 6903, 89, 1634, 1461, 261, 2109, 31230, 13263, 384, 1665, 424, 20944, 7753, 1111, 1050, 49689, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09562987623543574, "compression_ratio": 1.4247491638795986, "no_speech_prob": 0.8414344787597656}, {"id": 46, "seek": 32250, "start": 335.5, "end": 340.5, "text": " Zaraz, zaraz. Czyli model og\u00f3lnego przeznaczenia, kt\u00f3remu po prostu inaczej zadan pytanie?", "tokens": [51014, 41580, 921, 11, 22675, 921, 13, 37099, 2316, 5360, 15741, 11858, 14064, 77, 326, 14320, 11, 4695, 2579, 84, 714, 19518, 33230, 16920, 710, 11338, 36610, 30, 51264], "temperature": 0.0, "avg_logprob": -0.09562987623543574, "compression_ratio": 1.4247491638795986, "no_speech_prob": 0.8414344787597656}, {"id": 47, "seek": 32250, "start": 340.5, "end": 341.5, "text": " Tak.", "tokens": [51264, 9118, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09562987623543574, "compression_ratio": 1.4247491638795986, "no_speech_prob": 0.8414344787597656}, {"id": 48, "seek": 32250, "start": 341.5, "end": 347.5, "text": " Pokona\u0142 wyspecjalizowanego zawodnika na jego w\u0142asnym boisku? To nie jest znalezienie lepszego algorytmu.", "tokens": [51314, 14958, 4037, 1221, 27062, 494, 66, 22600, 590, 37345, 6308, 28165, 378, 77, 5439, 1667, 26542, 43572, 12996, 748, 271, 5279, 30, 1407, 2838, 3492, 15397, 37646, 27385, 476, 1878, 27725, 3501, 827, 83, 20140, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09562987623543574, "compression_ratio": 1.4247491638795986, "no_speech_prob": 0.8414344787597656}, {"id": 49, "seek": 34750, "start": 347.5, "end": 352.5, "text": " To jak odkrycie, \u017ce w silniku, kt\u00f3ry mieli\u015bmy od dawno, jest ukryty dodatkowy bieg?", "tokens": [50364, 1407, 4207, 3611, 43298, 4260, 11, 3561, 261, 3425, 13123, 84, 11, 9913, 41214, 10513, 3611, 43438, 1771, 11, 3492, 26769, 627, 874, 13886, 33525, 10089, 272, 20408, 30, 50614], "temperature": 0.0, "avg_logprob": -0.07143517155801096, "compression_ratio": 1.5080906148867315, "no_speech_prob": 0.12908101081848145}, {"id": 50, "seek": 34750, "start": 352.5, "end": 360.5, "text": " To jest idealna analogia. A co wi\u0119cej, badacze odkryli, \u017ce ten dodatkowy bieg nie istnieje w mniejszych silnikach.", "tokens": [50614, 1407, 3492, 7157, 629, 16660, 654, 13, 316, 598, 26004, 11, 1578, 326, 1381, 3611, 43298, 2081, 11, 3561, 2064, 13886, 33525, 10089, 272, 20408, 2838, 1418, 2766, 2884, 261, 39513, 45021, 3425, 13123, 608, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07143517155801096, "compression_ratio": 1.5080906148867315, "no_speech_prob": 0.12908101081848145}, {"id": 51, "seek": 34750, "start": 360.5, "end": 364.5, "text": " Nazwali to emergent ability z dolno\u015bci\u0105 wy\u0142aniaj\u0105c\u0105 si\u0119.", "tokens": [51014, 11870, 40054, 281, 4345, 6930, 3485, 710, 17858, 16438, 1611, 4628, 1221, 5609, 8555, 32557, 3244, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07143517155801096, "compression_ratio": 1.5080906148867315, "no_speech_prob": 0.12908101081848145}, {"id": 52, "seek": 34750, "start": 364.5, "end": 369.5, "text": " Ona po prostu nie pojawia si\u0119 dop\u00f3ki model nie osi\u0105gnie pewnej krytycznej masy.", "tokens": [51214, 49793, 714, 19518, 2838, 30655, 654, 3244, 21900, 812, 2984, 2316, 2838, 3003, 11404, 70, 2766, 25889, 11794, 34847, 874, 3689, 11794, 2300, 88, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07143517155801096, "compression_ratio": 1.5080906148867315, "no_speech_prob": 0.12908101081848145}, {"id": 53, "seek": 34750, "start": 369.5, "end": 375.5, "text": " Chwila, zatrzymajmy si\u0119 tutaj. Czyli je\u015bli spr\u00f3buj\u0119 tej samej techniki na mniejszym modelu, to to nie zadzia\u0142a.", "tokens": [51464, 761, 86, 7371, 11, 35802, 13047, 1696, 73, 2226, 3244, 12749, 13, 37099, 25630, 6103, 14216, 18258, 12573, 912, 73, 1537, 9850, 1667, 39513, 7706, 76, 2316, 84, 11, 281, 281, 2838, 42788, 89, 25605, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07143517155801096, "compression_ratio": 1.5080906148867315, "no_speech_prob": 0.12908101081848145}, {"id": 54, "seek": 37550, "start": 375.5, "end": 381.5, "text": " Co wi\u0119cej, mo\u017ce nawet pogorszy\u0107 jego wyniki. Jak spojrzymy na figur 4, wida\u0107 to doskonale.", "tokens": [50364, 3066, 26004, 11, 12034, 22696, 32037, 830, 27150, 26542, 31936, 9850, 13, 15029, 8243, 73, 13047, 2226, 1667, 31094, 1017, 11, 261, 46898, 281, 4491, 18295, 1220, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06353813493755502, "compression_ratio": 1.4049844236760125, "no_speech_prob": 0.2523839473724365}, {"id": 55, "seek": 37550, "start": 381.5, "end": 388.5, "text": " Poni\u017cej progu oko\u0142o 100 miliard\u00f3w parametr\u00f3w, Chain of Thought Prompting prowadzi do gorszych odpowiedzi ni\u017c standardowe pytanie.", "tokens": [50664, 31756, 72, 38493, 447, 2794, 45730, 5249, 2319, 1962, 72, 515, 3901, 6220, 27965, 3901, 11, 33252, 295, 23058, 15833, 662, 278, 36590, 3992, 360, 290, 830, 28051, 36574, 3992, 28502, 3832, 6880, 36610, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06353813493755502, "compression_ratio": 1.4049844236760125, "no_speech_prob": 0.2523839473724365}, {"id": 56, "seek": 37550, "start": 388.5, "end": 395.5, "text": " To jest kompletnie wbrew intuicji. Dlaczego mniejszy model, pr\u00f3buj\u0105c na\u015bladowa\u0107 ten proces, staje si\u0119 gorszy?", "tokens": [51014, 1407, 3492, 5207, 14657, 2766, 261, 65, 2236, 560, 84, 299, 4013, 13, 413, 75, 39329, 39513, 7706, 2316, 11, 8565, 65, 44733, 1667, 1788, 9290, 11445, 2064, 17565, 11, 342, 11153, 3244, 290, 830, 1229, 30, 51364], "temperature": 0.0, "avg_logprob": -0.06353813493755502, "compression_ratio": 1.4049844236760125, "no_speech_prob": 0.2523839473724365}, {"id": 57, "seek": 37550, "start": 395.5, "end": 400.5, "text": " Brzmi to tak, jakby pr\u00f3ba nauczenia dziecka algebryzbyt wcze\u015bniej sprawia\u0142a, \u017ce zapomina jak dodawa\u0107.", "tokens": [51364, 1603, 89, 3057, 281, 991, 11, 28976, 8565, 4231, 49103, 14320, 17953, 39342, 419, 432, 65, 627, 89, 2322, 83, 40785, 22734, 25605, 11, 3561, 14223, 49217, 4207, 13886, 10449, 2162, 13, 51614], "temperature": 0.0, "avg_logprob": -0.06353813493755502, "compression_ratio": 1.4049844236760125, "no_speech_prob": 0.2523839473724365}, {"id": 58, "seek": 40050, "start": 400.5, "end": 409.5, "text": " W\u0142a\u015bnie tak. Mniejszy model potrafi wygenerowa\u0107 tekst, kt\u00f3ry wygl\u0105da jak \u0142a\u0144cuch my\u015bli, jest p\u0142ynny, gramatyczny, ma pozory logiki.", "tokens": [50364, 343, 5024, 12221, 991, 13, 376, 10402, 7706, 2316, 1847, 10437, 72, 4628, 21848, 11445, 16624, 372, 11, 9913, 32015, 4207, 220, 5024, 5248, 66, 625, 452, 15350, 11, 3492, 28695, 2534, 1634, 11, 21353, 267, 17466, 1634, 11, 463, 21281, 827, 3565, 9850, 13, 50814], "temperature": 0.0, "avg_logprob": -0.0692581757903099, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.1725301593542099}, {"id": 59, "seek": 40050, "start": 409.5, "end": 416.5, "text": " Ale w rzeczywisto\u015bci jest be\u0142kotem. Generuje niepoprawne kroki po\u015brednie, kt\u00f3re prowadz\u0105 go na manowce.", "tokens": [50814, 9366, 261, 26297, 86, 9334, 6199, 3492, 312, 1221, 74, 310, 443, 13, 15409, 13008, 2838, 13872, 5131, 716, 45909, 2984, 714, 1788, 986, 2766, 11, 8864, 36590, 8925, 352, 1667, 587, 305, 384, 13, 51164], "temperature": 0.0, "avg_logprob": -0.0692581757903099, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.1725301593542099}, {"id": 60, "seek": 40050, "start": 416.5, "end": 422.5, "text": " Zdolno\u015b\u0107 do generowania faktycznie sp\u00f3jnego, logicznego rozumowania pojawia si\u0119 skokowo.", "tokens": [51164, 1176, 67, 401, 23293, 360, 1337, 21308, 33647, 45586, 637, 18999, 11858, 11, 9952, 89, 11858, 48797, 21308, 30655, 654, 3244, 1110, 453, 19941, 13, 51464], "temperature": 0.0, "avg_logprob": -0.0692581757903099, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.1725301593542099}, {"id": 61, "seek": 40050, "start": 422.5, "end": 425.5, "text": " Dopiero po przekroczeniu pewnego progu skali.", "tokens": [51464, 42657, 12030, 714, 29785, 24174, 39651, 25889, 11858, 447, 2794, 1110, 5103, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0692581757903099, "compression_ratio": 1.4057971014492754, "no_speech_prob": 0.1725301593542099}, {"id": 62, "seek": 42550, "start": 425.5, "end": 431.5, "text": " To sugeruje, \u017ce w tych najwi\u0119kszych modelach drzemie ukryty potencja\u0142, kt\u00f3ry trzeba po prostu umie\u0107 aktywowa\u0107.", "tokens": [50364, 1407, 459, 1321, 13008, 11, 3561, 261, 15180, 48636, 1694, 28051, 2316, 608, 1224, 24313, 414, 26769, 627, 874, 1847, 22660, 2938, 1221, 11, 9913, 25860, 714, 19518, 1105, 414, 2162, 9308, 874, 86, 11445, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07577755491612322, "compression_ratio": 1.4741641337386018, "no_speech_prob": 0.05792892351746559}, {"id": 63, "seek": 42550, "start": 431.5, "end": 436.5, "text": " Nie wystarczy mie\u0107 pot\u0119\u017cny silnik. Trzeba te\u017c wiedzie\u0107, jak go uruchomi\u0107.", "tokens": [50664, 12016, 4628, 9710, 6522, 35612, 1847, 1274, 1427, 1634, 3425, 13123, 13, 1765, 1381, 4231, 9516, 261, 22078, 11, 4207, 352, 4038, 625, 9220, 2162, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07577755491612322, "compression_ratio": 1.4741641337386018, "no_speech_prob": 0.05792892351746559}, {"id": 64, "seek": 42550, "start": 436.5, "end": 442.5, "text": " Okej, ale to brzmi prawie zbyt prosto. Ka\u017cdy z ceptyk zapyta\u0142 by, a mo\u017ce to jaka\u015b statystyczna sztuczka.", "tokens": [50914, 29094, 73, 11, 6775, 281, 738, 89, 3057, 3206, 8699, 710, 2322, 83, 10293, 78, 13, 10988, 1427, 3173, 710, 45026, 874, 74, 14223, 88, 46426, 538, 11, 257, 12034, 281, 4207, 64, 1788, 2219, 38593, 17466, 629, 262, 2682, 1311, 89, 2330, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07577755491612322, "compression_ratio": 1.4741641337386018, "no_speech_prob": 0.05792892351746559}, {"id": 65, "seek": 42550, "start": 442.5, "end": 446.5, "text": " Mo\u017ce model wcale nie my\u015bli, tylko, no wiesz, oszukuje.", "tokens": [51214, 43774, 2316, 261, 37088, 2838, 452, 15350, 11, 13219, 11, 572, 261, 15347, 11, 3003, 43994, 13008, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07577755491612322, "compression_ratio": 1.4741641337386018, "no_speech_prob": 0.05792892351746559}, {"id": 66, "seek": 42550, "start": 446.5, "end": 453.5, "text": " Na szczu\u015bcie badaczy wcielili si\u0119 w rol\u0119 detektyw\u00f3w i postanowili obali\u0107 w\u0142asn\u0105 tez\u0119 na kilka sprytnych sposob\u00f3w.", "tokens": [51414, 6056, 22090, 84, 9815, 1578, 14691, 261, 537, 338, 2312, 3244, 261, 34109, 1274, 1141, 916, 874, 86, 3901, 741, 2183, 282, 305, 2312, 1111, 5103, 2162, 43572, 13113, 535, 11052, 1667, 36466, 637, 627, 83, 9399, 20443, 996, 3901, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07577755491612322, "compression_ratio": 1.4741641337386018, "no_speech_prob": 0.05792892351746559}, {"id": 67, "seek": 45350, "start": 453.5, "end": 464.5, "text": " Dok\u0142adnie. Przeprowadzili seri\u0119 genialnych eksperyment\u00f3w kontrolnych, tak zwane ablation study, \u017ceby wyizorowa\u0107 ten kluczowy czyn n\u00f3g.", "tokens": [50364, 29768, 10358, 2766, 13, 2114, 46342, 1892, 345, 89, 2312, 816, 5034, 48228, 9399, 30724, 610, 88, 518, 3901, 14373, 6623, 9399, 11, 991, 11873, 1929, 410, 24278, 2979, 11, 11316, 4628, 590, 284, 11445, 2064, 9671, 1311, 89, 10089, 6430, 77, 6604, 70, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07046056929088775, "compression_ratio": 1.3903345724907064, "no_speech_prob": 0.055500660091638565}, {"id": 68, "seek": 45350, "start": 464.5, "end": 470.5, "text": " Chcieli sprawdzi\u0107, czy to na pewno ten logiczny, j\u0119zykowy proces my\u015blowy jest odpowiedzialny za sukces.", "tokens": [50914, 761, 537, 10148, 46192, 28496, 11, 6430, 281, 1667, 33002, 2064, 9952, 89, 1634, 11, 49055, 74, 10089, 17565, 452, 19212, 10089, 3492, 24314, 15338, 831, 1634, 7949, 46432, 887, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07046056929088775, "compression_ratio": 1.3903345724907064, "no_speech_prob": 0.055500660091638565}, {"id": 69, "seek": 45350, "start": 470.5, "end": 474.5, "text": " Figure 5 w pracy \u015bwietnie to ilustruje.", "tokens": [51214, 43225, 1025, 261, 35591, 8299, 39083, 2766, 281, 1930, 381, 894, 2884, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07046056929088775, "compression_ratio": 1.3903345724907064, "no_speech_prob": 0.055500660091638565}, {"id": 70, "seek": 45350, "start": 474.5, "end": 479.5, "text": " Pierwsza hipoteza na celowniku detektyw\u00f3w. A mo\u017ce nie trzeba tej ca\u0142ej opowie\u015bci.", "tokens": [51414, 16676, 14358, 2394, 8103, 1370, 2394, 1667, 9277, 648, 24320, 1141, 916, 874, 86, 3901, 13, 316, 12034, 2838, 25860, 12573, 47631, 73, 999, 13998, 6199, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07046056929088775, "compression_ratio": 1.3903345724907064, "no_speech_prob": 0.055500660091638565}, {"id": 71, "seek": 47950, "start": 479.5, "end": 484.5, "text": " Mo\u017ce wystaraczy poda\u0107 modelowi samor\u00f3wnanie matematyczne bez tego ca\u0142ego opisu s\u0142ownego.", "tokens": [50364, 43774, 4628, 9710, 14691, 2497, 43379, 2316, 24503, 3247, 284, 812, 895, 7155, 3803, 8615, 17466, 716, 10782, 8627, 35224, 6308, 999, 25871, 15116, 648, 6308, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1139941169220267, "compression_ratio": 1.3148936170212766, "no_speech_prob": 0.2767932116985321}, {"id": 72, "seek": 47950, "start": 484.5, "end": 489.5, "text": " Sprawdzili to. Nazwali ten variant equation only.", "tokens": [50614, 1738, 15889, 89, 2312, 281, 13, 11870, 40054, 2064, 17501, 5367, 787, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1139941169220267, "compression_ratio": 1.3148936170212766, "no_speech_prob": 0.2767932116985321}, {"id": 73, "seek": 47950, "start": 489.5, "end": 501.5, "text": " I okaza\u0142o si\u0119, \u017ce to nie wystarcza. Model, widz\u0105c tylko suche 20 plus 6 r\u00f3wna si\u0119 9, nie potrafi\u0142 przenie\u015b\u0107 tej logiki na nowe, z\u0142o\u017cone zadania tekstrowe.", "tokens": [50864, 286, 3133, 12257, 5249, 3244, 11, 3561, 281, 2838, 4628, 9710, 41524, 13, 17105, 11, 5274, 8925, 66, 13219, 1270, 68, 945, 1804, 1386, 367, 3901, 629, 3244, 1722, 11, 2838, 1847, 10437, 40622, 582, 16778, 7753, 12573, 3565, 9850, 1667, 586, 68, 11, 710, 5249, 1427, 546, 42788, 5609, 16624, 372, 1892, 68, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1139941169220267, "compression_ratio": 1.3148936170212766, "no_speech_prob": 0.2767932116985321}, {"id": 74, "seek": 50150, "start": 501.5, "end": 506.5, "text": " Gubi\u0142 si\u0119 w semantyce, nie wiedzia\u0142, kt\u00f3r\u0105 liczb\u0119 odj\u0105\u0107, a kt\u00f3r\u0105 doda\u0107.", "tokens": [50364, 460, 836, 40622, 3244, 261, 4361, 394, 88, 384, 11, 2838, 261, 15338, 8908, 11, 37415, 6169, 89, 65, 1274, 3611, 8555, 2162, 11, 257, 37415, 360, 2675, 2162, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07432638251263163, "compression_ratio": 1.3629343629343629, "no_speech_prob": 0.3571386933326721}, {"id": 75, "seek": 50150, "start": 506.5, "end": 514.5, "text": " To w\u0142a\u015bnie kroki opisanej j\u0119zykiem naturalnym zosta\u0142o dokupiono, okaza\u0142y si\u0119 kluczowe, \u017ceby zrozumia\u0142 kontekst problemu.", "tokens": [50614, 1407, 14234, 45909, 2984, 45477, 1929, 73, 49055, 26116, 3303, 12996, 23154, 5249, 25037, 1010, 49020, 11, 3133, 12257, 6825, 3244, 9671, 1311, 89, 6880, 11, 11316, 710, 27857, 449, 8908, 14373, 916, 372, 1154, 84, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07432638251263163, "compression_ratio": 1.3629343629343629, "no_speech_prob": 0.3571386933326721}, {"id": 76, "seek": 50150, "start": 514.5, "end": 519.5, "text": " Dobrze, czyli nie chodzi o sam\u0105 matematyk\u0119. To prowadzi do drugiej hipotezy.", "tokens": [51014, 29679, 13503, 11, 16591, 2838, 23998, 277, 3247, 1611, 3803, 8615, 88, 15724, 13, 1407, 36590, 3992, 360, 47373, 8103, 1370, 1229, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07432638251263163, "compression_ratio": 1.3629343629343629, "no_speech_prob": 0.3571386933326721}, {"id": 77, "seek": 50150, "start": 519.5, "end": 524.5, "text": " A mo\u017ce model po prostu potrzebuje wi\u0119cej czasu na my\u015blenie.", "tokens": [51264, 316, 12034, 2316, 714, 19518, 28577, 6021, 2884, 26004, 40860, 1667, 48633, 6698, 414, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07432638251263163, "compression_ratio": 1.3629343629343629, "no_speech_prob": 0.3571386933326721}, {"id": 78, "seek": 52450, "start": 525.5, "end": 532.5, "text": " Wi\u0119cej przestrzeni obliczeniowej. Mo\u017ce samo generowanie d\u0142u\u017cszego tekstu, nie wa\u017cne jakiego, pomaga mu doj\u015b\u0107 do poprawnego wyniku.", "tokens": [50414, 30127, 20811, 44264, 81, 42124, 1111, 1050, 42124, 21091, 13, 43774, 36422, 1337, 22028, 274, 24066, 1427, 15453, 6308, 16624, 372, 84, 11, 2838, 46110, 4207, 12200, 11, 12991, 9286, 2992, 360, 44536, 360, 1665, 5131, 11858, 31936, 24320, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08302412101690718, "compression_ratio": 1.470790378006873, "no_speech_prob": 0.23552250862121582}, {"id": 79, "seek": 52450, "start": 532.5, "end": 535.5, "text": " Nazwali to variable compute only.", "tokens": [50764, 11870, 40054, 281, 7006, 14722, 787, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08302412101690718, "compression_ratio": 1.470790378006873, "no_speech_prob": 0.23552250862121582}, {"id": 80, "seek": 52450, "start": 535.5, "end": 538.5, "text": " To by\u0142 naprawd\u0119 sprytny eksperyment.", "tokens": [50914, 1407, 16673, 20970, 637, 627, 83, 1634, 30724, 610, 88, 518, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08302412101690718, "compression_ratio": 1.470790378006873, "no_speech_prob": 0.23552250862121582}, {"id": 81, "seek": 52450, "start": 538.5, "end": 543.5, "text": " Zamiast kaza\u0107 modelowi generowa\u0107 logiczne kroki, poleci mu wygenerowa\u0107 seri\u0105 kropek.", "tokens": [51064, 1176, 4526, 525, 350, 12257, 2162, 2316, 24503, 1337, 11445, 9952, 43077, 45909, 2984, 11, 13208, 537, 2992, 4628, 21848, 11445, 816, 11404, 45909, 32659, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08302412101690718, "compression_ratio": 1.470790378006873, "no_speech_prob": 0.23552250862121582}, {"id": 82, "seek": 52450, "start": 543.5, "end": 544.5, "text": " Kropek.", "tokens": [51314, 591, 340, 32659, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08302412101690718, "compression_ratio": 1.470790378006873, "no_speech_prob": 0.23552250862121582}, {"id": 83, "seek": 52450, "start": 544.5, "end": 552.5, "text": " Tak, kropek. Tyle kropek ile znak\u00f3w mia\u0142oby poprawne rozumowanie. Chcieli sprawdzi\u0107, czy samo wysilenie si\u0119 co\u015b da.", "tokens": [51364, 9118, 11, 45909, 32659, 13, 314, 2072, 45909, 32659, 15465, 15397, 514, 3901, 27989, 13944, 1665, 5131, 716, 48797, 22028, 13, 761, 537, 10148, 46192, 28496, 11, 6430, 36422, 27062, 17471, 414, 3244, 19241, 1120, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08302412101690718, "compression_ratio": 1.470790378006873, "no_speech_prob": 0.23552250862121582}, {"id": 84, "seek": 55250, "start": 552.5, "end": 556.5, "text": " Czyli w zasadzie kazali mu pomedytowa\u0107 nad problemem, generuj\u0105c kropki.", "tokens": [50364, 37099, 261, 44585, 3283, 30623, 5103, 2992, 12991, 6038, 83, 11445, 12617, 1154, 443, 11, 1337, 44733, 350, 1513, 2984, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08304359926191789, "compression_ratio": 1.4942528735632183, "no_speech_prob": 0.04479303956031799}, {"id": 85, "seek": 55250, "start": 556.5, "end": 558.5, "text": " I okaza\u0142o si\u0119, \u017ce to nic nie da\u0142o.", "tokens": [50564, 286, 3133, 12257, 5249, 3244, 11, 3561, 281, 6201, 2838, 1120, 5249, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08304359926191789, "compression_ratio": 1.4942528735632183, "no_speech_prob": 0.04479303956031799}, {"id": 86, "seek": 55250, "start": 558.5, "end": 562.5, "text": " To chyba z\u0142a wiadomo\u015b\u0107 dla fan\u00f3w pustego patrzenia w \u015bcian\u0119 w poszukiwaniu natchnienia.", "tokens": [50664, 1407, 31532, 710, 5024, 26393, 40633, 7753, 12285, 3429, 3901, 280, 381, 6308, 1947, 81, 14320, 261, 220, 6199, 282, 1274, 261, 1366, 89, 11788, 86, 25849, 297, 852, 77, 18811, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08304359926191789, "compression_ratio": 1.4942528735632183, "no_speech_prob": 0.04479303956031799}, {"id": 87, "seek": 55250, "start": 562.5, "end": 563.5, "text": " Niestety tak.", "tokens": [50864, 426, 6495, 2210, 991, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08304359926191789, "compression_ratio": 1.4942528735632183, "no_speech_prob": 0.04479303956031799}, {"id": 88, "seek": 55250, "start": 563.5, "end": 568.5, "text": " Wynik \u017cadnej poprawy w stosunku do standardowego podej\u015bcia.", "tokens": [50914, 343, 2534, 1035, 39628, 11794, 1665, 5131, 88, 261, 43581, 49910, 360, 3832, 26576, 7468, 73, 1788, 2755, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08304359926191789, "compression_ratio": 1.4942528735632183, "no_speech_prob": 0.04479303956031799}, {"id": 89, "seek": 55250, "start": 568.5, "end": 574.5, "text": " To by\u0142 nokautuj\u0105cy dow\u00f3d, \u017ce nie chodzi o sam\u0105 ilo\u015b\u0107 oblicze\u0144 czy wygenerowanych token\u00f3w,", "tokens": [51164, 1407, 16673, 33811, 1375, 13263, 1344, 9459, 17081, 11, 3561, 2838, 23998, 277, 3247, 1611, 1930, 78, 7753, 1111, 1050, 49689, 6430, 4628, 21848, 23341, 339, 14862, 3901, 11, 51464], "temperature": 0.0, "avg_logprob": -0.08304359926191789, "compression_ratio": 1.4942528735632183, "no_speech_prob": 0.04479303956031799}, {"id": 90, "seek": 55250, "start": 574.5, "end": 578.5, "text": " ale o semantyczn\u0105 i logiczn\u0105 struktur\u0119 tego, co jest generowane.", "tokens": [51464, 6775, 277, 4361, 394, 17466, 13113, 741, 9952, 89, 13113, 342, 31543, 1274, 8627, 11, 598, 3492, 1337, 23066, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08304359926191789, "compression_ratio": 1.4942528735632183, "no_speech_prob": 0.04479303956031799}, {"id": 91, "seek": 55250, "start": 578.5, "end": 581.5, "text": " Nie wystarczy my\u015ble\u0107 d\u0142u\u017cej. Trzeba my\u015ble\u0107 w w\u0142a\u015bciwy spos\u00f3b.", "tokens": [51664, 12016, 4628, 9710, 6522, 48633, 306, 2162, 274, 24066, 38493, 13, 1765, 1381, 4231, 48633, 306, 2162, 261, 40112, 9726, 22904, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08304359926191789, "compression_ratio": 1.4942528735632183, "no_speech_prob": 0.04479303956031799}, {"id": 92, "seek": 58150, "start": 581.5, "end": 585.5, "text": " I ostatni cios, chyba najbardziej podchwytli\u0142a hipoteza.", "tokens": [50364, 286, 32686, 3722, 269, 2717, 11, 31532, 41857, 2497, 34655, 4328, 2081, 5024, 8103, 1370, 2394, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07097119701151945, "compression_ratio": 1.468013468013468, "no_speech_prob": 0.07846765965223312}, {"id": 93, "seek": 58150, "start": 585.5, "end": 590.5, "text": " A co, je\u015bli ten ca\u0142y proces my\u015blowy wcale nie s\u0142u\u017cy do rozwi\u0105zania problemu,", "tokens": [50564, 316, 598, 11, 25630, 2064, 35226, 17565, 452, 19212, 10089, 261, 37088, 2838, 48459, 7735, 360, 9544, 22620, 5609, 1154, 84, 11, 50814], "temperature": 0.0, "avg_logprob": -0.07097119701151945, "compression_ratio": 1.468013468013468, "no_speech_prob": 0.07846765965223312}, {"id": 94, "seek": 58150, "start": 590.5, "end": 594.5, "text": " a jedynie do aktywowania odpowiedniej wiedzy w modelu?", "tokens": [50814, 257, 5232, 2534, 414, 360, 9308, 874, 86, 21308, 36574, 10402, 46894, 1229, 261, 2316, 84, 30, 51014], "temperature": 0.0, "avg_logprob": -0.07097119701151945, "compression_ratio": 1.468013468013468, "no_speech_prob": 0.07846765965223312}, {"id": 95, "seek": 58150, "start": 594.5, "end": 596.5, "text": " Taka rozgrzewka przed odpowiedzi\u0105.", "tokens": [51014, 314, 7849, 9544, 861, 43551, 2330, 18334, 36574, 3992, 1611, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07097119701151945, "compression_ratio": 1.468013468013468, "no_speech_prob": 0.07846765965223312}, {"id": 96, "seek": 58150, "start": 596.5, "end": 602.5, "text": " Sprawdzili to, karz\u0105c modelowi najpierw poda\u0107 odpowied\u017a, a dopiero potem wygenerowa\u0107 wyja\u015bnienie.", "tokens": [51114, 1738, 15889, 89, 2312, 281, 11, 7917, 8925, 66, 2316, 24503, 11212, 45119, 86, 2497, 43379, 36574, 10659, 11, 257, 21900, 12030, 36513, 4628, 21848, 11445, 4628, 2938, 1788, 77, 27385, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07097119701151945, "compression_ratio": 1.468013468013468, "no_speech_prob": 0.07846765965223312}, {"id": 97, "seek": 58150, "start": 602.5, "end": 605.5, "text": " Nazwali to chain of thought after answer.", "tokens": [51414, 11870, 40054, 281, 5021, 295, 1194, 934, 1867, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07097119701151945, "compression_ratio": 1.468013468013468, "no_speech_prob": 0.07846765965223312}, {"id": 98, "seek": 58150, "start": 605.5, "end": 607.5, "text": " I ta metoda r\u00f3wnie\u017c spektakularnie zawiod\u0142a.", "tokens": [51564, 286, 1846, 1131, 13449, 20532, 768, 2320, 514, 1040, 2766, 28165, 2695, 5024, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07097119701151945, "compression_ratio": 1.468013468013468, "no_speech_prob": 0.07846765965223312}, {"id": 99, "seek": 58150, "start": 607.5, "end": 608.5, "text": " Serio?", "tokens": [51664, 4210, 1004, 30, 51714], "temperature": 0.0, "avg_logprob": -0.07097119701151945, "compression_ratio": 1.468013468013468, "no_speech_prob": 0.07846765965223312}, {"id": 100, "seek": 58150, "start": 608.5, "end": 609.5, "text": " Tak.", "tokens": [51714, 9118, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07097119701151945, "compression_ratio": 1.468013468013468, "no_speech_prob": 0.07846765965223312}, {"id": 101, "seek": 60950, "start": 609.5, "end": 613.5, "text": " To chyba najmocniejszy dow\u00f3d z ca\u0142ego badania.", "tokens": [50364, 1407, 31532, 11212, 76, 905, 10402, 7706, 9459, 17081, 710, 35224, 6308, 1578, 5609, 13, 50564], "temperature": 0.0, "avg_logprob": -0.05639522192908115, "compression_ratio": 1.434108527131783, "no_speech_prob": 0.02363935485482216}, {"id": 102, "seek": 60950, "start": 613.5, "end": 617.5, "text": " Pokazuje, \u017ce kolejno\u015b\u0107 ma fundamentalne znaczenie.", "tokens": [50564, 14958, 43317, 11, 3561, 23749, 23293, 463, 8088, 716, 15397, 326, 16778, 13, 50764], "temperature": 0.0, "avg_logprob": -0.05639522192908115, "compression_ratio": 1.434108527131783, "no_speech_prob": 0.02363935485482216}, {"id": 103, "seek": 60950, "start": 617.5, "end": 622.5, "text": " Model faktycznie wykorzystuje wygenerowane przez siebie kolejne kroki,", "tokens": [50764, 17105, 33647, 45586, 43606, 36049, 13008, 4628, 21848, 23066, 14064, 39137, 23749, 716, 45909, 2984, 11, 51014], "temperature": 0.0, "avg_logprob": -0.05639522192908115, "compression_ratio": 1.434108527131783, "no_speech_prob": 0.02363935485482216}, {"id": 104, "seek": 60950, "start": 622.5, "end": 625.5, "text": " aby doj\u015b\u0107 do poprawnego wyniku na ko\u0144cu.", "tokens": [51014, 24457, 360, 44536, 360, 1665, 5131, 11858, 31936, 24320, 1667, 26470, 12032, 13, 51164], "temperature": 0.0, "avg_logprob": -0.05639522192908115, "compression_ratio": 1.434108527131783, "no_speech_prob": 0.02363935485482216}, {"id": 105, "seek": 60950, "start": 625.5, "end": 629.5, "text": " To nie jest postracjonalizacja, to nie jest uzasadnianie z g\u00f3ry podj\u0119tej decyzji.", "tokens": [51164, 1407, 2838, 3492, 2183, 12080, 15735, 304, 590, 23395, 11, 281, 2838, 3492, 16851, 296, 345, 77, 952, 414, 710, 290, 812, 627, 2497, 11115, 975, 73, 979, 37433, 4013, 13, 51364], "temperature": 0.0, "avg_logprob": -0.05639522192908115, "compression_ratio": 1.434108527131783, "no_speech_prob": 0.02363935485482216}, {"id": 106, "seek": 60950, "start": 629.5, "end": 634.5, "text": " To jest autentyczny krok po kroku, proces dochodzenia do odpowiedzi.", "tokens": [51364, 1407, 3492, 1476, 4179, 3689, 1634, 350, 31621, 714, 45909, 5279, 11, 17565, 9243, 378, 14320, 360, 36574, 3992, 13, 51614], "temperature": 0.0, "avg_logprob": -0.05639522192908115, "compression_ratio": 1.434108527131783, "no_speech_prob": 0.02363935485482216}, {"id": 107, "seek": 63450, "start": 634.5, "end": 637.5, "text": " Czyli podsumowuj\u0105c t\u0119 detektywistyczn\u0105 prac\u0119,", "tokens": [50364, 37099, 31925, 449, 305, 44733, 32489, 1141, 916, 874, 86, 468, 17466, 13113, 22404, 1274, 11, 50514], "temperature": 0.0, "avg_logprob": -0.07060220055546321, "compression_ratio": 1.4305084745762713, "no_speech_prob": 0.07688663899898529}, {"id": 108, "seek": 63450, "start": 637.5, "end": 642.5, "text": " wygl\u0105da na to, \u017ce model rzeczywi\u015bcie, w pewnym sensie, my\u015bli na g\u0142os.", "tokens": [50514, 32015, 1667, 281, 11, 3561, 2316, 44922, 11, 261, 47160, 4199, 2923, 414, 11, 452, 15350, 1667, 43767, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07060220055546321, "compression_ratio": 1.4305084745762713, "no_speech_prob": 0.07688663899898529}, {"id": 109, "seek": 63450, "start": 642.5, "end": 649.5, "text": " Generuje po\u015bredni stan, analizuje go i na jego podstawie generuje kolejny, a\u017c dojdzie to fina\u0142u.", "tokens": [50764, 15409, 13008, 714, 1788, 986, 3722, 27984, 11, 2624, 590, 13008, 352, 741, 1667, 26542, 43443, 414, 1337, 13008, 23749, 1634, 11, 48134, 360, 73, 13096, 281, 962, 64, 24066, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07060220055546321, "compression_ratio": 1.4305084745762713, "no_speech_prob": 0.07688663899898529}, {"id": 110, "seek": 63450, "start": 649.5, "end": 654.5, "text": " To co\u015b znacznie wi\u0119cej ni\u017c tylko odnajdywanie statystycznych wzorc\u00f3w w tak\u015bcie.", "tokens": [51114, 1407, 19241, 15397, 14875, 2766, 26004, 28502, 13219, 3611, 20981, 3173, 86, 7155, 2219, 38593, 17466, 9399, 24809, 284, 29268, 261, 991, 9815, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07060220055546321, "compression_ratio": 1.4305084745762713, "no_speech_prob": 0.07688663899898529}, {"id": 111, "seek": 63450, "start": 654.5, "end": 657.5, "text": " I tu dochodzimy do sedna pot\u0119gi tej metody.", "tokens": [51364, 286, 2604, 9243, 378, 89, 13189, 360, 9643, 629, 1847, 1274, 7834, 12573, 1131, 843, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07060220055546321, "compression_ratio": 1.4305084745762713, "no_speech_prob": 0.07688663899898529}, {"id": 112, "seek": 63450, "start": 657.5, "end": 661.5, "text": " Poniewa\u017c ten \u0142a\u0144cuch my\u015bli jest wyra\u017cony w j\u0119zyku naturalnym,", "tokens": [51514, 31756, 27806, 2064, 220, 5024, 5248, 66, 625, 452, 15350, 3492, 4628, 424, 1427, 2526, 261, 49055, 5279, 3303, 12996, 11, 51714], "temperature": 0.0, "avg_logprob": -0.07060220055546321, "compression_ratio": 1.4305084745762713, "no_speech_prob": 0.07688663899898529}, {"id": 113, "seek": 66150, "start": 661.5, "end": 665.5, "text": " a nie w j\u0119zyku formalnym, matematycznym czy programistycznym,", "tokens": [50364, 257, 2838, 261, 49055, 5279, 9860, 12996, 11, 3803, 8615, 17466, 12996, 6430, 1461, 468, 17466, 12996, 11, 50564], "temperature": 0.0, "avg_logprob": -0.054306488770705, "compression_ratio": 1.4464285714285714, "no_speech_prob": 0.011183055117726326}, {"id": 114, "seek": 66150, "start": 665.5, "end": 670.5, "text": " mo\u017cna go zastosowa\u0107 do niemal ka\u017cdego problemu, kt\u00f3ry wymaga rozumowania.", "tokens": [50564, 17790, 352, 36746, 329, 11445, 360, 2838, 5579, 21912, 67, 6308, 1154, 84, 11, 9913, 29764, 9286, 48797, 21308, 13, 50814], "temperature": 0.0, "avg_logprob": -0.054306488770705, "compression_ratio": 1.4464285714285714, "no_speech_prob": 0.011183055117726326}, {"id": 115, "seek": 66150, "start": 670.5, "end": 673.5, "text": " Nie ogranicza nas to do zada\u0144 arytmetycznych.", "tokens": [50814, 12016, 34416, 30732, 2394, 5382, 281, 360, 710, 1538, 5248, 594, 4328, 76, 2210, 3689, 9399, 13, 50964], "temperature": 0.0, "avg_logprob": -0.054306488770705, "compression_ratio": 1.4464285714285714, "no_speech_prob": 0.011183055117726326}, {"id": 116, "seek": 66150, "start": 673.5, "end": 677.5, "text": " W\u0142a\u015bnie badanie pokazuje niesamowit\u0105 skuteczno\u015b\u0107", "tokens": [50964, 343, 5024, 12221, 1578, 7155, 13010, 43317, 48100, 335, 305, 270, 1611, 1110, 1169, 3689, 23293, 51164], "temperature": 0.0, "avg_logprob": -0.054306488770705, "compression_ratio": 1.4464285714285714, "no_speech_prob": 0.011183055117726326}, {"id": 117, "seek": 66150, "start": 677.5, "end": 682.5, "text": " w zadaniach na tak zwany zdrowy rozs\u0105dek, czyli common sense reasoning,", "tokens": [51164, 261, 42788, 3782, 608, 1667, 991, 11873, 1325, 49745, 88, 9544, 82, 18962, 916, 11, 16591, 2689, 2020, 21577, 11, 51414], "temperature": 0.0, "avg_logprob": -0.054306488770705, "compression_ratio": 1.4464285714285714, "no_speech_prob": 0.011183055117726326}, {"id": 118, "seek": 66150, "start": 682.5, "end": 685.5, "text": " a tak\u017ce w rozumowaniu symbolicznym.", "tokens": [51414, 257, 23306, 261, 48797, 305, 25849, 5986, 17946, 12996, 13, 51564], "temperature": 0.0, "avg_logprob": -0.054306488770705, "compression_ratio": 1.4464285714285714, "no_speech_prob": 0.011183055117726326}, {"id": 119, "seek": 66150, "start": 685.5, "end": 688.5, "text": " Figure 3 w pracy jest pe\u0142ne \u015bwietnych przyk\u0142ad\u00f3w.", "tokens": [51564, 43225, 805, 261, 35591, 3492, 43205, 716, 8299, 39083, 9399, 23144, 3901, 13, 51714], "temperature": 0.0, "avg_logprob": -0.054306488770705, "compression_ratio": 1.4464285714285714, "no_speech_prob": 0.011183055117726326}, {"id": 120, "seek": 68850, "start": 688.5, "end": 690.5, "text": " Jeden z nich mnie zachwyci\u0142.", "tokens": [50364, 508, 6876, 710, 25570, 17661, 29303, 9726, 537, 1221, 13, 50464], "temperature": 0.0, "avg_logprob": -0.14478074867306776, "compression_ratio": 1.4048582995951417, "no_speech_prob": 0.06907634437084198}, {"id": 121, "seek": 68850, "start": 690.5, "end": 691.5, "text": " Pytanie.", "tokens": [50464, 430, 4328, 7155, 13, 50514], "temperature": 0.0, "avg_logprob": -0.14478074867306776, "compression_ratio": 1.4048582995951417, "no_speech_prob": 0.06907634437084198}, {"id": 122, "seek": 68850, "start": 691.5, "end": 697.5, "text": " Czy zdanie, Jua\u0142o Moutinho z\u0142apa\u0142 podanie w mistrzostwach NFC jest prawdopodobne?", "tokens": [50514, 19832, 16221, 7155, 11, 508, 4398, 5249, 376, 346, 7775, 31614, 7961, 1221, 2497, 7155, 261, 3544, 19390, 555, 50038, 13576, 34, 3492, 41175, 46684, 996, 716, 30, 50814], "temperature": 0.0, "avg_logprob": -0.14478074867306776, "compression_ratio": 1.4048582995951417, "no_speech_prob": 0.06907634437084198}, {"id": 123, "seek": 68850, "start": 697.5, "end": 700.5, "text": " Standardowy model m\u00f3g\u0142by si\u0119 pogubi\u0107.", "tokens": [50814, 21298, 10089, 2316, 275, 14047, 34635, 3244, 32037, 836, 12757, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14478074867306776, "compression_ratio": 1.4048582995951417, "no_speech_prob": 0.06907634437084198}, {"id": 124, "seek": 68850, "start": 700.5, "end": 705.5, "text": " Kojarzy sportowiec, mistrzostwa, podanie i odpowiedzie\u0107, \u017ce tak.", "tokens": [50964, 10509, 10150, 1229, 7282, 13998, 66, 11, 3544, 19390, 555, 4151, 11, 2497, 7155, 741, 24314, 22078, 11, 3561, 991, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14478074867306776, "compression_ratio": 1.4048582995951417, "no_speech_prob": 0.06907634437084198}, {"id": 125, "seek": 68850, "start": 705.5, "end": 709.5, "text": " Ale model u\u017cywaj\u0105cy Chain of Sold rozumuje tak.", "tokens": [51214, 9366, 2316, 34097, 86, 11133, 1344, 33252, 295, 20064, 48797, 13008, 991, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14478074867306776, "compression_ratio": 1.4048582995951417, "no_speech_prob": 0.06907634437084198}, {"id": 126, "seek": 68850, "start": 709.5, "end": 712.5, "text": " Jua\u0142o Moutinho to portugalski pi\u0142karz.", "tokens": [51414, 508, 4398, 5249, 376, 346, 7775, 281, 2436, 697, 1124, 2984, 3895, 1221, 12303, 89, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14478074867306776, "compression_ratio": 1.4048582995951417, "no_speech_prob": 0.06907634437084198}, {"id": 127, "seek": 68850, "start": 712.5, "end": 714.5, "text": " Pi\u0142ka no\u017cna to soker.", "tokens": [51564, 17741, 1221, 2330, 572, 1427, 629, 281, 370, 5767, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14478074867306776, "compression_ratio": 1.4048582995951417, "no_speech_prob": 0.06907634437084198}, {"id": 128, "seek": 71450, "start": 715.5, "end": 718.5, "text": " Mistrzostwa NFAC to futbol ameryka\u0144ski.", "tokens": [50414, 20166, 19390, 555, 4151, 13576, 4378, 281, 1877, 17460, 669, 2109, 2330, 5248, 18020, 13, 50564], "temperature": 0.0, "avg_logprob": -0.06507935429250958, "compression_ratio": 1.5054545454545454, "no_speech_prob": 0.07352885603904724}, {"id": 129, "seek": 71450, "start": 718.5, "end": 723.5, "text": " W futbolu ameryka\u0144skim u\u0142apie si\u0119 podania, ale Moutinho nie jest futbolist\u0105.", "tokens": [50564, 343, 1877, 17460, 84, 669, 2109, 2330, 27125, 332, 344, 1221, 569, 414, 3244, 2497, 5609, 11, 6775, 376, 346, 7775, 2838, 3492, 1877, 17460, 468, 1611, 13, 50814], "temperature": 0.0, "avg_logprob": -0.06507935429250958, "compression_ratio": 1.5054545454545454, "no_speech_prob": 0.07352885603904724}, {"id": 130, "seek": 71450, "start": 723.5, "end": 725.5, "text": " Zatem odpowied\u017a brzmi? Nie.", "tokens": [50814, 1176, 26851, 36574, 10659, 738, 89, 3057, 30, 12016, 13, 50914], "temperature": 0.0, "avg_logprob": -0.06507935429250958, "compression_ratio": 1.5054545454545454, "no_speech_prob": 0.07352885603904724}, {"id": 131, "seek": 71450, "start": 725.5, "end": 726.5, "text": " To jest niesamowite.", "tokens": [50914, 1407, 3492, 48100, 335, 305, 642, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06507935429250958, "compression_ratio": 1.5054545454545454, "no_speech_prob": 0.07352885603904724}, {"id": 132, "seek": 71450, "start": 726.5, "end": 728.5, "text": " To nie jest proste kojarzenie fakt\u00f3w.", "tokens": [50964, 1407, 2838, 3492, 10293, 68, 8384, 10150, 16778, 21310, 3901, 13, 51064], "temperature": 0.0, "avg_logprob": -0.06507935429250958, "compression_ratio": 1.5054545454545454, "no_speech_prob": 0.07352885603904724}, {"id": 133, "seek": 71450, "start": 728.5, "end": 732.5, "text": " To jest \u0142\u0105czenie wiedzy z dw\u00f3ch zupe\u0142nie r\u00f3\u017cnych domen.", "tokens": [51064, 1407, 3492, 220, 15926, 39043, 46894, 1229, 710, 27379, 812, 339, 49922, 42602, 3285, 268, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06507935429250958, "compression_ratio": 1.5054545454545454, "no_speech_prob": 0.07352885603904724}, {"id": 134, "seek": 71450, "start": 732.5, "end": 735.5, "text": " Europejskiej pi\u0142ki no\u017cnej i ameryka\u0144skiego futbolu,", "tokens": [51264, 3315, 32625, 7764, 3895, 1221, 2984, 572, 1427, 11794, 741, 669, 2109, 2330, 27125, 12200, 1877, 17460, 84, 11, 51414], "temperature": 0.0, "avg_logprob": -0.06507935429250958, "compression_ratio": 1.5054545454545454, "no_speech_prob": 0.07352885603904724}, {"id": 135, "seek": 71450, "start": 735.5, "end": 738.5, "text": " \u017ceby wyci\u0105gn\u0105\u0107 logiczny wniosek.", "tokens": [51414, 11316, 4628, 34381, 4568, 36374, 9952, 89, 1634, 261, 3722, 541, 74, 13, 51564], "temperature": 0.0, "avg_logprob": -0.06507935429250958, "compression_ratio": 1.5054545454545454, "no_speech_prob": 0.07352885603904724}, {"id": 136, "seek": 71450, "start": 738.5, "end": 741.5, "text": " To jest zupe\u0142nie inny poziom rozumienia \u015bwiata.", "tokens": [51564, 1407, 3492, 49922, 294, 1634, 38503, 298, 48797, 18811, 21485, 3274, 13, 51714], "temperature": 0.0, "avg_logprob": -0.06507935429250958, "compression_ratio": 1.5054545454545454, "no_speech_prob": 0.07352885603904724}, {"id": 137, "seek": 74150, "start": 741.5, "end": 744.5, "text": " Albo inny przyk\u0142ad z rozmowania symbolicznego.", "tokens": [50364, 967, 1763, 294, 1634, 23144, 710, 35234, 21308, 5986, 17946, 11858, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07328895543584761, "compression_ratio": 1.5246478873239437, "no_speech_prob": 0.050441522151231766}, {"id": 138, "seek": 74150, "start": 744.5, "end": 748.5, "text": " Zadanie po\u0142\u0105cz ostatnie litery imienia i nazwiska.", "tokens": [50514, 1176, 345, 7155, 714, 43558, 32686, 2766, 2733, 88, 566, 18811, 741, 20151, 86, 21945, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07328895543584761, "compression_ratio": 1.5246478873239437, "no_speech_prob": 0.050441522151231766}, {"id": 139, "seek": 74150, "start": 748.5, "end": 751.5, "text": " Na przyk\u0142ad dla Lady Gaga.", "tokens": [50714, 6056, 23144, 12285, 11256, 41465, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07328895543584761, "compression_ratio": 1.5246478873239437, "no_speech_prob": 0.050441522151231766}, {"id": 140, "seek": 74150, "start": 751.5, "end": 752.5, "text": " Model t\u0142umaczy.", "tokens": [50864, 17105, 256, 49166, 14691, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07328895543584761, "compression_ratio": 1.5246478873239437, "no_speech_prob": 0.050441522151231766}, {"id": 141, "seek": 74150, "start": 752.5, "end": 755.5, "text": " Ostatnia litera s\u0142owa Lady to Y.", "tokens": [50914, 422, 19435, 12679, 2733, 64, 15116, 5528, 11256, 281, 398, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07328895543584761, "compression_ratio": 1.5246478873239437, "no_speech_prob": 0.050441522151231766}, {"id": 142, "seek": 74150, "start": 755.5, "end": 757.5, "text": " Ostatnia litera s\u0142owa Gaga to A.", "tokens": [51064, 422, 19435, 12679, 2733, 64, 15116, 5528, 41465, 281, 316, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07328895543584761, "compression_ratio": 1.5246478873239437, "no_speech_prob": 0.050441522151231766}, {"id": 143, "seek": 74150, "start": 757.5, "end": 759.5, "text": " Po\u0142\u0105czenie ich daje ja.", "tokens": [51164, 6165, 15926, 39043, 1893, 1120, 2884, 2784, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07328895543584761, "compression_ratio": 1.5246478873239437, "no_speech_prob": 0.050441522151231766}, {"id": 144, "seek": 74150, "start": 759.5, "end": 762.5, "text": " To zadanie, kt\u00f3re dla cz\u0142owieka jest banalne.", "tokens": [51264, 1407, 42788, 7155, 11, 8864, 12285, 36282, 2330, 3492, 5643, 304, 716, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07328895543584761, "compression_ratio": 1.5246478873239437, "no_speech_prob": 0.050441522151231766}, {"id": 145, "seek": 74150, "start": 762.5, "end": 765.5, "text": " Dla modelu my\u015bl\u0105cego w kategoriach statystyki j\u0119zykowej", "tokens": [51414, 413, 875, 2316, 84, 452, 19212, 1611, 384, 1571, 261, 350, 2968, 7386, 608, 2219, 88, 25134, 2984, 49055, 74, 21091, 51564], "temperature": 0.0, "avg_logprob": -0.07328895543584761, "compression_ratio": 1.5246478873239437, "no_speech_prob": 0.050441522151231766}, {"id": 146, "seek": 74150, "start": 765.5, "end": 767.5, "text": " jest abstrakcyjn\u0105 operacj\u0105 na symbolach.", "tokens": [51564, 3492, 10823, 11272, 42949, 13113, 2208, 326, 8555, 1667, 5986, 608, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07328895543584761, "compression_ratio": 1.5246478873239437, "no_speech_prob": 0.050441522151231766}, {"id": 147, "seek": 74150, "start": 767.5, "end": 770.5, "text": " Chain of Sold pozwala moj\u0105 przeprowadzi\u0107.", "tokens": [51664, 33252, 295, 20064, 40557, 5159, 705, 8555, 30829, 1892, 345, 28496, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07328895543584761, "compression_ratio": 1.5246478873239437, "no_speech_prob": 0.050441522151231766}, {"id": 148, "seek": 77050, "start": 770.5, "end": 773.5, "text": " Ok, ale czy on naprawd\u0119 rozumie t\u0119 regu\u0142\u0119?", "tokens": [50364, 3477, 11, 6775, 6430, 322, 20970, 48797, 414, 32489, 1121, 84, 46564, 30, 50514], "temperature": 0.0, "avg_logprob": -0.0971716562906901, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.002719920827075839}, {"id": 149, "seek": 77050, "start": 773.5, "end": 777.5, "text": " Czy po prostu nauczy\u0142 si\u0119 na\u015bladowa\u0107 ten jeden konkretny trik,", "tokens": [50514, 19832, 714, 19518, 49103, 1229, 1221, 3244, 1667, 1788, 9290, 11445, 2064, 12906, 36500, 1634, 1376, 74, 11, 50714], "temperature": 0.0, "avg_logprob": -0.0971716562906901, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.002719920827075839}, {"id": 150, "seek": 77050, "start": 777.5, "end": 779.5, "text": " kt\u00f3ry mu pokazano w przyk\u0142adach?", "tokens": [50714, 9913, 2992, 13010, 921, 3730, 261, 23144, 608, 30, 50814], "temperature": 0.0, "avg_logprob": -0.0971716562906901, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.002719920827075839}, {"id": 151, "seek": 77050, "start": 779.5, "end": 782.5, "text": " Jak G\u0142\u0119boka jest ta zdolno\u015b\u0107 generalizacji?", "tokens": [50814, 15029, 460, 46564, 65, 15289, 3492, 1846, 16221, 401, 23293, 2674, 590, 13152, 30, 50964], "temperature": 0.0, "avg_logprob": -0.0971716562906901, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.002719920827075839}, {"id": 152, "seek": 77050, "start": 782.5, "end": 785.5, "text": " To jest kluczowe pytanie i badacze ja zadali.", "tokens": [50964, 1407, 3492, 9671, 1311, 89, 6880, 36610, 741, 1578, 326, 1381, 2784, 42788, 5103, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0971716562906901, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.002719920827075839}, {"id": 153, "seek": 77050, "start": 785.5, "end": 787.5, "text": " Sp\u00f3jrzmy na figure 8.", "tokens": [51114, 1738, 18999, 19390, 2226, 1667, 2573, 1649, 13, 51214], "temperature": 0.0, "avg_logprob": -0.0971716562906901, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.002719920827075839}, {"id": 154, "seek": 77050, "start": 787.5, "end": 790.5, "text": " Pokazali modelowi przyk\u0142ady \u0142\u0105czenia liter w imionach dwucz\u0142onowych,", "tokens": [51214, 14958, 921, 5103, 2316, 24503, 6501, 74, 1221, 880, 220, 15926, 38517, 2733, 261, 566, 313, 608, 27379, 1311, 89, 1221, 266, 19605, 11, 51364], "temperature": 0.0, "avg_logprob": -0.0971716562906901, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.002719920827075839}, {"id": 155, "seek": 77050, "start": 790.5, "end": 792.5, "text": " jak Lady Gaga.", "tokens": [51364, 4207, 11256, 41465, 13, 51464], "temperature": 0.0, "avg_logprob": -0.0971716562906901, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.002719920827075839}, {"id": 156, "seek": 77050, "start": 792.5, "end": 795.5, "text": " A nast\u0119pnie przetestowali go na imionach czterocz\u0142onowych,", "tokens": [51464, 316, 39662, 2766, 6541, 302, 377, 305, 5103, 352, 1667, 566, 313, 608, 6472, 391, 905, 89, 1221, 266, 19605, 11, 51614], "temperature": 0.0, "avg_logprob": -0.0971716562906901, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.002719920827075839}, {"id": 157, "seek": 77050, "start": 795.5, "end": 799.5, "text": " czyli na problemie o strukturze, kt\u00f3rej nigdy wcze\u015bniej nie widzia\u0142.", "tokens": [51614, 16591, 1667, 1154, 414, 277, 342, 31543, 1381, 11, 36023, 26996, 3173, 40785, 2838, 27486, 8908, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0971716562906901, "compression_ratio": 1.4727272727272727, "no_speech_prob": 0.002719920827075839}, {"id": 158, "seek": 79950, "start": 799.5, "end": 800.5, "text": " Jak posz\u0142o?", "tokens": [50364, 15029, 1366, 89, 5249, 30, 50414], "temperature": 0.0, "avg_logprob": -0.0775074495209588, "compression_ratio": 1.4208754208754208, "no_speech_prob": 0.012863430194556713}, {"id": 159, "seek": 79950, "start": 800.5, "end": 802.5, "text": " Z przypadku standard prompting model", "tokens": [50414, 1176, 41955, 3832, 12391, 278, 2316, 50514], "temperature": 0.0, "avg_logprob": -0.0775074495209588, "compression_ratio": 1.4208754208754208, "no_speech_prob": 0.012863430194556713}, {"id": 160, "seek": 79950, "start": 802.5, "end": 804.5, "text": " kompletnie sobie z tym nie radzi.", "tokens": [50514, 5207, 14657, 2766, 13652, 710, 8107, 2838, 2843, 3992, 13, 50614], "temperature": 0.0, "avg_logprob": -0.0775074495209588, "compression_ratio": 1.4208754208754208, "no_speech_prob": 0.012863430194556713}, {"id": 161, "seek": 79950, "start": 804.5, "end": 807.5, "text": " Jego skuteczno\u015b\u0107 spada praktycznie do zera.", "tokens": [50614, 508, 6308, 1110, 1169, 3689, 23293, 637, 1538, 3206, 74, 45586, 360, 710, 1663, 13, 50764], "temperature": 0.0, "avg_logprob": -0.0775074495209588, "compression_ratio": 1.4208754208754208, "no_speech_prob": 0.012863430194556713}, {"id": 162, "seek": 79950, "start": 807.5, "end": 809.5, "text": " A z Chain of Thought.", "tokens": [50764, 316, 710, 33252, 295, 23058, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0775074495209588, "compression_ratio": 1.4208754208754208, "no_speech_prob": 0.012863430194556713}, {"id": 163, "seek": 79950, "start": 809.5, "end": 813.5, "text": " Z Chain of Thought, model, kt\u00f3ry nauczy\u0142 si\u0119 procedury dla dw\u00f3ch s\u0142\u00f3w,", "tokens": [50864, 1176, 33252, 295, 23058, 11, 2316, 11, 9913, 49103, 1229, 1221, 3244, 6682, 2598, 12285, 27379, 812, 339, 15116, 3901, 11, 51064], "temperature": 0.0, "avg_logprob": -0.0775074495209588, "compression_ratio": 1.4208754208754208, "no_speech_prob": 0.012863430194556713}, {"id": 164, "seek": 79950, "start": 813.5, "end": 816.5, "text": " potrafi j\u0105 bezb\u0142\u0119dnie zastosowa\u0107 do czterech.", "tokens": [51064, 1847, 10437, 72, 35692, 10782, 65, 1221, 6298, 2766, 36746, 329, 11445, 360, 269, 2682, 323, 339, 13, 51214], "temperature": 0.0, "avg_logprob": -0.0775074495209588, "compression_ratio": 1.4208754208754208, "no_speech_prob": 0.012863430194556713}, {"id": 165, "seek": 79950, "start": 816.5, "end": 817.5, "text": " Wow.", "tokens": [51214, 3153, 13, 51264], "temperature": 0.0, "avg_logprob": -0.0775074495209588, "compression_ratio": 1.4208754208754208, "no_speech_prob": 0.012863430194556713}, {"id": 166, "seek": 79950, "start": 817.5, "end": 821.5, "text": " To jest dow\u00f3d na to, \u017ce on nie uczy si\u0119 na pami\u0119\u0107 konkretnego wzorca odpowiedzi,", "tokens": [51264, 1407, 3492, 9459, 17081, 1667, 281, 11, 3561, 322, 2838, 344, 6522, 3244, 1667, 31088, 2162, 36500, 11858, 24809, 284, 496, 36574, 3992, 11, 51464], "temperature": 0.0, "avg_logprob": -0.0775074495209588, "compression_ratio": 1.4208754208754208, "no_speech_prob": 0.012863430194556713}, {"id": 167, "seek": 79950, "start": 821.5, "end": 825.5, "text": " ale internalizuje og\u00f3ln\u0105, abstrakcyjn\u0105 procedur\u0119", "tokens": [51464, 6775, 6920, 590, 13008, 5360, 15741, 13113, 11, 10823, 11272, 42949, 13113, 6682, 374, 1274, 51664], "temperature": 0.0, "avg_logprob": -0.0775074495209588, "compression_ratio": 1.4208754208754208, "no_speech_prob": 0.012863430194556713}, {"id": 168, "seek": 82550, "start": 825.5, "end": 829.5, "text": " i potrafi j\u0105 ekstrapolowa\u0107 na nowe, bardziej z\u0142o\u017cone przypadki.", "tokens": [50364, 741, 1847, 10437, 72, 35692, 13359, 372, 4007, 401, 11445, 1667, 586, 68, 11, 27209, 710, 5249, 1427, 546, 33100, 2984, 13, 50564], "temperature": 0.0, "avg_logprob": -0.06687225614275251, "compression_ratio": 1.4422442244224423, "no_speech_prob": 0.11139674484729767}, {"id": 169, "seek": 82550, "start": 829.5, "end": 832.5, "text": " To jest prawdziwa si\u0142a tej metody.", "tokens": [50564, 1407, 3492, 41175, 3992, 4151, 1511, 5024, 12573, 1131, 843, 13, 50714], "temperature": 0.0, "avg_logprob": -0.06687225614275251, "compression_ratio": 1.4422442244224423, "no_speech_prob": 0.11139674484729767}, {"id": 170, "seek": 82550, "start": 832.5, "end": 835.5, "text": " Brzmi to niemal jak \u015bwi\u0119ty gra\u0142 AI,", "tokens": [50714, 1603, 89, 3057, 281, 2838, 5579, 4207, 8299, 22423, 874, 1295, 1221, 7318, 11, 50864], "temperature": 0.0, "avg_logprob": -0.06687225614275251, "compression_ratio": 1.4422442244224423, "no_speech_prob": 0.11139674484729767}, {"id": 171, "seek": 82550, "start": 835.5, "end": 838.5, "text": " ale musz\u0119 zapyta\u0107, jakie s\u0105 wady?", "tokens": [50864, 6775, 1038, 11052, 14223, 88, 42931, 11, 22124, 9015, 261, 880, 30, 51014], "temperature": 0.0, "avg_logprob": -0.06687225614275251, "compression_ratio": 1.4422442244224423, "no_speech_prob": 0.11139674484729767}, {"id": 172, "seek": 82550, "start": 838.5, "end": 839.5, "text": " Gdzie jest haczyk?", "tokens": [51014, 460, 13096, 3492, 324, 6522, 74, 30, 51064], "temperature": 0.0, "avg_logprob": -0.06687225614275251, "compression_ratio": 1.4422442244224423, "no_speech_prob": 0.11139674484729767}, {"id": 173, "seek": 82550, "start": 839.5, "end": 841.5, "text": " Bo zawsze jest jaki\u015b haczyk.", "tokens": [51064, 3286, 30964, 3492, 34721, 324, 6522, 74, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06687225614275251, "compression_ratio": 1.4422442244224423, "no_speech_prob": 0.11139674484729767}, {"id": 174, "seek": 82550, "start": 841.5, "end": 842.5, "text": " Oczywi\u015bcie.", "tokens": [51164, 42980, 13, 51214], "temperature": 0.0, "avg_logprob": -0.06687225614275251, "compression_ratio": 1.4422442244224423, "no_speech_prob": 0.11139674484729767}, {"id": 175, "seek": 82550, "start": 842.5, "end": 845.5, "text": " I autorzy badania s\u0105 w tej kwestii bardzo transparentni.", "tokens": [51214, 286, 19510, 1229, 1578, 5609, 9015, 261, 12573, 42035, 5597, 9034, 12737, 3722, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06687225614275251, "compression_ratio": 1.4422442244224423, "no_speech_prob": 0.11139674484729767}, {"id": 176, "seek": 82550, "start": 845.5, "end": 848.5, "text": " Wskazuj\u0105 na co najmniej trzy kluczowe ograniczenia.", "tokens": [51364, 343, 5161, 921, 13263, 1667, 598, 11212, 47658, 34573, 9671, 1311, 89, 6880, 34416, 30732, 14320, 13, 51514], "temperature": 0.0, "avg_logprob": -0.06687225614275251, "compression_ratio": 1.4422442244224423, "no_speech_prob": 0.11139674484729767}, {"id": 177, "seek": 82550, "start": 848.5, "end": 851.5, "text": " Pierwsze i najwa\u017cniejsze to skala.", "tokens": [51514, 16676, 14358, 1381, 741, 11212, 27111, 44258, 281, 1110, 5159, 13, 51664], "temperature": 0.0, "avg_logprob": -0.06687225614275251, "compression_ratio": 1.4422442244224423, "no_speech_prob": 0.11139674484729767}, {"id": 178, "seek": 82550, "start": 851.5, "end": 854.5, "text": " Jak ju\u017c m\u00f3wi\u0142y\u015bmy, to jest emergent ability.", "tokens": [51664, 15029, 10678, 24592, 6825, 10513, 11, 281, 3492, 4345, 6930, 3485, 13, 51814], "temperature": 0.0, "avg_logprob": -0.06687225614275251, "compression_ratio": 1.4422442244224423, "no_speech_prob": 0.11139674484729767}, {"id": 179, "seek": 85450, "start": 854.5, "end": 859.5, "text": " Chwila. Czyli wracamy do tego, \u017ce to dzia\u0142a tylko na gigantycznych, niezwykle kosztownych modelach.", "tokens": [50364, 761, 86, 7371, 13, 37099, 928, 326, 7804, 360, 8627, 11, 3561, 281, 37903, 13219, 1667, 8741, 394, 17466, 9399, 11, 33511, 9726, 14677, 19532, 2682, 648, 16384, 2316, 608, 13, 50614], "temperature": 0.0, "avg_logprob": -0.06997665801605621, "compression_ratio": 1.476340694006309, "no_speech_prob": 0.02558029256761074}, {"id": 180, "seek": 85450, "start": 859.5, "end": 863.5, "text": " Czy to nie oznacza, \u017ce to odkrycie jest w zasadzie bezu\u017cyteczne", "tokens": [50614, 19832, 281, 2838, 277, 22672, 326, 2394, 11, 3561, 281, 3611, 43298, 4260, 3492, 261, 44585, 3283, 10782, 84, 7735, 975, 38491, 50814], "temperature": 0.0, "avg_logprob": -0.06997665801605621, "compression_ratio": 1.476340694006309, "no_speech_prob": 0.02558029256761074}, {"id": 181, "seek": 85450, "start": 863.5, "end": 869.5, "text": " dla 99% developer\u00f3w i firm, kt\u00f3re nie maj\u0105 dost\u0119pu do infrastruktury Google'a?", "tokens": [50814, 12285, 11803, 4, 10754, 3901, 741, 6174, 11, 8864, 2838, 26064, 48209, 84, 360, 6534, 19977, 2598, 3329, 6, 64, 30, 51114], "temperature": 0.0, "avg_logprob": -0.06997665801605621, "compression_ratio": 1.476340694006309, "no_speech_prob": 0.02558029256761074}, {"id": 182, "seek": 85450, "start": 869.5, "end": 872.5, "text": " Czy to nie jest po prostu ciekawostka ze \u015bwiata gigant\u00f3w technologicznych?", "tokens": [51114, 19832, 281, 2838, 3492, 714, 19518, 46419, 1607, 555, 2330, 5277, 21485, 3274, 8741, 394, 3901, 1537, 1132, 17946, 9399, 30, 51264], "temperature": 0.0, "avg_logprob": -0.06997665801605621, "compression_ratio": 1.476340694006309, "no_speech_prob": 0.02558029256761074}, {"id": 183, "seek": 85450, "start": 872.5, "end": 874.5, "text": " To bardzo s\u0142uszna uwaga.", "tokens": [51264, 1407, 9034, 15116, 22378, 629, 23147, 9286, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06997665801605621, "compression_ratio": 1.476340694006309, "no_speech_prob": 0.02558029256761074}, {"id": 184, "seek": 85450, "start": 874.5, "end": 879.5, "text": " Na ten moment tak, jest to w du\u017cej mierze narz\u0119dzie dla najwi\u0119kszych graczy.", "tokens": [51364, 6056, 2064, 1623, 991, 11, 3492, 281, 261, 1581, 38493, 47448, 1381, 6714, 89, 42643, 12285, 48636, 1694, 28051, 11625, 1229, 13, 51614], "temperature": 0.0, "avg_logprob": -0.06997665801605621, "compression_ratio": 1.476340694006309, "no_speech_prob": 0.02558029256761074}, {"id": 185, "seek": 85450, "start": 879.5, "end": 882.5, "text": " Ale to odkrycie wyznacza kierunek.", "tokens": [51614, 9366, 281, 3611, 43298, 4260, 4628, 22672, 326, 2394, 38767, 409, 916, 13, 51764], "temperature": 0.0, "avg_logprob": -0.06997665801605621, "compression_ratio": 1.476340694006309, "no_speech_prob": 0.02558029256761074}, {"id": 186, "seek": 88250, "start": 882.5, "end": 886.5, "text": " Pokazuje, \u017ce celem nie musi by\u0107 tylko budowanie jeszcze wi\u0119kszych modeli,", "tokens": [50364, 14958, 43317, 11, 3561, 1769, 10386, 2838, 37587, 15069, 13219, 3265, 22028, 14168, 29968, 28051, 2316, 72, 11, 50564], "temperature": 0.0, "avg_logprob": -0.07189340460790347, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.05121517553925514}, {"id": 187, "seek": 88250, "start": 886.5, "end": 892.5, "text": " ale te\u017c szukanie sposob\u00f3w na wywo\u0142anie takich zdolno\u015bci w mniejszych, bardziej efektywnych architekturach.", "tokens": [50564, 6775, 9516, 7870, 2034, 7155, 20443, 996, 3901, 1667, 4628, 6120, 1221, 7155, 29607, 16221, 401, 16438, 261, 39513, 45021, 11, 27209, 31482, 916, 874, 895, 16384, 3912, 642, 2320, 374, 608, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07189340460790347, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.05121517553925514}, {"id": 188, "seek": 88250, "start": 892.5, "end": 895.5, "text": " To jest teraz otwarte pole do bada\u0144.", "tokens": [50864, 1407, 3492, 16854, 4337, 86, 11026, 13208, 360, 272, 1538, 5248, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07189340460790347, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.05121517553925514}, {"id": 189, "seek": 88250, "start": 895.5, "end": 898.5, "text": " Ok. Czyli to drogowska zna przysz\u0142o\u015b\u0107.", "tokens": [51014, 3477, 13, 37099, 281, 3789, 70, 1509, 2330, 710, 629, 44018, 44742, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07189340460790347, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.05121517553925514}, {"id": 190, "seek": 88250, "start": 898.5, "end": 899.5, "text": " A drugie ograniczenie?", "tokens": [51164, 316, 4110, 414, 34416, 30732, 16778, 30, 51214], "temperature": 0.0, "avg_logprob": -0.07189340460790347, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.05121517553925514}, {"id": 191, "seek": 88250, "start": 899.5, "end": 903.5, "text": " Tak, gwarancji poprawno\u015bci samego rozumowania.", "tokens": [51214, 9118, 11, 290, 6925, 4463, 4013, 1665, 424, 20944, 6199, 912, 1571, 48797, 21308, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07189340460790347, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.05121517553925514}, {"id": 192, "seek": 88250, "start": 903.5, "end": 907.5, "text": " Wygenerowany \u0142a\u0144cuch my\u015bli nie zawsze jest logiczny,", "tokens": [51414, 14458, 21848, 23341, 220, 5024, 5248, 66, 625, 452, 15350, 2838, 30964, 3492, 9952, 89, 1634, 11, 51614], "temperature": 0.0, "avg_logprob": -0.07189340460790347, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.05121517553925514}, {"id": 193, "seek": 88250, "start": 907.5, "end": 910.5, "text": " nawet je\u015bli ostateczna odpowied\u017a si\u0119 zgadza.", "tokens": [51614, 22696, 25630, 277, 15406, 3689, 629, 36574, 10659, 3244, 40948, 345, 2394, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07189340460790347, "compression_ratio": 1.4166666666666667, "no_speech_prob": 0.05121517553925514}, {"id": 194, "seek": 91050, "start": 910.5, "end": 917.5, "text": " Zw\u0142aszcza w zadaniach wielokrotnego wyboru model mo\u017ce doj\u015b\u0107 do prawid\u0142owego wyniku przez przypadek,", "tokens": [50364, 29385, 1221, 19601, 41524, 261, 42788, 3782, 608, 20570, 453, 10536, 11858, 4628, 3918, 84, 2316, 12034, 360, 44536, 360, 3206, 17697, 1221, 26576, 31936, 24320, 14064, 41780, 762, 74, 11, 50714], "temperature": 0.0, "avg_logprob": -0.042841018085748375, "compression_ratio": 1.4727891156462585, "no_speech_prob": 0.012909520417451859}, {"id": 195, "seek": 91050, "start": 917.5, "end": 922.5, "text": " a jego wyja\u015bnienie mo\u017ce by\u0107 pe\u0142ne b\u0142\u0119d\u00f3w i konfabulacji.", "tokens": [50714, 257, 26542, 4628, 2938, 1788, 77, 27385, 12034, 15069, 43205, 716, 272, 1221, 6298, 3901, 741, 5897, 69, 14057, 13152, 13, 50964], "temperature": 0.0, "avg_logprob": -0.042841018085748375, "compression_ratio": 1.4727891156462585, "no_speech_prob": 0.012909520417451859}, {"id": 196, "seek": 91050, "start": 922.5, "end": 926.5, "text": " Czyli model mo\u017ce uzyska\u0107 dobr\u0105 odpowied\u017a z zupe\u0142nie z\u0142ych powod\u00f3w,", "tokens": [50964, 37099, 2316, 12034, 16851, 749, 2330, 2162, 23067, 1611, 36574, 10659, 710, 49922, 710, 47655, 3388, 378, 3901, 11, 51164], "temperature": 0.0, "avg_logprob": -0.042841018085748375, "compression_ratio": 1.4727891156462585, "no_speech_prob": 0.012909520417451859}, {"id": 197, "seek": 91050, "start": 926.5, "end": 931.5, "text": " a potem z pe\u0142n\u0105 pewno\u015bci\u0105 siebie ok\u0142ama\u0107 mnie, jak do niej doszed\u0142.", "tokens": [51164, 257, 36513, 710, 43205, 13113, 33002, 50227, 39137, 3133, 1221, 2404, 2162, 17661, 11, 4207, 360, 2838, 73, 4491, 11312, 1221, 13, 51414], "temperature": 0.0, "avg_logprob": -0.042841018085748375, "compression_ratio": 1.4727891156462585, "no_speech_prob": 0.012909520417451859}, {"id": 198, "seek": 91050, "start": 931.5, "end": 933.5, "text": " To jest troch\u0119 niepokoj\u0105ce.", "tokens": [51414, 1407, 3492, 24926, 2838, 79, 13704, 8555, 384, 13, 51514], "temperature": 0.0, "avg_logprob": -0.042841018085748375, "compression_ratio": 1.4727891156462585, "no_speech_prob": 0.012909520417451859}, {"id": 199, "seek": 91050, "start": 933.5, "end": 938.5, "text": " Ta interpretowalno\u015b\u0107, kt\u00f3ra mia\u0142a by\u0107 zalet\u0105, okazuje si\u0119 mieczem obosiecznym.", "tokens": [51514, 6551, 7302, 305, 304, 23293, 11, 19456, 21290, 5024, 15069, 29599, 302, 1611, 11, 3133, 43317, 3244, 12597, 3689, 443, 1111, 329, 414, 3689, 12996, 13, 51764], "temperature": 0.0, "avg_logprob": -0.042841018085748375, "compression_ratio": 1.4727891156462585, "no_speech_prob": 0.012909520417451859}, {"id": 200, "seek": 93850, "start": 938.5, "end": 944.5, "text": " Dok\u0142adnie. To, \u017ce model pokazuje swoj\u0105 prac\u0119 nie znaczy, \u017ce ta praca jest zawsze dobrze wykonana.", "tokens": [50364, 29768, 10358, 2766, 13, 1407, 11, 3561, 2316, 13010, 43317, 49194, 22404, 1274, 2838, 36584, 11, 3561, 1846, 582, 6628, 3492, 30964, 28335, 46702, 2095, 13, 50664], "temperature": 0.0, "avg_logprob": -0.047604036958594075, "compression_ratio": 1.523961661341853, "no_speech_prob": 0.02284138649702072}, {"id": 201, "seek": 93850, "start": 944.5, "end": 947.5, "text": " Musimy podchodzi\u0107 do tych wyja\u015bni\u0107 z du\u017c\u0105 doz\u0105 krytyczyzmu.", "tokens": [50664, 3569, 13189, 2497, 34616, 2162, 360, 15180, 4628, 2938, 1788, 3722, 2162, 710, 21783, 1611, 360, 8925, 34847, 874, 6522, 89, 20140, 13, 50814], "temperature": 0.0, "avg_logprob": -0.047604036958594075, "compression_ratio": 1.523961661341853, "no_speech_prob": 0.02284138649702072}, {"id": 202, "seek": 93850, "start": 947.5, "end": 949.5, "text": " I wreszcie trzecia kwestia.", "tokens": [50814, 286, 261, 495, 89, 4260, 22266, 2755, 42035, 654, 13, 50914], "temperature": 0.0, "avg_logprob": -0.047604036958594075, "compression_ratio": 1.523961661341853, "no_speech_prob": 0.02284138649702072}, {"id": 203, "seek": 93850, "start": 949.5, "end": 950.5, "text": " Wp\u0142yw przyk\u0142ad\u00f3w.", "tokens": [50914, 343, 79, 6825, 86, 23144, 3901, 13, 50964], "temperature": 0.0, "avg_logprob": -0.047604036958594075, "compression_ratio": 1.523961661341853, "no_speech_prob": 0.02284138649702072}, {"id": 204, "seek": 93850, "start": 950.5, "end": 955.5, "text": " Mimo, \u017ce metoda jest do\u015b\u0107 odporna na styl pisania tych przyk\u0142ad\u00f3w przez r\u00f3\u017cne osoby,", "tokens": [50964, 376, 6934, 11, 3561, 1131, 13449, 3492, 49333, 3611, 2816, 629, 1667, 23736, 26584, 5609, 15180, 23144, 3901, 14064, 47760, 39737, 11, 51214], "temperature": 0.0, "avg_logprob": -0.047604036958594075, "compression_ratio": 1.523961661341853, "no_speech_prob": 0.02284138649702072}, {"id": 205, "seek": 93850, "start": 955.5, "end": 959.5, "text": " to jako\u015b\u0107 i dob\u00f3r tych kilku wzorc\u00f3w wci\u0105\u017c maj\u0105 ogromne znaczenie.", "tokens": [51214, 281, 17123, 7753, 741, 27082, 15614, 15180, 5128, 5279, 24809, 284, 29268, 261, 537, 27242, 26064, 34416, 298, 716, 15397, 326, 16778, 13, 51414], "temperature": 0.0, "avg_logprob": -0.047604036958594075, "compression_ratio": 1.523961661341853, "no_speech_prob": 0.02284138649702072}, {"id": 206, "seek": 93850, "start": 959.5, "end": 965.5, "text": " Jeden z\u0142y lub myl\u0105cy przyk\u0142ad mo\u017ce skierowa\u0107 ca\u0142y proces rozumowania modelu na manowce.", "tokens": [51414, 508, 6876, 710, 6825, 15980, 452, 75, 1611, 1344, 23144, 12034, 1110, 811, 11445, 35226, 17565, 48797, 21308, 2316, 84, 1667, 587, 305, 384, 13, 51714], "temperature": 0.0, "avg_logprob": -0.047604036958594075, "compression_ratio": 1.523961661341853, "no_speech_prob": 0.02284138649702072}, {"id": 207, "seek": 96550, "start": 965.5, "end": 969.5, "text": " Wci\u0105\u017c istnieje tu du\u017cy element tego, co nazywamy prompt engineering.", "tokens": [50364, 343, 537, 27242, 1418, 2766, 2884, 2604, 1581, 7735, 4478, 8627, 11, 598, 20151, 27112, 7804, 12391, 7043, 13, 50564], "temperature": 0.0, "avg_logprob": -0.035221408596999355, "compression_ratio": 1.456140350877193, "no_speech_prob": 0.014698735438287258}, {"id": 208, "seek": 96550, "start": 969.5, "end": 972.5, "text": " Dobrze. Zbierzmy to wszystko do kupy.", "tokens": [50564, 29679, 13503, 13, 1176, 65, 34602, 2226, 281, 22607, 360, 37534, 88, 13, 50714], "temperature": 0.0, "avg_logprob": -0.035221408596999355, "compression_ratio": 1.456140350877193, "no_speech_prob": 0.014698735438287258}, {"id": 209, "seek": 96550, "start": 972.5, "end": 974.5, "text": " Co z tego wynika w praktyce?", "tokens": [50714, 3066, 710, 8627, 31936, 5439, 261, 3206, 74, 874, 384, 30, 50814], "temperature": 0.0, "avg_logprob": -0.035221408596999355, "compression_ratio": 1.456140350877193, "no_speech_prob": 0.014698735438287258}, {"id": 210, "seek": 96550, "start": 974.5, "end": 978.5, "text": " Jaka jest ta jedna najwa\u017cniejsza lekcja, kt\u00f3r\u0105 powinni\u015bmy z tego wyci\u0105gn\u0105\u0107?", "tokens": [50814, 508, 7849, 3492, 1846, 5232, 629, 11212, 27111, 30295, 2394, 30863, 34056, 11, 37415, 27310, 3722, 10513, 710, 8627, 4628, 34381, 4568, 36374, 30, 51014], "temperature": 0.0, "avg_logprob": -0.035221408596999355, "compression_ratio": 1.456140350877193, "no_speech_prob": 0.014698735438287258}, {"id": 211, "seek": 96550, "start": 978.5, "end": 983.5, "text": " Najwa\u017cniejsza lekcja jest taka, \u017ce chain of thought prompting to prosta w za\u0142o\u017ceniach,", "tokens": [51014, 31576, 27111, 30295, 2394, 30863, 34056, 3492, 28017, 11, 3561, 5021, 295, 1194, 12391, 278, 281, 582, 8638, 261, 7949, 5249, 1427, 15711, 608, 11, 51264], "temperature": 0.0, "avg_logprob": -0.035221408596999355, "compression_ratio": 1.456140350877193, "no_speech_prob": 0.014698735438287258}, {"id": 212, "seek": 96550, "start": 983.5, "end": 989.5, "text": " ale rewolucyjna metoda na odblokowanie ukrytych zdolno\u015bci rozumowania w du\u017cych modelach j\u0119zykowych.", "tokens": [51264, 6775, 319, 48481, 1311, 88, 73, 629, 1131, 13449, 1667, 3611, 5199, 453, 22028, 26769, 627, 874, 339, 16221, 401, 16438, 48797, 21308, 261, 1581, 7735, 339, 2316, 608, 49055, 74, 19605, 13, 51564], "temperature": 0.0, "avg_logprob": -0.035221408596999355, "compression_ratio": 1.456140350877193, "no_speech_prob": 0.014698735438287258}, {"id": 213, "seek": 98950, "start": 989.5, "end": 995.5, "text": " Kluczo\u0142e jest to, \u017ce jest to emergent ability, co\u015b, co pojawia si\u0119 dopiero przy odpowiedniej skali,", "tokens": [50364, 16053, 1311, 4765, 19827, 3492, 281, 11, 3561, 3492, 281, 4345, 6930, 3485, 11, 19241, 11, 598, 30655, 654, 3244, 21900, 12030, 6501, 36574, 10402, 1110, 5103, 11, 50664], "temperature": 0.0, "avg_logprob": -0.05037667201115535, "compression_ratio": 1.4756554307116105, "no_speech_prob": 0.10629269480705261}, {"id": 214, "seek": 98950, "start": 995.5, "end": 1002.5, "text": " pozwalaj\u0105c modelom na dekompozycj\u0119 problem\u00f3w i m\u00f3wi\u0105c kolokwialnie pokazanie swojej pracy.", "tokens": [50664, 40557, 304, 38757, 2316, 298, 1667, 368, 20557, 2259, 1229, 41960, 1154, 3901, 741, 46591, 66, 17818, 453, 86, 831, 2766, 13010, 921, 7155, 29489, 73, 35591, 13, 51014], "temperature": 0.0, "avg_logprob": -0.05037667201115535, "compression_ratio": 1.4756554307116105, "no_speech_prob": 0.10629269480705261}, {"id": 215, "seek": 98950, "start": 1002.5, "end": 1007.5, "text": " A patrz\u0105c szerzej, co to zmienia w naszym my\u015bleniu o sztucznej inteligencji?", "tokens": [51014, 316, 1947, 81, 8925, 66, 36160, 16920, 11, 598, 281, 17020, 18811, 261, 48094, 48633, 6698, 5951, 277, 262, 2682, 1311, 89, 11794, 24777, 3213, 19649, 30, 51264], "temperature": 0.0, "avg_logprob": -0.05037667201115535, "compression_ratio": 1.4756554307116105, "no_speech_prob": 0.10629269480705261}, {"id": 216, "seek": 98950, "start": 1007.5, "end": 1010.5, "text": " To prowadzi do rewolucyjnego wniosku.", "tokens": [51264, 1407, 36590, 3992, 360, 319, 48481, 1311, 88, 73, 11858, 45368, 2717, 5279, 13, 51414], "temperature": 0.0, "avg_logprob": -0.05037667201115535, "compression_ratio": 1.4756554307116105, "no_speech_prob": 0.10629269480705261}, {"id": 217, "seek": 98950, "start": 1010.5, "end": 1015.5, "text": " By\u0107 mo\u017ce problemem nie jest to, \u017ce nasze modele s\u0105 za ma\u0142o inteligentne,", "tokens": [51414, 3146, 2162, 12034, 1154, 443, 2838, 3492, 281, 11, 3561, 43394, 4391, 306, 9015, 7949, 463, 5249, 24777, 25002, 716, 11, 51664], "temperature": 0.0, "avg_logprob": -0.05037667201115535, "compression_ratio": 1.4756554307116105, "no_speech_prob": 0.10629269480705261}, {"id": 218, "seek": 101550, "start": 1015.5, "end": 1019.5, "text": " ale to, \u017ce my jeste\u015bmy za ma\u0142o sprytni w rozmowie z nimi.", "tokens": [50364, 6775, 281, 11, 3561, 452, 35928, 7949, 463, 5249, 637, 627, 83, 3722, 261, 35234, 13998, 710, 297, 10121, 13, 50564], "temperature": 0.0, "avg_logprob": -0.044901367667671686, "compression_ratio": 1.4566666666666668, "no_speech_prob": 0.30736348032951355}, {"id": 219, "seek": 101550, "start": 1019.5, "end": 1024.5, "text": " Okazuje si\u0119, \u017ce w tych cyfrowych umys\u0142ach drzemi\u0105 ukryte zdolno\u015bci,", "tokens": [50564, 3477, 43317, 3244, 11, 3561, 261, 15180, 3185, 69, 1892, 16384, 1105, 39508, 608, 1224, 24313, 11404, 26769, 627, 975, 16221, 401, 16438, 11, 50814], "temperature": 0.0, "avg_logprob": -0.044901367667671686, "compression_ratio": 1.4566666666666668, "no_speech_prob": 0.30736348032951355}, {"id": 220, "seek": 101550, "start": 1024.5, "end": 1028.5, "text": " a naszym zadaniem jest sta\u0107 si\u0119 swego rodzaju zaklinaczami AI,", "tokens": [50814, 257, 48094, 710, 11338, 4907, 3492, 11135, 2162, 3244, 2484, 1571, 28607, 33166, 23810, 5045, 14875, 4526, 7318, 11, 51014], "temperature": 0.0, "avg_logprob": -0.044901367667671686, "compression_ratio": 1.4566666666666668, "no_speech_prob": 0.30736348032951355}, {"id": 221, "seek": 101550, "start": 1028.5, "end": 1032.5, "text": " kt\u00f3rzy potrafi\u0105 je wydoby\u0107 na powierzchni\u0119 za pomoc\u0105 odpowiednich s\u0142\u00f3w.", "tokens": [51014, 25382, 1847, 10437, 11404, 1506, 25984, 13944, 2162, 1667, 3388, 34602, 1377, 5034, 7949, 48962, 1611, 36574, 77, 480, 15116, 3901, 13, 51214], "temperature": 0.0, "avg_logprob": -0.044901367667671686, "compression_ratio": 1.4566666666666668, "no_speech_prob": 0.30736348032951355}, {"id": 222, "seek": 101550, "start": 1032.5, "end": 1036.5, "text": " To zmienia perspektyw\u0119 z samego budowania coraz wi\u0119kszych modeli", "tokens": [51214, 1407, 17020, 18811, 868, 32659, 874, 86, 1274, 710, 912, 1571, 3265, 21308, 25899, 29968, 28051, 2316, 72, 51414], "temperature": 0.0, "avg_logprob": -0.044901367667671686, "compression_ratio": 1.4566666666666668, "no_speech_prob": 0.30736348032951355}, {"id": 223, "seek": 101550, "start": 1036.5, "end": 1042.5, "text": " na szukanie lepszych, bardziej inteligentnych sposob\u00f3w interakcji z tymi, kt\u00f3re ju\u017c mamy.", "tokens": [51414, 1667, 7870, 2034, 7155, 476, 1878, 28051, 11, 27209, 24777, 25002, 9399, 20443, 996, 3901, 728, 514, 19649, 710, 1104, 3057, 11, 8864, 10678, 17335, 13, 51714], "temperature": 0.0, "avg_logprob": -0.044901367667671686, "compression_ratio": 1.4566666666666668, "no_speech_prob": 0.30736348032951355}, {"id": 224, "seek": 104250, "start": 1042.5, "end": 1047.5, "text": " I to zostawia nas z jedn\u0105, niezwykle prowokuj\u0105c\u0105 my\u015bl\u0105 na koniec.", "tokens": [50364, 286, 281, 31873, 34953, 5382, 710, 5232, 13113, 11, 33511, 9726, 14677, 45553, 453, 13263, 32557, 452, 19212, 1611, 1667, 5897, 35733, 13, 50614], "temperature": 0.0, "avg_logprob": -0.05908480196288138, "compression_ratio": 1.453531598513011, "no_speech_prob": 0.004725624807178974}, {"id": 225, "seek": 104250, "start": 1047.5, "end": 1051.5, "text": " Skoro tak prosta zmiana w sposobie zadawania pytania", "tokens": [50614, 7324, 10780, 991, 582, 8638, 17020, 8497, 261, 20443, 996, 414, 710, 1538, 86, 5609, 25878, 5609, 50814], "temperature": 0.0, "avg_logprob": -0.05908480196288138, "compression_ratio": 1.453531598513011, "no_speech_prob": 0.004725624807178974}, {"id": 226, "seek": 104250, "start": 1051.5, "end": 1055.5, "text": " mo\u017ce uwolni\u0107 tak zaawansowane zdolno\u015bci logicznego my\u015blenia,", "tokens": [50814, 12034, 23147, 401, 3722, 2162, 991, 7949, 1607, 599, 23066, 16221, 401, 16438, 9952, 89, 11858, 48633, 6698, 654, 11, 51014], "temperature": 0.0, "avg_logprob": -0.05908480196288138, "compression_ratio": 1.453531598513011, "no_speech_prob": 0.004725624807178974}, {"id": 227, "seek": 104250, "start": 1055.5, "end": 1061.5, "text": " to jakie inne, by\u0107 mo\u017ce znacznie pot\u0119\u017cniejsze ukryte talenty drzemi\u0105 w tych ogromnych sieciach neuronowych,", "tokens": [51014, 281, 22124, 24170, 11, 15069, 12034, 15397, 14875, 2766, 1847, 1274, 1427, 44258, 26769, 627, 975, 4023, 4179, 1224, 24313, 11404, 261, 15180, 34416, 298, 9399, 2804, 537, 608, 34090, 19605, 11, 51314], "temperature": 0.0, "avg_logprob": -0.05908480196288138, "compression_ratio": 1.453531598513011, "no_speech_prob": 0.004725624807178974}, {"id": 228, "seek": 104250, "start": 1061.5, "end": 1066.5, "text": " czekaj\u0105c jedynie na w\u0142a\u015bciwy klucz, na odpowiedni pr\u0105d, kt\u00f3ry otworzy kolejne drzwi.", "tokens": [51314, 6472, 916, 38757, 5232, 2534, 414, 1667, 40112, 9726, 9671, 1311, 89, 11, 1667, 36574, 3722, 582, 18962, 11, 9913, 4337, 28321, 1229, 23749, 716, 1224, 89, 6253, 13, 51564], "temperature": 0.0, "avg_logprob": -0.05908480196288138, "compression_ratio": 1.453531598513011, "no_speech_prob": 0.004725624807178974}], "language": "pl"}