{"text": " Jak to jest, \u017ce my ludzie rozumiemy si\u0119 bez trudu? Nawet gdy \u017cartujemy, u\u017cywamy sarkazmu albo, no wiesz, m\u00f3wimy w niedopowiedzeniach. A dla komputer\u00f3w? A dla komputer\u00f3w przez dekady to by\u0142a po prostu czarna magia. Dok\u0142adnie. Maszyna mog\u0142a zna\u0107 definicj\u0119 ka\u017cdego s\u0142owa, ale i tak nie rozumia\u0142a, \u017ce zdanie ale urwa\u0142. No po obejrzeniu jakiego\u015b ryzykownego manewru. Znaczy co\u015b zupe\u0142nie innego ni\u017c w instrukcji obs\u0142ugi pi\u0142y. To jest w\u0142a\u015bnie sedno problemu, kontekst. Zdecydowanie. Maszyna musi zrozumie\u0107, \u017ce z\u0142ama\u0107 komu\u015b serce, to nie jest problem dla kardiohirurga. A mie\u0107 muchy w nosie nie wymaga interwencji entomologa. Tak, ta intuicyjna ludzka zdolno\u015b\u0107 by\u0142a dla sztucznej inteligencji no takim \u015bwi\u0119tym gralem, a\u017c do 2018 roku. W\u0142a\u015bnie. I dzisiaj przyjrzymy si\u0119 pracy naukowej, kt\u00f3ra mo\u017cna powiedzie\u0107, tego Graala odnalaz\u0142a. Chodzi o artyku\u0142 z Google AI Language zatytu\u0142owany BERT. Pre-training of deep bidirectional transformers for language understanding. I to nie b\u0119dzie tylko taka sucha akademicka analiza. Nie. Spr\u00f3bujemy zrozumie\u0107, dlaczego ten jeden artyku\u0142 wywo\u0142a\u0142 prawdziwe trz\u0119sienie ziemi w \u015bwiecie AI. I sprawi\u0142, \u017ce dzi\u015b rozmawiamy z chatbotami, kt\u00f3re... No, kt\u00f3re wydaj\u0105 si\u0119 nas naprawd\u0119 rozumie\u0107. Dobrze, roz\u0142\u00f3\u017cmy to naczynniki pierwsze. Zacznijmy od tego, co by\u0142o wcze\u015bniej. Tak. Zanim pojawi\u0142 si\u0119 BERT, mieli\u015bmy ju\u017c przecie\u017c ca\u0142kiem niez\u0142e modele. Jak cho\u0107by test serii GPT od OpenAI czy ELMO. Co by\u0142o z nimi nie tak? Dlaczego potrzebowali\u015bmy rewolucji, a nie tylko ewolucji? G\u0142\u00f3wny problem, kt\u00f3ry zreszt\u0105 autorzy Berta \u015bwietnie zdiagnozowali. Mo\u017cna zamkn\u0105\u0107 w jednym s\u0142owie. I to jest? Jednokierunkowo\u015b\u0107. Te wcze\u015bniejsze modele, nawet te najbardziej zaawansowane, czyta\u0142y tekst tak, jak my czytamy ksi\u0105\u017ck\u0119 od lewej do prawej. S\u0142owo po s\u0142owie. Czyli... Czekaj, analizuj\u0105c jakie\u015b s\u0142owo, powiedzmy, w po\u0142owie zdania, model widzia\u0142 wszystko, co by\u0142o przed nim, ale by\u0142 kompletnie \u015blepy na to, co mia\u0142o dopiero nadej\u015b\u0107. W\u0142a\u015bnie tak. Wyobra\u017a sobie, \u017ce pr\u00f3bujesz rozwi\u0105za\u0107 zagadk\u0119 kryminaln\u0105, ale wolno ci czyta\u0107 akta sprawy tylko do strony, na kt\u00f3rej jeste\u015b. Bez mo\u017cliwo\u015bci zerkania na p\u00f3\u017aniejsze zeznania. Dok\u0142adnie. To gigantyczne ograniczenie. J\u0119zyk tak nie dzia\u0142a. Aby zrozumie\u0107 znaczenie s\u0142owa Pali w zdaniu Jan Pali Papierosa, musisz zobaczy\u0107 s\u0142owo Papieros, kt\u00f3re jest po nim. A wcze\u015bniejsze modele musia\u0142yby zgadywa\u0107. Zgadz\u0119 si\u0119. Owszem, by\u0142y pr\u00f3by obej\u015bcia tego. Na przyk\u0142ad w modelu Elmo. On pr\u00f3bowa\u0142 skleja\u0107 ze sob\u0105 dwie analizy jedn\u0105 od lewej do prawej i drug\u0105 od prawej do lewej. Ale to by\u0142o takie... powierzchowne. Troch\u0119 jakby dwie osoby czyta\u0142y ksi\u0105\u017ck\u0119, jedna od pocz\u0105tku, druga od ko\u0144ca i co jaki\u015b czas wymienia\u0142y si\u0119 notatkami. \u015awietna analogia. To nie to samo, co jedna osoba, kt\u00f3ra zna ca\u0142\u0105 tre\u015b\u0107 od razu. Idealnie. To by\u0142o tylko shallow concatenation, takie p\u0142ytkie po\u0142\u0105czenie. Brakowa\u0142o g\u0142\u0119bokiego i jednoczesnego rozumienia pe\u0142nego kontekstu na wszystkich warstwach sieci. I tu w\u0142a\u015bnie Bert wchodzi na scen\u0119. No dobrze, skoro problemem by\u0142a ta \u015blepo\u015b\u0107 na przysz\u0142o\u015b\u0107, to rozwi\u0105zanie wydaje si\u0119 proste. Poka\u017cmy modelowi od razu ca\u0142e zdanie, ale tu pojawia si\u0119 paradoks. Tak, standardowy trening polega\u0142 na przewidywaniu nast\u0119pnego s\u0142owa. Je\u015bli model widzi ca\u0142e zdanie, to przecie\u017c mo\u017ce bezczelnie \u015bci\u0105gn\u0105\u0107 i po prostu skopiowa\u0107 s\u0142owo, kt\u00f3re ma przewidzie\u0107. Jak oni to obe szli? I to jest ten moment, w kt\u00f3rym pojawia si\u0119 genialna w swojej prostocie innowacja Bertha. Stworzyli zupe\u0142nie now\u0105 gr\u0119, now\u0105 metod\u0119 treningu, kt\u00f3r\u0105 nazwali Masked Language Model. W skr\u00f3cie MLM. Czyli model j\u0119zykowy z maskowaniem. Tak, zamiast kaza\u0107 modelowi i przewidywa\u0107 nast\u0119pne s\u0142owo, postanowili da\u0107 mu zdanie i zrobi\u0107 w nim dziury. Jak w tych \u0107wiczeniach z podstawuski? Wczoraj poszed\u0142em i kupi\u0142em mleko. Dok\u0142adnie o to chodzi. To zadanie, znane w psycholinguistyce jako Close Task, sta\u0142o si\u0119 fundamentem Bertha. Podczas treningu model dostaje zdanie, w kt\u00f3rym losowo ukryto 15% s\u0142\u00f3w, zast\u0119puj\u0105c je specjalnym znacznikiem mask. I zadaniem modelu nie jest ju\u017c przewidzie\u0107, co b\u0119dzie dalej. Jego zadaniem jest spojrze\u0107 na ca\u0142e zdanie, na s\u0142owa przed mask\u0105 i po masce. I odgadn\u0105\u0107, co kryje si\u0119 w tej luce. Czyli zmuszamy go, \u017ceby sta\u0142 si\u0119 takim j\u0119zykowym detektywem. Musi analizowa\u0107 dowody z obu stron miejsca zbrowni, \u017ceby zidentyfikowa\u0107 ofiar\u0119. I co jest w tym fascynuj\u0105ce? To nie sama zgadywanka. To u\u015bwiadomienie sobie, \u017ce aby nauczy\u0107 maszyn\u0119 rozumie\u0107 j\u0119zyk, nie trzeba jej uczy\u0107 gramatyki i regu\u0142. Trzeba j\u0105 zmusi\u0107 do budowania intuicji. Tak. Ta prosta gra w zgadywania zmusza model do stworzenia wewn\u0119trznej, niezwykle z\u0142o\u017conej mapy powi\u0105za\u0144 mi\u0119dzy s\u0142owami. To jest krok od bycia s\u0142ownikiem do bycia partnerem w rozmowie. To jest w\u0142a\u015bnie ta g\u0142\u0119boka dwukierunkowo\u015b\u0107, to Deep Be Directionality z tytu\u0142u artyku\u0142u. OK. Ten pomys\u0142 z maskowaniem jest genialny. Ale czyta\u0142am w artykule, \u017ce autorzy puszli okrok dalej. Obawiali si\u0119, \u017ce je\u015bli model b\u0119dzie trenowany tylko na zdaniach z tym specjalnym tokenem mask, to potem w praktyce mo\u017ce sobie nie radzi\u0107. Dok\u0142adnie. W praktycznym u\u017cyciu, gdzie ten token nie wyst\u0119puje, pojawi si\u0119 niezgodno\u015b\u0107 mi\u0119dzy treningiem a faz\u0105 fine tuning. S\u0142oszna uwaga. I to pokazuje, jak bardzo przemy\u015blana by\u0142a ta architektura. Rozwi\u0105zali to w bardzo sprytny spos\u00f3b. Z tych 15% s\u0142\u00f3w wybranych do zamaskowania nie wszystkie s\u0105 traktowane tak samo. Jak to? W 80% przypadk\u00f3w s\u0142owo jest faktycznie zast\u0119powane przez mask. Standardowa procedura. A pozosta\u0142e 20%? I tu jest ciekawie. W 10% przypadk\u00f3w wybrane s\u0142owo jest zast\u0119powane innym, kompletnie losowym s\u0142owem. Aha. A w ostatnich 10% nie zmienia si\u0119 nic. S\u0142owo zostaje na swoim miejscu. Chwila. To znaczy, \u017ce czasem zadaniem modelu jest przewidzie\u0107 s\u0142owo, kt\u00f3re ju\u017c widzi, ale kt\u00f3re mo\u017ce by\u0107 b\u0142\u0119dem, to jak podrzucenie fa\u0142szywych trop\u00f3w w \u015bledztwie. Dok\u0142adnie. Dzi\u0119ki temu model uczy si\u0119 czego\u015b znacznie wa\u017cniejszego ni\u017c tylko wype\u0142nianie luk. Czego? Uczy si\u0119 nie ufa\u0107 \u015blepo danym wej\u015bciowym. Musi oceni\u0107 ka\u017cde s\u0142owo w kontek\u015bcie ca\u0142ego zdania i zdecydowa\u0107, czy ono tam pasuje. To zmusza go do budowania znacznie bogatszej, bardziej odpornej na b\u0142\u0119dy reprezentacji j\u0119zyka. To jest naprawd\u0119 sprytne. Uczy\u0107 modelu niuans\u00f3w zmuszaj\u0105c go do pracy detektywistycznej wewn\u0105trz zdania. Ale j\u0119zyk to co\u015b wi\u0119cej ni\u017c zdania w izolacji. To ca\u0142e akapity, historie, dialogi. No w\u0142a\u015bnie. Czy sam ten mechanizm MLM wystarczy\u0142, \u017ceby Bert rozumia\u0142 na przyk\u0142ad ironi\u0119 w rozmowie, albo logiczny ci\u0105g argument\u00f3w? \u015awietne pytanie. I odpowied\u017a brzmi nie. Nie wystarczy\u0142. MLM da\u0142 Bertowi niesamowit\u0105 zdolno\u015b\u0107 rozumienia relacji wewn\u0105trz zda\u0144. Ale \u017ceby rozumie\u0107 d\u0142u\u017csze formy potrzebny by\u0142 drugi filar. Drugi filar? Autorzy zdawali sobie spraw\u0119, \u017ce wiele zada\u0144, jak question answering czy analiza logiczna, wymaga rozumienia zwi\u0105zk\u00f3w pomi\u0119dzy zdaniami. Dlatego wprowadzili drugie zadanie treningowe. Next sentence prediction w skr\u00f3cie NSP. Czyli przewidywanie nast\u0119pnego zdania. Jak to dzia\u0142a\u0142o w praktyce? Dawali mu po prostu dwa zdania. Dok\u0142adnie. Model dostaje par\u0119 zda\u0144, nazwijmy je A i B. I teraz najlepsze. W po\u0142owie przypadk\u00f3w zdanie B to faktycznie to, kt\u00f3re nast\u0119powa\u0142o po A w oryginalnym tek\u015bcie. A w drugiej po\u0142owie to jest jakie\u015b zupe\u0142nie losowe zdanie, wyrwane z kontekstu z innego miejsca w internecie. W\u0142a\u015bnie tak. I model musi odpowiedzie\u0107 na proste, binarne pytanie. Czy ta para zda\u0144 ma sens? Czy B jest logiczn\u0105 kontynuacj\u0105 A? Czyli uczy si\u0119 tego niewidzialnego kleju, kt\u00f3ry \u0142\u0105czym my\u015bli i wsp\u00f3jno ca\u0142o\u015b\u0107. Dok\u0142adnie. Uczy si\u0119 \u015bledzi\u0107 narracj\u0119, rozumie\u0107 przyczyn\u0119 i skutek, rozpoznawa\u0107, kiedy rozmowa z Bacza Storu. Mo\u017cna powiedzie\u0107, \u017ce MMM to by\u0142a nauka anatomii s\u0142\u00f3w, a NSP to nauka logiki konwersacji. Co to wszystko znacza w praktyce? Mamy wi\u0119c dwa genialne pomys\u0142y treningowe, ale papier to jedno, a rzeczywisto\u015b\u0107 drugie. Czy to faktycznie zadzia\u0142a\u0142o tak dobrze? Dobrze, to ma\u0142o powiedziane. Kiedy opublikowali wyniki w \u015brodowisku NLP, no opad\u0142y szcz\u0119ki. Bert po prostu wszed\u0142 i pozamiata\u0142. A\u017c tak? Ustanowi\u0142 nowe rekordy, czyli State of the Art, w 11 r\u00f3\u017cnych zadaniach. I nie m\u00f3wimy tu o kosmetycznych poprawkach. Na przyk\u0142ad w GLU, to taki olimpijski wielob\u00f3j dla modeli j\u0119zykowych. Bert nie pobi\u0142 rekordu o w\u0142os, on go zmia\u017cczy\u0142. O ile? Poprawi\u0142 wynik o prawie 8 punkt\u00f3w procentowych. W tym \u015bwiecie to jest jak przebiec setk\u0119 o sekund\u0119 szybciej od poprzedniego rekordzisty. Wszyscy wiedzieli, \u017ce to pocz\u0105tek nowej ery. A co z zadaniami, kt\u00f3re by\u0142y pi\u0119t\u0105 achillesow\u0105 z starych modeli? Na przyk\u0142ad to odpowiadanie na pytania, gdzie pe\u0142ny dwukierunkowy kontekst jest absolutnie kluczowy. Tam efekty by\u0142y jeszcze bardziej dramatyczne. W popularnym te\u015bcie sk\u0142od, gdzie model musi przeczyta\u0107 akapic wikipedii i znale\u017a\u0107 w nim odpowied\u017a, Bert by\u0142 pierwszym systemem, kt\u00f3ry pobi\u0142 ludzk\u0105 wydajno\u015b\u0107. Niesamowite. A w trudniejszej wersji sk\u0142od 2.0, gdzie niekt\u00f3re pytania celowo nie maj\u0105 odpowiedzi w tek\u015bcie, Bert zdeklasowa\u0142 konkurencj\u0119, poprawiaj\u0105c wynik a\u017c o pi\u0119\u0107 punkt\u00f3w F1. To by\u0142 knockout. To faktycznie brzmi jak prze\u0142om. Ale jest jeszcze jedna kwestia. Trening takich potwor\u00f3w musi by\u0107 niewyobra\u017calnie drogi i czasoch\u0142onny. Czy to oznacza\u0142o, \u017ce tylko giganci jak Google mogli z tego korzysta\u0107? I tu dochodzimy do drugiego genialnego aspektu tej pracy, kt\u00f3ry zdemokratyzowa\u0142 ca\u0142\u0105 dziedzin\u0119. Proces u\u017cywania Berta sk\u0142ada si\u0119 z dw\u00f3ch etap\u00f3w. Pierwszy to pre-training. Otwornie drogi i d\u0142ugi proces, w kt\u00f3rym Google na swoich superkomputerach trenowa\u0142o model na gigantycznym zbiorze danych ca\u0142ej angielskiej wikipedii i tysi\u0105cach ksi\u0105\u017cek. Tak, budowali ten uniwersalny, wszechwiedz\u0105cy m\u00f3zg j\u0119zykowy. A drugi etap. A potem nast\u0119puje drugi etap, czyli Fine Tuning. I to jest czysta magia. Wyobra\u017a sobie, \u017ce Google sp\u0119dzi\u0142o lata i miliony dolar\u00f3w, ucz\u0105c kogo\u015b biegle wszystkich j\u0119zyk\u00f3w \u015bwiata. To jest gotowy model Bert. A ty teraz podchodzisz do tego poligloty, dajesz mu kilkaset przyk\u0142ad\u00f3w recenzji filmowych i m\u00f3wisz. Od teraz jeste\u015b krytykiem filmowym. I on nie uczy si\u0119 j\u0119zyka od nowa. Nie, on tylko uczy si\u0119 stosowa\u0107 swoj\u0105 oglomn\u0105 wiedz\u0119 w tej jednej w\u0105skiej dziedzinie i robi to w kilka godzin na jednym zwyk\u0142ym GPU. OK, wi\u0119c mamy dwa sprytne zadania treningowe, kt\u00f3re da\u0142y \u015bwietne wyniki. Ale dla mnie to, co jest w tym artykulem naprawd\u0119 fascynuj\u0105ce, to nie tyle co zrobili, ale czego dowiedzieli si\u0119 przy okazji, rozk\u0142adaj\u0105c cen model na cz\u0119\u015bci. Bo tu zaczyna si\u0119 robi\u0107 naprawd\u0119 ciekawie. Te badania ablacyjne, tak? Co si\u0119 okaza\u0142o, kiedy zacz\u0119li wy\u0142\u0105cza\u0107 poszczeg\u00f3lne elementy tej maszyny? Te badania to, mo\u017cna powiedzie\u0107, gwo\u015b\u0107 dotrumny dla starych metod. Po pierwsze sprawdzili, co si\u0119 stanie, je\u015bli usun\u0105\u0142 zadanie Next Sentence Prediction. I b\u0119d\u0105 trenowa\u0107 model tylko przez maskowanie s\u0142\u00f3w? Tak, wyniki. W zadaniach wymagaj\u0105cych rozumienia zwi\u0105zk\u00f3w logicznych mi\u0119dzy zdaniami, jak QNLI czy MNLI, model radzi\u0142 sobie znacznie gorzej. To by\u0142 dow\u00f3d, \u017ce NSP nie jest tylko mi\u0142ym dodatkiem, ale kluczowym sk\u0142adnikiem. A co z najwa\u017cniejszym, czyli por\u00f3wnaniem do modeli jednokierunkowych? To by\u0142 ostateczny dow\u00f3d. Zbudowali model, kt\u00f3ry by\u0142 w zasadzie kopi\u0105 Bertha, ale trenowany tylko w jedn\u0105 stron\u0119. Od lewej do krawy, w stylu GPT. I we wszystkich zadaniach, bez wyj\u0105tku, ten jednokierunkowy model przegrywa\u0142 z Gretesem. Najbardziej w tych zadaniach wymagaj\u0105cych kontekstu? Najbardziej dramatyczny spadek by\u0142 w zadaniu sk\u0142ad, tym z odpowiadaniem na pytania, co jest logiczne, bo tam w\u0142a\u015bnie umiej\u0119tno\u015b\u0107 patrzenia w prz\u00f3d i w ty\u0142 jest absolutnie krytyczna. To bezdyskusyjnie pokaza\u0142o, \u017ce deep bidirectionality by\u0142o tajnym sk\u0142adnikiem sukcesu. W artykule jest te\u017c mowa o wp\u0142owie wielko\u015bci modelu. Por\u00f3wnuj\u0105 tam wersje BERT Base ze 110 milionami parametr\u00f3w do BERT Large z 340 milionami. I wyniki s\u0105 jednoznaczne. Wi\u0119kszy jest lepszy. Ale chwila, to jest wbrew wszystkiemu, czego uczono nas o maszinerningu. Zawsze si\u0119 powtarza\u0142o. Uwa\u017caj, bo za du\u017cy model na ma\u0142ym zbiorze danych to prosta droga do overfittingu. Do zapami\u0119tania danych na pami\u0119\u0107, a nie do nauki. Dok\u0142adnie, dlatego tutaj to nie tylko nie by\u0142 problem, ale wr\u0119cz zaleta. I to jest w\u0142a\u015bnie sedno jednego z najwa\u017cniejszych odkry\u0107 tego artyku\u0142u. Odkrycia, kt\u00f3re zapocz\u0105tkowa\u0142o obecny wy\u015bcig zbrojeni na coraz wi\u0119ksze modele. Okaza\u0142o si\u0119, \u017ce je\u015bli model przesied\u0142 przez wystarczaj\u0105co d\u0142ugi i wszechstronny pretraining na ogromnej ilo\u015bci danych, to jego gigantyczna pojemno\u015b\u0107 staje si\u0119 zalet\u0105. Nie wad\u0105. Nie. Taki wst\u0119pnie wytrenowany m\u00f3zg nie musi ju\u017c uczy\u0107 si\u0119 podstawowych koncepcji j\u0119zykowych na ma\u0142ym zbiorze. On ju\u017c je zna. On tylko adaptuje t\u0119 ogromn\u0105 zgeneralizowan\u0105 wiedz\u0119 do nowego problemu. Czyli ten pot\u0119\u017cny pretraining dzia\u0142a jak taka szczepionka przeciwko overfittingowi. Mo\u017cna tak powiedzie\u0107. Okaza\u0142o si\u0119, \u017ce te dodatkowe miliony parametr\u00f3w nie s\u0142u\u017c\u0105 do zapami\u0119tywania danych, ale do budowania bardziej subtylnych, bardziej z\u0142o\u017conych i bardziej og\u00f3lnych reprezentacji j\u0119zyka. Nawet na ma\u0142ych zbiorach danych? Tak. Nawet w zadaniach z zaledwie kilkoma tysi\u0105cami przyk\u0142ad\u00f3w jak MRPC Bert Larch by\u0142 znacznie lepszy od swojego mniejszego brata. Artyku\u0142 udowodni\u0142, \u017ce skalowanie w po\u0142\u0105czeniu z odpowiednim pretrainingiem jest drog\u0105 do prawdziwej inteligencji. I reszta \u015bwiata pos\u0142ucha\u0142a. Podsumowuj\u0105c, je\u015bli mieliby\u015bmy wskaza\u0107 te dwie absolutnie kluczowe innowacje, kt\u00f3re przyni\u00f3s\u0142 Bert, to by\u0142yby to. Po pierwsze, g\u0142\u0119boka dwukierunkowo\u015b\u0107 osi\u0105gni\u0119ta przez genialnie prost\u0105 gr\u0119 w zgadywanie zamaskowanych s\u0142\u00f3w. A po drugie? A po drugie, zdolno\u015b\u0107 rozumienia logiki mi\u0119dzy zdaniami. Dzi\u0119ki przewidywaniu czy dwa zdania do siebie pasuj\u0105. I co r\u00f3wnie wa\u017cne, Bert fundamentalnie zmieni\u0142 spos\u00f3b, w jaki podchodzimy do problem\u00f3w WNLP. Zamiast budowa\u0107 od zera skomplikowane, wyspecjalizowane architektury dla ka\u017cdego problemu, \u015bwiat przestawi\u0142 si\u0119 na paradygmat. We\u017a jeden pot\u0119\u017cny, wst\u0119pnie wytrenowany model fundament i w kilka godzin dostosuj go do swoich potrzeb. To by\u0142 prze\u0142om nie tylko technologiczny, ale te\u017c filozoficzny. Niesamowite, jak te dwa wydawa\u0142oby si\u0119 prosty pomys\u0142y, zgadywanie s\u0142\u00f3w i ocena sp\u00f3jno\u015bci zda\u0144 doprowadzi\u0142y do tak gigantycznego skoku naprz\u00f3d. To na s\u0142owa wa\u017cne pytanie. Bert pokaza\u0142, \u017ce aby rozwi\u0105za\u0107 wiele specyficznych problem\u00f3w j\u0119zykowych, najlepszym pierwszym krokiem jest rozwi\u0105zanie jednego bardzo og\u00f3lnego problemu. Zrozumienia j\u0119zyka w jego pe\u0142nym dwukierunkowym kontak\u015bcie. I co w zwi\u0105zku z tym? Zastan\u00f3wmy si\u0119, jakie inne z\u0142o\u017cone dziedziny, mo\u017ce poza j\u0119zykiem, mog\u0142yby zosta\u0107 zrewolucjonizowane nie przez budowanie tysi\u0119cy specjalistycznych narz\u0119dzi, ale przez stworzenie jednego, niezwykle pot\u0119\u017cnego uniwersalnego fundamentu.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.4, "text": " Jak to jest, \u017ce my ludzie rozumiemy si\u0119 bez trudu?", "tokens": [50364, 15029, 281, 3492, 11, 3561, 452, 37025, 48797, 414, 2226, 3244, 10782, 32007, 84, 30, 50534], "temperature": 0.0, "avg_logprob": -0.12181574181665349, "compression_ratio": 1.4733333333333334, "no_speech_prob": 0.004391769412904978}, {"id": 1, "seek": 0, "start": 3.6, "end": 9.4, "text": " Nawet gdy \u017cartujemy, u\u017cywamy sarkazmu albo, no wiesz, m\u00f3wimy w niedopowiedzeniach.", "tokens": [50544, 40315, 302, 28405, 19625, 446, 21767, 11, 34097, 86, 7804, 262, 809, 921, 20140, 22622, 11, 572, 261, 15347, 11, 13489, 13189, 261, 32488, 404, 305, 1091, 42124, 608, 13, 50834], "temperature": 0.0, "avg_logprob": -0.12181574181665349, "compression_ratio": 1.4733333333333334, "no_speech_prob": 0.004391769412904978}, {"id": 2, "seek": 0, "start": 9.6, "end": 10.6, "text": " A dla komputer\u00f3w?", "tokens": [50844, 316, 12285, 5207, 13849, 3901, 30, 50894], "temperature": 0.0, "avg_logprob": -0.12181574181665349, "compression_ratio": 1.4733333333333334, "no_speech_prob": 0.004391769412904978}, {"id": 3, "seek": 0, "start": 10.8, "end": 14.8, "text": " A dla komputer\u00f3w przez dekady to by\u0142a po prostu czarna magia.", "tokens": [50904, 316, 12285, 5207, 13849, 3901, 14064, 368, 74, 880, 281, 23936, 714, 19518, 6472, 21394, 2258, 654, 13, 51104], "temperature": 0.0, "avg_logprob": -0.12181574181665349, "compression_ratio": 1.4733333333333334, "no_speech_prob": 0.004391769412904978}, {"id": 4, "seek": 0, "start": 15.0, "end": 18.400000000000002, "text": " Dok\u0142adnie. Maszyna mog\u0142a zna\u0107 definicj\u0119 ka\u017cdego s\u0142owa,", "tokens": [51114, 29768, 10358, 2766, 13, 5224, 1229, 629, 13172, 5024, 710, 629, 2162, 1561, 299, 11115, 21912, 67, 6308, 15116, 5528, 11, 51284], "temperature": 0.0, "avg_logprob": -0.12181574181665349, "compression_ratio": 1.4733333333333334, "no_speech_prob": 0.004391769412904978}, {"id": 5, "seek": 0, "start": 18.6, "end": 21.8, "text": " ale i tak nie rozumia\u0142a, \u017ce zdanie ale urwa\u0142.", "tokens": [51294, 6775, 741, 991, 2838, 48797, 25605, 11, 3561, 16221, 7155, 6775, 4038, 44603, 13, 51454], "temperature": 0.0, "avg_logprob": -0.12181574181665349, "compression_ratio": 1.4733333333333334, "no_speech_prob": 0.004391769412904978}, {"id": 6, "seek": 0, "start": 22.0, "end": 24.8, "text": " No po obejrzeniu jakiego\u015b ryzykownego manewru.", "tokens": [51464, 883, 714, 36346, 73, 81, 39651, 4207, 12200, 1788, 20791, 1229, 74, 648, 6308, 587, 1023, 894, 13, 51604], "temperature": 0.0, "avg_logprob": -0.12181574181665349, "compression_ratio": 1.4733333333333334, "no_speech_prob": 0.004391769412904978}, {"id": 7, "seek": 0, "start": 25.0, "end": 28.400000000000002, "text": " Znaczy co\u015b zupe\u0142nie innego ni\u017c w instrukcji obs\u0142ugi pi\u0142y.", "tokens": [51614, 1176, 77, 14691, 19241, 49922, 294, 11858, 28502, 261, 1058, 25126, 19649, 3181, 1221, 24780, 3895, 6825, 13, 51784], "temperature": 0.0, "avg_logprob": -0.12181574181665349, "compression_ratio": 1.4733333333333334, "no_speech_prob": 0.004391769412904978}, {"id": 8, "seek": 2840, "start": 28.4, "end": 31.799999999999997, "text": " To jest w\u0142a\u015bnie sedno problemu, kontekst.", "tokens": [50364, 1407, 3492, 14234, 9643, 1771, 1154, 84, 11, 14373, 916, 372, 13, 50534], "temperature": 0.0, "avg_logprob": -0.12403465906778971, "compression_ratio": 1.3675213675213675, "no_speech_prob": 0.010235575027763844}, {"id": 9, "seek": 2840, "start": 32.0, "end": 36.199999999999996, "text": " Zdecydowanie. Maszyna musi zrozumie\u0107, \u017ce z\u0142ama\u0107 komu\u015b serce,", "tokens": [50544, 1176, 1479, 1344, 67, 22028, 13, 5224, 1229, 629, 37587, 710, 27857, 449, 414, 2162, 11, 3561, 31614, 2404, 2162, 5207, 84, 1788, 816, 384, 11, 50754], "temperature": 0.0, "avg_logprob": -0.12403465906778971, "compression_ratio": 1.3675213675213675, "no_speech_prob": 0.010235575027763844}, {"id": 10, "seek": 2840, "start": 36.4, "end": 38.4, "text": " to nie jest problem dla kardiohirurga.", "tokens": [50764, 281, 2838, 3492, 1154, 12285, 350, 38126, 1445, 347, 374, 3680, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12403465906778971, "compression_ratio": 1.3675213675213675, "no_speech_prob": 0.010235575027763844}, {"id": 11, "seek": 2840, "start": 38.6, "end": 41.8, "text": " A mie\u0107 muchy w nosie nie wymaga interwencji entomologa.", "tokens": [50874, 316, 35612, 2992, 28629, 261, 3269, 414, 2838, 29764, 9286, 728, 15615, 19649, 948, 298, 1132, 64, 13, 51034], "temperature": 0.0, "avg_logprob": -0.12403465906778971, "compression_ratio": 1.3675213675213675, "no_speech_prob": 0.010235575027763844}, {"id": 12, "seek": 2840, "start": 42.0, "end": 48.0, "text": " Tak, ta intuicyjna ludzka zdolno\u015b\u0107 by\u0142a dla sztucznej inteligencji", "tokens": [51044, 9118, 11, 1846, 560, 84, 2632, 73, 629, 15946, 89, 2330, 16221, 401, 23293, 23936, 12285, 262, 2682, 1311, 89, 11794, 24777, 3213, 19649, 51344], "temperature": 0.0, "avg_logprob": -0.12403465906778971, "compression_ratio": 1.3675213675213675, "no_speech_prob": 0.010235575027763844}, {"id": 13, "seek": 2840, "start": 48.2, "end": 53.4, "text": " no takim \u015bwi\u0119tym gralem, a\u017c do 2018 roku.", "tokens": [51354, 572, 31732, 8299, 22423, 874, 76, 1295, 10386, 11, 48134, 360, 6096, 19451, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12403465906778971, "compression_ratio": 1.3675213675213675, "no_speech_prob": 0.010235575027763844}, {"id": 14, "seek": 5340, "start": 53.8, "end": 57.4, "text": " W\u0142a\u015bnie. I dzisiaj przyjrzymy si\u0119 pracy naukowej,", "tokens": [50384, 343, 5024, 12221, 13, 286, 25772, 6501, 73, 13047, 2226, 3244, 35591, 35616, 74, 21091, 11, 50564], "temperature": 0.0, "avg_logprob": -0.13884724837083082, "compression_ratio": 1.3910034602076125, "no_speech_prob": 0.11300785094499588}, {"id": 15, "seek": 5340, "start": 57.6, "end": 60.8, "text": " kt\u00f3ra mo\u017cna powiedzie\u0107, tego Graala odnalaz\u0142a.", "tokens": [50574, 19456, 17790, 27886, 11, 8627, 8985, 5159, 3611, 4660, 921, 5024, 13, 50734], "temperature": 0.0, "avg_logprob": -0.13884724837083082, "compression_ratio": 1.3910034602076125, "no_speech_prob": 0.11300785094499588}, {"id": 16, "seek": 5340, "start": 61.0, "end": 65.0, "text": " Chodzi o artyku\u0142 z Google AI Language zatytu\u0142owany BERT.", "tokens": [50744, 761, 14543, 277, 594, 874, 5279, 1221, 710, 3329, 7318, 24445, 35802, 4328, 84, 1221, 23341, 363, 31479, 13, 50944], "temperature": 0.0, "avg_logprob": -0.13884724837083082, "compression_ratio": 1.3910034602076125, "no_speech_prob": 0.11300785094499588}, {"id": 17, "seek": 5340, "start": 65.2, "end": 70.2, "text": " Pre-training of deep bidirectional transformers for language understanding.", "tokens": [50954, 6001, 12, 17227, 1760, 295, 2452, 12957, 621, 41048, 4088, 433, 337, 2856, 3701, 13, 51204], "temperature": 0.0, "avg_logprob": -0.13884724837083082, "compression_ratio": 1.3910034602076125, "no_speech_prob": 0.11300785094499588}, {"id": 18, "seek": 5340, "start": 70.4, "end": 73.6, "text": " I to nie b\u0119dzie tylko taka sucha akademicka analiza.", "tokens": [51214, 286, 281, 2838, 10562, 13219, 28017, 1270, 64, 9308, 49290, 618, 64, 2624, 13427, 13, 51374], "temperature": 0.0, "avg_logprob": -0.13884724837083082, "compression_ratio": 1.3910034602076125, "no_speech_prob": 0.11300785094499588}, {"id": 19, "seek": 5340, "start": 73.8, "end": 75.8, "text": " Nie. Spr\u00f3bujemy zrozumie\u0107,", "tokens": [51384, 12016, 13, 7702, 14216, 21767, 710, 27857, 449, 414, 2162, 11, 51484], "temperature": 0.0, "avg_logprob": -0.13884724837083082, "compression_ratio": 1.3910034602076125, "no_speech_prob": 0.11300785094499588}, {"id": 20, "seek": 5340, "start": 76.0, "end": 80.6, "text": " dlaczego ten jeden artyku\u0142 wywo\u0142a\u0142 prawdziwe trz\u0119sienie ziemi w \u015bwiecie AI.", "tokens": [51494, 37873, 39329, 2064, 12906, 594, 874, 5279, 1221, 4628, 6120, 5024, 1221, 41175, 3992, 826, 504, 11052, 82, 27385, 16503, 3057, 261, 40078, 4260, 7318, 13, 51724], "temperature": 0.0, "avg_logprob": -0.13884724837083082, "compression_ratio": 1.3910034602076125, "no_speech_prob": 0.11300785094499588}, {"id": 21, "seek": 8060, "start": 80.6, "end": 84.0, "text": " I sprawi\u0142, \u017ce dzi\u015b rozmawiamy z chatbotami, kt\u00f3re...", "tokens": [50364, 286, 22734, 40622, 11, 3561, 31981, 1788, 35234, 1607, 2918, 88, 710, 5081, 18870, 4526, 11, 8864, 485, 50534], "temperature": 0.0, "avg_logprob": -0.1319800649370466, "compression_ratio": 1.3875432525951557, "no_speech_prob": 0.015348180197179317}, {"id": 22, "seek": 8060, "start": 84.19999999999999, "end": 86.8, "text": " No, kt\u00f3re wydaj\u0105 si\u0119 nas naprawd\u0119 rozumie\u0107.", "tokens": [50544, 883, 11, 8864, 25984, 11133, 3244, 5382, 20970, 48797, 414, 2162, 13, 50674], "temperature": 0.0, "avg_logprob": -0.1319800649370466, "compression_ratio": 1.3875432525951557, "no_speech_prob": 0.015348180197179317}, {"id": 23, "seek": 8060, "start": 87.0, "end": 89.39999999999999, "text": " Dobrze, roz\u0142\u00f3\u017cmy to naczynniki pierwsze.", "tokens": [50684, 29679, 13503, 11, 9544, 1221, 812, 1427, 2226, 281, 297, 14691, 26384, 9850, 45994, 13, 50804], "temperature": 0.0, "avg_logprob": -0.1319800649370466, "compression_ratio": 1.3875432525951557, "no_speech_prob": 0.015348180197179317}, {"id": 24, "seek": 8060, "start": 89.6, "end": 91.6, "text": " Zacznijmy od tego, co by\u0142o wcze\u015bniej.", "tokens": [50814, 1176, 14875, 77, 1718, 2226, 3611, 8627, 11, 598, 14811, 40785, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1319800649370466, "compression_ratio": 1.3875432525951557, "no_speech_prob": 0.015348180197179317}, {"id": 25, "seek": 8060, "start": 91.8, "end": 96.6, "text": " Tak. Zanim pojawi\u0142 si\u0119 BERT, mieli\u015bmy ju\u017c przecie\u017c ca\u0142kiem niez\u0142e modele.", "tokens": [50924, 9118, 13, 1176, 17869, 30655, 40622, 3244, 363, 31479, 11, 41214, 10513, 10678, 8325, 40082, 35224, 26116, 33511, 19827, 4391, 306, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1319800649370466, "compression_ratio": 1.3875432525951557, "no_speech_prob": 0.015348180197179317}, {"id": 26, "seek": 8060, "start": 96.8, "end": 100.6, "text": " Jak cho\u0107by test serii GPT od OpenAI czy ELMO.", "tokens": [51174, 15029, 1586, 2162, 2322, 1500, 816, 5597, 26039, 51, 3611, 7238, 48698, 6430, 14426, 18976, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1319800649370466, "compression_ratio": 1.3875432525951557, "no_speech_prob": 0.015348180197179317}, {"id": 27, "seek": 8060, "start": 100.8, "end": 102.39999999999999, "text": " Co by\u0142o z nimi nie tak?", "tokens": [51374, 3066, 14811, 710, 297, 10121, 2838, 991, 30, 51454], "temperature": 0.0, "avg_logprob": -0.1319800649370466, "compression_ratio": 1.3875432525951557, "no_speech_prob": 0.015348180197179317}, {"id": 28, "seek": 8060, "start": 102.6, "end": 106.8, "text": " Dlaczego potrzebowali\u015bmy rewolucji, a nie tylko ewolucji?", "tokens": [51464, 413, 75, 39329, 37595, 305, 33955, 319, 48481, 1311, 4013, 11, 257, 2838, 13219, 43364, 401, 1311, 4013, 30, 51674], "temperature": 0.0, "avg_logprob": -0.1319800649370466, "compression_ratio": 1.3875432525951557, "no_speech_prob": 0.015348180197179317}, {"id": 29, "seek": 10680, "start": 107.0, "end": 111.2, "text": " G\u0142\u00f3wny problem, kt\u00f3ry zreszt\u0105 autorzy Berta \u015bwietnie zdiagnozowali.", "tokens": [50374, 460, 1221, 812, 43682, 1154, 11, 9913, 710, 495, 2682, 1611, 19510, 1229, 363, 22765, 8299, 39083, 2766, 710, 4504, 559, 1771, 89, 305, 5103, 13, 50584], "temperature": 0.0, "avg_logprob": -0.12615002881760565, "compression_ratio": 1.4076655052264808, "no_speech_prob": 0.14346358180046082}, {"id": 30, "seek": 10680, "start": 111.39999999999999, "end": 113.6, "text": " Mo\u017cna zamkn\u0105\u0107 w jednym s\u0142owie.", "tokens": [50594, 44736, 629, 19876, 5457, 36374, 261, 5232, 12996, 15116, 13998, 13, 50704], "temperature": 0.0, "avg_logprob": -0.12615002881760565, "compression_ratio": 1.4076655052264808, "no_speech_prob": 0.14346358180046082}, {"id": 31, "seek": 10680, "start": 113.8, "end": 114.8, "text": " I to jest?", "tokens": [50714, 286, 281, 3492, 30, 50764], "temperature": 0.0, "avg_logprob": -0.12615002881760565, "compression_ratio": 1.4076655052264808, "no_speech_prob": 0.14346358180046082}, {"id": 32, "seek": 10680, "start": 115.0, "end": 117.0, "text": " Jednokierunkowo\u015b\u0107.", "tokens": [50774, 27076, 77, 453, 811, 3197, 19941, 7753, 13, 50874], "temperature": 0.0, "avg_logprob": -0.12615002881760565, "compression_ratio": 1.4076655052264808, "no_speech_prob": 0.14346358180046082}, {"id": 33, "seek": 10680, "start": 117.2, "end": 120.39999999999999, "text": " Te wcze\u015bniejsze modele, nawet te najbardziej zaawansowane,", "tokens": [50884, 1989, 38533, 1788, 44258, 4391, 306, 11, 22696, 535, 41857, 7949, 1607, 599, 23066, 11, 51044], "temperature": 0.0, "avg_logprob": -0.12615002881760565, "compression_ratio": 1.4076655052264808, "no_speech_prob": 0.14346358180046082}, {"id": 34, "seek": 10680, "start": 120.6, "end": 124.6, "text": " czyta\u0142y tekst tak, jak my czytamy ksi\u0105\u017ck\u0119 od lewej do prawej.", "tokens": [51054, 6430, 1328, 6825, 16624, 372, 991, 11, 4207, 452, 6430, 83, 7804, 39311, 15724, 3611, 476, 826, 73, 360, 3206, 826, 73, 13, 51254], "temperature": 0.0, "avg_logprob": -0.12615002881760565, "compression_ratio": 1.4076655052264808, "no_speech_prob": 0.14346358180046082}, {"id": 35, "seek": 10680, "start": 124.8, "end": 126.4, "text": " S\u0142owo po s\u0142owie.", "tokens": [51264, 318, 1221, 19941, 714, 15116, 13998, 13, 51344], "temperature": 0.0, "avg_logprob": -0.12615002881760565, "compression_ratio": 1.4076655052264808, "no_speech_prob": 0.14346358180046082}, {"id": 36, "seek": 10680, "start": 126.6, "end": 127.6, "text": " Czyli...", "tokens": [51354, 37099, 485, 51404], "temperature": 0.0, "avg_logprob": -0.12615002881760565, "compression_ratio": 1.4076655052264808, "no_speech_prob": 0.14346358180046082}, {"id": 37, "seek": 10680, "start": 127.8, "end": 132.0, "text": " Czekaj, analizuj\u0105c jakie\u015b s\u0142owo, powiedzmy, w po\u0142owie zdania,", "tokens": [51414, 383, 19878, 1805, 11, 2624, 590, 44733, 31163, 15116, 19941, 11, 27617, 2226, 11, 261, 714, 1221, 13998, 16221, 5609, 11, 51624], "temperature": 0.0, "avg_logprob": -0.12615002881760565, "compression_ratio": 1.4076655052264808, "no_speech_prob": 0.14346358180046082}, {"id": 38, "seek": 10680, "start": 132.2, "end": 135.0, "text": " model widzia\u0142 wszystko, co by\u0142o przed nim,", "tokens": [51634, 2316, 27486, 8908, 22607, 11, 598, 14811, 18334, 24887, 11, 51774], "temperature": 0.0, "avg_logprob": -0.12615002881760565, "compression_ratio": 1.4076655052264808, "no_speech_prob": 0.14346358180046082}, {"id": 39, "seek": 13500, "start": 135.2, "end": 138.6, "text": " ale by\u0142 kompletnie \u015blepy na to, co mia\u0142o dopiero nadej\u015b\u0107.", "tokens": [50374, 6775, 16673, 5207, 14657, 2766, 8299, 306, 8200, 1667, 281, 11, 598, 21290, 5249, 21900, 12030, 297, 762, 44536, 13, 50544], "temperature": 0.0, "avg_logprob": -0.118898097379708, "compression_ratio": 1.439102564102564, "no_speech_prob": 0.0052991872653365135}, {"id": 40, "seek": 13500, "start": 138.8, "end": 139.8, "text": " W\u0142a\u015bnie tak.", "tokens": [50554, 343, 5024, 12221, 991, 13, 50604], "temperature": 0.0, "avg_logprob": -0.118898097379708, "compression_ratio": 1.439102564102564, "no_speech_prob": 0.0052991872653365135}, {"id": 41, "seek": 13500, "start": 140.0, "end": 143.4, "text": " Wyobra\u017a sobie, \u017ce pr\u00f3bujesz rozwi\u0105za\u0107 zagadk\u0119 kryminaln\u0105,", "tokens": [50614, 14458, 24393, 10659, 13652, 11, 3561, 8565, 65, 4579, 10430, 9544, 18234, 35873, 27001, 345, 15724, 34847, 76, 2071, 13113, 11, 50784], "temperature": 0.0, "avg_logprob": -0.118898097379708, "compression_ratio": 1.439102564102564, "no_speech_prob": 0.0052991872653365135}, {"id": 42, "seek": 13500, "start": 143.6, "end": 147.4, "text": " ale wolno ci czyta\u0107 akta sprawy tylko do strony, na kt\u00f3rej jeste\u015b.", "tokens": [50794, 6775, 20960, 1771, 6983, 6430, 42931, 13680, 64, 22734, 88, 13219, 360, 32406, 11, 1667, 36023, 25255, 1788, 13, 50984], "temperature": 0.0, "avg_logprob": -0.118898097379708, "compression_ratio": 1.439102564102564, "no_speech_prob": 0.0052991872653365135}, {"id": 43, "seek": 13500, "start": 147.6, "end": 150.6, "text": " Bez mo\u017cliwo\u015bci zerkania na p\u00f3\u017aniejsze zeznania.", "tokens": [50994, 879, 89, 30854, 36476, 44746, 5225, 654, 1667, 28157, 10659, 44258, 5277, 22672, 5609, 13, 51144], "temperature": 0.0, "avg_logprob": -0.118898097379708, "compression_ratio": 1.439102564102564, "no_speech_prob": 0.0052991872653365135}, {"id": 44, "seek": 13500, "start": 150.8, "end": 151.8, "text": " Dok\u0142adnie.", "tokens": [51154, 29768, 10358, 2766, 13, 51204], "temperature": 0.0, "avg_logprob": -0.118898097379708, "compression_ratio": 1.439102564102564, "no_speech_prob": 0.0052991872653365135}, {"id": 45, "seek": 13500, "start": 152.0, "end": 154.0, "text": " To gigantyczne ograniczenie.", "tokens": [51214, 1407, 8741, 394, 17466, 716, 34416, 30732, 16778, 13, 51314], "temperature": 0.0, "avg_logprob": -0.118898097379708, "compression_ratio": 1.439102564102564, "no_speech_prob": 0.0052991872653365135}, {"id": 46, "seek": 13500, "start": 154.2, "end": 155.4, "text": " J\u0119zyk tak nie dzia\u0142a.", "tokens": [51324, 508, 1274, 1229, 74, 991, 2838, 37903, 13, 51384], "temperature": 0.0, "avg_logprob": -0.118898097379708, "compression_ratio": 1.439102564102564, "no_speech_prob": 0.0052991872653365135}, {"id": 47, "seek": 13500, "start": 155.6, "end": 159.6, "text": " Aby zrozumie\u0107 znaczenie s\u0142owa Pali w zdaniu Jan Pali Papierosa,", "tokens": [51394, 316, 2322, 710, 27857, 449, 414, 2162, 15397, 326, 16778, 15116, 5528, 430, 5103, 261, 16221, 25849, 4956, 430, 5103, 15919, 811, 6447, 11, 51594], "temperature": 0.0, "avg_logprob": -0.118898097379708, "compression_ratio": 1.439102564102564, "no_speech_prob": 0.0052991872653365135}, {"id": 48, "seek": 13500, "start": 159.8, "end": 162.8, "text": " musisz zobaczy\u0107 s\u0142owo Papieros, kt\u00f3re jest po nim.", "tokens": [51604, 1038, 23848, 37273, 2162, 15116, 19941, 15919, 811, 329, 11, 8864, 3492, 714, 24887, 13, 51754], "temperature": 0.0, "avg_logprob": -0.118898097379708, "compression_ratio": 1.439102564102564, "no_speech_prob": 0.0052991872653365135}, {"id": 49, "seek": 16280, "start": 163.20000000000002, "end": 165.4, "text": " A wcze\u015bniejsze modele musia\u0142yby zgadywa\u0107.", "tokens": [50384, 316, 38533, 1788, 44258, 4391, 306, 1038, 654, 6825, 2322, 40948, 880, 25234, 13, 50494], "temperature": 0.0, "avg_logprob": -0.10901992797851562, "compression_ratio": 1.4874213836477987, "no_speech_prob": 0.021605560556054115}, {"id": 50, "seek": 16280, "start": 165.60000000000002, "end": 166.60000000000002, "text": " Zgadz\u0119 si\u0119.", "tokens": [50504, 1176, 70, 345, 11052, 3244, 13, 50554], "temperature": 0.0, "avg_logprob": -0.10901992797851562, "compression_ratio": 1.4874213836477987, "no_speech_prob": 0.021605560556054115}, {"id": 51, "seek": 16280, "start": 166.8, "end": 169.4, "text": " Owszem, by\u0142y pr\u00f3by obej\u015bcia tego.", "tokens": [50564, 12773, 15453, 443, 11, 26366, 8565, 2322, 36346, 73, 1788, 2755, 8627, 13, 50694], "temperature": 0.0, "avg_logprob": -0.10901992797851562, "compression_ratio": 1.4874213836477987, "no_speech_prob": 0.021605560556054115}, {"id": 52, "seek": 16280, "start": 169.60000000000002, "end": 171.4, "text": " Na przyk\u0142ad w modelu Elmo.", "tokens": [50704, 6056, 23144, 261, 2316, 84, 38722, 13, 50794], "temperature": 0.0, "avg_logprob": -0.10901992797851562, "compression_ratio": 1.4874213836477987, "no_speech_prob": 0.021605560556054115}, {"id": 53, "seek": 16280, "start": 171.60000000000002, "end": 177.60000000000002, "text": " On pr\u00f3bowa\u0142 skleja\u0107 ze sob\u0105 dwie analizy jedn\u0105 od lewej do prawej i drug\u0105 od prawej do lewej.", "tokens": [50804, 1282, 8565, 65, 30105, 1110, 306, 2938, 2162, 5277, 18253, 1611, 274, 8699, 2624, 590, 88, 5232, 13113, 3611, 476, 826, 73, 360, 3206, 826, 73, 741, 4110, 1611, 3611, 3206, 826, 73, 360, 476, 826, 73, 13, 51104], "temperature": 0.0, "avg_logprob": -0.10901992797851562, "compression_ratio": 1.4874213836477987, "no_speech_prob": 0.021605560556054115}, {"id": 54, "seek": 16280, "start": 177.8, "end": 179.60000000000002, "text": " Ale to by\u0142o takie...", "tokens": [51114, 9366, 281, 14811, 15963, 485, 51204], "temperature": 0.0, "avg_logprob": -0.10901992797851562, "compression_ratio": 1.4874213836477987, "no_speech_prob": 0.021605560556054115}, {"id": 55, "seek": 16280, "start": 179.8, "end": 180.8, "text": " powierzchowne.", "tokens": [51214, 3388, 34602, 339, 648, 68, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10901992797851562, "compression_ratio": 1.4874213836477987, "no_speech_prob": 0.021605560556054115}, {"id": 56, "seek": 16280, "start": 181.0, "end": 185.20000000000002, "text": " Troch\u0119 jakby dwie osoby czyta\u0142y ksi\u0105\u017ck\u0119, jedna od pocz\u0105tku, druga od ko\u0144ca", "tokens": [51274, 19406, 23006, 28976, 274, 8699, 39737, 6430, 1328, 6825, 39311, 15724, 11, 5232, 629, 3611, 43959, 11, 4110, 64, 3611, 26470, 496, 51484], "temperature": 0.0, "avg_logprob": -0.10901992797851562, "compression_ratio": 1.4874213836477987, "no_speech_prob": 0.021605560556054115}, {"id": 57, "seek": 16280, "start": 185.4, "end": 187.4, "text": " i co jaki\u015b czas wymienia\u0142y si\u0119 notatkami.", "tokens": [51494, 741, 598, 34721, 13190, 29764, 18811, 6825, 3244, 406, 33525, 4526, 13, 51594], "temperature": 0.0, "avg_logprob": -0.10901992797851562, "compression_ratio": 1.4874213836477987, "no_speech_prob": 0.021605560556054115}, {"id": 58, "seek": 16280, "start": 187.60000000000002, "end": 188.60000000000002, "text": " \u015awietna analogia.", "tokens": [51604, 27933, 39083, 629, 16660, 654, 13, 51654], "temperature": 0.0, "avg_logprob": -0.10901992797851562, "compression_ratio": 1.4874213836477987, "no_speech_prob": 0.021605560556054115}, {"id": 59, "seek": 16280, "start": 188.8, "end": 192.0, "text": " To nie to samo, co jedna osoba, kt\u00f3ra zna ca\u0142\u0105 tre\u015b\u0107 od razu.", "tokens": [51664, 1407, 2838, 281, 36422, 11, 598, 5232, 629, 19116, 4231, 11, 19456, 710, 629, 1335, 15926, 2192, 7753, 3611, 367, 8813, 13, 51824], "temperature": 0.0, "avg_logprob": -0.10901992797851562, "compression_ratio": 1.4874213836477987, "no_speech_prob": 0.021605560556054115}, {"id": 60, "seek": 19200, "start": 192.2, "end": 193.2, "text": " Idealnie.", "tokens": [50374, 13090, 304, 2766, 13, 50424], "temperature": 0.0, "avg_logprob": -0.13506005650801625, "compression_ratio": 1.49185667752443, "no_speech_prob": 0.0018082050373777747}, {"id": 61, "seek": 19200, "start": 193.4, "end": 197.4, "text": " To by\u0142o tylko shallow concatenation, takie p\u0142ytkie po\u0142\u0105czenie.", "tokens": [50434, 1407, 14811, 13219, 20488, 1588, 7186, 399, 11, 15963, 28695, 4328, 22872, 714, 15926, 39043, 13, 50634], "temperature": 0.0, "avg_logprob": -0.13506005650801625, "compression_ratio": 1.49185667752443, "no_speech_prob": 0.0018082050373777747}, {"id": 62, "seek": 19200, "start": 197.6, "end": 203.4, "text": " Brakowa\u0142o g\u0142\u0119bokiego i jednoczesnego rozumienia pe\u0142nego kontekstu na wszystkich warstwach sieci.", "tokens": [50644, 4991, 74, 5528, 5249, 18117, 1274, 21666, 12200, 741, 5232, 26694, 12214, 11858, 48797, 18811, 43205, 11858, 14373, 916, 372, 84, 1667, 34234, 1516, 372, 50038, 2804, 537, 13, 50934], "temperature": 0.0, "avg_logprob": -0.13506005650801625, "compression_ratio": 1.49185667752443, "no_speech_prob": 0.0018082050373777747}, {"id": 63, "seek": 19200, "start": 203.6, "end": 206.2, "text": " I tu w\u0142a\u015bnie Bert wchodzi na scen\u0119.", "tokens": [50944, 286, 2604, 14234, 29594, 261, 34616, 1667, 4191, 1274, 13, 51074], "temperature": 0.0, "avg_logprob": -0.13506005650801625, "compression_ratio": 1.49185667752443, "no_speech_prob": 0.0018082050373777747}, {"id": 64, "seek": 19200, "start": 206.4, "end": 212.0, "text": " No dobrze, skoro problemem by\u0142a ta \u015blepo\u015b\u0107 na przysz\u0142o\u015b\u0107, to rozwi\u0105zanie wydaje si\u0119 proste.", "tokens": [51084, 883, 28335, 11, 1110, 10780, 1154, 443, 23936, 1846, 8299, 306, 2259, 7753, 1667, 44018, 44742, 11, 281, 9544, 22620, 7155, 49165, 3244, 10293, 68, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13506005650801625, "compression_ratio": 1.49185667752443, "no_speech_prob": 0.0018082050373777747}, {"id": 65, "seek": 19200, "start": 212.2, "end": 217.4, "text": " Poka\u017cmy modelowi od razu ca\u0142e zdanie, ale tu pojawia si\u0119 paradoks.", "tokens": [51374, 14958, 18264, 2226, 2316, 24503, 3611, 367, 8813, 47631, 16221, 7155, 11, 6775, 2604, 30655, 654, 3244, 13480, 25500, 13, 51634], "temperature": 0.0, "avg_logprob": -0.13506005650801625, "compression_ratio": 1.49185667752443, "no_speech_prob": 0.0018082050373777747}, {"id": 66, "seek": 19200, "start": 217.6, "end": 221.8, "text": " Tak, standardowy trening polega\u0142 na przewidywaniu nast\u0119pnego s\u0142owa.", "tokens": [51644, 9118, 11, 3832, 10089, 2192, 773, 13208, 3680, 1221, 1667, 39758, 327, 27112, 25849, 39662, 11858, 15116, 5528, 13, 51854], "temperature": 0.0, "avg_logprob": -0.13506005650801625, "compression_ratio": 1.49185667752443, "no_speech_prob": 0.0018082050373777747}, {"id": 67, "seek": 22200, "start": 222.0, "end": 227.4, "text": " Je\u015bli model widzi ca\u0142e zdanie, to przecie\u017c mo\u017ce bezczelnie \u015bci\u0105gn\u0105\u0107", "tokens": [50364, 37086, 2316, 5274, 3992, 47631, 16221, 7155, 11, 281, 8325, 40082, 12034, 10782, 3689, 338, 2766, 220, 50227, 4568, 36374, 50634], "temperature": 0.0, "avg_logprob": -0.10850370675325394, "compression_ratio": 1.407942238267148, "no_speech_prob": 0.0010828125523403287}, {"id": 68, "seek": 22200, "start": 227.6, "end": 230.0, "text": " i po prostu skopiowa\u0107 s\u0142owo, kt\u00f3re ma przewidzie\u0107.", "tokens": [50644, 741, 714, 19518, 1110, 404, 72, 11445, 15116, 19941, 11, 8864, 463, 39758, 327, 21214, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10850370675325394, "compression_ratio": 1.407942238267148, "no_speech_prob": 0.0010828125523403287}, {"id": 69, "seek": 22200, "start": 230.2, "end": 231.4, "text": " Jak oni to obe szli?", "tokens": [50774, 15029, 36317, 281, 36346, 7870, 2081, 30, 50834], "temperature": 0.0, "avg_logprob": -0.10850370675325394, "compression_ratio": 1.407942238267148, "no_speech_prob": 0.0010828125523403287}, {"id": 70, "seek": 22200, "start": 231.6, "end": 236.4, "text": " I to jest ten moment, w kt\u00f3rym pojawia si\u0119 genialna w swojej prostocie innowacja Bertha.", "tokens": [50844, 286, 281, 3492, 2064, 1623, 11, 261, 30120, 30655, 654, 3244, 48228, 629, 261, 29489, 73, 10293, 905, 414, 294, 3785, 23395, 5637, 13571, 13, 51084], "temperature": 0.0, "avg_logprob": -0.10850370675325394, "compression_ratio": 1.407942238267148, "no_speech_prob": 0.0010828125523403287}, {"id": 71, "seek": 22200, "start": 236.6, "end": 243.0, "text": " Stworzyli zupe\u0142nie now\u0105 gr\u0119, now\u0105 metod\u0119 treningu, kt\u00f3r\u0105 nazwali Masked Language Model.", "tokens": [51094, 745, 28321, 1229, 2081, 49922, 586, 1611, 677, 1274, 11, 586, 1611, 1131, 378, 1274, 2192, 773, 84, 11, 37415, 20151, 40054, 25414, 292, 24445, 17105, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10850370675325394, "compression_ratio": 1.407942238267148, "no_speech_prob": 0.0010828125523403287}, {"id": 72, "seek": 22200, "start": 243.2, "end": 244.4, "text": " W skr\u00f3cie MLM.", "tokens": [51424, 343, 1110, 11721, 4260, 21601, 44, 13, 51484], "temperature": 0.0, "avg_logprob": -0.10850370675325394, "compression_ratio": 1.407942238267148, "no_speech_prob": 0.0010828125523403287}, {"id": 73, "seek": 22200, "start": 244.6, "end": 247.6, "text": " Czyli model j\u0119zykowy z maskowaniem.", "tokens": [51494, 37099, 2316, 49055, 74, 10089, 710, 6094, 37345, 4907, 13, 51644], "temperature": 0.0, "avg_logprob": -0.10850370675325394, "compression_ratio": 1.407942238267148, "no_speech_prob": 0.0010828125523403287}, {"id": 74, "seek": 24760, "start": 247.6, "end": 254.79999999999998, "text": " Tak, zamiast kaza\u0107 modelowi i przewidywa\u0107 nast\u0119pne s\u0142owo, postanowili da\u0107 mu zdanie i zrobi\u0107 w nim dziury.", "tokens": [50364, 9118, 11, 710, 4526, 525, 350, 12257, 2162, 2316, 24503, 741, 39758, 38836, 25234, 39662, 716, 15116, 19941, 11, 2183, 282, 305, 2312, 1120, 2162, 2992, 16221, 7155, 741, 31785, 261, 24887, 31981, 2598, 13, 50724], "temperature": 0.0, "avg_logprob": -0.10467082262039185, "compression_ratio": 1.4013377926421404, "no_speech_prob": 0.02972692996263504}, {"id": 75, "seek": 24760, "start": 255.0, "end": 256.8, "text": " Jak w tych \u0107wiczeniach z podstawuski?", "tokens": [50734, 15029, 261, 15180, 45854, 22295, 42124, 608, 710, 43443, 301, 2984, 30, 50824], "temperature": 0.0, "avg_logprob": -0.10467082262039185, "compression_ratio": 1.4013377926421404, "no_speech_prob": 0.02972692996263504}, {"id": 76, "seek": 24760, "start": 257.0, "end": 259.6, "text": " Wczoraj poszed\u0142em i kupi\u0142em mleko.", "tokens": [50834, 343, 3689, 284, 1805, 1366, 11312, 11126, 741, 37534, 72, 11126, 275, 306, 4093, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10467082262039185, "compression_ratio": 1.4013377926421404, "no_speech_prob": 0.02972692996263504}, {"id": 77, "seek": 24760, "start": 259.8, "end": 261.4, "text": " Dok\u0142adnie o to chodzi.", "tokens": [50974, 29768, 10358, 2766, 277, 281, 23998, 13, 51054], "temperature": 0.0, "avg_logprob": -0.10467082262039185, "compression_ratio": 1.4013377926421404, "no_speech_prob": 0.02972692996263504}, {"id": 78, "seek": 24760, "start": 261.6, "end": 267.2, "text": " To zadanie, znane w psycholinguistyce jako Close Task, sta\u0142o si\u0119 fundamentem Bertha.", "tokens": [51064, 1407, 42788, 7155, 11, 15397, 1929, 261, 4681, 401, 7050, 38618, 384, 17123, 16346, 30428, 11, 11135, 5249, 3244, 6073, 443, 5637, 13571, 13, 51344], "temperature": 0.0, "avg_logprob": -0.10467082262039185, "compression_ratio": 1.4013377926421404, "no_speech_prob": 0.02972692996263504}, {"id": 79, "seek": 24760, "start": 267.4, "end": 272.6, "text": " Podczas treningu model dostaje zdanie, w kt\u00f3rym losowo ukryto 15% s\u0142\u00f3w,", "tokens": [51354, 12646, 30989, 2192, 773, 84, 2316, 20568, 11153, 16221, 7155, 11, 261, 30120, 1750, 19941, 26769, 627, 1353, 2119, 4, 15116, 3901, 11, 51614], "temperature": 0.0, "avg_logprob": -0.10467082262039185, "compression_ratio": 1.4013377926421404, "no_speech_prob": 0.02972692996263504}, {"id": 80, "seek": 24760, "start": 272.8, "end": 276.2, "text": " zast\u0119puj\u0105c je specjalnym znacznikiem mask.", "tokens": [51624, 36746, 18085, 44733, 1506, 46433, 12996, 15397, 14875, 13123, 4907, 6094, 13, 51794], "temperature": 0.0, "avg_logprob": -0.10467082262039185, "compression_ratio": 1.4013377926421404, "no_speech_prob": 0.02972692996263504}, {"id": 81, "seek": 27620, "start": 276.4, "end": 280.2, "text": " I zadaniem modelu nie jest ju\u017c przewidzie\u0107, co b\u0119dzie dalej.", "tokens": [50374, 286, 710, 11338, 4907, 2316, 84, 2838, 3492, 10678, 39758, 327, 21214, 11, 598, 10562, 34257, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08618932374766175, "compression_ratio": 1.4440298507462686, "no_speech_prob": 0.02070852369070053}, {"id": 82, "seek": 27620, "start": 280.4, "end": 286.4, "text": " Jego zadaniem jest spojrze\u0107 na ca\u0142e zdanie, na s\u0142owa przed mask\u0105 i po masce.", "tokens": [50574, 508, 6308, 710, 11338, 4907, 3492, 8243, 73, 13503, 2162, 1667, 47631, 16221, 7155, 11, 1667, 15116, 5528, 18334, 6094, 1611, 741, 714, 2300, 384, 13, 50874], "temperature": 0.0, "avg_logprob": -0.08618932374766175, "compression_ratio": 1.4440298507462686, "no_speech_prob": 0.02070852369070053}, {"id": 83, "seek": 27620, "start": 286.59999999999997, "end": 289.2, "text": " I odgadn\u0105\u0107, co kryje si\u0119 w tej luce.", "tokens": [50884, 286, 3611, 70, 345, 13113, 2162, 11, 598, 34847, 2884, 3244, 261, 12573, 10438, 384, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08618932374766175, "compression_ratio": 1.4440298507462686, "no_speech_prob": 0.02070852369070053}, {"id": 84, "seek": 27620, "start": 289.4, "end": 293.59999999999997, "text": " Czyli zmuszamy go, \u017ceby sta\u0142 si\u0119 takim j\u0119zykowym detektywem.", "tokens": [51024, 37099, 17020, 22378, 7804, 352, 11, 11316, 11135, 1221, 3244, 31732, 49055, 74, 31691, 1141, 916, 874, 86, 443, 13, 51234], "temperature": 0.0, "avg_logprob": -0.08618932374766175, "compression_ratio": 1.4440298507462686, "no_speech_prob": 0.02070852369070053}, {"id": 85, "seek": 27620, "start": 293.8, "end": 298.8, "text": " Musi analizowa\u0107 dowody z obu stron miejsca zbrowni, \u017ceby zidentyfikowa\u0107 ofiar\u0119.", "tokens": [51244, 3569, 72, 2624, 590, 11445, 9459, 843, 710, 1111, 84, 45766, 18522, 44239, 710, 9120, 895, 72, 11, 11316, 710, 1078, 88, 31230, 11445, 295, 9448, 1274, 13, 51494], "temperature": 0.0, "avg_logprob": -0.08618932374766175, "compression_ratio": 1.4440298507462686, "no_speech_prob": 0.02070852369070053}, {"id": 86, "seek": 27620, "start": 299.0, "end": 300.8, "text": " I co jest w tym fascynuj\u0105ce?", "tokens": [51504, 286, 598, 3492, 261, 8107, 30632, 1344, 77, 13263, 384, 30, 51594], "temperature": 0.0, "avg_logprob": -0.08618932374766175, "compression_ratio": 1.4440298507462686, "no_speech_prob": 0.02070852369070053}, {"id": 87, "seek": 27620, "start": 301.0, "end": 302.8, "text": " To nie sama zgadywanka.", "tokens": [51604, 1407, 2838, 17768, 40948, 880, 86, 21729, 13, 51694], "temperature": 0.0, "avg_logprob": -0.08618932374766175, "compression_ratio": 1.4440298507462686, "no_speech_prob": 0.02070852369070053}, {"id": 88, "seek": 30280, "start": 303.0, "end": 306.8, "text": " To u\u015bwiadomienie sobie, \u017ce aby nauczy\u0107 maszyn\u0119 rozumie\u0107 j\u0119zyk,", "tokens": [50374, 1407, 344, 37750, 298, 27385, 13652, 11, 3561, 24457, 49103, 27150, 2300, 1229, 77, 1274, 48797, 414, 2162, 49055, 74, 11, 50564], "temperature": 0.0, "avg_logprob": -0.108110934127996, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.016633667051792145}, {"id": 89, "seek": 30280, "start": 307.0, "end": 309.40000000000003, "text": " nie trzeba jej uczy\u0107 gramatyki i regu\u0142.", "tokens": [50574, 2838, 25860, 28924, 344, 33967, 21353, 21398, 2984, 741, 1121, 84, 1221, 13, 50694], "temperature": 0.0, "avg_logprob": -0.108110934127996, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.016633667051792145}, {"id": 90, "seek": 30280, "start": 309.6, "end": 312.0, "text": " Trzeba j\u0105 zmusi\u0107 do budowania intuicji.", "tokens": [50704, 1765, 1381, 4231, 35692, 17020, 301, 12757, 360, 3265, 21308, 560, 84, 299, 4013, 13, 50824], "temperature": 0.0, "avg_logprob": -0.108110934127996, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.016633667051792145}, {"id": 91, "seek": 30280, "start": 312.2, "end": 312.8, "text": " Tak.", "tokens": [50834, 9118, 13, 50864], "temperature": 0.0, "avg_logprob": -0.108110934127996, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.016633667051792145}, {"id": 92, "seek": 30280, "start": 313.0, "end": 316.8, "text": " Ta prosta gra w zgadywania zmusza model do stworzenia wewn\u0119trznej,", "tokens": [50874, 6551, 582, 8638, 1295, 261, 40948, 880, 86, 5609, 17020, 301, 2394, 2316, 360, 342, 28321, 14320, 321, 895, 1274, 6903, 89, 11794, 11, 51064], "temperature": 0.0, "avg_logprob": -0.108110934127996, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.016633667051792145}, {"id": 93, "seek": 30280, "start": 317.0, "end": 320.40000000000003, "text": " niezwykle z\u0142o\u017conej mapy powi\u0105za\u0144 mi\u0119dzy s\u0142owami.", "tokens": [51074, 33511, 9726, 14677, 710, 5249, 1427, 546, 73, 4471, 88, 3388, 11404, 2394, 5248, 33964, 15116, 305, 4526, 13, 51244], "temperature": 0.0, "avg_logprob": -0.108110934127996, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.016633667051792145}, {"id": 94, "seek": 30280, "start": 320.6, "end": 324.40000000000003, "text": " To jest krok od bycia s\u0142ownikiem do bycia partnerem w rozmowie.", "tokens": [51254, 1407, 3492, 350, 31621, 3611, 538, 2755, 15116, 44895, 4907, 360, 538, 2755, 644, 77, 7333, 261, 35234, 13998, 13, 51444], "temperature": 0.0, "avg_logprob": -0.108110934127996, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.016633667051792145}, {"id": 95, "seek": 30280, "start": 324.6, "end": 330.6, "text": " To jest w\u0142a\u015bnie ta g\u0142\u0119boka dwukierunkowo\u015b\u0107, to Deep Be Directionality z tytu\u0142u artyku\u0142u.", "tokens": [51454, 1407, 3492, 14234, 1846, 18117, 1274, 65, 15289, 27379, 2034, 811, 3197, 19941, 7753, 11, 281, 14895, 879, 5822, 882, 1860, 710, 1104, 9179, 24066, 594, 874, 5279, 24066, 13, 51754], "temperature": 0.0, "avg_logprob": -0.108110934127996, "compression_ratio": 1.4932432432432432, "no_speech_prob": 0.016633667051792145}, {"id": 96, "seek": 33060, "start": 330.8, "end": 334.20000000000005, "text": " OK. Ten pomys\u0142 z maskowaniem jest genialny.", "tokens": [50374, 2264, 13, 9380, 12991, 39508, 710, 6094, 37345, 4907, 3492, 48228, 1634, 13, 50544], "temperature": 0.0, "avg_logprob": -0.12264989634029201, "compression_ratio": 1.414179104477612, "no_speech_prob": 0.05238362401723862}, {"id": 97, "seek": 33060, "start": 334.40000000000003, "end": 338.0, "text": " Ale czyta\u0142am w artykule, \u017ce autorzy puszli okrok dalej.", "tokens": [50554, 9366, 6430, 1328, 20177, 261, 594, 874, 74, 2271, 11, 3561, 19510, 1229, 280, 22378, 2081, 3133, 31621, 34257, 13, 50734], "temperature": 0.0, "avg_logprob": -0.12264989634029201, "compression_ratio": 1.414179104477612, "no_speech_prob": 0.05238362401723862}, {"id": 98, "seek": 33060, "start": 338.20000000000005, "end": 345.20000000000005, "text": " Obawiali si\u0119, \u017ce je\u015bli model b\u0119dzie trenowany tylko na zdaniach z tym specjalnym tokenem mask,", "tokens": [50744, 4075, 1607, 831, 72, 3244, 11, 3561, 25630, 2316, 10562, 23136, 23341, 13219, 1667, 16221, 3782, 608, 710, 8107, 46433, 12996, 14862, 443, 6094, 11, 51094], "temperature": 0.0, "avg_logprob": -0.12264989634029201, "compression_ratio": 1.414179104477612, "no_speech_prob": 0.05238362401723862}, {"id": 99, "seek": 33060, "start": 345.40000000000003, "end": 348.20000000000005, "text": " to potem w praktyce mo\u017ce sobie nie radzi\u0107.", "tokens": [51104, 281, 36513, 261, 3206, 74, 874, 384, 12034, 13652, 2838, 2843, 28496, 13, 51244], "temperature": 0.0, "avg_logprob": -0.12264989634029201, "compression_ratio": 1.414179104477612, "no_speech_prob": 0.05238362401723862}, {"id": 100, "seek": 33060, "start": 348.40000000000003, "end": 352.0, "text": " Dok\u0142adnie. W praktycznym u\u017cyciu, gdzie ten token nie wyst\u0119puje,", "tokens": [51254, 29768, 10358, 2766, 13, 343, 3206, 74, 874, 3689, 12996, 34097, 30795, 11, 18922, 2064, 14862, 2838, 48255, 18085, 13008, 11, 51434], "temperature": 0.0, "avg_logprob": -0.12264989634029201, "compression_ratio": 1.414179104477612, "no_speech_prob": 0.05238362401723862}, {"id": 101, "seek": 33060, "start": 352.20000000000005, "end": 356.40000000000003, "text": " pojawi si\u0119 niezgodno\u015b\u0107 mi\u0119dzy treningiem a faz\u0105 fine tuning.", "tokens": [51444, 30655, 72, 3244, 33511, 21787, 23293, 33964, 2192, 773, 4907, 257, 4375, 1611, 2489, 15164, 13, 51654], "temperature": 0.0, "avg_logprob": -0.12264989634029201, "compression_ratio": 1.414179104477612, "no_speech_prob": 0.05238362401723862}, {"id": 102, "seek": 35640, "start": 356.4, "end": 360.79999999999995, "text": " S\u0142oszna uwaga. I to pokazuje, jak bardzo przemy\u015blana by\u0142a ta architektura.", "tokens": [50364, 318, 1221, 329, 35458, 23147, 9286, 13, 286, 281, 13010, 43317, 11, 4207, 9034, 6541, 3633, 19212, 2095, 23936, 1846, 3912, 642, 2320, 2991, 13, 50584], "temperature": 0.0, "avg_logprob": -0.10978408420787138, "compression_ratio": 1.4646840148698885, "no_speech_prob": 0.42908987402915955}, {"id": 103, "seek": 35640, "start": 361.0, "end": 363.4, "text": " Rozwi\u0105zali to w bardzo sprytny spos\u00f3b.", "tokens": [50594, 43313, 22620, 5103, 281, 261, 9034, 637, 627, 83, 1634, 22904, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10978408420787138, "compression_ratio": 1.4646840148698885, "no_speech_prob": 0.42908987402915955}, {"id": 104, "seek": 35640, "start": 363.59999999999997, "end": 369.4, "text": " Z tych 15% s\u0142\u00f3w wybranych do zamaskowania nie wszystkie s\u0105 traktowane tak samo.", "tokens": [50724, 1176, 15180, 2119, 4, 15116, 3901, 4628, 1443, 34644, 360, 19876, 3863, 21308, 2838, 31723, 9015, 944, 2320, 23066, 991, 36422, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10978408420787138, "compression_ratio": 1.4646840148698885, "no_speech_prob": 0.42908987402915955}, {"id": 105, "seek": 35640, "start": 369.59999999999997, "end": 370.0, "text": " Jak to?", "tokens": [51024, 15029, 281, 30, 51044], "temperature": 0.0, "avg_logprob": -0.10978408420787138, "compression_ratio": 1.4646840148698885, "no_speech_prob": 0.42908987402915955}, {"id": 106, "seek": 35640, "start": 370.2, "end": 375.59999999999997, "text": " W 80% przypadk\u00f3w s\u0142owo jest faktycznie zast\u0119powane przez mask.", "tokens": [51054, 343, 4688, 4, 33100, 23849, 15116, 19941, 3492, 33647, 45586, 36746, 1274, 14701, 1929, 14064, 6094, 13, 51324], "temperature": 0.0, "avg_logprob": -0.10978408420787138, "compression_ratio": 1.4646840148698885, "no_speech_prob": 0.42908987402915955}, {"id": 107, "seek": 35640, "start": 375.79999999999995, "end": 377.0, "text": " Standardowa procedura.", "tokens": [51334, 21298, 5528, 6682, 2991, 13, 51394], "temperature": 0.0, "avg_logprob": -0.10978408420787138, "compression_ratio": 1.4646840148698885, "no_speech_prob": 0.42908987402915955}, {"id": 108, "seek": 35640, "start": 377.2, "end": 378.59999999999997, "text": " A pozosta\u0142e 20%?", "tokens": [51404, 316, 21281, 8638, 19827, 945, 4, 30, 51474], "temperature": 0.0, "avg_logprob": -0.10978408420787138, "compression_ratio": 1.4646840148698885, "no_speech_prob": 0.42908987402915955}, {"id": 109, "seek": 35640, "start": 378.79999999999995, "end": 379.79999999999995, "text": " I tu jest ciekawie.", "tokens": [51484, 286, 2604, 3492, 46419, 1607, 414, 13, 51534], "temperature": 0.0, "avg_logprob": -0.10978408420787138, "compression_ratio": 1.4646840148698885, "no_speech_prob": 0.42908987402915955}, {"id": 110, "seek": 35640, "start": 380.0, "end": 385.0, "text": " W 10% przypadk\u00f3w wybrane s\u0142owo jest zast\u0119powane innym,", "tokens": [51544, 343, 1266, 4, 33100, 23849, 4628, 1443, 1929, 15116, 19941, 3492, 36746, 1274, 14701, 1929, 294, 12996, 11, 51794], "temperature": 0.0, "avg_logprob": -0.10978408420787138, "compression_ratio": 1.4646840148698885, "no_speech_prob": 0.42908987402915955}, {"id": 111, "seek": 38500, "start": 385.0, "end": 387.0, "text": " kompletnie losowym s\u0142owem.", "tokens": [50364, 5207, 14657, 2766, 1750, 31691, 15116, 305, 443, 13, 50464], "temperature": 0.0, "avg_logprob": -0.12135106744900556, "compression_ratio": 1.4145454545454546, "no_speech_prob": 0.006058141123503447}, {"id": 112, "seek": 38500, "start": 387.2, "end": 387.6, "text": " Aha.", "tokens": [50474, 27448, 13, 50494], "temperature": 0.0, "avg_logprob": -0.12135106744900556, "compression_ratio": 1.4145454545454546, "no_speech_prob": 0.006058141123503447}, {"id": 113, "seek": 38500, "start": 387.8, "end": 391.6, "text": " A w ostatnich 10% nie zmienia si\u0119 nic.", "tokens": [50504, 316, 261, 32686, 77, 480, 1266, 4, 2838, 17020, 18811, 3244, 6201, 13, 50694], "temperature": 0.0, "avg_logprob": -0.12135106744900556, "compression_ratio": 1.4145454545454546, "no_speech_prob": 0.006058141123503447}, {"id": 114, "seek": 38500, "start": 391.8, "end": 394.4, "text": " S\u0142owo zostaje na swoim miejscu.", "tokens": [50704, 318, 1221, 19941, 31873, 11153, 1667, 13291, 332, 32754, 84, 13, 50834], "temperature": 0.0, "avg_logprob": -0.12135106744900556, "compression_ratio": 1.4145454545454546, "no_speech_prob": 0.006058141123503447}, {"id": 115, "seek": 38500, "start": 394.6, "end": 400.4, "text": " Chwila. To znaczy, \u017ce czasem zadaniem modelu jest przewidzie\u0107 s\u0142owo, kt\u00f3re ju\u017c widzi,", "tokens": [50844, 761, 86, 7371, 13, 1407, 36584, 11, 3561, 13190, 443, 710, 11338, 4907, 2316, 84, 3492, 39758, 327, 21214, 15116, 19941, 11, 8864, 10678, 5274, 3992, 11, 51134], "temperature": 0.0, "avg_logprob": -0.12135106744900556, "compression_ratio": 1.4145454545454546, "no_speech_prob": 0.006058141123503447}, {"id": 116, "seek": 38500, "start": 400.6, "end": 405.0, "text": " ale kt\u00f3re mo\u017ce by\u0107 b\u0142\u0119dem, to jak podrzucenie fa\u0142szywych trop\u00f3w w \u015bledztwie.", "tokens": [51144, 6775, 8864, 12034, 15069, 272, 1221, 6298, 443, 11, 281, 4207, 15305, 89, 1311, 268, 414, 2050, 1221, 7706, 9726, 339, 9006, 3901, 261, 8299, 1493, 2682, 8699, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12135106744900556, "compression_ratio": 1.4145454545454546, "no_speech_prob": 0.006058141123503447}, {"id": 117, "seek": 38500, "start": 405.2, "end": 411.4, "text": " Dok\u0142adnie. Dzi\u0119ki temu model uczy si\u0119 czego\u015b znacznie wa\u017cniejszego ni\u017c tylko wype\u0142nianie luk.", "tokens": [51374, 29768, 10358, 2766, 13, 413, 34546, 33346, 2316, 344, 6522, 3244, 36559, 1788, 15397, 14875, 2766, 27777, 10402, 15453, 6308, 28502, 13219, 4628, 31457, 77, 952, 414, 287, 2034, 13, 51684], "temperature": 0.0, "avg_logprob": -0.12135106744900556, "compression_ratio": 1.4145454545454546, "no_speech_prob": 0.006058141123503447}, {"id": 118, "seek": 38500, "start": 411.6, "end": 412.0, "text": " Czego?", "tokens": [51694, 383, 27725, 30, 51714], "temperature": 0.0, "avg_logprob": -0.12135106744900556, "compression_ratio": 1.4145454545454546, "no_speech_prob": 0.006058141123503447}, {"id": 119, "seek": 41200, "start": 412.2, "end": 415.2, "text": " Uczy si\u0119 nie ufa\u0107 \u015blepo danym wej\u015bciowym.", "tokens": [50374, 624, 6522, 3244, 2838, 344, 11771, 2162, 8299, 306, 2259, 274, 1325, 76, 321, 73, 6199, 31691, 13, 50524], "temperature": 0.0, "avg_logprob": -0.07955529139592098, "compression_ratio": 1.4363636363636363, "no_speech_prob": 0.016903990879654884}, {"id": 120, "seek": 41200, "start": 415.4, "end": 422.0, "text": " Musi oceni\u0107 ka\u017cde s\u0142owo w kontek\u015bcie ca\u0142ego zdania i zdecydowa\u0107, czy ono tam pasuje.", "tokens": [50534, 3569, 72, 10409, 268, 12757, 21912, 1479, 15116, 19941, 261, 14373, 916, 9815, 35224, 6308, 16221, 5609, 741, 49749, 1344, 67, 11445, 11, 6430, 322, 78, 7677, 1736, 13008, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07955529139592098, "compression_ratio": 1.4363636363636363, "no_speech_prob": 0.016903990879654884}, {"id": 121, "seek": 41200, "start": 422.2, "end": 428.2, "text": " To zmusza go do budowania znacznie bogatszej, bardziej odpornej na b\u0142\u0119dy reprezentacji j\u0119zyka.", "tokens": [50874, 1407, 17020, 301, 2394, 352, 360, 3265, 21308, 15397, 14875, 2766, 26132, 1720, 16920, 11, 27209, 3611, 2816, 11794, 1667, 272, 46564, 3173, 1085, 265, 14185, 13152, 42309, 40940, 13, 51174], "temperature": 0.0, "avg_logprob": -0.07955529139592098, "compression_ratio": 1.4363636363636363, "no_speech_prob": 0.016903990879654884}, {"id": 122, "seek": 41200, "start": 428.4, "end": 430.0, "text": " To jest naprawd\u0119 sprytne.", "tokens": [51184, 1407, 3492, 20970, 637, 627, 83, 716, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07955529139592098, "compression_ratio": 1.4363636363636363, "no_speech_prob": 0.016903990879654884}, {"id": 123, "seek": 41200, "start": 430.2, "end": 435.8, "text": " Uczy\u0107 modelu niuans\u00f3w zmuszaj\u0105c go do pracy detektywistycznej wewn\u0105trz zdania.", "tokens": [51274, 624, 33967, 2316, 84, 3867, 84, 599, 3901, 17020, 22378, 38757, 352, 360, 35591, 1141, 916, 874, 86, 468, 17466, 11794, 321, 895, 1611, 6903, 89, 16221, 5609, 13, 51554], "temperature": 0.0, "avg_logprob": -0.07955529139592098, "compression_ratio": 1.4363636363636363, "no_speech_prob": 0.016903990879654884}, {"id": 124, "seek": 41200, "start": 436.0, "end": 440.2, "text": " Ale j\u0119zyk to co\u015b wi\u0119cej ni\u017c zdania w izolacji.", "tokens": [51564, 9366, 49055, 74, 281, 19241, 26004, 28502, 16221, 5609, 261, 14736, 401, 13152, 13, 51774], "temperature": 0.0, "avg_logprob": -0.07955529139592098, "compression_ratio": 1.4363636363636363, "no_speech_prob": 0.016903990879654884}, {"id": 125, "seek": 44020, "start": 440.2, "end": 443.2, "text": " To ca\u0142e akapity, historie, dialogi.", "tokens": [50364, 1407, 47631, 9308, 569, 507, 11, 4058, 414, 11, 19308, 72, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11899027617081352, "compression_ratio": 1.4145454545454546, "no_speech_prob": 0.03586764261126518}, {"id": 126, "seek": 44020, "start": 443.4, "end": 449.4, "text": " No w\u0142a\u015bnie. Czy sam ten mechanizm MLM wystarczy\u0142, \u017ceby Bert rozumia\u0142 na przyk\u0142ad ironi\u0119 w rozmowie,", "tokens": [50524, 883, 14234, 13, 19832, 3247, 2064, 4236, 590, 76, 21601, 44, 4628, 9710, 6522, 1221, 11, 11316, 29594, 48797, 8908, 1667, 23144, 6497, 5034, 261, 35234, 13998, 11, 50824], "temperature": 0.0, "avg_logprob": -0.11899027617081352, "compression_ratio": 1.4145454545454546, "no_speech_prob": 0.03586764261126518}, {"id": 127, "seek": 44020, "start": 449.59999999999997, "end": 451.4, "text": " albo logiczny ci\u0105g argument\u00f3w?", "tokens": [50834, 22622, 9952, 89, 1634, 42398, 70, 6770, 3901, 30, 50924], "temperature": 0.0, "avg_logprob": -0.11899027617081352, "compression_ratio": 1.4145454545454546, "no_speech_prob": 0.03586764261126518}, {"id": 128, "seek": 44020, "start": 451.59999999999997, "end": 453.0, "text": " \u015awietne pytanie.", "tokens": [50934, 27933, 39083, 716, 36610, 13, 51004], "temperature": 0.0, "avg_logprob": -0.11899027617081352, "compression_ratio": 1.4145454545454546, "no_speech_prob": 0.03586764261126518}, {"id": 129, "seek": 44020, "start": 453.2, "end": 456.8, "text": " I odpowied\u017a brzmi nie. Nie wystarczy\u0142.", "tokens": [51014, 286, 36574, 10659, 738, 89, 3057, 2838, 13, 12016, 4628, 9710, 6522, 1221, 13, 51194], "temperature": 0.0, "avg_logprob": -0.11899027617081352, "compression_ratio": 1.4145454545454546, "no_speech_prob": 0.03586764261126518}, {"id": 130, "seek": 44020, "start": 457.0, "end": 462.0, "text": " MLM da\u0142 Bertowi niesamowit\u0105 zdolno\u015b\u0107 rozumienia relacji wewn\u0105trz zda\u0144.", "tokens": [51204, 21601, 44, 1120, 1221, 29594, 24503, 48100, 335, 305, 270, 1611, 16221, 401, 23293, 48797, 18811, 1039, 13152, 321, 895, 1611, 6903, 89, 710, 2675, 5248, 13, 51454], "temperature": 0.0, "avg_logprob": -0.11899027617081352, "compression_ratio": 1.4145454545454546, "no_speech_prob": 0.03586764261126518}, {"id": 131, "seek": 44020, "start": 462.2, "end": 465.4, "text": " Ale \u017ceby rozumie\u0107 d\u0142u\u017csze formy potrzebny by\u0142 drugi filar.", "tokens": [51464, 9366, 11316, 48797, 414, 2162, 274, 24066, 1427, 82, 1381, 1254, 88, 37595, 1634, 16673, 4110, 72, 1387, 289, 13, 51624], "temperature": 0.0, "avg_logprob": -0.11899027617081352, "compression_ratio": 1.4145454545454546, "no_speech_prob": 0.03586764261126518}, {"id": 132, "seek": 44020, "start": 465.59999999999997, "end": 466.8, "text": " Drugi filar?", "tokens": [51634, 2491, 24780, 1387, 289, 30, 51694], "temperature": 0.0, "avg_logprob": -0.11899027617081352, "compression_ratio": 1.4145454545454546, "no_speech_prob": 0.03586764261126518}, {"id": 133, "seek": 46680, "start": 467.2, "end": 471.8, "text": " Autorzy zdawali sobie spraw\u0119, \u017ce wiele zada\u0144, jak question answering czy analiza logiczna,", "tokens": [50384, 6049, 284, 1229, 16221, 1607, 5103, 13652, 22734, 1274, 11, 3561, 33137, 710, 1538, 5248, 11, 4207, 1168, 13430, 6430, 2624, 13427, 9952, 35458, 11, 50614], "temperature": 0.0, "avg_logprob": -0.13702787970104358, "compression_ratio": 1.410344827586207, "no_speech_prob": 0.018956152722239494}, {"id": 134, "seek": 46680, "start": 472.0, "end": 474.8, "text": " wymaga rozumienia zwi\u0105zk\u00f3w pomi\u0119dzy zdaniami.", "tokens": [50624, 29764, 9286, 48797, 18811, 27741, 23849, 12991, 49485, 710, 10312, 15568, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13702787970104358, "compression_ratio": 1.410344827586207, "no_speech_prob": 0.018956152722239494}, {"id": 135, "seek": 46680, "start": 475.0, "end": 477.40000000000003, "text": " Dlatego wprowadzili drugie zadanie treningowe.", "tokens": [50774, 47184, 46733, 89, 2312, 4110, 414, 42788, 7155, 2192, 773, 6880, 13, 50894], "temperature": 0.0, "avg_logprob": -0.13702787970104358, "compression_ratio": 1.410344827586207, "no_speech_prob": 0.018956152722239494}, {"id": 136, "seek": 46680, "start": 477.6, "end": 481.2, "text": " Next sentence prediction w skr\u00f3cie NSP.", "tokens": [50904, 3087, 8174, 17630, 261, 1110, 11721, 4260, 15943, 47, 13, 51084], "temperature": 0.0, "avg_logprob": -0.13702787970104358, "compression_ratio": 1.410344827586207, "no_speech_prob": 0.018956152722239494}, {"id": 137, "seek": 46680, "start": 481.40000000000003, "end": 483.6, "text": " Czyli przewidywanie nast\u0119pnego zdania.", "tokens": [51094, 37099, 39758, 38836, 86, 7155, 39662, 11858, 16221, 5609, 13, 51204], "temperature": 0.0, "avg_logprob": -0.13702787970104358, "compression_ratio": 1.410344827586207, "no_speech_prob": 0.018956152722239494}, {"id": 138, "seek": 46680, "start": 483.8, "end": 485.40000000000003, "text": " Jak to dzia\u0142a\u0142o w praktyce?", "tokens": [51214, 15029, 281, 37903, 5249, 261, 3206, 74, 874, 384, 30, 51294], "temperature": 0.0, "avg_logprob": -0.13702787970104358, "compression_ratio": 1.410344827586207, "no_speech_prob": 0.018956152722239494}, {"id": 139, "seek": 46680, "start": 485.6, "end": 487.40000000000003, "text": " Dawali mu po prostu dwa zdania.", "tokens": [51304, 28407, 5103, 2992, 714, 19518, 35045, 16221, 5609, 13, 51394], "temperature": 0.0, "avg_logprob": -0.13702787970104358, "compression_ratio": 1.410344827586207, "no_speech_prob": 0.018956152722239494}, {"id": 140, "seek": 46680, "start": 487.6, "end": 488.40000000000003, "text": " Dok\u0142adnie.", "tokens": [51404, 29768, 10358, 2766, 13, 51444], "temperature": 0.0, "avg_logprob": -0.13702787970104358, "compression_ratio": 1.410344827586207, "no_speech_prob": 0.018956152722239494}, {"id": 141, "seek": 46680, "start": 488.6, "end": 492.0, "text": " Model dostaje par\u0119 zda\u0144, nazwijmy je A i B.", "tokens": [51454, 17105, 20568, 11153, 971, 1274, 710, 2675, 5248, 11, 20151, 36652, 2226, 1506, 316, 741, 363, 13, 51624], "temperature": 0.0, "avg_logprob": -0.13702787970104358, "compression_ratio": 1.410344827586207, "no_speech_prob": 0.018956152722239494}, {"id": 142, "seek": 46680, "start": 492.2, "end": 493.2, "text": " I teraz najlepsze.", "tokens": [51634, 286, 16854, 41903, 1878, 1381, 13, 51684], "temperature": 0.0, "avg_logprob": -0.13702787970104358, "compression_ratio": 1.410344827586207, "no_speech_prob": 0.018956152722239494}, {"id": 143, "seek": 49320, "start": 493.2, "end": 498.4, "text": " W po\u0142owie przypadk\u00f3w zdanie B to faktycznie to, kt\u00f3re nast\u0119powa\u0142o po A w oryginalnym tek\u015bcie.", "tokens": [50364, 343, 714, 1221, 13998, 33100, 23849, 16221, 7155, 363, 281, 33647, 45586, 281, 11, 8864, 39662, 5528, 5249, 714, 316, 261, 420, 88, 1494, 304, 12996, 16624, 9815, 13, 50624], "temperature": 0.0, "avg_logprob": -0.1059977235020818, "compression_ratio": 1.4612794612794613, "no_speech_prob": 0.010214352048933506}, {"id": 144, "seek": 49320, "start": 498.59999999999997, "end": 501.8, "text": " A w drugiej po\u0142owie to jest jakie\u015b zupe\u0142nie losowe zdanie,", "tokens": [50634, 316, 261, 47373, 714, 1221, 13998, 281, 3492, 31163, 49922, 1750, 6880, 16221, 7155, 11, 50794], "temperature": 0.0, "avg_logprob": -0.1059977235020818, "compression_ratio": 1.4612794612794613, "no_speech_prob": 0.010214352048933506}, {"id": 145, "seek": 49320, "start": 502.0, "end": 504.59999999999997, "text": " wyrwane z kontekstu z innego miejsca w internecie.", "tokens": [50804, 4628, 81, 86, 1929, 710, 14373, 916, 372, 84, 710, 294, 11858, 18522, 44239, 261, 728, 716, 4260, 13, 50934], "temperature": 0.0, "avg_logprob": -0.1059977235020818, "compression_ratio": 1.4612794612794613, "no_speech_prob": 0.010214352048933506}, {"id": 146, "seek": 49320, "start": 504.8, "end": 505.59999999999997, "text": " W\u0142a\u015bnie tak.", "tokens": [50944, 343, 5024, 12221, 991, 13, 50984], "temperature": 0.0, "avg_logprob": -0.1059977235020818, "compression_ratio": 1.4612794612794613, "no_speech_prob": 0.010214352048933506}, {"id": 147, "seek": 49320, "start": 505.8, "end": 509.4, "text": " I model musi odpowiedzie\u0107 na proste, binarne pytanie.", "tokens": [50994, 286, 2316, 37587, 24314, 22078, 1667, 10293, 68, 11, 5171, 289, 716, 36610, 13, 51174], "temperature": 0.0, "avg_logprob": -0.1059977235020818, "compression_ratio": 1.4612794612794613, "no_speech_prob": 0.010214352048933506}, {"id": 148, "seek": 49320, "start": 509.59999999999997, "end": 512.0, "text": " Czy ta para zda\u0144 ma sens?", "tokens": [51184, 19832, 1846, 1690, 710, 2675, 5248, 463, 2923, 30, 51304], "temperature": 0.0, "avg_logprob": -0.1059977235020818, "compression_ratio": 1.4612794612794613, "no_speech_prob": 0.010214352048933506}, {"id": 149, "seek": 49320, "start": 512.2, "end": 514.8, "text": " Czy B jest logiczn\u0105 kontynuacj\u0105 A?", "tokens": [51314, 19832, 363, 3492, 9952, 89, 13113, 5897, 874, 16241, 326, 8555, 316, 30, 51444], "temperature": 0.0, "avg_logprob": -0.1059977235020818, "compression_ratio": 1.4612794612794613, "no_speech_prob": 0.010214352048933506}, {"id": 150, "seek": 49320, "start": 515.0, "end": 519.4, "text": " Czyli uczy si\u0119 tego niewidzialnego kleju, kt\u00f3ry \u0142\u0105czym my\u015bli i wsp\u00f3jno ca\u0142o\u015b\u0107.", "tokens": [51454, 37099, 344, 6522, 3244, 8627, 43622, 327, 17787, 11858, 9318, 8954, 11, 9913, 220, 15926, 6522, 76, 452, 15350, 741, 17757, 18999, 1771, 1335, 44742, 13, 51674], "temperature": 0.0, "avg_logprob": -0.1059977235020818, "compression_ratio": 1.4612794612794613, "no_speech_prob": 0.010214352048933506}, {"id": 151, "seek": 51940, "start": 519.6, "end": 520.4, "text": " Dok\u0142adnie.", "tokens": [50374, 29768, 10358, 2766, 13, 50414], "temperature": 0.0, "avg_logprob": -0.09999451571947908, "compression_ratio": 1.4186851211072664, "no_speech_prob": 0.008293797262012959}, {"id": 152, "seek": 51940, "start": 520.6, "end": 524.6, "text": " Uczy si\u0119 \u015bledzi\u0107 narracj\u0119, rozumie\u0107 przyczyn\u0119 i skutek,", "tokens": [50424, 624, 6522, 3244, 8299, 1493, 28496, 6397, 29924, 11, 48797, 414, 2162, 6501, 6522, 77, 1274, 741, 1110, 325, 916, 11, 50624], "temperature": 0.0, "avg_logprob": -0.09999451571947908, "compression_ratio": 1.4186851211072664, "no_speech_prob": 0.008293797262012959}, {"id": 153, "seek": 51940, "start": 524.8, "end": 527.4, "text": " rozpoznawa\u0107, kiedy rozmowa z Bacza Storu.", "tokens": [50634, 9544, 2259, 35458, 25234, 11, 18777, 35234, 5528, 710, 363, 326, 2394, 745, 32963, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09999451571947908, "compression_ratio": 1.4186851211072664, "no_speech_prob": 0.008293797262012959}, {"id": 154, "seek": 51940, "start": 527.6, "end": 533.8, "text": " Mo\u017cna powiedzie\u0107, \u017ce MMM to by\u0142a nauka anatomii s\u0142\u00f3w, a NSP to nauka logiki konwersacji.", "tokens": [50774, 44736, 629, 27886, 11, 3561, 376, 17365, 281, 23936, 35616, 2330, 21618, 298, 5597, 15116, 3901, 11, 257, 15943, 47, 281, 35616, 2330, 3565, 9850, 5897, 5364, 13152, 13, 51084], "temperature": 0.0, "avg_logprob": -0.09999451571947908, "compression_ratio": 1.4186851211072664, "no_speech_prob": 0.008293797262012959}, {"id": 155, "seek": 51940, "start": 534.0, "end": 535.6, "text": " Co to wszystko znacza w praktyce?", "tokens": [51094, 3066, 281, 22607, 15397, 326, 2394, 261, 3206, 74, 874, 384, 30, 51174], "temperature": 0.0, "avg_logprob": -0.09999451571947908, "compression_ratio": 1.4186851211072664, "no_speech_prob": 0.008293797262012959}, {"id": 156, "seek": 51940, "start": 535.8, "end": 540.8, "text": " Mamy wi\u0119c dwa genialne pomys\u0142y treningowe, ale papier to jedno, a rzeczywisto\u015b\u0107 drugie.", "tokens": [51184, 376, 7804, 16677, 35045, 48228, 716, 12991, 749, 6825, 2192, 773, 6880, 11, 6775, 37410, 281, 5232, 1771, 11, 257, 26297, 86, 9334, 7753, 4110, 414, 13, 51434], "temperature": 0.0, "avg_logprob": -0.09999451571947908, "compression_ratio": 1.4186851211072664, "no_speech_prob": 0.008293797262012959}, {"id": 157, "seek": 51940, "start": 541.0, "end": 543.0, "text": " Czy to faktycznie zadzia\u0142a\u0142o tak dobrze?", "tokens": [51444, 19832, 281, 33647, 45586, 42788, 89, 25605, 5249, 991, 28335, 30, 51544], "temperature": 0.0, "avg_logprob": -0.09999451571947908, "compression_ratio": 1.4186851211072664, "no_speech_prob": 0.008293797262012959}, {"id": 158, "seek": 51940, "start": 543.1999999999999, "end": 545.0, "text": " Dobrze, to ma\u0142o powiedziane.", "tokens": [51554, 29679, 13503, 11, 281, 463, 5249, 27617, 21133, 13, 51644], "temperature": 0.0, "avg_logprob": -0.09999451571947908, "compression_ratio": 1.4186851211072664, "no_speech_prob": 0.008293797262012959}, {"id": 159, "seek": 54500, "start": 545.2, "end": 550.0, "text": " Kiedy opublikowali wyniki w \u015brodowisku NLP, no opad\u0142y szcz\u0119ki.", "tokens": [50374, 591, 16446, 999, 48620, 305, 5103, 31936, 9850, 261, 28580, 305, 271, 5279, 426, 45196, 11, 572, 999, 345, 6825, 22090, 1274, 2984, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12438193345681214, "compression_ratio": 1.389261744966443, "no_speech_prob": 0.4091433584690094}, {"id": 160, "seek": 54500, "start": 550.2, "end": 552.6, "text": " Bert po prostu wszed\u0142 i pozamiata\u0142.", "tokens": [50624, 29594, 714, 19518, 37647, 11312, 1221, 741, 21281, 4526, 3274, 1221, 13, 50744], "temperature": 0.0, "avg_logprob": -0.12438193345681214, "compression_ratio": 1.389261744966443, "no_speech_prob": 0.4091433584690094}, {"id": 161, "seek": 54500, "start": 552.8, "end": 553.4, "text": " A\u017c tak?", "tokens": [50754, 316, 1427, 991, 30, 50784], "temperature": 0.0, "avg_logprob": -0.12438193345681214, "compression_ratio": 1.389261744966443, "no_speech_prob": 0.4091433584690094}, {"id": 162, "seek": 54500, "start": 553.6, "end": 558.2, "text": " Ustanowi\u0142 nowe rekordy, czyli State of the Art, w 11 r\u00f3\u017cnych zadaniach.", "tokens": [50794, 624, 18758, 24503, 1221, 586, 68, 33881, 765, 88, 11, 16591, 4533, 295, 264, 5735, 11, 261, 2975, 42602, 42788, 3782, 608, 13, 51024], "temperature": 0.0, "avg_logprob": -0.12438193345681214, "compression_ratio": 1.389261744966443, "no_speech_prob": 0.4091433584690094}, {"id": 163, "seek": 54500, "start": 558.4, "end": 560.8, "text": " I nie m\u00f3wimy tu o kosmetycznych poprawkach.", "tokens": [51034, 286, 2838, 13489, 13189, 2604, 277, 19532, 76, 2210, 3689, 9399, 1665, 5131, 41326, 13, 51154], "temperature": 0.0, "avg_logprob": -0.12438193345681214, "compression_ratio": 1.389261744966443, "no_speech_prob": 0.4091433584690094}, {"id": 164, "seek": 54500, "start": 561.0, "end": 565.2, "text": " Na przyk\u0142ad w GLU, to taki olimpijski wielob\u00f3j dla modeli j\u0119zykowych.", "tokens": [51164, 6056, 23144, 261, 16225, 52, 11, 281, 20065, 2545, 8814, 1718, 18020, 20570, 996, 18999, 12285, 2316, 72, 49055, 74, 19605, 13, 51374], "temperature": 0.0, "avg_logprob": -0.12438193345681214, "compression_ratio": 1.389261744966443, "no_speech_prob": 0.4091433584690094}, {"id": 165, "seek": 54500, "start": 565.4, "end": 568.8, "text": " Bert nie pobi\u0142 rekordu o w\u0142os, on go zmia\u017cczy\u0142.", "tokens": [51384, 29594, 2838, 714, 5614, 1221, 33881, 28655, 277, 34696, 329, 11, 322, 352, 17020, 654, 1427, 6522, 1221, 13, 51554], "temperature": 0.0, "avg_logprob": -0.12438193345681214, "compression_ratio": 1.389261744966443, "no_speech_prob": 0.4091433584690094}, {"id": 166, "seek": 54500, "start": 569.0, "end": 569.4, "text": " O ile?", "tokens": [51564, 422, 15465, 30, 51584], "temperature": 0.0, "avg_logprob": -0.12438193345681214, "compression_ratio": 1.389261744966443, "no_speech_prob": 0.4091433584690094}, {"id": 167, "seek": 54500, "start": 569.6, "end": 572.6, "text": " Poprawi\u0142 wynik o prawie 8 punkt\u00f3w procentowych.", "tokens": [51594, 10215, 5131, 40622, 31936, 1035, 277, 3206, 8699, 1649, 39561, 3901, 38826, 19605, 13, 51744], "temperature": 0.0, "avg_logprob": -0.12438193345681214, "compression_ratio": 1.389261744966443, "no_speech_prob": 0.4091433584690094}, {"id": 168, "seek": 57260, "start": 572.8000000000001, "end": 577.4, "text": " W tym \u015bwiecie to jest jak przebiec setk\u0119 o sekund\u0119 szybciej od poprzedniego rekordzisty.", "tokens": [50374, 343, 8107, 40078, 4260, 281, 3492, 4207, 8325, 7392, 66, 992, 15724, 277, 17215, 997, 1274, 36456, 4260, 73, 3611, 1665, 81, 11312, 2766, 1571, 33881, 765, 89, 38618, 13, 50604], "temperature": 0.0, "avg_logprob": -0.09680592936854208, "compression_ratio": 1.4651898734177216, "no_speech_prob": 0.0037546479143202305}, {"id": 169, "seek": 57260, "start": 577.6, "end": 580.2, "text": " Wszyscy wiedzieli, \u017ce to pocz\u0105tek nowej ery.", "tokens": [50614, 343, 15453, 38966, 261, 15338, 23099, 11, 3561, 281, 34397, 916, 586, 40779, 1189, 88, 13, 50744], "temperature": 0.0, "avg_logprob": -0.09680592936854208, "compression_ratio": 1.4651898734177216, "no_speech_prob": 0.0037546479143202305}, {"id": 170, "seek": 57260, "start": 580.4, "end": 584.6, "text": " A co z zadaniami, kt\u00f3re by\u0142y pi\u0119t\u0105 achillesow\u0105 z starych modeli?", "tokens": [50754, 316, 598, 710, 710, 11338, 15568, 11, 8864, 26366, 32677, 83, 1611, 2800, 14835, 30297, 710, 342, 822, 339, 2316, 72, 30, 50964], "temperature": 0.0, "avg_logprob": -0.09680592936854208, "compression_ratio": 1.4651898734177216, "no_speech_prob": 0.0037546479143202305}, {"id": 171, "seek": 57260, "start": 584.8000000000001, "end": 591.2, "text": " Na przyk\u0142ad to odpowiadanie na pytania, gdzie pe\u0142ny dwukierunkowy kontekst jest absolutnie kluczowy.", "tokens": [50974, 6056, 23144, 281, 24314, 38069, 7155, 1667, 25878, 5609, 11, 18922, 43205, 1634, 27379, 2034, 811, 3197, 10089, 14373, 916, 372, 3492, 18757, 2766, 9671, 1311, 89, 10089, 13, 51294], "temperature": 0.0, "avg_logprob": -0.09680592936854208, "compression_ratio": 1.4651898734177216, "no_speech_prob": 0.0037546479143202305}, {"id": 172, "seek": 57260, "start": 591.4, "end": 594.0, "text": " Tam efekty by\u0142y jeszcze bardziej dramatyczne.", "tokens": [51304, 8540, 31482, 916, 874, 26366, 14168, 27209, 42749, 17466, 716, 13, 51434], "temperature": 0.0, "avg_logprob": -0.09680592936854208, "compression_ratio": 1.4651898734177216, "no_speech_prob": 0.0037546479143202305}, {"id": 173, "seek": 57260, "start": 594.2, "end": 600.0, "text": " W popularnym te\u015bcie sk\u0142od, gdzie model musi przeczyta\u0107 akapic wikipedii i znale\u017a\u0107 w nim odpowied\u017a,", "tokens": [51444, 343, 3743, 12996, 535, 9815, 1110, 1221, 378, 11, 18922, 2316, 37587, 8325, 6522, 42931, 9308, 569, 299, 261, 1035, 647, 292, 5597, 741, 15397, 1220, 10659, 2162, 261, 24887, 36574, 10659, 11, 51734], "temperature": 0.0, "avg_logprob": -0.09680592936854208, "compression_ratio": 1.4651898734177216, "no_speech_prob": 0.0037546479143202305}, {"id": 174, "seek": 60000, "start": 600.2, "end": 603.2, "text": " Bert by\u0142 pierwszym systemem, kt\u00f3ry pobi\u0142 ludzk\u0105 wydajno\u015b\u0107.", "tokens": [50374, 29594, 16673, 34016, 76, 1185, 443, 11, 9913, 714, 5614, 1221, 15946, 89, 26304, 25984, 1805, 23293, 13, 50524], "temperature": 0.0, "avg_logprob": -0.09293482984815325, "compression_ratio": 1.4169096209912537, "no_speech_prob": 0.01064077578485012}, {"id": 175, "seek": 60000, "start": 603.4, "end": 604.2, "text": " Niesamowite.", "tokens": [50534, 426, 530, 335, 305, 642, 13, 50574], "temperature": 0.0, "avg_logprob": -0.09293482984815325, "compression_ratio": 1.4169096209912537, "no_speech_prob": 0.01064077578485012}, {"id": 176, "seek": 60000, "start": 604.4, "end": 609.4, "text": " A w trudniejszej wersji sk\u0142od 2.0, gdzie niekt\u00f3re pytania celowo nie maj\u0105 odpowiedzi w tek\u015bcie,", "tokens": [50584, 316, 261, 32007, 30295, 16920, 261, 433, 4013, 1110, 1221, 378, 568, 13, 15, 11, 18922, 2838, 43073, 265, 25878, 5609, 9277, 19941, 2838, 26064, 36574, 3992, 261, 16624, 9815, 11, 50834], "temperature": 0.0, "avg_logprob": -0.09293482984815325, "compression_ratio": 1.4169096209912537, "no_speech_prob": 0.01064077578485012}, {"id": 177, "seek": 60000, "start": 609.6, "end": 614.4, "text": " Bert zdeklasowa\u0142 konkurencj\u0119, poprawiaj\u0105c wynik a\u017c o pi\u0119\u0107 punkt\u00f3w F1.", "tokens": [50844, 29594, 49749, 74, 7743, 30105, 21428, 9873, 41960, 11, 1665, 5131, 48125, 66, 31936, 1035, 48134, 277, 32677, 2162, 39561, 3901, 479, 16, 13, 51084], "temperature": 0.0, "avg_logprob": -0.09293482984815325, "compression_ratio": 1.4169096209912537, "no_speech_prob": 0.01064077578485012}, {"id": 178, "seek": 60000, "start": 614.6, "end": 615.6, "text": " To by\u0142 knockout.", "tokens": [51094, 1407, 16673, 6728, 346, 13, 51144], "temperature": 0.0, "avg_logprob": -0.09293482984815325, "compression_ratio": 1.4169096209912537, "no_speech_prob": 0.01064077578485012}, {"id": 179, "seek": 60000, "start": 615.8, "end": 618.4, "text": " To faktycznie brzmi jak prze\u0142om.", "tokens": [51154, 1407, 33647, 45586, 738, 89, 3057, 4207, 8325, 1221, 298, 13, 51284], "temperature": 0.0, "avg_logprob": -0.09293482984815325, "compression_ratio": 1.4169096209912537, "no_speech_prob": 0.01064077578485012}, {"id": 180, "seek": 60000, "start": 618.6, "end": 620.4, "text": " Ale jest jeszcze jedna kwestia.", "tokens": [51294, 9366, 3492, 14168, 5232, 629, 42035, 654, 13, 51384], "temperature": 0.0, "avg_logprob": -0.09293482984815325, "compression_ratio": 1.4169096209912537, "no_speech_prob": 0.01064077578485012}, {"id": 181, "seek": 60000, "start": 620.6, "end": 625.2, "text": " Trening takich potwor\u00f3w musi by\u0107 niewyobra\u017calnie drogi i czasoch\u0142onny.", "tokens": [51394, 8648, 773, 29607, 1847, 28321, 3901, 37587, 15069, 43622, 88, 24393, 1427, 304, 2766, 3789, 7834, 741, 13190, 8997, 1221, 266, 1634, 13, 51624], "temperature": 0.0, "avg_logprob": -0.09293482984815325, "compression_ratio": 1.4169096209912537, "no_speech_prob": 0.01064077578485012}, {"id": 182, "seek": 60000, "start": 625.4, "end": 629.2, "text": " Czy to oznacza\u0142o, \u017ce tylko giganci jak Google mogli z tego korzysta\u0107?", "tokens": [51634, 19832, 281, 277, 22672, 326, 2394, 5249, 11, 3561, 13219, 8741, 38840, 4207, 3329, 13172, 2081, 710, 8627, 14784, 49590, 2162, 30, 51824], "temperature": 0.0, "avg_logprob": -0.09293482984815325, "compression_ratio": 1.4169096209912537, "no_speech_prob": 0.01064077578485012}, {"id": 183, "seek": 62920, "start": 629.4000000000001, "end": 634.4000000000001, "text": " I tu dochodzimy do drugiego genialnego aspektu tej pracy, kt\u00f3ry zdemokratyzowa\u0142 ca\u0142\u0105 dziedzin\u0119.", "tokens": [50374, 286, 2604, 9243, 378, 89, 13189, 360, 4110, 12200, 48228, 11858, 382, 23533, 84, 12573, 35591, 11, 9913, 710, 10730, 19795, 37433, 30105, 1335, 15926, 9758, 15338, 259, 1274, 13, 50624], "temperature": 0.0, "avg_logprob": -0.09640639788144595, "compression_ratio": 1.4262295081967213, "no_speech_prob": 0.0014248767402023077}, {"id": 184, "seek": 62920, "start": 634.6, "end": 637.4000000000001, "text": " Proces u\u017cywania Berta sk\u0142ada si\u0119 z dw\u00f3ch etap\u00f3w.", "tokens": [50634, 1705, 887, 34097, 86, 5609, 29594, 64, 1110, 46217, 3244, 710, 27379, 812, 339, 47634, 3901, 13, 50774], "temperature": 0.0, "avg_logprob": -0.09640639788144595, "compression_ratio": 1.4262295081967213, "no_speech_prob": 0.0014248767402023077}, {"id": 185, "seek": 62920, "start": 637.6, "end": 639.4000000000001, "text": " Pierwszy to pre-training.", "tokens": [50784, 16676, 30012, 281, 659, 12, 17227, 1760, 13, 50874], "temperature": 0.0, "avg_logprob": -0.09640639788144595, "compression_ratio": 1.4262295081967213, "no_speech_prob": 0.0014248767402023077}, {"id": 186, "seek": 62920, "start": 639.6, "end": 645.6, "text": " Otwornie drogi i d\u0142ugi proces, w kt\u00f3rym Google na swoich superkomputerach trenowa\u0142o model", "tokens": [50884, 12936, 86, 1865, 414, 3789, 7834, 741, 44042, 24780, 17565, 11, 261, 30120, 3329, 1667, 13291, 480, 1687, 20557, 13849, 608, 23136, 5528, 5249, 2316, 51184], "temperature": 0.0, "avg_logprob": -0.09640639788144595, "compression_ratio": 1.4262295081967213, "no_speech_prob": 0.0014248767402023077}, {"id": 187, "seek": 62920, "start": 645.8000000000001, "end": 649.8000000000001, "text": " na gigantycznym zbiorze danych ca\u0142ej angielskiej wikipedii i tysi\u0105cach ksi\u0105\u017cek.", "tokens": [51194, 1667, 8741, 394, 17466, 12996, 710, 33362, 1381, 274, 34644, 47631, 73, 2562, 1187, 5161, 7764, 261, 1035, 647, 292, 5597, 741, 38156, 11404, 66, 608, 39311, 916, 13, 51394], "temperature": 0.0, "avg_logprob": -0.09640639788144595, "compression_ratio": 1.4262295081967213, "no_speech_prob": 0.0014248767402023077}, {"id": 188, "seek": 62920, "start": 650.0, "end": 653.6, "text": " Tak, budowali ten uniwersalny, wszechwiedz\u0105cy m\u00f3zg j\u0119zykowy.", "tokens": [51404, 9118, 11, 3265, 305, 5103, 2064, 36435, 5364, 304, 1634, 11, 37647, 19439, 86, 1091, 8925, 1344, 32515, 89, 70, 49055, 74, 10089, 13, 51584], "temperature": 0.0, "avg_logprob": -0.09640639788144595, "compression_ratio": 1.4262295081967213, "no_speech_prob": 0.0014248767402023077}, {"id": 189, "seek": 62920, "start": 653.8000000000001, "end": 654.8000000000001, "text": " A drugi etap.", "tokens": [51594, 316, 4110, 72, 47634, 13, 51644], "temperature": 0.0, "avg_logprob": -0.09640639788144595, "compression_ratio": 1.4262295081967213, "no_speech_prob": 0.0014248767402023077}, {"id": 190, "seek": 65480, "start": 654.8, "end": 657.8, "text": " A potem nast\u0119puje drugi etap, czyli Fine Tuning.", "tokens": [50364, 316, 36513, 39662, 13008, 4110, 72, 47634, 11, 16591, 12024, 21363, 278, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10366830708068094, "compression_ratio": 1.4606060606060607, "no_speech_prob": 0.009671802632510662}, {"id": 191, "seek": 65480, "start": 658.0, "end": 659.5999999999999, "text": " I to jest czysta magia.", "tokens": [50524, 286, 281, 3492, 6430, 9140, 2258, 654, 13, 50604], "temperature": 0.0, "avg_logprob": -0.10366830708068094, "compression_ratio": 1.4606060606060607, "no_speech_prob": 0.009671802632510662}, {"id": 192, "seek": 65480, "start": 659.8, "end": 665.4, "text": " Wyobra\u017a sobie, \u017ce Google sp\u0119dzi\u0142o lata i miliony dolar\u00f3w, ucz\u0105c kogo\u015b biegle wszystkich j\u0119zyk\u00f3w \u015bwiata.", "tokens": [50614, 14458, 24393, 10659, 13652, 11, 3561, 3329, 637, 6298, 3992, 5249, 46722, 741, 1962, 46184, 360, 2200, 3901, 11, 35403, 1611, 66, 350, 23515, 1788, 272, 20408, 306, 34234, 49055, 23849, 21485, 3274, 13, 50894], "temperature": 0.0, "avg_logprob": -0.10366830708068094, "compression_ratio": 1.4606060606060607, "no_speech_prob": 0.009671802632510662}, {"id": 193, "seek": 65480, "start": 665.5999999999999, "end": 667.1999999999999, "text": " To jest gotowy model Bert.", "tokens": [50904, 1407, 3492, 658, 10089, 2316, 29594, 13, 50984], "temperature": 0.0, "avg_logprob": -0.10366830708068094, "compression_ratio": 1.4606060606060607, "no_speech_prob": 0.009671802632510662}, {"id": 194, "seek": 65480, "start": 667.4, "end": 673.4, "text": " A ty teraz podchodzisz do tego poligloty, dajesz mu kilkaset przyk\u0142ad\u00f3w recenzji filmowych i m\u00f3wisz.", "tokens": [50994, 316, 1104, 16854, 2497, 29914, 89, 23848, 360, 8627, 1180, 328, 75, 6737, 11, 1120, 73, 10430, 2992, 5128, 32876, 302, 23144, 3901, 850, 11368, 4013, 2007, 19605, 741, 13489, 23848, 13, 51294], "temperature": 0.0, "avg_logprob": -0.10366830708068094, "compression_ratio": 1.4606060606060607, "no_speech_prob": 0.009671802632510662}, {"id": 195, "seek": 65480, "start": 673.5999999999999, "end": 676.4, "text": " Od teraz jeste\u015b krytykiem filmowym.", "tokens": [51304, 12210, 16854, 25255, 1788, 34847, 874, 26116, 2007, 31691, 13, 51444], "temperature": 0.0, "avg_logprob": -0.10366830708068094, "compression_ratio": 1.4606060606060607, "no_speech_prob": 0.009671802632510662}, {"id": 196, "seek": 65480, "start": 676.5999999999999, "end": 678.8, "text": " I on nie uczy si\u0119 j\u0119zyka od nowa.", "tokens": [51454, 286, 322, 2838, 344, 6522, 3244, 42309, 40940, 3611, 586, 64, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10366830708068094, "compression_ratio": 1.4606060606060607, "no_speech_prob": 0.009671802632510662}, {"id": 197, "seek": 65480, "start": 679.0, "end": 684.1999999999999, "text": " Nie, on tylko uczy si\u0119 stosowa\u0107 swoj\u0105 oglomn\u0105 wiedz\u0119 w tej jednej w\u0105skiej dziedzinie", "tokens": [51574, 12016, 11, 322, 13219, 344, 6522, 3244, 43581, 11445, 49194, 5360, 75, 298, 13113, 46894, 11052, 261, 12573, 5232, 11794, 261, 1611, 5161, 7764, 9758, 15338, 259, 414, 51834], "temperature": 0.0, "avg_logprob": -0.10366830708068094, "compression_ratio": 1.4606060606060607, "no_speech_prob": 0.009671802632510662}, {"id": 198, "seek": 68420, "start": 684.2, "end": 687.8000000000001, "text": " i robi to w kilka godzin na jednym zwyk\u0142ym GPU.", "tokens": [50364, 741, 47380, 281, 261, 36466, 3044, 23584, 1667, 5232, 12996, 43436, 74, 1221, 4199, 18407, 13, 50544], "temperature": 0.0, "avg_logprob": -0.100923646291097, "compression_ratio": 1.4448051948051948, "no_speech_prob": 0.012092688120901585}, {"id": 199, "seek": 68420, "start": 688.0, "end": 693.4000000000001, "text": " OK, wi\u0119c mamy dwa sprytne zadania treningowe, kt\u00f3re da\u0142y \u015bwietne wyniki.", "tokens": [50554, 2264, 11, 16677, 17335, 35045, 637, 627, 83, 716, 42788, 5609, 2192, 773, 6880, 11, 8864, 1120, 6825, 8299, 39083, 716, 31936, 9850, 13, 50824], "temperature": 0.0, "avg_logprob": -0.100923646291097, "compression_ratio": 1.4448051948051948, "no_speech_prob": 0.012092688120901585}, {"id": 200, "seek": 68420, "start": 693.6, "end": 698.0, "text": " Ale dla mnie to, co jest w tym artykulem naprawd\u0119 fascynuj\u0105ce,", "tokens": [50834, 9366, 12285, 17661, 281, 11, 598, 3492, 261, 8107, 594, 874, 74, 2271, 76, 20970, 30632, 1344, 77, 13263, 384, 11, 51054], "temperature": 0.0, "avg_logprob": -0.100923646291097, "compression_ratio": 1.4448051948051948, "no_speech_prob": 0.012092688120901585}, {"id": 201, "seek": 68420, "start": 698.2, "end": 704.8000000000001, "text": " to nie tyle co zrobili, ale czego dowiedzieli si\u0119 przy okazji, rozk\u0142adaj\u0105c cen model na cz\u0119\u015bci.", "tokens": [51064, 281, 2838, 39293, 598, 44399, 2312, 11, 6775, 36559, 9459, 15338, 23099, 3244, 6501, 3133, 921, 4013, 11, 9544, 74, 46217, 8555, 66, 27900, 2316, 1667, 41314, 13, 51394], "temperature": 0.0, "avg_logprob": -0.100923646291097, "compression_ratio": 1.4448051948051948, "no_speech_prob": 0.012092688120901585}, {"id": 202, "seek": 68420, "start": 705.0, "end": 707.2, "text": " Bo tu zaczyna si\u0119 robi\u0107 naprawd\u0119 ciekawie.", "tokens": [51404, 3286, 2604, 43811, 629, 3244, 46900, 20970, 46419, 1607, 414, 13, 51514], "temperature": 0.0, "avg_logprob": -0.100923646291097, "compression_ratio": 1.4448051948051948, "no_speech_prob": 0.012092688120901585}, {"id": 203, "seek": 68420, "start": 707.4000000000001, "end": 709.2, "text": " Te badania ablacyjne, tak?", "tokens": [51524, 1989, 1578, 5609, 410, 75, 31285, 716, 11, 991, 30, 51614], "temperature": 0.0, "avg_logprob": -0.100923646291097, "compression_ratio": 1.4448051948051948, "no_speech_prob": 0.012092688120901585}, {"id": 204, "seek": 68420, "start": 709.4000000000001, "end": 714.0, "text": " Co si\u0119 okaza\u0142o, kiedy zacz\u0119li wy\u0142\u0105cza\u0107 poszczeg\u00f3lne elementy tej maszyny?", "tokens": [51624, 3066, 3244, 3133, 12257, 5249, 11, 18777, 34430, 11052, 2081, 4628, 15926, 66, 35873, 1366, 43771, 38079, 716, 4478, 88, 12573, 2300, 1229, 1634, 30, 51854], "temperature": 0.0, "avg_logprob": -0.100923646291097, "compression_ratio": 1.4448051948051948, "no_speech_prob": 0.012092688120901585}, {"id": 205, "seek": 71420, "start": 714.2, "end": 718.0, "text": " Te badania to, mo\u017cna powiedzie\u0107, gwo\u015b\u0107 dotrumny dla starych metod.", "tokens": [50364, 1989, 1578, 5609, 281, 11, 17790, 27886, 11, 290, 6120, 7753, 5893, 6247, 1634, 12285, 342, 822, 339, 1131, 378, 13, 50554], "temperature": 0.0, "avg_logprob": -0.10294364715789582, "compression_ratio": 1.4056603773584906, "no_speech_prob": 0.0023705102503299713}, {"id": 206, "seek": 71420, "start": 718.2, "end": 723.0, "text": " Po pierwsze sprawdzili, co si\u0119 stanie, je\u015bli usun\u0105\u0142 zadanie Next Sentence Prediction.", "tokens": [50564, 6165, 45994, 46192, 89, 2312, 11, 598, 3244, 40013, 11, 25630, 505, 409, 1611, 1221, 42788, 7155, 3087, 23652, 655, 32969, 4105, 13, 50804], "temperature": 0.0, "avg_logprob": -0.10294364715789582, "compression_ratio": 1.4056603773584906, "no_speech_prob": 0.0023705102503299713}, {"id": 207, "seek": 71420, "start": 723.2, "end": 726.2, "text": " I b\u0119d\u0105 trenowa\u0107 model tylko przez maskowanie s\u0142\u00f3w?", "tokens": [50814, 286, 26239, 23136, 11445, 2316, 13219, 14064, 6094, 22028, 15116, 3901, 30, 50964], "temperature": 0.0, "avg_logprob": -0.10294364715789582, "compression_ratio": 1.4056603773584906, "no_speech_prob": 0.0023705102503299713}, {"id": 208, "seek": 71420, "start": 726.4000000000001, "end": 731.6, "text": " Tak, wyniki. W zadaniach wymagaj\u0105cych rozumienia zwi\u0105zk\u00f3w logicznych mi\u0119dzy zdaniami,", "tokens": [50974, 9118, 11, 31936, 9850, 13, 343, 42788, 3782, 608, 29764, 559, 11133, 31306, 48797, 18811, 27741, 23849, 9952, 89, 9399, 33964, 710, 10312, 15568, 11, 51234], "temperature": 0.0, "avg_logprob": -0.10294364715789582, "compression_ratio": 1.4056603773584906, "no_speech_prob": 0.0023705102503299713}, {"id": 209, "seek": 71420, "start": 731.8000000000001, "end": 736.2, "text": " jak QNLI czy MNLI, model radzi\u0142 sobie znacznie gorzej.", "tokens": [51244, 4207, 1249, 45, 48718, 6430, 376, 45, 48718, 11, 2316, 2843, 3992, 1221, 13652, 15397, 14875, 2766, 24012, 16920, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10294364715789582, "compression_ratio": 1.4056603773584906, "no_speech_prob": 0.0023705102503299713}, {"id": 210, "seek": 71420, "start": 736.4000000000001, "end": 741.2, "text": " To by\u0142 dow\u00f3d, \u017ce NSP nie jest tylko mi\u0142ym dodatkiem, ale kluczowym sk\u0142adnikiem.", "tokens": [51474, 1407, 16673, 9459, 17081, 11, 3561, 15943, 47, 2838, 3492, 13219, 2752, 1221, 4199, 13886, 267, 26116, 11, 6775, 9671, 1311, 89, 31691, 1110, 10358, 13123, 4907, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10294364715789582, "compression_ratio": 1.4056603773584906, "no_speech_prob": 0.0023705102503299713}, {"id": 211, "seek": 74120, "start": 741.2, "end": 745.4000000000001, "text": " A co z najwa\u017cniejszym, czyli por\u00f3wnaniem do modeli jednokierunkowych?", "tokens": [50364, 316, 598, 710, 11212, 27111, 10402, 7706, 76, 11, 16591, 1515, 812, 895, 282, 4907, 360, 2316, 72, 5232, 77, 453, 811, 3197, 19605, 30, 50574], "temperature": 0.0, "avg_logprob": -0.10402993928818476, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.03638940304517746}, {"id": 212, "seek": 74120, "start": 745.6, "end": 747.2, "text": " To by\u0142 ostateczny dow\u00f3d.", "tokens": [50584, 1407, 16673, 277, 15406, 3689, 1634, 9459, 17081, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10402993928818476, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.03638940304517746}, {"id": 213, "seek": 74120, "start": 747.4000000000001, "end": 753.0, "text": " Zbudowali model, kt\u00f3ry by\u0142 w zasadzie kopi\u0105 Bertha, ale trenowany tylko w jedn\u0105 stron\u0119.", "tokens": [50674, 1176, 18281, 305, 5103, 2316, 11, 9913, 16673, 261, 44585, 3283, 28920, 11404, 5637, 13571, 11, 6775, 23136, 23341, 13219, 261, 5232, 13113, 45766, 1274, 13, 50954], "temperature": 0.0, "avg_logprob": -0.10402993928818476, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.03638940304517746}, {"id": 214, "seek": 74120, "start": 753.2, "end": 755.4000000000001, "text": " Od lewej do krawy, w stylu GPT.", "tokens": [50964, 12210, 476, 826, 73, 360, 350, 5131, 88, 11, 261, 7952, 2781, 26039, 51, 13, 51074], "temperature": 0.0, "avg_logprob": -0.10402993928818476, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.03638940304517746}, {"id": 215, "seek": 74120, "start": 755.6, "end": 761.0, "text": " I we wszystkich zadaniach, bez wyj\u0105tku, ten jednokierunkowy model przegrywa\u0142 z Gretesem.", "tokens": [51084, 286, 321, 34234, 42788, 3782, 608, 11, 10782, 4628, 8555, 83, 5279, 11, 2064, 5232, 77, 453, 811, 3197, 10089, 2316, 6541, 1146, 627, 44603, 710, 460, 1505, 279, 443, 13, 51354], "temperature": 0.0, "avg_logprob": -0.10402993928818476, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.03638940304517746}, {"id": 216, "seek": 74120, "start": 761.2, "end": 763.8000000000001, "text": " Najbardziej w tych zadaniach wymagaj\u0105cych kontekstu?", "tokens": [51364, 31576, 40392, 261, 15180, 42788, 3782, 608, 29764, 559, 11133, 31306, 14373, 916, 372, 84, 30, 51494], "temperature": 0.0, "avg_logprob": -0.10402993928818476, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.03638940304517746}, {"id": 217, "seek": 74120, "start": 764.0, "end": 769.8000000000001, "text": " Najbardziej dramatyczny spadek by\u0142 w zadaniu sk\u0142ad, tym z odpowiadaniem na pytania, co jest logiczne,", "tokens": [51504, 31576, 40392, 42749, 17466, 1634, 637, 762, 74, 16673, 261, 42788, 25849, 1110, 10358, 11, 8107, 710, 24314, 38069, 282, 4907, 1667, 25878, 5609, 11, 598, 3492, 9952, 43077, 11, 51794], "temperature": 0.0, "avg_logprob": -0.10402993928818476, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.03638940304517746}, {"id": 218, "seek": 76980, "start": 769.8, "end": 774.8, "text": " bo tam w\u0142a\u015bnie umiej\u0119tno\u015b\u0107 patrzenia w prz\u00f3d i w ty\u0142 jest absolutnie krytyczna.", "tokens": [50364, 748, 7677, 14234, 1105, 7764, 46788, 23293, 1947, 81, 14320, 261, 6541, 17081, 741, 261, 1104, 1221, 3492, 18757, 2766, 34847, 874, 3689, 629, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12273925893446978, "compression_ratio": 1.36, "no_speech_prob": 0.010445892810821533}, {"id": 219, "seek": 76980, "start": 775.0, "end": 780.4, "text": " To bezdyskusyjnie pokaza\u0142o, \u017ce deep bidirectionality by\u0142o tajnym sk\u0142adnikiem sukcesu.", "tokens": [50624, 1407, 10782, 67, 749, 35080, 88, 73, 2766, 13010, 12257, 5249, 11, 3561, 2452, 12957, 621, 882, 1860, 14811, 256, 1805, 12996, 1110, 10358, 13123, 4907, 46432, 887, 84, 13, 50894], "temperature": 0.0, "avg_logprob": -0.12273925893446978, "compression_ratio": 1.36, "no_speech_prob": 0.010445892810821533}, {"id": 220, "seek": 76980, "start": 780.5999999999999, "end": 784.0, "text": " W artykule jest te\u017c mowa o wp\u0142owie wielko\u015bci modelu.", "tokens": [50904, 343, 594, 874, 74, 2271, 3492, 9516, 275, 5528, 277, 32444, 1221, 13998, 20570, 4093, 6199, 2316, 84, 13, 51074], "temperature": 0.0, "avg_logprob": -0.12273925893446978, "compression_ratio": 1.36, "no_speech_prob": 0.010445892810821533}, {"id": 221, "seek": 76980, "start": 784.1999999999999, "end": 791.8, "text": " Por\u00f3wnuj\u0105 tam wersje BERT Base ze 110 milionami parametr\u00f3w do BERT Large z 340 milionami.", "tokens": [51084, 5269, 812, 895, 13263, 7677, 261, 433, 2884, 363, 31479, 21054, 5277, 20154, 1962, 313, 4526, 6220, 27965, 3901, 360, 363, 31479, 33092, 710, 805, 5254, 1962, 313, 4526, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12273925893446978, "compression_ratio": 1.36, "no_speech_prob": 0.010445892810821533}, {"id": 222, "seek": 76980, "start": 792.0, "end": 793.8, "text": " I wyniki s\u0105 jednoznaczne.", "tokens": [51474, 286, 31936, 9850, 9015, 5232, 1771, 22672, 14875, 716, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12273925893446978, "compression_ratio": 1.36, "no_speech_prob": 0.010445892810821533}, {"id": 223, "seek": 76980, "start": 794.0, "end": 795.5999999999999, "text": " Wi\u0119kszy jest lepszy.", "tokens": [51574, 30127, 1694, 1229, 3492, 476, 1878, 1229, 13, 51654], "temperature": 0.0, "avg_logprob": -0.12273925893446978, "compression_ratio": 1.36, "no_speech_prob": 0.010445892810821533}, {"id": 224, "seek": 79560, "start": 795.8000000000001, "end": 800.0, "text": " Ale chwila, to jest wbrew wszystkiemu, czego uczono nas o maszinerningu.", "tokens": [50374, 9366, 26237, 7371, 11, 281, 3492, 261, 65, 2236, 14615, 4907, 84, 11, 36559, 35403, 8957, 5382, 277, 2300, 89, 4564, 773, 84, 13, 50584], "temperature": 0.0, "avg_logprob": -0.08646550612016157, "compression_ratio": 1.475, "no_speech_prob": 0.011614608578383923}, {"id": 225, "seek": 79560, "start": 800.2, "end": 801.8000000000001, "text": " Zawsze si\u0119 powtarza\u0142o.", "tokens": [50594, 1176, 28354, 3244, 3388, 23480, 2394, 5249, 13, 50674], "temperature": 0.0, "avg_logprob": -0.08646550612016157, "compression_ratio": 1.475, "no_speech_prob": 0.011614608578383923}, {"id": 226, "seek": 79560, "start": 802.0, "end": 807.8000000000001, "text": " Uwa\u017caj, bo za du\u017cy model na ma\u0142ym zbiorze danych to prosta droga do overfittingu.", "tokens": [50684, 624, 27111, 1805, 11, 748, 7949, 1581, 7735, 2316, 1667, 463, 1221, 4199, 710, 33362, 1381, 274, 34644, 281, 582, 8638, 3789, 3680, 360, 670, 69, 2414, 84, 13, 50974], "temperature": 0.0, "avg_logprob": -0.08646550612016157, "compression_ratio": 1.475, "no_speech_prob": 0.011614608578383923}, {"id": 227, "seek": 79560, "start": 808.0, "end": 810.8000000000001, "text": " Do zapami\u0119tania danych na pami\u0119\u0107, a nie do nauki.", "tokens": [50984, 1144, 14223, 23806, 83, 5609, 274, 34644, 1667, 31088, 2162, 11, 257, 2838, 360, 35616, 2984, 13, 51124], "temperature": 0.0, "avg_logprob": -0.08646550612016157, "compression_ratio": 1.475, "no_speech_prob": 0.011614608578383923}, {"id": 228, "seek": 79560, "start": 811.0, "end": 815.8000000000001, "text": " Dok\u0142adnie, dlatego tutaj to nie tylko nie by\u0142 problem, ale wr\u0119cz zaleta.", "tokens": [51134, 29768, 10358, 2766, 11, 32205, 12749, 281, 2838, 13219, 2838, 16673, 1154, 11, 6775, 928, 1274, 3689, 29599, 7664, 13, 51374], "temperature": 0.0, "avg_logprob": -0.08646550612016157, "compression_ratio": 1.475, "no_speech_prob": 0.011614608578383923}, {"id": 229, "seek": 79560, "start": 816.0, "end": 820.0, "text": " I to jest w\u0142a\u015bnie sedno jednego z najwa\u017cniejszych odkry\u0107 tego artyku\u0142u.", "tokens": [51384, 286, 281, 3492, 14234, 9643, 1771, 5232, 11858, 710, 11212, 27111, 10402, 45021, 3611, 43298, 2162, 8627, 594, 874, 5279, 24066, 13, 51584], "temperature": 0.0, "avg_logprob": -0.08646550612016157, "compression_ratio": 1.475, "no_speech_prob": 0.011614608578383923}, {"id": 230, "seek": 79560, "start": 820.2, "end": 825.2, "text": " Odkrycia, kt\u00f3re zapocz\u0105tkowa\u0142o obecny wy\u015bcig zbrojeni na coraz wi\u0119ksze modele.", "tokens": [51594, 12210, 43298, 2755, 11, 8864, 14223, 905, 8925, 83, 74, 5528, 5249, 49141, 1634, 4628, 1788, 66, 328, 710, 9120, 15378, 72, 1667, 25899, 29968, 1381, 4391, 306, 13, 51844], "temperature": 0.0, "avg_logprob": -0.08646550612016157, "compression_ratio": 1.475, "no_speech_prob": 0.011614608578383923}, {"id": 231, "seek": 82520, "start": 825.4000000000001, "end": 832.0, "text": " Okaza\u0142o si\u0119, \u017ce je\u015bli model przesied\u0142 przez wystarczaj\u0105co d\u0142ugi i wszechstronny pretraining na ogromnej ilo\u015bci danych,", "tokens": [50374, 3477, 12257, 5249, 3244, 11, 3561, 25630, 2316, 6541, 279, 1091, 1221, 14064, 4628, 9710, 3689, 11133, 1291, 44042, 24780, 741, 37647, 19439, 372, 2044, 1634, 1162, 424, 1760, 1667, 34416, 298, 11794, 1930, 44468, 274, 34644, 11, 50704], "temperature": 0.0, "avg_logprob": -0.07375894684389413, "compression_ratio": 1.4953846153846153, "no_speech_prob": 0.0024479327257722616}, {"id": 232, "seek": 82520, "start": 832.2, "end": 835.0, "text": " to jego gigantyczna pojemno\u015b\u0107 staje si\u0119 zalet\u0105.", "tokens": [50714, 281, 26542, 8741, 394, 17466, 629, 714, 30833, 23293, 342, 11153, 3244, 29599, 302, 1611, 13, 50854], "temperature": 0.0, "avg_logprob": -0.07375894684389413, "compression_ratio": 1.4953846153846153, "no_speech_prob": 0.0024479327257722616}, {"id": 233, "seek": 82520, "start": 835.2, "end": 836.0, "text": " Nie wad\u0105.", "tokens": [50864, 12016, 261, 345, 1611, 13, 50904], "temperature": 0.0, "avg_logprob": -0.07375894684389413, "compression_ratio": 1.4953846153846153, "no_speech_prob": 0.0024479327257722616}, {"id": 234, "seek": 82520, "start": 836.2, "end": 837.2, "text": " Nie.", "tokens": [50914, 12016, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07375894684389413, "compression_ratio": 1.4953846153846153, "no_speech_prob": 0.0024479327257722616}, {"id": 235, "seek": 82520, "start": 837.4000000000001, "end": 842.4000000000001, "text": " Taki wst\u0119pnie wytrenowany m\u00f3zg nie musi ju\u017c uczy\u0107 si\u0119 podstawowych koncepcji j\u0119zykowych na ma\u0142ym zbiorze.", "tokens": [50974, 314, 7421, 261, 372, 18085, 2766, 261, 4328, 1095, 23341, 32515, 89, 70, 2838, 37587, 10678, 344, 33967, 3244, 43443, 19605, 5897, 27493, 19649, 49055, 74, 19605, 1667, 463, 1221, 4199, 710, 33362, 1381, 13, 51224], "temperature": 0.0, "avg_logprob": -0.07375894684389413, "compression_ratio": 1.4953846153846153, "no_speech_prob": 0.0024479327257722616}, {"id": 236, "seek": 82520, "start": 842.6, "end": 843.8000000000001, "text": " On ju\u017c je zna.", "tokens": [51234, 1282, 10678, 1506, 710, 629, 13, 51294], "temperature": 0.0, "avg_logprob": -0.07375894684389413, "compression_ratio": 1.4953846153846153, "no_speech_prob": 0.0024479327257722616}, {"id": 237, "seek": 82520, "start": 844.0, "end": 847.8000000000001, "text": " On tylko adaptuje t\u0119 ogromn\u0105 zgeneralizowan\u0105 wiedz\u0119 do nowego problemu.", "tokens": [51304, 1282, 13219, 6231, 13008, 32489, 34416, 298, 13113, 710, 1766, 2790, 590, 37345, 1611, 46894, 11052, 360, 586, 6308, 1154, 84, 13, 51494], "temperature": 0.0, "avg_logprob": -0.07375894684389413, "compression_ratio": 1.4953846153846153, "no_speech_prob": 0.0024479327257722616}, {"id": 238, "seek": 82520, "start": 848.0, "end": 854.2, "text": " Czyli ten pot\u0119\u017cny pretraining dzia\u0142a jak taka szczepionka przeciwko overfittingowi.", "tokens": [51504, 37099, 2064, 1847, 1274, 1427, 1634, 1162, 424, 1760, 37903, 4207, 28017, 22090, 595, 313, 2330, 39622, 86, 4093, 670, 69, 2414, 24503, 13, 51814], "temperature": 0.0, "avg_logprob": -0.07375894684389413, "compression_ratio": 1.4953846153846153, "no_speech_prob": 0.0024479327257722616}, {"id": 239, "seek": 85420, "start": 854.4000000000001, "end": 855.6, "text": " Mo\u017cna tak powiedzie\u0107.", "tokens": [50374, 44736, 629, 991, 27886, 13, 50434], "temperature": 0.0, "avg_logprob": -0.09588219970464706, "compression_ratio": 1.4317343173431734, "no_speech_prob": 0.009752476587891579}, {"id": 240, "seek": 85420, "start": 855.8000000000001, "end": 860.8000000000001, "text": " Okaza\u0142o si\u0119, \u017ce te dodatkowe miliony parametr\u00f3w nie s\u0142u\u017c\u0105 do zapami\u0119tywania danych,", "tokens": [50444, 3477, 12257, 5249, 3244, 11, 3561, 535, 13886, 33525, 6880, 1962, 46184, 6220, 27965, 3901, 2838, 48459, 1427, 1611, 360, 14223, 23806, 874, 86, 5609, 274, 34644, 11, 50694], "temperature": 0.0, "avg_logprob": -0.09588219970464706, "compression_ratio": 1.4317343173431734, "no_speech_prob": 0.009752476587891579}, {"id": 241, "seek": 85420, "start": 861.0, "end": 867.8000000000001, "text": " ale do budowania bardziej subtylnych, bardziej z\u0142o\u017conych i bardziej og\u00f3lnych reprezentacji j\u0119zyka.", "tokens": [50704, 6775, 360, 3265, 21308, 27209, 7257, 5088, 9399, 11, 27209, 710, 5249, 1427, 2526, 339, 741, 27209, 5360, 15741, 9399, 1085, 265, 14185, 13152, 42309, 40940, 13, 51044], "temperature": 0.0, "avg_logprob": -0.09588219970464706, "compression_ratio": 1.4317343173431734, "no_speech_prob": 0.009752476587891579}, {"id": 242, "seek": 85420, "start": 868.0, "end": 869.8000000000001, "text": " Nawet na ma\u0142ych zbiorach danych?", "tokens": [51054, 40315, 302, 1667, 463, 47655, 710, 33362, 608, 274, 34644, 30, 51144], "temperature": 0.0, "avg_logprob": -0.09588219970464706, "compression_ratio": 1.4317343173431734, "no_speech_prob": 0.009752476587891579}, {"id": 243, "seek": 85420, "start": 870.0, "end": 870.4000000000001, "text": " Tak.", "tokens": [51154, 9118, 13, 51174], "temperature": 0.0, "avg_logprob": -0.09588219970464706, "compression_ratio": 1.4317343173431734, "no_speech_prob": 0.009752476587891579}, {"id": 244, "seek": 85420, "start": 870.6, "end": 878.4000000000001, "text": " Nawet w zadaniach z zaledwie kilkoma tysi\u0105cami przyk\u0142ad\u00f3w jak MRPC Bert Larch by\u0142 znacznie lepszy od swojego mniejszego brata.", "tokens": [51184, 40315, 302, 261, 42788, 3782, 608, 710, 710, 5573, 8699, 5128, 74, 6440, 38156, 11404, 66, 4526, 23144, 3901, 4207, 9808, 12986, 29594, 441, 1178, 16673, 15397, 14875, 2766, 476, 1878, 1229, 3611, 13291, 39738, 275, 30295, 27725, 738, 3274, 13, 51574], "temperature": 0.0, "avg_logprob": -0.09588219970464706, "compression_ratio": 1.4317343173431734, "no_speech_prob": 0.009752476587891579}, {"id": 245, "seek": 87840, "start": 878.6, "end": 885.1999999999999, "text": " Artyku\u0142 udowodni\u0142, \u017ce skalowanie w po\u0142\u0105czeniu z odpowiednim pretrainingiem jest drog\u0105 do prawdziwej inteligencji.", "tokens": [50374, 1587, 874, 5279, 1221, 11727, 305, 378, 3722, 1221, 11, 3561, 16890, 22028, 261, 714, 15926, 66, 39651, 710, 36574, 39223, 1162, 424, 1760, 4907, 3492, 3789, 70, 1611, 360, 41175, 3992, 826, 73, 24777, 3213, 19649, 13, 50704], "temperature": 0.0, "avg_logprob": -0.07768061720294717, "compression_ratio": 1.4290123456790123, "no_speech_prob": 0.13719435036182404}, {"id": 246, "seek": 87840, "start": 885.4, "end": 886.8, "text": " I reszta \u015bwiata pos\u0142ucha\u0142a.", "tokens": [50714, 286, 725, 89, 1328, 21485, 3274, 1366, 1221, 26042, 5024, 13, 50784], "temperature": 0.0, "avg_logprob": -0.07768061720294717, "compression_ratio": 1.4290123456790123, "no_speech_prob": 0.13719435036182404}, {"id": 247, "seek": 87840, "start": 887.0, "end": 894.1999999999999, "text": " Podsumowuj\u0105c, je\u015bli mieliby\u015bmy wskaza\u0107 te dwie absolutnie kluczowe innowacje, kt\u00f3re przyni\u00f3s\u0142 Bert, to by\u0142yby to.", "tokens": [50794, 12646, 82, 449, 305, 44733, 11, 25630, 41392, 897, 88, 10513, 261, 5161, 12257, 2162, 535, 274, 8699, 18757, 2766, 9671, 1311, 89, 6880, 294, 3785, 29293, 11, 8864, 6501, 3722, 12994, 1221, 29594, 11, 281, 26366, 2322, 281, 13, 51154], "temperature": 0.0, "avg_logprob": -0.07768061720294717, "compression_ratio": 1.4290123456790123, "no_speech_prob": 0.13719435036182404}, {"id": 248, "seek": 87840, "start": 894.4, "end": 900.8, "text": " Po pierwsze, g\u0142\u0119boka dwukierunkowo\u015b\u0107 osi\u0105gni\u0119ta przez genialnie prost\u0105 gr\u0119 w zgadywanie zamaskowanych s\u0142\u00f3w.", "tokens": [51164, 6165, 45994, 11, 18117, 1274, 65, 15289, 27379, 2034, 811, 3197, 19941, 7753, 3003, 11404, 70, 35938, 1328, 14064, 48228, 2766, 10293, 1611, 677, 1274, 261, 40948, 880, 86, 7155, 19876, 3863, 23341, 339, 15116, 3901, 13, 51484], "temperature": 0.0, "avg_logprob": -0.07768061720294717, "compression_ratio": 1.4290123456790123, "no_speech_prob": 0.13719435036182404}, {"id": 249, "seek": 87840, "start": 901.0, "end": 901.8, "text": " A po drugie?", "tokens": [51494, 316, 714, 4110, 414, 30, 51534], "temperature": 0.0, "avg_logprob": -0.07768061720294717, "compression_ratio": 1.4290123456790123, "no_speech_prob": 0.13719435036182404}, {"id": 250, "seek": 87840, "start": 902.0, "end": 906.0, "text": " A po drugie, zdolno\u015b\u0107 rozumienia logiki mi\u0119dzy zdaniami.", "tokens": [51544, 316, 714, 4110, 414, 11, 16221, 401, 23293, 48797, 18811, 3565, 9850, 33964, 710, 10312, 15568, 13, 51744], "temperature": 0.0, "avg_logprob": -0.07768061720294717, "compression_ratio": 1.4290123456790123, "no_speech_prob": 0.13719435036182404}, {"id": 251, "seek": 90600, "start": 906.0, "end": 909.0, "text": " Dzi\u0119ki przewidywaniu czy dwa zdania do siebie pasuj\u0105.", "tokens": [50364, 413, 34546, 39758, 327, 27112, 25849, 6430, 35045, 16221, 5609, 360, 39137, 1736, 13263, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07737329843881968, "compression_ratio": 1.4254658385093169, "no_speech_prob": 0.12144949287176132}, {"id": 252, "seek": 90600, "start": 909.2, "end": 915.4, "text": " I co r\u00f3wnie wa\u017cne, Bert fundamentalnie zmieni\u0142 spos\u00f3b, w jaki podchodzimy do problem\u00f3w WNLP.", "tokens": [50524, 286, 598, 11416, 14215, 46110, 11, 29594, 8088, 2766, 17020, 35462, 1221, 22904, 11, 261, 24492, 2497, 29914, 89, 13189, 360, 1154, 3901, 343, 45, 45196, 13, 50834], "temperature": 0.0, "avg_logprob": -0.07737329843881968, "compression_ratio": 1.4254658385093169, "no_speech_prob": 0.12144949287176132}, {"id": 253, "seek": 90600, "start": 915.6, "end": 924.2, "text": " Zamiast budowa\u0107 od zera skomplikowane, wyspecjalizowane architektury dla ka\u017cdego problemu, \u015bwiat przestawi\u0142 si\u0119 na paradygmat.", "tokens": [50844, 1176, 4526, 525, 3265, 11445, 3611, 710, 1663, 1110, 298, 564, 1035, 23066, 11, 27062, 494, 66, 22600, 590, 23066, 3912, 642, 2320, 2598, 12285, 21912, 67, 6308, 1154, 84, 11, 36425, 44264, 38402, 1221, 3244, 1667, 13480, 18103, 15677, 13, 51274], "temperature": 0.0, "avg_logprob": -0.07737329843881968, "compression_ratio": 1.4254658385093169, "no_speech_prob": 0.12144949287176132}, {"id": 254, "seek": 90600, "start": 924.4, "end": 931.0, "text": " We\u017a jeden pot\u0119\u017cny, wst\u0119pnie wytrenowany model fundament i w kilka godzin dostosuj go do swoich potrzeb.", "tokens": [51284, 492, 10659, 12906, 1847, 1274, 1427, 1634, 11, 261, 372, 18085, 2766, 261, 4328, 1095, 23341, 2316, 6073, 741, 261, 36466, 3044, 23584, 20568, 329, 4579, 352, 360, 13291, 480, 37595, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07737329843881968, "compression_ratio": 1.4254658385093169, "no_speech_prob": 0.12144949287176132}, {"id": 255, "seek": 90600, "start": 931.2, "end": 934.6, "text": " To by\u0142 prze\u0142om nie tylko technologiczny, ale te\u017c filozoficzny.", "tokens": [51624, 1407, 16673, 8325, 1221, 298, 2838, 13219, 1537, 1132, 17946, 1634, 11, 6775, 9516, 1387, 78, 4765, 1786, 89, 1634, 13, 51794], "temperature": 0.0, "avg_logprob": -0.07737329843881968, "compression_ratio": 1.4254658385093169, "no_speech_prob": 0.12144949287176132}, {"id": 256, "seek": 93460, "start": 935.0, "end": 944.4, "text": " Niesamowite, jak te dwa wydawa\u0142oby si\u0119 prosty pomys\u0142y, zgadywanie s\u0142\u00f3w i ocena sp\u00f3jno\u015bci zda\u0144 doprowadzi\u0142y do tak gigantycznego skoku naprz\u00f3d.", "tokens": [50384, 426, 530, 335, 305, 642, 11, 4207, 535, 35045, 25984, 10449, 1221, 13944, 3244, 10293, 88, 12991, 749, 6825, 11, 40948, 880, 86, 7155, 15116, 3901, 741, 10409, 4118, 637, 18999, 16438, 710, 2675, 5248, 360, 35019, 3992, 6825, 360, 991, 8741, 394, 17466, 11858, 1110, 13275, 9296, 19390, 17081, 13, 50854], "temperature": 0.0, "avg_logprob": -0.08544905408680868, "compression_ratio": 1.436241610738255, "no_speech_prob": 0.0044802166521549225}, {"id": 257, "seek": 93460, "start": 944.6, "end": 946.2, "text": " To na s\u0142owa wa\u017cne pytanie.", "tokens": [50864, 1407, 1667, 15116, 5528, 46110, 36610, 13, 50944], "temperature": 0.0, "avg_logprob": -0.08544905408680868, "compression_ratio": 1.436241610738255, "no_speech_prob": 0.0044802166521549225}, {"id": 258, "seek": 93460, "start": 946.4, "end": 955.8000000000001, "text": " Bert pokaza\u0142, \u017ce aby rozwi\u0105za\u0107 wiele specyficznych problem\u00f3w j\u0119zykowych, najlepszym pierwszym krokiem jest rozwi\u0105zanie jednego bardzo og\u00f3lnego problemu.", "tokens": [50954, 29594, 13010, 12257, 1221, 11, 3561, 24457, 9544, 18234, 35873, 33137, 768, 1344, 1786, 89, 9399, 1154, 3901, 49055, 74, 19605, 11, 41903, 1878, 26681, 34016, 76, 45909, 26116, 3492, 9544, 22620, 7155, 5232, 11858, 9034, 5360, 15741, 11858, 1154, 84, 13, 51424], "temperature": 0.0, "avg_logprob": -0.08544905408680868, "compression_ratio": 1.436241610738255, "no_speech_prob": 0.0044802166521549225}, {"id": 259, "seek": 93460, "start": 956.0, "end": 959.8000000000001, "text": " Zrozumienia j\u0119zyka w jego pe\u0142nym dwukierunkowym kontak\u015bcie.", "tokens": [51434, 1176, 27857, 449, 18811, 42309, 40940, 261, 26542, 43205, 12996, 27379, 2034, 811, 3197, 31691, 14373, 514, 9815, 13, 51624], "temperature": 0.0, "avg_logprob": -0.08544905408680868, "compression_ratio": 1.436241610738255, "no_speech_prob": 0.0044802166521549225}, {"id": 260, "seek": 93460, "start": 960.0, "end": 961.4, "text": " I co w zwi\u0105zku z tym?", "tokens": [51634, 286, 598, 261, 27741, 5279, 710, 8107, 30, 51704], "temperature": 0.0, "avg_logprob": -0.08544905408680868, "compression_ratio": 1.436241610738255, "no_speech_prob": 0.0044802166521549225}, {"id": 261, "seek": 96140, "start": 961.4, "end": 972.6, "text": " Zastan\u00f3wmy si\u0119, jakie inne z\u0142o\u017cone dziedziny, mo\u017ce poza j\u0119zykiem, mog\u0142yby zosta\u0107 zrewolucjonizowane nie przez budowanie tysi\u0119cy specjalistycznych narz\u0119dzi,", "tokens": [50364, 1176, 525, 282, 3901, 2226, 3244, 11, 22124, 24170, 710, 5249, 1427, 546, 9758, 15338, 3519, 11, 12034, 714, 2394, 49055, 26116, 11, 13172, 6825, 2322, 23154, 2162, 710, 2236, 401, 1311, 15735, 590, 23066, 2838, 14064, 3265, 22028, 38156, 47303, 46433, 468, 17466, 9399, 6714, 89, 6298, 3992, 11, 50924], "temperature": 0.0, "avg_logprob": -0.07000171983396852, "compression_ratio": 1.3278688524590163, "no_speech_prob": 0.0004340301384218037}, {"id": 262, "seek": 96140, "start": 972.8, "end": 978.8, "text": " ale przez stworzenie jednego, niezwykle pot\u0119\u017cnego uniwersalnego fundamentu.", "tokens": [50934, 6775, 14064, 342, 28321, 16778, 5232, 11858, 11, 33511, 9726, 14677, 1847, 1274, 1427, 11858, 36435, 5364, 304, 11858, 6073, 84, 13, 51234], "temperature": 0.0, "avg_logprob": -0.07000171983396852, "compression_ratio": 1.3278688524590163, "no_speech_prob": 0.0004340301384218037}], "language": "pl"}