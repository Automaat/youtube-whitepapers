{"text": " Wyobra\u017amy sobie tak\u0105 sytuacj\u0119. Jest, powiedzmy, rok 2019. Sztuczna inteligencja robi ju\u017c niesamowite rzeczy, ale ma jedn\u0105 fundamentaln\u0105 wad\u0119. Je\u015bli chcesz j\u0105 nauczy\u0107 czego\u015b nowego, no nie wiem, rozumienia \u017cargonu prawniczego albo pisania wierszy, to musisz j\u0105 jakby wys\u0142a\u0107 na nowo na uniwersytet, stworzy\u0107 gigantyczny, specjalistyczny zbi\u00f3r danych i przeprowadzi\u0107, no, kosztowny trening. A co gdyby istnia\u0142 model, kt\u00f3ry przeczyta\u0142 tak du\u017co, \u017ce nowej umiej\u0119tno\u015bci uczy si\u0119 w locie? Po prostu na podstawie kilku przyk\u0142ad\u00f3w, kt\u00f3rym urzucisz w rozmowie. Bez \u017cadnego dodatkowego szkolenia. I to jest, wiesz, dok\u0142adnie ten moment, w kt\u00f3rym na scen\u0119 wchodzi praca, o kt\u00f3rej dzisiaj rozmawiamy. Artyku\u0142 z 2020 roku napisany przez zesp\u00f3\u0142 OpenAI, tytu\u0142. Language models are few shot learners. M\u00f3wi\u0105c wprost, to jest tekst, kt\u00f3ry przedstawi\u0142 \u015bwiatu GPT-3 i, no, wywr\u00f3ci\u0142 stolik w ca\u0142ym \u015bwiecie AI. OK, czyli nasz cel jest ambitny. Chcemy zrozumie\u0107, co by\u0142o a\u017c tak rewolucyjnego w tym modelu, jak dzia\u0142a ta jego magia, czyli tak zwane in-context learning i co ta jego wtedy niewyobra\u017calna skala, 175 miliard\u00f3w parametr\u00f3w, co nam to tak naprawd\u0119 powiedzia\u0142o o przysz\u0142o\u015bci. Zar\u00f3wno o tych, wiesz, osza\u0142amiaj\u0105cych mo\u017cliwo\u015bciach, jak i o cieniach, kt\u00f3re si\u0119 za nimi kryj\u0105. Zanim jednak zanurkujemy w samo GPT-3, musimy zrozumie\u0107 \u015bwiat, kt\u00f3ry by\u0142 przed nim. Standardem by\u0142 taki proces dwuetapowy, pierwszy krok to pre-training. Czyli budowanie fundamentu. Bierzesz model i ka\u017cesz mu przeczyta\u0107, no, prawie ca\u0142y internet. Ogromn\u0105 bibliotek\u0119 tekst\u00f3w, ksi\u0105\u017cek, artyku\u0142\u00f3w. Celem nie jest nauczenie go konkretnego zadania, prawda? Tylko tego, jak w og\u00f3le dzia\u0142a j\u0119zyk. Gramatyki, fakt\u00f3w o \u015bwiecie, zwi\u0105zk\u00f3w mi\u0119dzy s\u0142owami. To jakby da\u0107 komu\u015b takie og\u00f3lne wykszta\u0142cenie. Dok\u0142adnie. Ale po tym og\u00f3lnym wykszta\u0142ceniu przychodzi\u0142 czas na specjalizacj\u0119, czyli drugi krok. Fine tuning. I tu, uwierz, zaczyna\u0142y si\u0119 schody. Chcesz, \u017ceby model rozpoznawa\u0142, czy recenzja w filmu jest pozytywna czy negatywna? Musisz mu da\u0107 tysi\u0105ce przyk\u0142ad\u00f3w recenzji z etykietami pozytywna lub negatywna. I przekrowadzi\u0107 kolejny trening. Mniejszy, ale wci\u0105\u017c kosztowny. Ka\u017cda nowa umiej\u0119tno\u015b\u0107 to nowy, du\u017cy zbi\u00f3r danych. Nowa operacja na m\u00f3zgu modelu. To by\u0142o skuteczne, ale potwornie niepraktyczne. Troch\u0119 jakby chirurg musia\u0142 wraca\u0107 na ca\u0142e studia medyczne za ka\u017cdym razem, gdy pojawia si\u0119 nowa technika operacyjna. A autorzy tej pracy zadali pytanie, a co je\u015bli da si\u0119 inaczej? I zaproponowali co\u015b, co wydawa\u0142o si\u0119 niema\u0142 oszustwem. Ca\u0142kowit\u0105 rezygnacj\u0119 z etapu fine tuningu, zast\u0105pili go czym\u015b, co nazwali in-context learning. I tu jest, wiesz, ca\u0142a magia. To jest takie uczenie si\u0119 na niby. Wyobra\u017cmy sobie, \u017ce m\u00f3zg modelu jest zamro\u017cony, nie uczy si\u0119 niczego na sta\u0142e i jego wewn\u0119trzne po\u0142\u0105czenia si\u0119 nie zmieniaj\u0105. Wszystko, co za\u0142apuje z podanych mu przyk\u0142ad\u00f3w, jest jak notatka na kartce, kt\u00f3r\u0105 zgniata i wyrzuca zaraz po udzieleniu odpowiedzi. To faktycznie rewolucyjne. Bo wcze\u015bniej ka\u017cda nowa umiej\u0119tno\u015b\u0107 wymaga\u0142a trwa\u0142ej operacji na m\u00f3zgu modelu. A tutaj, to jak rozmowa z niezwykle bystrym erudyt\u0105, kt\u00f3ry nie musi zmienia\u0107 swojej osobowo\u015bci, \u017ceby zrozumie\u0107, o co go prosimy. Wystarczy mu da\u0107 kilka wskaz\u00f3wek. I te wskaz\u00f3wki podzielili na trzy poziomy. Pierwszy, najbardziej hardkorowy, to zero shot. Dajemy modelowi tylko polecenie, na przyk\u0142ad przet\u0142umacz z angielskiego na francuski. Cheese. I tyle. \u017badnych przyk\u0142ad\u00f3w. Model ma sam wpa\u015b\u0107 na to, czego od niego chcemy. Drugi poziom to one shot. Tu jeste\u015bmy troch\u0119 milsi. Dajemy jeden przyk\u0142ad, \u017ceby go nakierowa\u0107. M\u00f3wimy. See Otter Lauter de Merch. Cheese. I ciekamy, a\u017c za\u0142apie schemat. I wreszcie few shot, czyli serce tej pracy. Tutaj dajemy modelowi od kilku do kilkudziesi\u0119ciu przyk\u0142ad\u00f3w. Wszystko w jednym zapytaniu, w jego oknie kontekstowym. A potem zadanie do wykonania. No i tu dochodzimy do g\u0142\u00f3wnej hipotezy, do wielkiego zak\u0142adu, kt\u00f3ry postawili badacze z OpenAI. A brzmia\u0142 on? Postawili tez\u0119, \u017ce je\u015bli we\u017cmiemy architektur\u0119 modelu j\u0119zykowego i przeskalujemy j\u0105 do absurdalnych rozmiar\u00f3w, tych 175 miliard\u00f3w parametr\u00f3w. To ta zdolno\u015b\u0107 do uczenia si\u0119 w locie, czyli in-context learning, po prostu eksploduje. Stanie si\u0119 tak dobra, \u017ce w wielu zadaniach dor\u00f3wna, albo nawet przebije modele, kt\u00f3re przesz\u0142y ten ca\u0142y \u017cmudny proces fine tuningu. A wszystko bez jednej trwa\u0142ej zmiany w modelu. Ok, rozumiem. Wi\u0119kszy model to lepsze wyniki, to ma jaki\u015b intuicyjny sens. Ale szukam tego momentu opadni\u0119cia szcz\u0119ki. Co by\u0142o tym jednym konkretnym osi\u0105gni\u0119ciem, o kt\u00f3rym ludzie w OpenAI otworzyli szampana, a ich konkurenci z\u0142apali si\u0119 za g\u0142ow\u0119. W tych moment\u00f3w by\u0142o kilka, ale zacznijmy od czego\u015b, co nazywa si\u0119 trywia QA. To jest benchmark, kt\u00f3ry dzia\u0142a troch\u0119 jak teleturniej. Zadajesz modelowi pytania z wiedzy og\u00f3lnej. Ale kluczowy jest tu tryb closed book. Model nie mo\u017ce szuka\u0107 odpowiedzi w internecie. Musi polega\u0107 wy\u0142\u0105cznie na wiedzy, kt\u00f3r\u0105 zapami\u0119ta\u0142 podczas swojego pierwotnego treningu. Jak sobie poradzi\u0142? Jaki by\u0142 wynik? GPT-3 w trybie Fue Shot osi\u0105gn\u0119\u0142o 71,20% dok\u0142adno\u015bci. I teraz, wiesz, ta liczba sama w sobie mo\u017ce niewiele m\u00f3wi\u0107. No w\u0142a\u015bnie, 71,20%. To brzmi dobrze, ale liczby potrafi\u0105 by\u0107 myl\u0105ce. Musimy to umie\u015bci\u0107 w jakim\u015b kontek\u015bcie. Kto by\u0142 poprzednim mistrzem w tej dziedzinie i z jakim wynikiem? Czy to by\u0142 ma\u0142y kroczek naprz\u00f3d, czy raczej knockout? To by\u0142 knockout. GPT-3 pobi\u0142o dotychczasowy najlepszy model TP-11B, kt\u00f3ry by\u0142 specjalnie dostrajany fine-tuned do tego konkretnego zadania i tu jest ta fundamentalna r\u00f3\u017cnica. To nie by\u0142o tylko pobicie rekordu. To by\u0142 jakby amator, kt\u00f3ry przeczyta\u0142 ca\u0142\u0105 bibliotek\u0119, wszed\u0142 na Olimpiad\u0119 i pobi\u0142 w biegu na 100 metr\u00f3w sportowc\u00f3w, kt\u00f3rzy trenowali tylko t\u0105 jedn\u0105 dyscyplin\u0119 przez ca\u0142e \u017cycie. To podwa\u017cy\u0142o sensobno\u015b\u0107 ca\u0142ego tego specjalistycznego treningu. Rozumiem, to pokazuje, \u017ce wiedza og\u00f3lna na masow\u0105 skal\u0119 mo\u017ce by\u0107 pot\u0119\u017cniejsza ni\u017c w\u0105ska specjalizacja. A co z innymi zadaniami? Gdzie jeszcze ta skala zrobi\u0142a tak\u0105 r\u00f3\u017cnic\u0119? Byli\u015bmy zadaniem lambada. Polega ono na przewidzeniu ostatniego s\u0142owa w d\u0142u\u017cszym akapicie. To jest test na rozumienie szerokiego kontekstu. Nie wystarczy spojrze\u0107 na kilka ostatnich s\u0142\u00f3w. Trzeba zrozumie\u0107 ca\u0142\u0105 histori\u0119. Poprzedni State of the Art wynosi\u0142 tam oko\u0142o 60% dok\u0142adno\u015bci. GPT-3 w trybie Fuel Shot osi\u0105gn\u0119\u0142o ponad 76%. Poprawa o ponad 18 punkt\u00f3w procentowych. W \u015bwiecie AI to nie jest krok, to jest skok przez przepa\u015b\u0107. OK, to wszystko s\u0105 wyniki w benchmarkach, kt\u00f3re robi\u0105 wra\u017cenia na badaczach. Ale by\u0142 te\u017c test, kt\u00f3ry zszokowa\u0142 chyba wszystkich, bo dotyczy\u0142 czego\u015b bardzo ludzkiego oceny autentyczno\u015bci tekstu. Zdecydowanie. To by\u0142 chyba ten najbardziej medialny i nieukrywajmy niepokoj\u0105cy eksperyment. Badaczy wygenerowali za pomoc\u0105 GPT-3 kr\u00f3tkie artyku\u0142y prasowe na oko\u0142o 200 s\u0142\u00f3w. Nast\u0119pnie pokazali je ludziom wymieszane z prawdziwymi artyku\u0142ami napisanymi przez dziennikarzy i zadali proste pytanie. Kt\u00f3ry tekst napisa\u0142 cz\u0142owiek, a kt\u00f3ry maszyna? A jaki by\u0142 wynik? Ludzie potrafili poprawnie wskaza\u0107 tekst maszyny w zaledwie 52% przypadk\u00f3w. Czekaj, tylko 52% to jest na granicy b\u0142\u0119du statystycznego. 50% to rzut monet\u0105, czyli w zasadzie r\u00f3wnie dobrze mogliby\u015bmy pyta\u0107 o zdanie mojego psa. To pokazuje, jak bardzo nasze intuicje dotycz\u0105ce ludzkiego pisania sta\u0142y si\u0119 niewiarygodne. Dok\u0142adnie, a co jeszcze ciekawsze i co wida\u0107 na jednym z wykres\u00f3w pracy, ta zdolno\u015b\u0107 odr\u00f3\u017cniania spada\u0142a dramatycznie wraz ze wzrostem wielko\u015bci modelu. Dla mniejszych modeli ludzie radzili sobie ca\u0142kiem nie\u017ale. Dla najwi\u0119kszego GPT-3 byli niema\u0142 bezradni. I to jest co\u015b, co autorzy sami nazwali niepokoj\u0105cym kamieniem milowym. A propos kali. Wr\u00f3\u0107my na chwil\u0119 do tych wykres\u00f3w. Bo one pokazywa\u0142y co\u015b wi\u0119cej ni\u017c tylko to, \u017ce wi\u0119kszy znaczy lepszy. By\u0142 tam pewien subtelny, ale kluczowy trend. Tak, to wida\u0107 na samym gocz\u0105tku na figure 1.1. W wykres po lewej stronie pokazuje, \u017ce wraz ze wzrostem liczby parametr\u00f3w ro\u015bnie wydajno\u015b\u0107 we wszystkich trybach zero shot, one shot i few shot. Ale i to jest kluczowe, r\u00f3\u017cnica mi\u0119dzy wydajno\u015bci\u0105 few shot a zero shot te\u017c si\u0119 powi\u0119ksza. Co to w\u0142a\u015bciwie oznacza w praktyce? Oznacza to, \u017ce wi\u0119ksze modele staj\u0105 si\u0119 nieproporcjonalnie lepsze w uczeniu si\u0119 w locie z kontekstu. To nie jest liniowy przyrost. One nie tylko wiedz\u0105 wi\u0119cej. One staj\u0105 si\u0119 sprawniejsze w wykorzystywaniu nowych informacji. Autorzy nazywaj\u0105 je bardziej bieg\u0142ymi metal learners. Metal learners? To brzmi jak co\u015b z podr\u0119cznika do filozofii. Co to tak naprawd\u0119 znaczy? To znaczy, \u017ce te modele nie tylko nauczy\u0142y si\u0119 j\u0119zyka, ale nauczy\u0142y si\u0119 samej sztuki uczenia si\u0119. Jak ucze\u0144, kt\u00f3ry nie tylko wkuwa fakty, ale zrozumia\u0142, jak si\u0119 efektywnie uczy\u0107 z dowolnego materia\u0142u, kt\u00f3ry mu si\u0119 podsunie. Im wi\u0119kszy model, tym szybciej \u0142apie w locie o co chodzi w nowej grze, nawet je\u015bli nikt nie zmienia\u0142 zasad w jego g\u0142owie na sta\u0142e. To wszystko brzni niemal jak science fiction. Wyniki, kt\u00f3re bij\u0105 specjalist\u00f3w, tekst nieodr\u00f3\u017cnialny od ludzkiego, a\u017c trudno uwierzy\u0107, \u017ce to nie jest jaka\u015b forma sztucznej og\u00f3lnej inteligencji. Ale autorzy sami sprowadzaj\u0105 nas na ziemi\u0119, po\u015bwi\u0119caj\u0105c sporo miejsca na to, gdzie ta magia przestaje dzia\u0142a\u0107. I to jest jedyna z najwi\u0119kszych zalet tej pracy, ta intelektualna uczciwo\u015b\u0107. Nie pr\u00f3bowali sprzeda\u0107 GPT-3 jako magicznego rozwi\u0105zania wszystkich problem\u00f3w. Po\u015bwi\u0119cili ca\u0142\u0105 sekcj\u0119 na szczeg\u00f3\u0142owe om\u00f3wienie jego s\u0142abo\u015bci. No w\u0142a\u015bnie. Gdzie s\u0105 te p\u0119kni\u0119cia wzbroi? Co by\u0142o najwi\u0119ksz\u0105 pora\u017ck\u0105 GPT-3? Pierwszy fundamentalny problem to sp\u00f3jno\u015b\u0107 na d\u0142u\u017csz\u0105 met\u0119. Model potrafi\u0142 wygenerowa\u0107 genialne akapit, a nawet kilka. Ale w tek\u015bcie na kilka stron zaczyna\u0142 si\u0119 gubi\u0107. Potrafi\u0142 si\u0119 powtarza\u0107, traci\u0107 g\u0142\u00f3wny w\u0105tek, a czasem nawet zaprzeczy\u0107 temu, co napisa\u0142 kilka akapit\u00f3w wcze\u015bniej. By\u0142 sprinterem niemarat\u0105czykiem. Druga wada, jak rozumiem, by\u0142a wpisana w sam\u0105 jego architektur\u0119? Tak. GPT-3, podobnie jak jego poprzednicy, jest modelem jednokierunkowym. Przetwarza tekst od lewej do prawej, jak cz\u0142owiek czytaj\u0105cy ksi\u0105\u017ck\u0119. To jest \u015bwietne dogenerowania tekstu, ale fatalne do zada\u0144, kt\u00f3re wymagaj\u0105 por\u00f3wnania dw\u00f3ch fragment\u00f3w tekstu i spojrzenia na nie z obu stron jednocze\u015bnie. Mo\u017cesz poda\u0107 jaki\u015b przyk\u0142ad? Klasyczny przyk\u0142ad to zadanie WIS z benchmarku Super Glue. Model dostaje dwa zdania, w kt\u00f3rych wyst\u0119puje to samo s\u0142owo i ma oceni\u0107, czy zosta\u0142o u\u017cyte w tym samym znaczeniu. Na przyk\u0142ad rzuci\u0142em pi\u0142k\u0119 i urz\u0105dzili\u015bmy wielki bal. Czy bal znaczy to samo? Dla nas to oczywiste, \u017ce nie. Dla GPT-3, kt\u00f3re nie mo\u017ce efektywnie skaka\u0107 mi\u0119dzy zdaniami, by\u0142o to niemal niemo\u017cliwe. Jego wyniki by\u0142y tam bliskie losowym. I wreszcie trzecia chyba najg\u0142\u0119bsza wada, kt\u00f3r\u0105 autorzy nazwali brakiem ugruntowania w rzeczywisto\u015bci. Dok\u0142adnie GPT-3 jest mistrzem tekstu, \u017cyje w \u015bwiecie s\u0142\u00f3w, ale nie ma poj\u0119cia o \u015bwiecie fizycznym. Nigdy nie widzia\u0142 czerwonego jab\u0142ka, nie poczu\u0142 zimna, nie podnios\u0142 ci\u0119\u017ckiego kamienia. Brakuje mu ogromnej cz\u0119\u015bci kontekstu, kt\u00f3re my ludzie czerpiemy ze zmys\u0142\u00f3w. To prowadzi do b\u0142\u0119d\u00f3w, kt\u00f3re dla nas wydaj\u0105 si\u0119 absurdalne. B\u0142\u0119d\u00f3w wynikaj\u0105cych z braku zdrowego rozs\u0105dku opartego na fizycznym do\u015bwiadczeniu. To jak kto\u015b, kto przeczyta\u0142 ka\u017cd\u0105 ksi\u0105\u017ck\u0119 o p\u0142ywaniu, ale nigdy nie wszed\u0142 do wody. OK, czyli mamy ograniczenia techniczne, ale poza nimi autorzy po\u015bwi\u0119cili te\u017c bardzo du\u017co miejsca na analiz\u0119 znacznie trudniejszego problemu. Co si\u0119 stanie, gdy ta technologia trafi w r\u0119ce ludzi? M\u00f3wimy o sekcji Broader Impact. Autorzy nie uciekali od odpowiedzialno\u015bci, otwarcie analizowali potencjalne nadu\u017cycia i, co niezwykle wa\u017cne, problem wbudowanej w model stronniczo\u015bci, czyli bias. Zacznijmy od nadu\u017cy\u0107, czego si\u0119 obawiali w 2020 roku. Wskazywali na te oczywiste ryzyka, masowe generowanie dezinformacji z personalizowanego spamu czy fishingu, plagiaty na skal\u0119 przemys\u0142ow\u0105. Wtedy w 2020 oceniali, \u017ce zagro\u017cenie nie jest jeszcze bezpo\u015brednie, bo model wci\u0105\u017c pope\u0142nia\u0142 b\u0142\u0119dy i nie by\u0142 wystarczaj\u0105co niezale\u017cny, \u017ceby oszukiwa\u0107 na du\u017c\u0105 skal\u0119. Ale jasno stwierdzili, \u017ce dalszy post\u0119p mo\u017ce to zmieni\u0107. A ten wynik 52% w te\u015bcie na odr\u00f3\u017cnianie tekst\u00f3w nazwali, jak ju\u017c m\u00f3wi\u0142y\u015bmy, niepokoj\u0105cym kamieniem milowym, kt\u00f3ry pokazuje dok\u0105d to zmierza. Ale jeszcze wi\u0119kszym problemem, bo bardziej subtelnym, okaza\u0142 si\u0119 bias. Model, ucz\u0105c si\u0119 z internetu, nauczy\u0142 si\u0119 te\u017c wszystkich naszych ludzkich uprzedze\u0144. To jest kluczowa lekcja z tej pracy. Model jest jak lustro, odbija to, co mu pokazano, a internet, na kt\u00f3rym si\u0119 uczy\u0142, no nie jest miejscem neutralnym ani sprawiedliwym. Autorzy przeprowadzili seri\u0119 test\u00f3w, \u017ceby zmierzy\u0107 to uprzedzenia i wyniki by\u0142y do\u015b\u0107 ponure. Odkryli, \u017ce model bardzo silnie skojarzy\u0142 okre\u015blone zawody z p\u0142ci\u0105. A\u017c 83% testowanych profesji mia\u0142o m\u0119skie konotacje. Zawody wymagaj\u0105ce wy\u017cszego wykszta\u0142cenia, jak legislator, bankier, profesor, by\u0142y w jego oczach zdecydowanie m\u0119skie. Z kolei rol\u0119, takie jak piel\u0119gniarka, recepcjonistka czy gospodzia, kobiece. Co wi\u0119cej, model znacznie cz\u0119\u015bciej opisywa\u0142 kobiety u\u017cywaj\u0105c przymiotnik\u00f3w zwi\u0105zanych z wygl\u0105dem, jak beautiful czy gorgeous. Czyli odtworzy\u0142 klasyczne stereotypy. A co z ras\u0105? Tu podobno wyniki by\u0142y jeszcze bardziej jednoznaczne. Tak, tutaj przeprowadzili analiz\u0119 sentymentu. Sprawdzali, jakie nacechowanie emocjonalne maj\u0105 teksty generowane przez model w kontek\u015bcie r\u00f3\u017cnych grup rasowych. I jeden z wykres\u00f3w, figure 7.1, pokazuje to bardzo wyra\u017anie. Tre\u015bci powi\u0105zane z ras\u0105 azjatyczk\u0105 mia\u0142y konsekwentnie pozytywny wyd\u017awi\u0119k. Z kolei te dotycz\u0105ce os\u00f3b czarnosku\u0142ych konsekwentnie negatywny. To nie jest tak, \u017ce model ma pogl\u0105dy. On po prostu statystycznie odtwarza wzorce, kt\u00f3re znalaz\u0142 w miliardach tekst\u00f3w z internetu. I na koniec religia. Tutaj by\u0142o podobnie. S\u0142owa takie jak terrorism czy violent pojawia\u0142y si\u0119 ze znacznie wi\u0119ksz\u0105 cz\u0119stotliwo\u015bci\u0105 w kontek\u015bcie Islamu ni\u017c chrze\u015bcija\u0144stwa, budyzmu czy judaizmu. To znowu jest odbicie tego, jak te tematy s\u0105 reprezentowane w danych treningowych. Model jest po prostu niezwykle skutecznym na\u015bladowc\u0105 ze wszystkimi tego konsekwencjami. Podsumowuj\u0105c, dok\u0105d nas to wszystko prowadzi? Z jednej strony mamy prze\u0142om. GPT-3 pokaza\u0142o, \u017ce sama skala w po\u0142\u0105czeniu z nowym podej\u015bciem in-context learning odblokowuje zdolno\u015bci, kt\u00f3re wcze\u015bniej wydawa\u0142y si\u0119 domen\u0105 wyspecjalizowanych modeli. To jest zmiana paradygmatu. I co wi\u0119cej, ta praca udowodni\u0142a, \u017ce ogromne modele nie s\u0105 tylko ilo\u015bciowo leksze. One staj\u0105 si\u0119 jako\u015bciowo inne. Zaczynaj\u0105 dzia\u0142a\u0107 jak meta-learners, ucz\u0105 si\u0119 jak si\u0119 uczy\u0107. Ale jednocze\u015bnie jak lustro o ogromnej mocy powi\u0119kszaj\u0105cej pokaza\u0142a, \u017ce skala wzmaznia te\u017c istniej\u0105ce problemy. Uprzedzenia staj\u0105 si\u0119 ostrzejsze, a potencjalne nadu\u017cycia bardziej realne. Te kwestie, podniesione tak wyra\u017anie w 2020 roku, s\u0105 dzi\u015b w samym centrum debaty o odpowiedzialnym rozwoju AI. Autorzy ko\u0144cz\u0105 swoj\u0105 prac\u0119 bardzo ciekaw\u0105 refleksj\u0105 na przysz\u0142o\u015b\u0107. Stwierdzaj\u0105, \u017ce samo przewidywanie kolejnego s\u0142owa nawet na niewyobra\u017caln\u0105 skal\u0119 prawdopodobnie kiedy\u015b dojdzie do \u015bciany. \u017be b\u0119dzie to trzeba uzupe\u0142ni\u0107 o co\u015b wi\u0119cej. Tak, sugeruj\u0105, \u017ce przysz\u0142o\u015b\u0107 mo\u017ce le\u017cy\u0107 w \u0142\u0105czeniu tego podej\u015bcia z innymi. Na przyk\u0142ad z uczeniem si\u0119 bezpo\u015brednio z ludzkich opinii albo z dodaniem innych zmys\u0142\u00f3w, modalno\u015bci, takich jak obrazy, d\u017awi\u0119k czy wideo. I to prowadzi nas do ostatniej prowokacyjnej my\u015bli na koniec. Zatem co to za my\u015bl? Urd\u00f3w w teleturniejach, popisanie wiarygodnych artyku\u0142\u00f3w. To jakie zupe\u0142nie nowe, dzi\u015b niewyobra\u017calne mo\u017cliwo\u015bci zostan\u0105 odblokowane, gdy takie systemy zostan\u0105 wreszcie osadzone w bogatszym \u015bwiecie. W \u015bwiecie obrazu, d\u017awi\u0119ku i fizycznej interakcji. I jakie zupe\u0142nie nowe, nieznane nam jeszcze uprzedzenia wnios\u0105 ze sob\u0105 te nowe rodzaje danych.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.7, "text": " Wyobra\u017amy sobie tak\u0105 sytuacj\u0119. Jest, powiedzmy, rok 2019.", "tokens": [50364, 14458, 24393, 10659, 2226, 13652, 31069, 28275, 29924, 13, 24918, 11, 27617, 2226, 11, 35135, 6071, 13, 50599], "temperature": 0.0, "avg_logprob": -0.11282360131013477, "compression_ratio": 1.40625, "no_speech_prob": 0.00824408046901226}, {"id": 1, "seek": 0, "start": 4.7, "end": 7.4, "text": " Sztuczna inteligencja robi ju\u017c niesamowite rzeczy,", "tokens": [50599, 318, 2682, 1311, 35458, 24777, 3213, 34056, 47380, 10678, 48100, 335, 305, 642, 26297, 11, 50734], "temperature": 0.0, "avg_logprob": -0.11282360131013477, "compression_ratio": 1.40625, "no_speech_prob": 0.00824408046901226}, {"id": 2, "seek": 0, "start": 7.4, "end": 10.3, "text": " ale ma jedn\u0105 fundamentaln\u0105 wad\u0119.", "tokens": [50734, 6775, 463, 5232, 13113, 8088, 13113, 261, 345, 1274, 13, 50879], "temperature": 0.0, "avg_logprob": -0.11282360131013477, "compression_ratio": 1.40625, "no_speech_prob": 0.00824408046901226}, {"id": 3, "seek": 0, "start": 10.3, "end": 13.1, "text": " Je\u015bli chcesz j\u0105 nauczy\u0107 czego\u015b nowego,", "tokens": [50879, 37086, 417, 887, 89, 35692, 49103, 27150, 36559, 1788, 586, 6308, 11, 51019], "temperature": 0.0, "avg_logprob": -0.11282360131013477, "compression_ratio": 1.40625, "no_speech_prob": 0.00824408046901226}, {"id": 4, "seek": 0, "start": 13.1, "end": 16.9, "text": " no nie wiem, rozumienia \u017cargonu prawniczego albo pisania wierszy,", "tokens": [51019, 572, 2838, 26522, 11, 48797, 18811, 19625, 289, 10660, 84, 37047, 17946, 6308, 22622, 26584, 5609, 261, 4890, 1229, 11, 51209], "temperature": 0.0, "avg_logprob": -0.11282360131013477, "compression_ratio": 1.40625, "no_speech_prob": 0.00824408046901226}, {"id": 5, "seek": 0, "start": 16.9, "end": 20.3, "text": " to musisz j\u0105 jakby wys\u0142a\u0107 na nowo na uniwersytet,", "tokens": [51209, 281, 1038, 23848, 35692, 28976, 27062, 5024, 2162, 1667, 586, 78, 1667, 36435, 5364, 4328, 302, 11, 51379], "temperature": 0.0, "avg_logprob": -0.11282360131013477, "compression_ratio": 1.40625, "no_speech_prob": 0.00824408046901226}, {"id": 6, "seek": 0, "start": 20.3, "end": 23.6, "text": " stworzy\u0107 gigantyczny, specjalistyczny zbi\u00f3r danych", "tokens": [51379, 342, 28321, 27150, 8741, 394, 17466, 1634, 11, 46433, 468, 17466, 1634, 710, 5614, 15614, 274, 34644, 51544], "temperature": 0.0, "avg_logprob": -0.11282360131013477, "compression_ratio": 1.40625, "no_speech_prob": 0.00824408046901226}, {"id": 7, "seek": 0, "start": 23.6, "end": 27.3, "text": " i przeprowadzi\u0107, no, kosztowny trening.", "tokens": [51544, 741, 30829, 1892, 345, 28496, 11, 572, 11, 19532, 2682, 648, 88, 2192, 773, 13, 51729], "temperature": 0.0, "avg_logprob": -0.11282360131013477, "compression_ratio": 1.40625, "no_speech_prob": 0.00824408046901226}, {"id": 8, "seek": 2730, "start": 27.400000000000002, "end": 30.7, "text": " A co gdyby istnia\u0142 model, kt\u00f3ry przeczyta\u0142 tak du\u017co,", "tokens": [50369, 316, 598, 28405, 2322, 1418, 77, 8908, 2316, 11, 9913, 8325, 6522, 46426, 991, 26673, 11, 50534], "temperature": 0.0, "avg_logprob": -0.12879740241115079, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.05904153361916542}, {"id": 9, "seek": 2730, "start": 30.7, "end": 33.6, "text": " \u017ce nowej umiej\u0119tno\u015bci uczy si\u0119 w locie?", "tokens": [50534, 3561, 586, 40779, 1105, 7764, 46788, 16438, 344, 6522, 3244, 261, 1628, 414, 30, 50679], "temperature": 0.0, "avg_logprob": -0.12879740241115079, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.05904153361916542}, {"id": 10, "seek": 2730, "start": 33.6, "end": 35.8, "text": " Po prostu na podstawie kilku przyk\u0142ad\u00f3w,", "tokens": [50679, 6165, 19518, 1667, 43443, 414, 5128, 5279, 23144, 3901, 11, 50789], "temperature": 0.0, "avg_logprob": -0.12879740241115079, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.05904153361916542}, {"id": 11, "seek": 2730, "start": 35.8, "end": 37.5, "text": " kt\u00f3rym urzucisz w rozmowie.", "tokens": [50789, 30120, 4038, 89, 1311, 23848, 261, 35234, 13998, 13, 50874], "temperature": 0.0, "avg_logprob": -0.12879740241115079, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.05904153361916542}, {"id": 12, "seek": 2730, "start": 37.5, "end": 39.6, "text": " Bez \u017cadnego dodatkowego szkolenia.", "tokens": [50874, 879, 89, 39628, 11858, 13886, 33525, 26576, 7870, 74, 11940, 654, 13, 50979], "temperature": 0.0, "avg_logprob": -0.12879740241115079, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.05904153361916542}, {"id": 13, "seek": 2730, "start": 39.6, "end": 41.6, "text": " I to jest, wiesz, dok\u0142adnie ten moment,", "tokens": [50979, 286, 281, 3492, 11, 261, 15347, 11, 45864, 2766, 2064, 1623, 11, 51079], "temperature": 0.0, "avg_logprob": -0.12879740241115079, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.05904153361916542}, {"id": 14, "seek": 2730, "start": 41.6, "end": 44.900000000000006, "text": " w kt\u00f3rym na scen\u0119 wchodzi praca, o kt\u00f3rej dzisiaj rozmawiamy.", "tokens": [51079, 261, 30120, 1667, 4191, 1274, 261, 34616, 582, 6628, 11, 277, 36023, 25772, 35234, 1607, 2918, 88, 13, 51244], "temperature": 0.0, "avg_logprob": -0.12879740241115079, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.05904153361916542}, {"id": 15, "seek": 2730, "start": 44.900000000000006, "end": 49.7, "text": " Artyku\u0142 z 2020 roku napisany przez zesp\u00f3\u0142 OpenAI, tytu\u0142.", "tokens": [51244, 1587, 874, 5279, 1221, 710, 4808, 19451, 9296, 271, 1325, 14064, 710, 13361, 16181, 7238, 48698, 11, 1104, 9179, 1221, 13, 51484], "temperature": 0.0, "avg_logprob": -0.12879740241115079, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.05904153361916542}, {"id": 16, "seek": 2730, "start": 49.7, "end": 52.6, "text": " Language models are few shot learners.", "tokens": [51484, 24445, 5245, 366, 1326, 3347, 23655, 13, 51629], "temperature": 0.0, "avg_logprob": -0.12879740241115079, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.05904153361916542}, {"id": 17, "seek": 2730, "start": 52.6, "end": 56.3, "text": " M\u00f3wi\u0105c wprost, to jest tekst, kt\u00f3ry przedstawi\u0142 \u015bwiatu GPT-3", "tokens": [51629, 376, 3901, 11404, 66, 261, 1424, 555, 11, 281, 3492, 16624, 372, 11, 9913, 45616, 40622, 21485, 20546, 26039, 51, 12, 18, 51814], "temperature": 0.0, "avg_logprob": -0.12879740241115079, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.05904153361916542}, {"id": 18, "seek": 5630, "start": 56.3, "end": 59.5, "text": " i, no, wywr\u00f3ci\u0142 stolik w ca\u0142ym \u015bwiecie AI.", "tokens": [50364, 741, 11, 572, 11, 4628, 7449, 812, 537, 1221, 43553, 1035, 261, 35224, 4199, 40078, 4260, 7318, 13, 50524], "temperature": 0.0, "avg_logprob": -0.0925149849482945, "compression_ratio": 1.3892857142857142, "no_speech_prob": 0.016062410548329353}, {"id": 19, "seek": 5630, "start": 59.5, "end": 62.699999999999996, "text": " OK, czyli nasz cel jest ambitny.", "tokens": [50524, 2264, 11, 16591, 5382, 89, 9277, 3492, 3913, 270, 1634, 13, 50684], "temperature": 0.0, "avg_logprob": -0.0925149849482945, "compression_ratio": 1.3892857142857142, "no_speech_prob": 0.016062410548329353}, {"id": 20, "seek": 5630, "start": 62.699999999999996, "end": 66.39999999999999, "text": " Chcemy zrozumie\u0107, co by\u0142o a\u017c tak rewolucyjnego w tym modelu,", "tokens": [50684, 761, 384, 2226, 710, 27857, 449, 414, 2162, 11, 598, 14811, 48134, 991, 319, 48481, 1311, 88, 73, 11858, 261, 8107, 2316, 84, 11, 50869], "temperature": 0.0, "avg_logprob": -0.0925149849482945, "compression_ratio": 1.3892857142857142, "no_speech_prob": 0.016062410548329353}, {"id": 21, "seek": 5630, "start": 66.39999999999999, "end": 70.7, "text": " jak dzia\u0142a ta jego magia, czyli tak zwane in-context learning", "tokens": [50869, 4207, 37903, 1846, 26542, 2258, 654, 11, 16591, 991, 11873, 1929, 294, 12, 9000, 3828, 2539, 51084], "temperature": 0.0, "avg_logprob": -0.0925149849482945, "compression_ratio": 1.3892857142857142, "no_speech_prob": 0.016062410548329353}, {"id": 22, "seek": 5630, "start": 70.7, "end": 74.2, "text": " i co ta jego wtedy niewyobra\u017calna skala,", "tokens": [51084, 741, 598, 1846, 26542, 26959, 43622, 88, 24393, 1427, 304, 629, 1110, 5159, 11, 51259], "temperature": 0.0, "avg_logprob": -0.0925149849482945, "compression_ratio": 1.3892857142857142, "no_speech_prob": 0.016062410548329353}, {"id": 23, "seek": 5630, "start": 74.2, "end": 77.1, "text": " 175 miliard\u00f3w parametr\u00f3w,", "tokens": [51259, 41165, 1962, 72, 515, 3901, 6220, 27965, 3901, 11, 51404], "temperature": 0.0, "avg_logprob": -0.0925149849482945, "compression_ratio": 1.3892857142857142, "no_speech_prob": 0.016062410548329353}, {"id": 24, "seek": 5630, "start": 77.1, "end": 80.3, "text": " co nam to tak naprawd\u0119 powiedzia\u0142o o przysz\u0142o\u015bci.", "tokens": [51404, 598, 8835, 281, 991, 20970, 27617, 654, 5249, 277, 44018, 35059, 13, 51564], "temperature": 0.0, "avg_logprob": -0.0925149849482945, "compression_ratio": 1.3892857142857142, "no_speech_prob": 0.016062410548329353}, {"id": 25, "seek": 5630, "start": 80.3, "end": 83.7, "text": " Zar\u00f3wno o tych, wiesz, osza\u0142amiaj\u0105cych mo\u017cliwo\u015bciach,", "tokens": [51564, 41580, 812, 20944, 277, 15180, 11, 261, 15347, 11, 3003, 2394, 20177, 48125, 31306, 30854, 36476, 608, 11, 51734], "temperature": 0.0, "avg_logprob": -0.0925149849482945, "compression_ratio": 1.3892857142857142, "no_speech_prob": 0.016062410548329353}, {"id": 26, "seek": 8370, "start": 83.7, "end": 86.3, "text": " jak i o cieniach, kt\u00f3re si\u0119 za nimi kryj\u0105.", "tokens": [50364, 4207, 741, 277, 269, 35462, 608, 11, 8864, 3244, 7949, 297, 10121, 34847, 8555, 13, 50494], "temperature": 0.0, "avg_logprob": -0.08075100144529654, "compression_ratio": 1.393846153846154, "no_speech_prob": 0.007476169150322676}, {"id": 27, "seek": 8370, "start": 86.3, "end": 89.2, "text": " Zanim jednak zanurkujemy w samo GPT-3,", "tokens": [50494, 1176, 17869, 25897, 710, 282, 374, 74, 21767, 261, 36422, 26039, 51, 12, 18, 11, 50639], "temperature": 0.0, "avg_logprob": -0.08075100144529654, "compression_ratio": 1.393846153846154, "no_speech_prob": 0.007476169150322676}, {"id": 28, "seek": 8370, "start": 89.2, "end": 91.9, "text": " musimy zrozumie\u0107 \u015bwiat, kt\u00f3ry by\u0142 przed nim.", "tokens": [50639, 43449, 710, 27857, 449, 414, 2162, 36425, 11, 9913, 16673, 18334, 24887, 13, 50774], "temperature": 0.0, "avg_logprob": -0.08075100144529654, "compression_ratio": 1.393846153846154, "no_speech_prob": 0.007476169150322676}, {"id": 29, "seek": 8370, "start": 91.9, "end": 96.9, "text": " Standardem by\u0142 taki proces dwuetapowy, pierwszy krok to pre-training.", "tokens": [50774, 21298, 443, 16673, 20065, 17565, 27379, 15382, 569, 10089, 11, 34016, 350, 31621, 281, 659, 12, 17227, 1760, 13, 51024], "temperature": 0.0, "avg_logprob": -0.08075100144529654, "compression_ratio": 1.393846153846154, "no_speech_prob": 0.007476169150322676}, {"id": 30, "seek": 8370, "start": 96.9, "end": 98.9, "text": " Czyli budowanie fundamentu.", "tokens": [51024, 37099, 3265, 22028, 6073, 84, 13, 51124], "temperature": 0.0, "avg_logprob": -0.08075100144529654, "compression_ratio": 1.393846153846154, "no_speech_prob": 0.007476169150322676}, {"id": 31, "seek": 8370, "start": 98.9, "end": 103.7, "text": " Bierzesz model i ka\u017cesz mu przeczyta\u0107, no, prawie ca\u0142y internet.", "tokens": [51124, 363, 34602, 10430, 2316, 741, 21912, 10430, 2992, 8325, 6522, 42931, 11, 572, 11, 3206, 8699, 35226, 4705, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08075100144529654, "compression_ratio": 1.393846153846154, "no_speech_prob": 0.007476169150322676}, {"id": 32, "seek": 8370, "start": 103.7, "end": 107.4, "text": " Ogromn\u0105 bibliotek\u0119 tekst\u00f3w, ksi\u0105\u017cek, artyku\u0142\u00f3w.", "tokens": [51364, 422, 861, 298, 13113, 34344, 310, 916, 1274, 16624, 372, 3901, 11, 39311, 916, 11, 594, 874, 5279, 1221, 3901, 13, 51549], "temperature": 0.0, "avg_logprob": -0.08075100144529654, "compression_ratio": 1.393846153846154, "no_speech_prob": 0.007476169150322676}, {"id": 33, "seek": 8370, "start": 107.4, "end": 111.0, "text": " Celem nie jest nauczenie go konkretnego zadania, prawda?", "tokens": [51549, 8257, 10386, 2838, 3492, 49103, 16778, 352, 36500, 11858, 42788, 5609, 11, 43607, 30, 51729], "temperature": 0.0, "avg_logprob": -0.08075100144529654, "compression_ratio": 1.393846153846154, "no_speech_prob": 0.007476169150322676}, {"id": 34, "seek": 8370, "start": 111.0, "end": 113.4, "text": " Tylko tego, jak w og\u00f3le dzia\u0142a j\u0119zyk.", "tokens": [51729, 49286, 4093, 8627, 11, 4207, 261, 29229, 37903, 49055, 74, 13, 51849], "temperature": 0.0, "avg_logprob": -0.08075100144529654, "compression_ratio": 1.393846153846154, "no_speech_prob": 0.007476169150322676}, {"id": 35, "seek": 11340, "start": 113.4, "end": 117.7, "text": " Gramatyki, fakt\u00f3w o \u015bwiecie, zwi\u0105zk\u00f3w mi\u0119dzy s\u0142owami.", "tokens": [50364, 22130, 21398, 2984, 11, 21310, 3901, 277, 40078, 4260, 11, 27741, 23849, 33964, 15116, 305, 4526, 13, 50579], "temperature": 0.0, "avg_logprob": -0.10184009346419466, "compression_ratio": 1.4935897435897436, "no_speech_prob": 0.0021132072433829308}, {"id": 36, "seek": 11340, "start": 117.7, "end": 120.5, "text": " To jakby da\u0107 komu\u015b takie og\u00f3lne wykszta\u0142cenie.", "tokens": [50579, 1407, 28976, 1120, 2162, 5207, 84, 1788, 15963, 5360, 15741, 716, 4628, 1694, 89, 46426, 13037, 414, 13, 50719], "temperature": 0.0, "avg_logprob": -0.10184009346419466, "compression_ratio": 1.4935897435897436, "no_speech_prob": 0.0021132072433829308}, {"id": 37, "seek": 11340, "start": 120.5, "end": 126.60000000000001, "text": " Dok\u0142adnie. Ale po tym og\u00f3lnym wykszta\u0142ceniu przychodzi\u0142 czas na specjalizacj\u0119, czyli drugi krok.", "tokens": [50719, 29768, 10358, 2766, 13, 9366, 714, 8107, 5360, 15741, 12996, 4628, 1694, 89, 46426, 13037, 5951, 6501, 34616, 1221, 13190, 1667, 46433, 590, 29924, 11, 16591, 4110, 72, 350, 31621, 13, 51024], "temperature": 0.0, "avg_logprob": -0.10184009346419466, "compression_ratio": 1.4935897435897436, "no_speech_prob": 0.0021132072433829308}, {"id": 38, "seek": 11340, "start": 126.60000000000001, "end": 128.1, "text": " Fine tuning.", "tokens": [51024, 12024, 15164, 13, 51099], "temperature": 0.0, "avg_logprob": -0.10184009346419466, "compression_ratio": 1.4935897435897436, "no_speech_prob": 0.0021132072433829308}, {"id": 39, "seek": 11340, "start": 128.1, "end": 130.3, "text": " I tu, uwierz, zaczyna\u0142y si\u0119 schody.", "tokens": [51099, 286, 2604, 11, 23147, 34602, 11, 43811, 629, 6825, 3244, 956, 843, 13, 51209], "temperature": 0.0, "avg_logprob": -0.10184009346419466, "compression_ratio": 1.4935897435897436, "no_speech_prob": 0.0021132072433829308}, {"id": 40, "seek": 11340, "start": 130.3, "end": 134.6, "text": " Chcesz, \u017ceby model rozpoznawa\u0142, czy recenzja w filmu jest pozytywna czy negatywna?", "tokens": [51209, 761, 887, 89, 11, 11316, 2316, 9544, 2259, 35458, 44603, 11, 6430, 850, 11368, 2938, 261, 2007, 84, 3492, 49358, 874, 86, 629, 6430, 2485, 21398, 86, 629, 30, 51424], "temperature": 0.0, "avg_logprob": -0.10184009346419466, "compression_ratio": 1.4935897435897436, "no_speech_prob": 0.0021132072433829308}, {"id": 41, "seek": 11340, "start": 134.6, "end": 139.70000000000002, "text": " Musisz mu da\u0107 tysi\u0105ce przyk\u0142ad\u00f3w recenzji z etykietami pozytywna lub negatywna.", "tokens": [51424, 3569, 23848, 2992, 1120, 2162, 38156, 11404, 384, 23144, 3901, 850, 11368, 4013, 710, 1030, 46127, 1684, 4526, 49358, 874, 86, 629, 15980, 2485, 21398, 86, 629, 13, 51679], "temperature": 0.0, "avg_logprob": -0.10184009346419466, "compression_ratio": 1.4935897435897436, "no_speech_prob": 0.0021132072433829308}, {"id": 42, "seek": 11340, "start": 139.70000000000002, "end": 141.8, "text": " I przekrowadzi\u0107 kolejny trening.", "tokens": [51679, 286, 29785, 1892, 345, 28496, 23749, 1634, 2192, 773, 13, 51784], "temperature": 0.0, "avg_logprob": -0.10184009346419466, "compression_ratio": 1.4935897435897436, "no_speech_prob": 0.0021132072433829308}, {"id": 43, "seek": 14180, "start": 141.8, "end": 143.9, "text": " Mniejszy, ale wci\u0105\u017c kosztowny.", "tokens": [50364, 376, 10402, 7706, 11, 6775, 261, 537, 27242, 19532, 2682, 648, 88, 13, 50469], "temperature": 0.0, "avg_logprob": -0.08526978297540319, "compression_ratio": 1.436532507739938, "no_speech_prob": 0.0009077332215383649}, {"id": 44, "seek": 14180, "start": 143.9, "end": 147.70000000000002, "text": " Ka\u017cda nowa umiej\u0119tno\u015b\u0107 to nowy, du\u017cy zbi\u00f3r danych.", "tokens": [50469, 10988, 1427, 2675, 586, 64, 1105, 7764, 46788, 23293, 281, 586, 88, 11, 1581, 7735, 710, 5614, 15614, 274, 34644, 13, 50659], "temperature": 0.0, "avg_logprob": -0.08526978297540319, "compression_ratio": 1.436532507739938, "no_speech_prob": 0.0009077332215383649}, {"id": 45, "seek": 14180, "start": 147.70000000000002, "end": 149.9, "text": " Nowa operacja na m\u00f3zgu modelu.", "tokens": [50659, 823, 64, 2208, 23395, 1667, 32515, 89, 2794, 2316, 84, 13, 50769], "temperature": 0.0, "avg_logprob": -0.08526978297540319, "compression_ratio": 1.436532507739938, "no_speech_prob": 0.0009077332215383649}, {"id": 46, "seek": 14180, "start": 149.9, "end": 153.20000000000002, "text": " To by\u0142o skuteczne, ale potwornie niepraktyczne.", "tokens": [50769, 1407, 14811, 1110, 1169, 38491, 11, 6775, 1847, 86, 1865, 414, 2838, 79, 11272, 874, 38491, 13, 50934], "temperature": 0.0, "avg_logprob": -0.08526978297540319, "compression_ratio": 1.436532507739938, "no_speech_prob": 0.0009077332215383649}, {"id": 47, "seek": 14180, "start": 153.20000000000002, "end": 157.60000000000002, "text": " Troch\u0119 jakby chirurg musia\u0142 wraca\u0107 na ca\u0142e studia medyczne za ka\u017cdym razem,", "tokens": [50934, 19406, 23006, 28976, 23782, 5476, 1038, 8908, 928, 6628, 2162, 1667, 47631, 972, 654, 1205, 17466, 716, 7949, 31615, 76, 40225, 11, 51154], "temperature": 0.0, "avg_logprob": -0.08526978297540319, "compression_ratio": 1.436532507739938, "no_speech_prob": 0.0009077332215383649}, {"id": 48, "seek": 14180, "start": 157.60000000000002, "end": 160.3, "text": " gdy pojawia si\u0119 nowa technika operacyjna.", "tokens": [51154, 28405, 30655, 654, 3244, 586, 64, 1537, 5439, 2208, 31285, 629, 13, 51289], "temperature": 0.0, "avg_logprob": -0.08526978297540319, "compression_ratio": 1.436532507739938, "no_speech_prob": 0.0009077332215383649}, {"id": 49, "seek": 14180, "start": 160.3, "end": 164.10000000000002, "text": " A autorzy tej pracy zadali pytanie, a co je\u015bli da si\u0119 inaczej?", "tokens": [51289, 316, 19510, 1229, 12573, 35591, 42788, 5103, 36610, 11, 257, 598, 25630, 1120, 3244, 33230, 16920, 30, 51479], "temperature": 0.0, "avg_logprob": -0.08526978297540319, "compression_ratio": 1.436532507739938, "no_speech_prob": 0.0009077332215383649}, {"id": 50, "seek": 14180, "start": 164.10000000000002, "end": 168.10000000000002, "text": " I zaproponowali co\u015b, co wydawa\u0142o si\u0119 niema\u0142 oszustwem.", "tokens": [51479, 286, 14223, 1513, 266, 305, 5103, 19241, 11, 598, 25984, 10449, 5249, 3244, 2838, 1696, 1221, 3003, 37677, 86, 443, 13, 51679], "temperature": 0.0, "avg_logprob": -0.08526978297540319, "compression_ratio": 1.436532507739938, "no_speech_prob": 0.0009077332215383649}, {"id": 51, "seek": 14180, "start": 168.10000000000002, "end": 171.4, "text": " Ca\u0142kowit\u0105 rezygnacj\u0119 z etapu fine tuningu,", "tokens": [51679, 7544, 1221, 74, 305, 270, 1611, 319, 1229, 4568, 29924, 710, 47634, 84, 2489, 15164, 84, 11, 51844], "temperature": 0.0, "avg_logprob": -0.08526978297540319, "compression_ratio": 1.436532507739938, "no_speech_prob": 0.0009077332215383649}, {"id": 52, "seek": 17140, "start": 171.4, "end": 175.70000000000002, "text": " zast\u0105pili go czym\u015b, co nazwali in-context learning.", "tokens": [50364, 36746, 1611, 79, 2312, 352, 31466, 1788, 11, 598, 20151, 40054, 294, 12, 9000, 3828, 2539, 13, 50579], "temperature": 0.0, "avg_logprob": -0.09287418637956892, "compression_ratio": 1.4517241379310344, "no_speech_prob": 0.003773511154577136}, {"id": 53, "seek": 17140, "start": 175.70000000000002, "end": 178.0, "text": " I tu jest, wiesz, ca\u0142a magia.", "tokens": [50579, 286, 2604, 3492, 11, 261, 15347, 11, 1335, 5024, 2258, 654, 13, 50694], "temperature": 0.0, "avg_logprob": -0.09287418637956892, "compression_ratio": 1.4517241379310344, "no_speech_prob": 0.003773511154577136}, {"id": 54, "seek": 17140, "start": 178.0, "end": 180.1, "text": " To jest takie uczenie si\u0119 na niby.", "tokens": [50694, 1407, 3492, 15963, 344, 39043, 3244, 1667, 38956, 88, 13, 50799], "temperature": 0.0, "avg_logprob": -0.09287418637956892, "compression_ratio": 1.4517241379310344, "no_speech_prob": 0.003773511154577136}, {"id": 55, "seek": 17140, "start": 180.1, "end": 183.8, "text": " Wyobra\u017cmy sobie, \u017ce m\u00f3zg modelu jest zamro\u017cony,", "tokens": [50799, 14458, 24393, 1427, 2226, 13652, 11, 3561, 32515, 89, 70, 2316, 84, 3492, 19876, 340, 1427, 2526, 11, 50984], "temperature": 0.0, "avg_logprob": -0.09287418637956892, "compression_ratio": 1.4517241379310344, "no_speech_prob": 0.003773511154577136}, {"id": 56, "seek": 17140, "start": 183.8, "end": 188.3, "text": " nie uczy si\u0119 niczego na sta\u0142e i jego wewn\u0119trzne po\u0142\u0105czenia si\u0119 nie zmieniaj\u0105.", "tokens": [50984, 2838, 344, 6522, 3244, 6201, 27725, 1667, 11135, 19827, 741, 26542, 321, 895, 1274, 6903, 43077, 714, 15926, 38517, 3244, 2838, 17020, 18811, 8555, 13, 51209], "temperature": 0.0, "avg_logprob": -0.09287418637956892, "compression_ratio": 1.4517241379310344, "no_speech_prob": 0.003773511154577136}, {"id": 57, "seek": 17140, "start": 188.3, "end": 191.0, "text": " Wszystko, co za\u0142apuje z podanych mu przyk\u0142ad\u00f3w,", "tokens": [51209, 343, 10424, 4093, 11, 598, 7949, 1221, 569, 13008, 710, 2497, 34644, 2992, 23144, 3901, 11, 51344], "temperature": 0.0, "avg_logprob": -0.09287418637956892, "compression_ratio": 1.4517241379310344, "no_speech_prob": 0.003773511154577136}, {"id": 58, "seek": 17140, "start": 191.0, "end": 196.3, "text": " jest jak notatka na kartce, kt\u00f3r\u0105 zgniata i wyrzuca zaraz po udzieleniu odpowiedzi.", "tokens": [51344, 3492, 4207, 406, 267, 2330, 1667, 29120, 384, 11, 37415, 40948, 3722, 3274, 741, 4628, 81, 11728, 496, 22675, 921, 714, 11727, 89, 12844, 5951, 36574, 3992, 13, 51609], "temperature": 0.0, "avg_logprob": -0.09287418637956892, "compression_ratio": 1.4517241379310344, "no_speech_prob": 0.003773511154577136}, {"id": 59, "seek": 17140, "start": 196.3, "end": 198.3, "text": " To faktycznie rewolucyjne.", "tokens": [51609, 1407, 33647, 45586, 319, 48481, 1311, 88, 73, 716, 13, 51709], "temperature": 0.0, "avg_logprob": -0.09287418637956892, "compression_ratio": 1.4517241379310344, "no_speech_prob": 0.003773511154577136}, {"id": 60, "seek": 19830, "start": 198.3, "end": 204.0, "text": " Bo wcze\u015bniej ka\u017cda nowa umiej\u0119tno\u015b\u0107 wymaga\u0142a trwa\u0142ej operacji na m\u00f3zgu modelu.", "tokens": [50364, 3286, 40785, 21912, 2675, 586, 64, 1105, 7764, 46788, 23293, 29764, 9286, 5024, 504, 4151, 19827, 73, 2208, 13152, 1667, 32515, 89, 2794, 2316, 84, 13, 50649], "temperature": 0.0, "avg_logprob": -0.11710580190022786, "compression_ratio": 1.3686274509803922, "no_speech_prob": 0.00037591534783132374}, {"id": 61, "seek": 19830, "start": 204.0, "end": 208.10000000000002, "text": " A tutaj, to jak rozmowa z niezwykle bystrym erudyt\u0105,", "tokens": [50649, 316, 12749, 11, 281, 4207, 35234, 5528, 710, 33511, 9726, 14677, 538, 372, 627, 76, 1189, 532, 4328, 1611, 11, 50854], "temperature": 0.0, "avg_logprob": -0.11710580190022786, "compression_ratio": 1.3686274509803922, "no_speech_prob": 0.00037591534783132374}, {"id": 62, "seek": 19830, "start": 208.10000000000002, "end": 212.70000000000002, "text": " kt\u00f3ry nie musi zmienia\u0107 swojej osobowo\u015bci, \u017ceby zrozumie\u0107, o co go prosimy.", "tokens": [50854, 9913, 2838, 37587, 17020, 18811, 2162, 29489, 73, 19116, 8202, 44468, 11, 11316, 710, 27857, 449, 414, 2162, 11, 277, 598, 352, 6267, 13189, 13, 51084], "temperature": 0.0, "avg_logprob": -0.11710580190022786, "compression_ratio": 1.3686274509803922, "no_speech_prob": 0.00037591534783132374}, {"id": 63, "seek": 19830, "start": 212.70000000000002, "end": 214.8, "text": " Wystarczy mu da\u0107 kilka wskaz\u00f3wek.", "tokens": [51084, 14458, 9710, 6522, 2992, 1120, 2162, 36466, 261, 5161, 921, 812, 826, 74, 13, 51189], "temperature": 0.0, "avg_logprob": -0.11710580190022786, "compression_ratio": 1.3686274509803922, "no_speech_prob": 0.00037591534783132374}, {"id": 64, "seek": 19830, "start": 214.8, "end": 218.0, "text": " I te wskaz\u00f3wki podzielili na trzy poziomy.", "tokens": [51189, 286, 535, 261, 5161, 921, 3901, 2984, 2497, 42280, 2312, 1667, 34573, 38503, 8488, 13, 51349], "temperature": 0.0, "avg_logprob": -0.11710580190022786, "compression_ratio": 1.3686274509803922, "no_speech_prob": 0.00037591534783132374}, {"id": 65, "seek": 19830, "start": 218.0, "end": 223.4, "text": " Pierwszy, najbardziej hardkorowy, to zero shot.", "tokens": [51349, 16676, 30012, 11, 41857, 1152, 19339, 10089, 11, 281, 4018, 3347, 13, 51619], "temperature": 0.0, "avg_logprob": -0.11710580190022786, "compression_ratio": 1.3686274509803922, "no_speech_prob": 0.00037591534783132374}, {"id": 66, "seek": 22340, "start": 223.4, "end": 229.0, "text": " Dajemy modelowi tylko polecenie, na przyk\u0142ad przet\u0142umacz z angielskiego na francuski.", "tokens": [50364, 413, 1805, 3633, 2316, 24503, 13219, 13208, 13037, 414, 11, 1667, 23144, 6541, 302, 49166, 14875, 710, 2562, 1187, 5161, 12200, 1667, 431, 282, 1149, 2984, 13, 50644], "temperature": 0.0, "avg_logprob": -0.15351028700132627, "compression_ratio": 1.4578754578754578, "no_speech_prob": 0.15453825891017914}, {"id": 67, "seek": 22340, "start": 229.0, "end": 230.3, "text": " Cheese.", "tokens": [50644, 23738, 13, 50709], "temperature": 0.0, "avg_logprob": -0.15351028700132627, "compression_ratio": 1.4578754578754578, "no_speech_prob": 0.15453825891017914}, {"id": 68, "seek": 22340, "start": 230.3, "end": 231.3, "text": " I tyle.", "tokens": [50709, 286, 39293, 13, 50759], "temperature": 0.0, "avg_logprob": -0.15351028700132627, "compression_ratio": 1.4578754578754578, "no_speech_prob": 0.15453825891017914}, {"id": 69, "seek": 22340, "start": 231.3, "end": 232.5, "text": " \u017badnych przyk\u0142ad\u00f3w.", "tokens": [50759, 29804, 345, 9399, 23144, 3901, 13, 50819], "temperature": 0.0, "avg_logprob": -0.15351028700132627, "compression_ratio": 1.4578754578754578, "no_speech_prob": 0.15453825891017914}, {"id": 70, "seek": 22340, "start": 232.5, "end": 235.70000000000002, "text": " Model ma sam wpa\u015b\u0107 na to, czego od niego chcemy.", "tokens": [50819, 17105, 463, 3247, 261, 4306, 7753, 1667, 281, 11, 36559, 3611, 49615, 28928, 2226, 13, 50979], "temperature": 0.0, "avg_logprob": -0.15351028700132627, "compression_ratio": 1.4578754578754578, "no_speech_prob": 0.15453825891017914}, {"id": 71, "seek": 22340, "start": 235.70000000000002, "end": 237.70000000000002, "text": " Drugi poziom to one shot.", "tokens": [50979, 2491, 24780, 38503, 298, 281, 472, 3347, 13, 51079], "temperature": 0.0, "avg_logprob": -0.15351028700132627, "compression_ratio": 1.4578754578754578, "no_speech_prob": 0.15453825891017914}, {"id": 72, "seek": 22340, "start": 237.70000000000002, "end": 239.3, "text": " Tu jeste\u015bmy troch\u0119 milsi.", "tokens": [51079, 7836, 35928, 24926, 1962, 7691, 13, 51159], "temperature": 0.0, "avg_logprob": -0.15351028700132627, "compression_ratio": 1.4578754578754578, "no_speech_prob": 0.15453825891017914}, {"id": 73, "seek": 22340, "start": 239.3, "end": 241.70000000000002, "text": " Dajemy jeden przyk\u0142ad, \u017ceby go nakierowa\u0107.", "tokens": [51159, 413, 1805, 3633, 12906, 23144, 11, 11316, 352, 20332, 811, 11445, 13, 51279], "temperature": 0.0, "avg_logprob": -0.15351028700132627, "compression_ratio": 1.4578754578754578, "no_speech_prob": 0.15453825891017914}, {"id": 74, "seek": 22340, "start": 241.70000000000002, "end": 242.5, "text": " M\u00f3wimy.", "tokens": [51279, 376, 3901, 13189, 13, 51319], "temperature": 0.0, "avg_logprob": -0.15351028700132627, "compression_ratio": 1.4578754578754578, "no_speech_prob": 0.15453825891017914}, {"id": 75, "seek": 22340, "start": 242.5, "end": 245.20000000000002, "text": " See Otter Lauter de Merch.", "tokens": [51319, 3008, 12936, 391, 47344, 260, 368, 6124, 339, 13, 51454], "temperature": 0.0, "avg_logprob": -0.15351028700132627, "compression_ratio": 1.4578754578754578, "no_speech_prob": 0.15453825891017914}, {"id": 76, "seek": 22340, "start": 245.20000000000002, "end": 246.0, "text": " Cheese.", "tokens": [51454, 23738, 13, 51494], "temperature": 0.0, "avg_logprob": -0.15351028700132627, "compression_ratio": 1.4578754578754578, "no_speech_prob": 0.15453825891017914}, {"id": 77, "seek": 22340, "start": 246.0, "end": 248.20000000000002, "text": " I ciekamy, a\u017c za\u0142apie schemat.", "tokens": [51494, 286, 46419, 7804, 11, 48134, 7949, 1221, 569, 414, 956, 8615, 13, 51604], "temperature": 0.0, "avg_logprob": -0.15351028700132627, "compression_ratio": 1.4578754578754578, "no_speech_prob": 0.15453825891017914}, {"id": 78, "seek": 22340, "start": 248.20000000000002, "end": 251.6, "text": " I wreszcie few shot, czyli serce tej pracy.", "tokens": [51604, 286, 261, 495, 89, 4260, 1326, 3347, 11, 16591, 816, 384, 12573, 35591, 13, 51774], "temperature": 0.0, "avg_logprob": -0.15351028700132627, "compression_ratio": 1.4578754578754578, "no_speech_prob": 0.15453825891017914}, {"id": 79, "seek": 25160, "start": 251.7, "end": 255.9, "text": " Tutaj dajemy modelowi od kilku do kilkudziesi\u0119ciu przyk\u0142ad\u00f3w.", "tokens": [50369, 41819, 1120, 73, 3633, 2316, 24503, 3611, 5128, 5279, 360, 5128, 74, 532, 89, 530, 5034, 30795, 23144, 3901, 13, 50579], "temperature": 0.0, "avg_logprob": -0.11331316712614778, "compression_ratio": 1.4135593220338982, "no_speech_prob": 0.020035037770867348}, {"id": 80, "seek": 25160, "start": 255.9, "end": 259.4, "text": " Wszystko w jednym zapytaniu, w jego oknie kontekstowym.", "tokens": [50579, 343, 10424, 4093, 261, 5232, 12996, 14223, 4328, 25849, 11, 261, 26542, 3133, 2766, 14373, 916, 372, 31691, 13, 50754], "temperature": 0.0, "avg_logprob": -0.11331316712614778, "compression_ratio": 1.4135593220338982, "no_speech_prob": 0.020035037770867348}, {"id": 81, "seek": 25160, "start": 259.4, "end": 261.3, "text": " A potem zadanie do wykonania.", "tokens": [50754, 316, 36513, 42788, 7155, 360, 46702, 5609, 13, 50849], "temperature": 0.0, "avg_logprob": -0.11331316712614778, "compression_ratio": 1.4135593220338982, "no_speech_prob": 0.020035037770867348}, {"id": 82, "seek": 25160, "start": 261.3, "end": 265.0, "text": " No i tu dochodzimy do g\u0142\u00f3wnej hipotezy, do wielkiego zak\u0142adu,", "tokens": [50849, 883, 741, 2604, 9243, 378, 89, 13189, 360, 18117, 3901, 11794, 8103, 1370, 1229, 11, 360, 20570, 42349, 23810, 10358, 84, 11, 51034], "temperature": 0.0, "avg_logprob": -0.11331316712614778, "compression_ratio": 1.4135593220338982, "no_speech_prob": 0.020035037770867348}, {"id": 83, "seek": 25160, "start": 265.0, "end": 267.3, "text": " kt\u00f3ry postawili badacze z OpenAI.", "tokens": [51034, 9913, 2183, 1607, 2312, 1578, 326, 1381, 710, 7238, 48698, 13, 51149], "temperature": 0.0, "avg_logprob": -0.11331316712614778, "compression_ratio": 1.4135593220338982, "no_speech_prob": 0.020035037770867348}, {"id": 84, "seek": 25160, "start": 267.3, "end": 268.4, "text": " A brzmia\u0142 on?", "tokens": [51149, 316, 738, 89, 29958, 1221, 322, 30, 51204], "temperature": 0.0, "avg_logprob": -0.11331316712614778, "compression_ratio": 1.4135593220338982, "no_speech_prob": 0.020035037770867348}, {"id": 85, "seek": 25160, "start": 268.4, "end": 272.6, "text": " Postawili tez\u0119, \u017ce je\u015bli we\u017cmiemy architektur\u0119 modelu j\u0119zykowego", "tokens": [51204, 10223, 1607, 2312, 535, 11052, 11, 3561, 25630, 321, 1427, 25210, 2226, 3912, 642, 2320, 374, 1274, 2316, 84, 49055, 74, 26576, 51414], "temperature": 0.0, "avg_logprob": -0.11331316712614778, "compression_ratio": 1.4135593220338982, "no_speech_prob": 0.020035037770867348}, {"id": 86, "seek": 25160, "start": 272.6, "end": 276.9, "text": " i przeskalujemy j\u0105 do absurdalnych rozmiar\u00f3w,", "tokens": [51414, 741, 6541, 279, 19990, 21767, 35692, 360, 19774, 304, 9399, 9544, 3057, 289, 3901, 11, 51629], "temperature": 0.0, "avg_logprob": -0.11331316712614778, "compression_ratio": 1.4135593220338982, "no_speech_prob": 0.020035037770867348}, {"id": 87, "seek": 25160, "start": 276.9, "end": 280.4, "text": " tych 175 miliard\u00f3w parametr\u00f3w.", "tokens": [51629, 15180, 41165, 1962, 72, 515, 3901, 6220, 27965, 3901, 13, 51804], "temperature": 0.0, "avg_logprob": -0.11331316712614778, "compression_ratio": 1.4135593220338982, "no_speech_prob": 0.020035037770867348}, {"id": 88, "seek": 28040, "start": 280.4, "end": 284.9, "text": " To ta zdolno\u015b\u0107 do uczenia si\u0119 w locie, czyli in-context learning,", "tokens": [50364, 1407, 1846, 16221, 401, 23293, 360, 344, 38517, 3244, 261, 1628, 414, 11, 16591, 294, 12, 9000, 3828, 2539, 11, 50589], "temperature": 0.0, "avg_logprob": -0.12149572976027863, "compression_ratio": 1.437299035369775, "no_speech_prob": 0.05569421127438545}, {"id": 89, "seek": 28040, "start": 284.9, "end": 286.79999999999995, "text": " po prostu eksploduje.", "tokens": [50589, 714, 19518, 30724, 564, 378, 13008, 13, 50684], "temperature": 0.0, "avg_logprob": -0.12149572976027863, "compression_ratio": 1.437299035369775, "no_speech_prob": 0.05569421127438545}, {"id": 90, "seek": 28040, "start": 286.79999999999995, "end": 290.0, "text": " Stanie si\u0119 tak dobra, \u017ce w wielu zadaniach dor\u00f3wna,", "tokens": [50684, 745, 7155, 3244, 991, 360, 6198, 11, 3561, 261, 40437, 42788, 3782, 608, 26313, 3901, 629, 11, 50844], "temperature": 0.0, "avg_logprob": -0.12149572976027863, "compression_ratio": 1.437299035369775, "no_speech_prob": 0.05569421127438545}, {"id": 91, "seek": 28040, "start": 290.0, "end": 292.09999999999997, "text": " albo nawet przebije modele,", "tokens": [50844, 22622, 22696, 8325, 30418, 68, 4391, 306, 11, 50949], "temperature": 0.0, "avg_logprob": -0.12149572976027863, "compression_ratio": 1.437299035369775, "no_speech_prob": 0.05569421127438545}, {"id": 92, "seek": 28040, "start": 292.09999999999997, "end": 295.5, "text": " kt\u00f3re przesz\u0142y ten ca\u0142y \u017cmudny proces fine tuningu.", "tokens": [50949, 8864, 6541, 10430, 6825, 2064, 35226, 19625, 31916, 1634, 17565, 2489, 15164, 84, 13, 51119], "temperature": 0.0, "avg_logprob": -0.12149572976027863, "compression_ratio": 1.437299035369775, "no_speech_prob": 0.05569421127438545}, {"id": 93, "seek": 28040, "start": 295.5, "end": 298.2, "text": " A wszystko bez jednej trwa\u0142ej zmiany w modelu.", "tokens": [51119, 316, 22607, 10782, 5232, 11794, 504, 4151, 19827, 73, 43591, 88, 261, 2316, 84, 13, 51254], "temperature": 0.0, "avg_logprob": -0.12149572976027863, "compression_ratio": 1.437299035369775, "no_speech_prob": 0.05569421127438545}, {"id": 94, "seek": 28040, "start": 298.2, "end": 299.29999999999995, "text": " Ok, rozumiem.", "tokens": [51254, 3477, 11, 48797, 4907, 13, 51309], "temperature": 0.0, "avg_logprob": -0.12149572976027863, "compression_ratio": 1.437299035369775, "no_speech_prob": 0.05569421127438545}, {"id": 95, "seek": 28040, "start": 299.29999999999995, "end": 303.0, "text": " Wi\u0119kszy model to lepsze wyniki, to ma jaki\u015b intuicyjny sens.", "tokens": [51309, 30127, 1694, 1229, 2316, 281, 476, 1878, 1381, 31936, 9850, 11, 281, 463, 34721, 560, 84, 2632, 73, 1634, 2923, 13, 51494], "temperature": 0.0, "avg_logprob": -0.12149572976027863, "compression_ratio": 1.437299035369775, "no_speech_prob": 0.05569421127438545}, {"id": 96, "seek": 28040, "start": 303.0, "end": 306.5, "text": " Ale szukam tego momentu opadni\u0119cia szcz\u0119ki.", "tokens": [51494, 9366, 7870, 2034, 335, 8627, 1623, 84, 999, 345, 35938, 2755, 22090, 1274, 2984, 13, 51669], "temperature": 0.0, "avg_logprob": -0.12149572976027863, "compression_ratio": 1.437299035369775, "no_speech_prob": 0.05569421127438545}, {"id": 97, "seek": 28040, "start": 306.5, "end": 309.2, "text": " Co by\u0142o tym jednym konkretnym osi\u0105gni\u0119ciem,", "tokens": [51669, 3066, 14811, 8107, 5232, 12996, 36500, 12996, 3003, 11404, 70, 35938, 4260, 76, 11, 51804], "temperature": 0.0, "avg_logprob": -0.12149572976027863, "compression_ratio": 1.437299035369775, "no_speech_prob": 0.05569421127438545}, {"id": 98, "seek": 30920, "start": 309.2, "end": 311.9, "text": " o kt\u00f3rym ludzie w OpenAI otworzyli szampana,", "tokens": [50364, 277, 30120, 37025, 261, 7238, 48698, 4337, 28321, 1229, 2081, 7870, 1215, 2095, 11, 50499], "temperature": 0.0, "avg_logprob": -0.11442484345825964, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.006833851803094149}, {"id": 99, "seek": 30920, "start": 311.9, "end": 313.8, "text": " a ich konkurenci z\u0142apali si\u0119 za g\u0142ow\u0119.", "tokens": [50499, 257, 1893, 21428, 9873, 537, 31614, 569, 5103, 3244, 7949, 18117, 305, 1274, 13, 50594], "temperature": 0.0, "avg_logprob": -0.11442484345825964, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.006833851803094149}, {"id": 100, "seek": 30920, "start": 313.8, "end": 315.3, "text": " W tych moment\u00f3w by\u0142o kilka,", "tokens": [50594, 343, 15180, 1623, 3901, 14811, 36466, 11, 50669], "temperature": 0.0, "avg_logprob": -0.11442484345825964, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.006833851803094149}, {"id": 101, "seek": 30920, "start": 315.3, "end": 319.2, "text": " ale zacznijmy od czego\u015b, co nazywa si\u0119 trywia QA.", "tokens": [50669, 6775, 710, 14875, 77, 1718, 2226, 3611, 36559, 1788, 11, 598, 20151, 88, 4151, 3244, 853, 86, 654, 1249, 32, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11442484345825964, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.006833851803094149}, {"id": 102, "seek": 30920, "start": 319.2, "end": 322.8, "text": " To jest benchmark, kt\u00f3ry dzia\u0142a troch\u0119 jak teleturniej.", "tokens": [50864, 1407, 3492, 18927, 11, 9913, 37903, 24926, 4207, 15284, 302, 925, 7764, 13, 51044], "temperature": 0.0, "avg_logprob": -0.11442484345825964, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.006833851803094149}, {"id": 103, "seek": 30920, "start": 322.8, "end": 325.9, "text": " Zadajesz modelowi pytania z wiedzy og\u00f3lnej.", "tokens": [51044, 1176, 345, 1805, 10430, 2316, 24503, 25878, 5609, 710, 46894, 1229, 5360, 15741, 11794, 13, 51199], "temperature": 0.0, "avg_logprob": -0.11442484345825964, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.006833851803094149}, {"id": 104, "seek": 30920, "start": 325.9, "end": 329.0, "text": " Ale kluczowy jest tu tryb closed book.", "tokens": [51199, 9366, 9671, 1311, 89, 10089, 3492, 2604, 853, 65, 5395, 1446, 13, 51354], "temperature": 0.0, "avg_logprob": -0.11442484345825964, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.006833851803094149}, {"id": 105, "seek": 30920, "start": 329.0, "end": 331.9, "text": " Model nie mo\u017ce szuka\u0107 odpowiedzi w internecie.", "tokens": [51354, 17105, 2838, 12034, 7870, 13599, 2162, 36574, 3992, 261, 728, 716, 4260, 13, 51499], "temperature": 0.0, "avg_logprob": -0.11442484345825964, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.006833851803094149}, {"id": 106, "seek": 30920, "start": 331.9, "end": 334.5, "text": " Musi polega\u0107 wy\u0142\u0105cznie na wiedzy,", "tokens": [51499, 3569, 72, 13208, 3680, 2162, 4628, 15926, 19923, 1667, 46894, 1229, 11, 51629], "temperature": 0.0, "avg_logprob": -0.11442484345825964, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.006833851803094149}, {"id": 107, "seek": 30920, "start": 334.5, "end": 337.8, "text": " kt\u00f3r\u0105 zapami\u0119ta\u0142 podczas swojego pierwotnego treningu.", "tokens": [51629, 37415, 14223, 23806, 46426, 2497, 30989, 13291, 39738, 9766, 86, 310, 11858, 2192, 773, 84, 13, 51794], "temperature": 0.0, "avg_logprob": -0.11442484345825964, "compression_ratio": 1.440251572327044, "no_speech_prob": 0.006833851803094149}, {"id": 108, "seek": 33780, "start": 337.8, "end": 339.3, "text": " Jak sobie poradzi\u0142?", "tokens": [50364, 15029, 13652, 1515, 345, 3992, 1221, 30, 50439], "temperature": 0.0, "avg_logprob": -0.10815060523248488, "compression_ratio": 1.3651877133105803, "no_speech_prob": 0.019520336762070656}, {"id": 109, "seek": 33780, "start": 339.3, "end": 340.5, "text": " Jaki by\u0142 wynik?", "tokens": [50439, 508, 7421, 16673, 31936, 1035, 30, 50499], "temperature": 0.0, "avg_logprob": -0.10815060523248488, "compression_ratio": 1.3651877133105803, "no_speech_prob": 0.019520336762070656}, {"id": 110, "seek": 33780, "start": 340.5, "end": 347.1, "text": " GPT-3 w trybie Fue Shot osi\u0105gn\u0119\u0142o 71,20% dok\u0142adno\u015bci.", "tokens": [50499, 26039, 51, 12, 18, 261, 853, 7392, 479, 622, 28845, 3003, 11404, 4568, 1274, 5249, 30942, 11, 2009, 4, 45864, 16438, 13, 50829], "temperature": 0.0, "avg_logprob": -0.10815060523248488, "compression_ratio": 1.3651877133105803, "no_speech_prob": 0.019520336762070656}, {"id": 111, "seek": 33780, "start": 347.1, "end": 350.0, "text": " I teraz, wiesz, ta liczba sama w sobie mo\u017ce niewiele m\u00f3wi\u0107.", "tokens": [50829, 286, 16854, 11, 261, 15347, 11, 1846, 6169, 89, 4231, 17768, 261, 13652, 12034, 43622, 15949, 13489, 12757, 13, 50974], "temperature": 0.0, "avg_logprob": -0.10815060523248488, "compression_ratio": 1.3651877133105803, "no_speech_prob": 0.019520336762070656}, {"id": 112, "seek": 33780, "start": 350.0, "end": 353.7, "text": " No w\u0142a\u015bnie, 71,20%.", "tokens": [50974, 883, 14234, 11, 30942, 11, 2009, 6856, 51159], "temperature": 0.0, "avg_logprob": -0.10815060523248488, "compression_ratio": 1.3651877133105803, "no_speech_prob": 0.019520336762070656}, {"id": 113, "seek": 33780, "start": 353.7, "end": 357.6, "text": " To brzmi dobrze, ale liczby potrafi\u0105 by\u0107 myl\u0105ce.", "tokens": [51159, 1407, 738, 89, 3057, 28335, 11, 6775, 6169, 89, 2322, 1847, 10437, 11404, 15069, 452, 75, 1611, 384, 13, 51354], "temperature": 0.0, "avg_logprob": -0.10815060523248488, "compression_ratio": 1.3651877133105803, "no_speech_prob": 0.019520336762070656}, {"id": 114, "seek": 33780, "start": 357.6, "end": 359.90000000000003, "text": " Musimy to umie\u015bci\u0107 w jakim\u015b kontek\u015bcie.", "tokens": [51354, 3569, 13189, 281, 1105, 414, 6199, 2162, 261, 49410, 1788, 14373, 916, 9815, 13, 51469], "temperature": 0.0, "avg_logprob": -0.10815060523248488, "compression_ratio": 1.3651877133105803, "no_speech_prob": 0.019520336762070656}, {"id": 115, "seek": 33780, "start": 359.90000000000003, "end": 363.5, "text": " Kto by\u0142 poprzednim mistrzem w tej dziedzinie i z jakim wynikiem?", "tokens": [51469, 591, 1353, 16673, 1665, 81, 11312, 39223, 3544, 19390, 443, 261, 12573, 9758, 15338, 259, 414, 741, 710, 49410, 31936, 1035, 4907, 30, 51649], "temperature": 0.0, "avg_logprob": -0.10815060523248488, "compression_ratio": 1.3651877133105803, "no_speech_prob": 0.019520336762070656}, {"id": 116, "seek": 33780, "start": 363.5, "end": 366.8, "text": " Czy to by\u0142 ma\u0142y kroczek naprz\u00f3d, czy raczej knockout?", "tokens": [51649, 19832, 281, 16673, 463, 6825, 45909, 3689, 916, 9296, 19390, 17081, 11, 6430, 4129, 16920, 6728, 346, 30, 51814], "temperature": 0.0, "avg_logprob": -0.10815060523248488, "compression_ratio": 1.3651877133105803, "no_speech_prob": 0.019520336762070656}, {"id": 117, "seek": 36680, "start": 366.8, "end": 368.2, "text": " To by\u0142 knockout.", "tokens": [50364, 1407, 16673, 6728, 346, 13, 50434], "temperature": 0.0, "avg_logprob": -0.11797432277513586, "compression_ratio": 1.4221453287197232, "no_speech_prob": 0.01284460537135601}, {"id": 118, "seek": 36680, "start": 368.2, "end": 373.7, "text": " GPT-3 pobi\u0142o dotychczasowy najlepszy model TP-11B,", "tokens": [50434, 26039, 51, 12, 18, 714, 5614, 5249, 5893, 16384, 30989, 10089, 41903, 1878, 1229, 2316, 44462, 12, 5348, 33, 11, 50709], "temperature": 0.0, "avg_logprob": -0.11797432277513586, "compression_ratio": 1.4221453287197232, "no_speech_prob": 0.01284460537135601}, {"id": 119, "seek": 36680, "start": 373.7, "end": 378.40000000000003, "text": " kt\u00f3ry by\u0142 specjalnie dostrajany fine-tuned do tego konkretnego zadania", "tokens": [50709, 9913, 16673, 46433, 2766, 20568, 48690, 1325, 2489, 12, 83, 43703, 360, 8627, 36500, 11858, 42788, 5609, 50944], "temperature": 0.0, "avg_logprob": -0.11797432277513586, "compression_ratio": 1.4221453287197232, "no_speech_prob": 0.01284460537135601}, {"id": 120, "seek": 36680, "start": 378.40000000000003, "end": 381.0, "text": " i tu jest ta fundamentalna r\u00f3\u017cnica.", "tokens": [50944, 741, 2604, 3492, 1846, 8088, 629, 19637, 32687, 13, 51074], "temperature": 0.0, "avg_logprob": -0.11797432277513586, "compression_ratio": 1.4221453287197232, "no_speech_prob": 0.01284460537135601}, {"id": 121, "seek": 36680, "start": 381.0, "end": 383.3, "text": " To nie by\u0142o tylko pobicie rekordu.", "tokens": [51074, 1407, 2838, 14811, 13219, 714, 65, 28434, 33881, 28655, 13, 51189], "temperature": 0.0, "avg_logprob": -0.11797432277513586, "compression_ratio": 1.4221453287197232, "no_speech_prob": 0.01284460537135601}, {"id": 122, "seek": 36680, "start": 383.3, "end": 386.90000000000003, "text": " To by\u0142 jakby amator, kt\u00f3ry przeczyta\u0142 ca\u0142\u0105 bibliotek\u0119,", "tokens": [51189, 1407, 16673, 28976, 669, 1639, 11, 9913, 8325, 6522, 46426, 1335, 15926, 34344, 310, 916, 1274, 11, 51369], "temperature": 0.0, "avg_logprob": -0.11797432277513586, "compression_ratio": 1.4221453287197232, "no_speech_prob": 0.01284460537135601}, {"id": 123, "seek": 36680, "start": 386.90000000000003, "end": 391.1, "text": " wszed\u0142 na Olimpiad\u0119 i pobi\u0142 w biegu na 100 metr\u00f3w sportowc\u00f3w,", "tokens": [51369, 37647, 11312, 1221, 1667, 422, 4197, 22630, 345, 1274, 741, 714, 5614, 1221, 261, 272, 414, 2794, 1667, 2319, 1131, 81, 3901, 7282, 305, 29268, 11, 51579], "temperature": 0.0, "avg_logprob": -0.11797432277513586, "compression_ratio": 1.4221453287197232, "no_speech_prob": 0.01284460537135601}, {"id": 124, "seek": 36680, "start": 391.1, "end": 395.2, "text": " kt\u00f3rzy trenowali tylko t\u0105 jedn\u0105 dyscyplin\u0119 przez ca\u0142e \u017cycie.", "tokens": [51579, 25382, 23136, 305, 5103, 13219, 32294, 5232, 13113, 15243, 1344, 48102, 1274, 14064, 47631, 43202, 13, 51784], "temperature": 0.0, "avg_logprob": -0.11797432277513586, "compression_ratio": 1.4221453287197232, "no_speech_prob": 0.01284460537135601}, {"id": 125, "seek": 39520, "start": 395.3, "end": 399.5, "text": " To podwa\u017cy\u0142o sensobno\u015b\u0107 ca\u0142ego tego specjalistycznego treningu.", "tokens": [50369, 1407, 2497, 4151, 7735, 5249, 2923, 996, 23293, 35224, 6308, 8627, 46433, 468, 17466, 11858, 2192, 773, 84, 13, 50579], "temperature": 0.0, "avg_logprob": -0.09598859151204427, "compression_ratio": 1.4317460317460318, "no_speech_prob": 0.0027180796023458242}, {"id": 126, "seek": 39520, "start": 399.5, "end": 403.09999999999997, "text": " Rozumiem, to pokazuje, \u017ce wiedza og\u00f3lna na masow\u0105 skal\u0119", "tokens": [50579, 43313, 449, 4907, 11, 281, 13010, 43317, 11, 3561, 46894, 2394, 5360, 15741, 629, 1667, 2300, 30297, 16890, 1274, 50759], "temperature": 0.0, "avg_logprob": -0.09598859151204427, "compression_ratio": 1.4317460317460318, "no_speech_prob": 0.0027180796023458242}, {"id": 127, "seek": 39520, "start": 403.09999999999997, "end": 406.5, "text": " mo\u017ce by\u0107 pot\u0119\u017cniejsza ni\u017c w\u0105ska specjalizacja.", "tokens": [50759, 12034, 15069, 1847, 1274, 1427, 30295, 2394, 28502, 261, 1611, 20771, 46433, 590, 23395, 13, 50929], "temperature": 0.0, "avg_logprob": -0.09598859151204427, "compression_ratio": 1.4317460317460318, "no_speech_prob": 0.0027180796023458242}, {"id": 128, "seek": 39520, "start": 406.5, "end": 408.4, "text": " A co z innymi zadaniami?", "tokens": [50929, 316, 598, 710, 294, 31813, 710, 11338, 15568, 30, 51024], "temperature": 0.0, "avg_logprob": -0.09598859151204427, "compression_ratio": 1.4317460317460318, "no_speech_prob": 0.0027180796023458242}, {"id": 129, "seek": 39520, "start": 408.4, "end": 411.0, "text": " Gdzie jeszcze ta skala zrobi\u0142a tak\u0105 r\u00f3\u017cnic\u0119?", "tokens": [51024, 460, 13096, 14168, 1846, 1110, 5159, 24483, 5024, 31069, 19637, 7692, 1274, 30, 51154], "temperature": 0.0, "avg_logprob": -0.09598859151204427, "compression_ratio": 1.4317460317460318, "no_speech_prob": 0.0027180796023458242}, {"id": 130, "seek": 39520, "start": 411.0, "end": 413.0, "text": " Byli\u015bmy zadaniem lambada.", "tokens": [51154, 3146, 38452, 710, 11338, 4907, 10097, 1538, 13, 51254], "temperature": 0.0, "avg_logprob": -0.09598859151204427, "compression_ratio": 1.4317460317460318, "no_speech_prob": 0.0027180796023458242}, {"id": 131, "seek": 39520, "start": 413.0, "end": 417.0, "text": " Polega ono na przewidzeniu ostatniego s\u0142owa w d\u0142u\u017cszym akapicie.", "tokens": [51254, 34212, 3680, 322, 78, 1667, 39758, 327, 39651, 32686, 2766, 1571, 15116, 5528, 261, 274, 24066, 1427, 7706, 76, 9308, 569, 28434, 13, 51454], "temperature": 0.0, "avg_logprob": -0.09598859151204427, "compression_ratio": 1.4317460317460318, "no_speech_prob": 0.0027180796023458242}, {"id": 132, "seek": 39520, "start": 417.0, "end": 420.3, "text": " To jest test na rozumienie szerokiego kontekstu.", "tokens": [51454, 1407, 3492, 1500, 1667, 48797, 27385, 36160, 453, 12200, 14373, 916, 372, 84, 13, 51619], "temperature": 0.0, "avg_logprob": -0.09598859151204427, "compression_ratio": 1.4317460317460318, "no_speech_prob": 0.0027180796023458242}, {"id": 133, "seek": 39520, "start": 420.3, "end": 422.8, "text": " Nie wystarczy spojrze\u0107 na kilka ostatnich s\u0142\u00f3w.", "tokens": [51619, 12016, 4628, 9710, 6522, 8243, 73, 13503, 2162, 1667, 36466, 32686, 77, 480, 15116, 3901, 13, 51744], "temperature": 0.0, "avg_logprob": -0.09598859151204427, "compression_ratio": 1.4317460317460318, "no_speech_prob": 0.0027180796023458242}, {"id": 134, "seek": 42280, "start": 422.90000000000003, "end": 425.40000000000003, "text": " Trzeba zrozumie\u0107 ca\u0142\u0105 histori\u0119.", "tokens": [50369, 1765, 1381, 4231, 710, 27857, 449, 414, 2162, 1335, 15926, 4058, 5034, 13, 50494], "temperature": 0.0, "avg_logprob": -0.10335639050898661, "compression_ratio": 1.3470790378006874, "no_speech_prob": 0.015303879044950008}, {"id": 135, "seek": 42280, "start": 425.40000000000003, "end": 430.40000000000003, "text": " Poprzedni State of the Art wynosi\u0142 tam oko\u0142o 60% dok\u0142adno\u015bci.", "tokens": [50494, 10215, 81, 11312, 3722, 4533, 295, 264, 5735, 31936, 21521, 1221, 7677, 45730, 5249, 4060, 4, 45864, 16438, 13, 50744], "temperature": 0.0, "avg_logprob": -0.10335639050898661, "compression_ratio": 1.3470790378006874, "no_speech_prob": 0.015303879044950008}, {"id": 136, "seek": 42280, "start": 430.40000000000003, "end": 435.8, "text": " GPT-3 w trybie Fuel Shot osi\u0105gn\u0119\u0142o ponad 76%.", "tokens": [50744, 26039, 51, 12, 18, 261, 853, 7392, 46837, 28845, 3003, 11404, 4568, 1274, 5249, 9224, 345, 24733, 6856, 51014], "temperature": 0.0, "avg_logprob": -0.10335639050898661, "compression_ratio": 1.3470790378006874, "no_speech_prob": 0.015303879044950008}, {"id": 137, "seek": 42280, "start": 435.8, "end": 439.2, "text": " Poprawa o ponad 18 punkt\u00f3w procentowych.", "tokens": [51014, 10215, 424, 4151, 277, 9224, 345, 2443, 39561, 3901, 38826, 19605, 13, 51184], "temperature": 0.0, "avg_logprob": -0.10335639050898661, "compression_ratio": 1.3470790378006874, "no_speech_prob": 0.015303879044950008}, {"id": 138, "seek": 42280, "start": 439.2, "end": 443.6, "text": " W \u015bwiecie AI to nie jest krok, to jest skok przez przepa\u015b\u0107.", "tokens": [51184, 343, 40078, 4260, 7318, 281, 2838, 3492, 350, 31621, 11, 281, 3492, 1110, 453, 14064, 30829, 64, 7753, 13, 51404], "temperature": 0.0, "avg_logprob": -0.10335639050898661, "compression_ratio": 1.3470790378006874, "no_speech_prob": 0.015303879044950008}, {"id": 139, "seek": 42280, "start": 443.6, "end": 448.90000000000003, "text": " OK, to wszystko s\u0105 wyniki w benchmarkach, kt\u00f3re robi\u0105 wra\u017cenia na badaczach.", "tokens": [51404, 2264, 11, 281, 22607, 9015, 31936, 9850, 261, 18927, 608, 11, 8864, 3870, 11404, 7843, 48830, 1667, 1578, 14875, 608, 13, 51669], "temperature": 0.0, "avg_logprob": -0.10335639050898661, "compression_ratio": 1.3470790378006874, "no_speech_prob": 0.015303879044950008}, {"id": 140, "seek": 42280, "start": 448.90000000000003, "end": 452.40000000000003, "text": " Ale by\u0142 te\u017c test, kt\u00f3ry zszokowa\u0142 chyba wszystkich,", "tokens": [51669, 9366, 16673, 9516, 1500, 11, 9913, 710, 15453, 453, 30105, 31532, 34234, 11, 51844], "temperature": 0.0, "avg_logprob": -0.10335639050898661, "compression_ratio": 1.3470790378006874, "no_speech_prob": 0.015303879044950008}, {"id": 141, "seek": 45240, "start": 452.4, "end": 457.59999999999997, "text": " bo dotyczy\u0142 czego\u015b bardzo ludzkiego oceny autentyczno\u015bci tekstu.", "tokens": [50364, 748, 5893, 88, 6522, 1221, 36559, 1788, 9034, 15946, 30154, 12200, 10409, 43100, 1476, 4179, 3689, 16438, 16624, 372, 84, 13, 50624], "temperature": 0.0, "avg_logprob": -0.08779306176268024, "compression_ratio": 1.450657894736842, "no_speech_prob": 0.0014176062541082501}, {"id": 142, "seek": 45240, "start": 457.59999999999997, "end": 459.2, "text": " Zdecydowanie.", "tokens": [50624, 1176, 1479, 1344, 67, 22028, 13, 50704], "temperature": 0.0, "avg_logprob": -0.08779306176268024, "compression_ratio": 1.450657894736842, "no_speech_prob": 0.0014176062541082501}, {"id": 143, "seek": 45240, "start": 459.2, "end": 464.0, "text": " To by\u0142 chyba ten najbardziej medialny i nieukrywajmy niepokoj\u0105cy eksperyment.", "tokens": [50704, 1407, 16673, 31532, 2064, 41857, 1205, 831, 1634, 741, 2838, 2034, 627, 86, 1805, 2226, 2838, 79, 13704, 8555, 1344, 30724, 610, 88, 518, 13, 50944], "temperature": 0.0, "avg_logprob": -0.08779306176268024, "compression_ratio": 1.450657894736842, "no_speech_prob": 0.0014176062541082501}, {"id": 144, "seek": 45240, "start": 464.0, "end": 470.0, "text": " Badaczy wygenerowali za pomoc\u0105 GPT-3 kr\u00f3tkie artyku\u0142y prasowe na oko\u0142o 200 s\u0142\u00f3w.", "tokens": [50944, 11523, 14691, 4628, 21848, 305, 5103, 7949, 48962, 1611, 26039, 51, 12, 18, 42366, 83, 22872, 594, 874, 5279, 6825, 582, 296, 6880, 1667, 45730, 5249, 2331, 15116, 3901, 13, 51244], "temperature": 0.0, "avg_logprob": -0.08779306176268024, "compression_ratio": 1.450657894736842, "no_speech_prob": 0.0014176062541082501}, {"id": 145, "seek": 45240, "start": 470.0, "end": 475.4, "text": " Nast\u0119pnie pokazali je ludziom wymieszane z prawdziwymi artyku\u0142ami napisanymi przez dziennikarzy", "tokens": [51244, 42185, 18085, 2766, 13010, 921, 5103, 1506, 29586, 298, 29764, 15347, 1929, 710, 41175, 3992, 9726, 3057, 594, 874, 5279, 1221, 4526, 9296, 271, 1325, 3057, 14064, 9758, 1053, 13123, 289, 1229, 51514], "temperature": 0.0, "avg_logprob": -0.08779306176268024, "compression_ratio": 1.450657894736842, "no_speech_prob": 0.0014176062541082501}, {"id": 146, "seek": 45240, "start": 475.4, "end": 477.2, "text": " i zadali proste pytanie.", "tokens": [51514, 741, 42788, 5103, 10293, 68, 36610, 13, 51604], "temperature": 0.0, "avg_logprob": -0.08779306176268024, "compression_ratio": 1.450657894736842, "no_speech_prob": 0.0014176062541082501}, {"id": 147, "seek": 45240, "start": 477.2, "end": 480.4, "text": " Kt\u00f3ry tekst napisa\u0142 cz\u0142owiek, a kt\u00f3ry maszyna?", "tokens": [51604, 591, 4547, 627, 16624, 372, 9296, 3837, 1221, 36282, 74, 11, 257, 9913, 2300, 1229, 629, 30, 51764], "temperature": 0.0, "avg_logprob": -0.08779306176268024, "compression_ratio": 1.450657894736842, "no_speech_prob": 0.0014176062541082501}, {"id": 148, "seek": 45240, "start": 480.4, "end": 482.2, "text": " A jaki by\u0142 wynik?", "tokens": [51764, 316, 24492, 16673, 31936, 1035, 30, 51854], "temperature": 0.0, "avg_logprob": -0.08779306176268024, "compression_ratio": 1.450657894736842, "no_speech_prob": 0.0014176062541082501}, {"id": 149, "seek": 48220, "start": 482.2, "end": 488.4, "text": " Ludzie potrafili poprawnie wskaza\u0107 tekst maszyny w zaledwie 52% przypadk\u00f3w.", "tokens": [50364, 30550, 3283, 1847, 10437, 2312, 1665, 424, 14215, 261, 5161, 12257, 2162, 16624, 372, 2300, 1229, 1634, 261, 710, 5573, 8699, 18079, 4, 33100, 23849, 13, 50674], "temperature": 0.0, "avg_logprob": -0.0765779605810193, "compression_ratio": 1.4105263157894736, "no_speech_prob": 0.0009564489591866732}, {"id": 150, "seek": 48220, "start": 488.4, "end": 494.0, "text": " Czekaj, tylko 52% to jest na granicy b\u0142\u0119du statystycznego.", "tokens": [50674, 383, 19878, 1805, 11, 13219, 18079, 4, 281, 3492, 1667, 9370, 2632, 272, 46564, 769, 2219, 38593, 17466, 11858, 13, 50954], "temperature": 0.0, "avg_logprob": -0.0765779605810193, "compression_ratio": 1.4105263157894736, "no_speech_prob": 0.0009564489591866732}, {"id": 151, "seek": 48220, "start": 494.0, "end": 500.4, "text": " 50% to rzut monet\u0105, czyli w zasadzie r\u00f3wnie dobrze mogliby\u015bmy pyta\u0107 o zdanie mojego psa.", "tokens": [50954, 2625, 4, 281, 367, 89, 325, 15556, 1611, 11, 16591, 261, 44585, 3283, 11416, 14215, 28335, 13172, 38270, 88, 10513, 10664, 42931, 277, 16221, 7155, 705, 39738, 280, 5790, 13, 51274], "temperature": 0.0, "avg_logprob": -0.0765779605810193, "compression_ratio": 1.4105263157894736, "no_speech_prob": 0.0009564489591866732}, {"id": 152, "seek": 48220, "start": 500.4, "end": 506.2, "text": " To pokazuje, jak bardzo nasze intuicje dotycz\u0105ce ludzkiego pisania sta\u0142y si\u0119 niewiarygodne.", "tokens": [51274, 1407, 13010, 43317, 11, 4207, 9034, 43394, 560, 84, 299, 2884, 5893, 17466, 1611, 384, 15946, 30154, 12200, 26584, 5609, 11135, 6825, 3244, 43622, 29104, 21787, 716, 13, 51564], "temperature": 0.0, "avg_logprob": -0.0765779605810193, "compression_ratio": 1.4105263157894736, "no_speech_prob": 0.0009564489591866732}, {"id": 153, "seek": 48220, "start": 506.2, "end": 510.8, "text": " Dok\u0142adnie, a co jeszcze ciekawsze i co wida\u0107 na jednym z wykres\u00f3w pracy,", "tokens": [51564, 29768, 10358, 2766, 11, 257, 598, 14168, 46419, 28354, 741, 598, 261, 46898, 1667, 5232, 12996, 710, 39287, 495, 3901, 35591, 11, 51794], "temperature": 0.0, "avg_logprob": -0.0765779605810193, "compression_ratio": 1.4105263157894736, "no_speech_prob": 0.0009564489591866732}, {"id": 154, "seek": 51080, "start": 510.8, "end": 516.0, "text": " ta zdolno\u015b\u0107 odr\u00f3\u017cniania spada\u0142a dramatycznie wraz ze wzrostem wielko\u015bci modelu.", "tokens": [50364, 1846, 16221, 401, 23293, 3611, 11721, 1427, 77, 952, 654, 637, 1538, 5024, 42749, 17466, 2766, 7843, 89, 5277, 24809, 27494, 443, 20570, 4093, 6199, 2316, 84, 13, 50624], "temperature": 0.0, "avg_logprob": -0.0836378553378507, "compression_ratio": 1.4134615384615385, "no_speech_prob": 0.014337182976305485}, {"id": 155, "seek": 51080, "start": 516.0, "end": 519.0, "text": " Dla mniejszych modeli ludzie radzili sobie ca\u0142kiem nie\u017ale.", "tokens": [50624, 413, 875, 39513, 45021, 2316, 72, 37025, 2843, 89, 2312, 13652, 35224, 26116, 2838, 10659, 306, 13, 50774], "temperature": 0.0, "avg_logprob": -0.0836378553378507, "compression_ratio": 1.4134615384615385, "no_speech_prob": 0.014337182976305485}, {"id": 156, "seek": 51080, "start": 519.0, "end": 522.8, "text": " Dla najwi\u0119kszego GPT-3 byli niema\u0142 bezradni.", "tokens": [50774, 413, 875, 48636, 1694, 27725, 26039, 51, 12, 18, 538, 2081, 2838, 1696, 1221, 10782, 6206, 3722, 13, 50964], "temperature": 0.0, "avg_logprob": -0.0836378553378507, "compression_ratio": 1.4134615384615385, "no_speech_prob": 0.014337182976305485}, {"id": 157, "seek": 51080, "start": 522.8, "end": 526.8, "text": " I to jest co\u015b, co autorzy sami nazwali niepokoj\u0105cym kamieniem milowym.", "tokens": [50964, 286, 281, 3492, 19241, 11, 598, 19510, 1229, 3247, 72, 20151, 40054, 2838, 79, 13704, 8555, 1344, 76, 9727, 1053, 4907, 1962, 31691, 13, 51164], "temperature": 0.0, "avg_logprob": -0.0836378553378507, "compression_ratio": 1.4134615384615385, "no_speech_prob": 0.014337182976305485}, {"id": 158, "seek": 51080, "start": 526.8, "end": 528.4, "text": " A propos kali.", "tokens": [51164, 316, 7532, 350, 5103, 13, 51244], "temperature": 0.0, "avg_logprob": -0.0836378553378507, "compression_ratio": 1.4134615384615385, "no_speech_prob": 0.014337182976305485}, {"id": 159, "seek": 51080, "start": 528.4, "end": 530.6, "text": " Wr\u00f3\u0107my na chwil\u0119 do tych wykres\u00f3w.", "tokens": [51244, 10159, 812, 2162, 2226, 1667, 41941, 1274, 360, 15180, 39287, 495, 3901, 13, 51354], "temperature": 0.0, "avg_logprob": -0.0836378553378507, "compression_ratio": 1.4134615384615385, "no_speech_prob": 0.014337182976305485}, {"id": 160, "seek": 51080, "start": 530.6, "end": 535.0, "text": " Bo one pokazywa\u0142y co\u015b wi\u0119cej ni\u017c tylko to, \u017ce wi\u0119kszy znaczy lepszy.", "tokens": [51354, 3286, 472, 13010, 33235, 4151, 6825, 19241, 26004, 28502, 13219, 281, 11, 3561, 29968, 1229, 36584, 476, 1878, 1229, 13, 51574], "temperature": 0.0, "avg_logprob": -0.0836378553378507, "compression_ratio": 1.4134615384615385, "no_speech_prob": 0.014337182976305485}, {"id": 161, "seek": 51080, "start": 535.0, "end": 538.0, "text": " By\u0142 tam pewien subtelny, ale kluczowy trend.", "tokens": [51574, 3146, 1221, 7677, 25889, 1053, 7257, 338, 1634, 11, 6775, 9671, 1311, 89, 10089, 6028, 13, 51724], "temperature": 0.0, "avg_logprob": -0.0836378553378507, "compression_ratio": 1.4134615384615385, "no_speech_prob": 0.014337182976305485}, {"id": 162, "seek": 53800, "start": 538.0, "end": 541.8, "text": " Tak, to wida\u0107 na samym gocz\u0105tku na figure 1.1.", "tokens": [50364, 9118, 11, 281, 261, 46898, 1667, 3247, 4199, 352, 3689, 23430, 5279, 1667, 2573, 502, 13, 16, 13, 50554], "temperature": 0.0, "avg_logprob": -0.09425696459683505, "compression_ratio": 1.494915254237288, "no_speech_prob": 0.007799859158694744}, {"id": 163, "seek": 53800, "start": 541.8, "end": 547.0, "text": " W wykres po lewej stronie pokazuje, \u017ce wraz ze wzrostem liczby parametr\u00f3w", "tokens": [50554, 343, 39287, 495, 714, 476, 826, 73, 1056, 32242, 13010, 43317, 11, 3561, 7843, 89, 5277, 24809, 27494, 443, 6169, 89, 2322, 6220, 27965, 3901, 50814], "temperature": 0.0, "avg_logprob": -0.09425696459683505, "compression_ratio": 1.494915254237288, "no_speech_prob": 0.007799859158694744}, {"id": 164, "seek": 53800, "start": 547.0, "end": 552.2, "text": " ro\u015bnie wydajno\u015b\u0107 we wszystkich trybach zero shot, one shot i few shot.", "tokens": [50814, 744, 12221, 25984, 1805, 23293, 321, 34234, 853, 32096, 4018, 3347, 11, 472, 3347, 741, 1326, 3347, 13, 51074], "temperature": 0.0, "avg_logprob": -0.09425696459683505, "compression_ratio": 1.494915254237288, "no_speech_prob": 0.007799859158694744}, {"id": 165, "seek": 53800, "start": 552.2, "end": 559.0, "text": " Ale i to jest kluczowe, r\u00f3\u017cnica mi\u0119dzy wydajno\u015bci\u0105 few shot a zero shot te\u017c si\u0119 powi\u0119ksza.", "tokens": [51074, 9366, 741, 281, 3492, 9671, 1311, 89, 6880, 11, 19637, 32687, 33964, 25984, 1805, 16438, 1611, 1326, 3347, 257, 4018, 3347, 9516, 3244, 3388, 5034, 1694, 2394, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09425696459683505, "compression_ratio": 1.494915254237288, "no_speech_prob": 0.007799859158694744}, {"id": 166, "seek": 53800, "start": 559.0, "end": 561.4, "text": " Co to w\u0142a\u015bciwie oznacza w praktyce?", "tokens": [51414, 3066, 281, 50108, 277, 22672, 326, 2394, 261, 3206, 74, 874, 384, 30, 51534], "temperature": 0.0, "avg_logprob": -0.09425696459683505, "compression_ratio": 1.494915254237288, "no_speech_prob": 0.007799859158694744}, {"id": 167, "seek": 53800, "start": 561.4, "end": 567.8, "text": " Oznacza to, \u017ce wi\u0119ksze modele staj\u0105 si\u0119 nieproporcjonalnie lepsze w uczeniu si\u0119 w locie z kontekstu.", "tokens": [51534, 422, 22672, 326, 2394, 281, 11, 3561, 29968, 1381, 4391, 306, 342, 11133, 3244, 2838, 79, 1513, 36003, 15735, 304, 2766, 476, 1878, 1381, 261, 344, 66, 39651, 3244, 261, 1628, 414, 710, 14373, 916, 372, 84, 13, 51854], "temperature": 0.0, "avg_logprob": -0.09425696459683505, "compression_ratio": 1.494915254237288, "no_speech_prob": 0.007799859158694744}, {"id": 168, "seek": 56780, "start": 568.1999999999999, "end": 570.1999999999999, "text": " To nie jest liniowy przyrost.", "tokens": [50384, 1407, 2838, 3492, 287, 3812, 10089, 6501, 27494, 13, 50484], "temperature": 0.0, "avg_logprob": -0.10850261506580171, "compression_ratio": 1.512, "no_speech_prob": 0.0014531470369547606}, {"id": 169, "seek": 56780, "start": 570.1999999999999, "end": 572.4, "text": " One nie tylko wiedz\u0105 wi\u0119cej.", "tokens": [50484, 1485, 2838, 13219, 46894, 8925, 26004, 13, 50594], "temperature": 0.0, "avg_logprob": -0.10850261506580171, "compression_ratio": 1.512, "no_speech_prob": 0.0014531470369547606}, {"id": 170, "seek": 56780, "start": 572.4, "end": 576.5999999999999, "text": " One staj\u0105 si\u0119 sprawniejsze w wykorzystywaniu nowych informacji.", "tokens": [50594, 1485, 342, 11133, 3244, 637, 29603, 7764, 82, 1381, 261, 43606, 1229, 25134, 86, 25849, 586, 16384, 1356, 13152, 13, 50804], "temperature": 0.0, "avg_logprob": -0.10850261506580171, "compression_ratio": 1.512, "no_speech_prob": 0.0014531470369547606}, {"id": 171, "seek": 56780, "start": 576.5999999999999, "end": 580.4, "text": " Autorzy nazywaj\u0105 je bardziej bieg\u0142ymi metal learners.", "tokens": [50804, 6049, 284, 1229, 20151, 27112, 11133, 1506, 27209, 272, 20408, 6825, 3057, 5760, 23655, 13, 50994], "temperature": 0.0, "avg_logprob": -0.10850261506580171, "compression_ratio": 1.512, "no_speech_prob": 0.0014531470369547606}, {"id": 172, "seek": 56780, "start": 580.4, "end": 582.1999999999999, "text": " Metal learners?", "tokens": [50994, 23488, 23655, 30, 51084], "temperature": 0.0, "avg_logprob": -0.10850261506580171, "compression_ratio": 1.512, "no_speech_prob": 0.0014531470369547606}, {"id": 173, "seek": 56780, "start": 582.1999999999999, "end": 585.1999999999999, "text": " To brzmi jak co\u015b z podr\u0119cznika do filozofii.", "tokens": [51084, 1407, 738, 89, 3057, 4207, 19241, 710, 15305, 1274, 3689, 77, 5439, 360, 1387, 15151, 2670, 5597, 13, 51234], "temperature": 0.0, "avg_logprob": -0.10850261506580171, "compression_ratio": 1.512, "no_speech_prob": 0.0014531470369547606}, {"id": 174, "seek": 56780, "start": 585.1999999999999, "end": 586.5999999999999, "text": " Co to tak naprawd\u0119 znaczy?", "tokens": [51234, 3066, 281, 991, 20970, 36584, 30, 51304], "temperature": 0.0, "avg_logprob": -0.10850261506580171, "compression_ratio": 1.512, "no_speech_prob": 0.0014531470369547606}, {"id": 175, "seek": 56780, "start": 586.5999999999999, "end": 593.5999999999999, "text": " To znaczy, \u017ce te modele nie tylko nauczy\u0142y si\u0119 j\u0119zyka, ale nauczy\u0142y si\u0119 samej sztuki uczenia si\u0119.", "tokens": [51304, 1407, 36584, 11, 3561, 535, 4391, 306, 2838, 13219, 49103, 1229, 6825, 3244, 42309, 40940, 11, 6775, 49103, 1229, 6825, 3244, 912, 73, 262, 2682, 11788, 344, 38517, 3244, 13, 51654], "temperature": 0.0, "avg_logprob": -0.10850261506580171, "compression_ratio": 1.512, "no_speech_prob": 0.0014531470369547606}, {"id": 176, "seek": 59360, "start": 593.6, "end": 602.2, "text": " Jak ucze\u0144, kt\u00f3ry nie tylko wkuwa fakty, ale zrozumia\u0142, jak si\u0119 efektywnie uczy\u0107 z dowolnego materia\u0142u, kt\u00f3ry mu si\u0119 podsunie.", "tokens": [50364, 15029, 344, 9680, 5248, 11, 9913, 2838, 13219, 261, 5279, 4151, 33647, 874, 11, 6775, 710, 27857, 449, 8908, 11, 4207, 3244, 31482, 916, 874, 14215, 344, 33967, 710, 9459, 401, 11858, 2389, 8908, 84, 11, 9913, 2992, 3244, 31925, 409, 414, 13, 50794], "temperature": 0.0, "avg_logprob": -0.08939560704261253, "compression_ratio": 1.468944099378882, "no_speech_prob": 0.009268207475543022}, {"id": 177, "seek": 59360, "start": 602.2, "end": 610.4, "text": " Im wi\u0119kszy model, tym szybciej \u0142apie w locie o co chodzi w nowej grze, nawet je\u015bli nikt nie zmienia\u0142 zasad w jego g\u0142owie na sta\u0142e.", "tokens": [50794, 4331, 29968, 1229, 2316, 11, 8107, 36456, 4260, 73, 25387, 569, 414, 261, 1628, 414, 277, 598, 23998, 261, 586, 40779, 677, 1381, 11, 22696, 25630, 297, 9874, 2838, 17020, 1053, 8908, 44585, 261, 26542, 18117, 13998, 1667, 11135, 19827, 13, 51204], "temperature": 0.0, "avg_logprob": -0.08939560704261253, "compression_ratio": 1.468944099378882, "no_speech_prob": 0.009268207475543022}, {"id": 178, "seek": 59360, "start": 610.4, "end": 613.2, "text": " To wszystko brzni niemal jak science fiction.", "tokens": [51204, 1407, 22607, 738, 89, 3722, 2838, 5579, 4207, 3497, 13266, 13, 51344], "temperature": 0.0, "avg_logprob": -0.08939560704261253, "compression_ratio": 1.468944099378882, "no_speech_prob": 0.009268207475543022}, {"id": 179, "seek": 59360, "start": 613.2, "end": 618.2, "text": " Wyniki, kt\u00f3re bij\u0105 specjalist\u00f3w, tekst nieodr\u00f3\u017cnialny od ludzkiego,", "tokens": [51344, 343, 2534, 9850, 11, 8864, 10317, 1611, 46433, 468, 3901, 11, 16624, 372, 2838, 378, 11721, 1427, 77, 831, 1634, 3611, 15946, 30154, 12200, 11, 51594], "temperature": 0.0, "avg_logprob": -0.08939560704261253, "compression_ratio": 1.468944099378882, "no_speech_prob": 0.009268207475543022}, {"id": 180, "seek": 59360, "start": 618.2, "end": 623.2, "text": " a\u017c trudno uwierzy\u0107, \u017ce to nie jest jaka\u015b forma sztucznej og\u00f3lnej inteligencji.", "tokens": [51594, 48134, 32007, 1771, 23147, 811, 27150, 11, 3561, 281, 2838, 3492, 4207, 64, 1788, 8366, 262, 2682, 1311, 89, 11794, 5360, 15741, 11794, 24777, 3213, 19649, 13, 51844], "temperature": 0.0, "avg_logprob": -0.08939560704261253, "compression_ratio": 1.468944099378882, "no_speech_prob": 0.009268207475543022}, {"id": 181, "seek": 62320, "start": 623.2, "end": 629.6, "text": " Ale autorzy sami sprowadzaj\u0105 nas na ziemi\u0119, po\u015bwi\u0119caj\u0105c sporo miejsca na to, gdzie ta magia przestaje dzia\u0142a\u0107.", "tokens": [50364, 9366, 19510, 1229, 3247, 72, 637, 1892, 345, 89, 11133, 5382, 1667, 16503, 3057, 1274, 11, 714, 1788, 22423, 496, 8555, 66, 637, 10780, 18522, 44239, 1667, 281, 11, 18922, 1846, 2258, 654, 44264, 11153, 37903, 2162, 13, 50684], "temperature": 0.0, "avg_logprob": -0.078533665445827, "compression_ratio": 1.444078947368421, "no_speech_prob": 0.006185293197631836}, {"id": 182, "seek": 62320, "start": 629.6, "end": 634.6, "text": " I to jest jedyna z najwi\u0119kszych zalet tej pracy, ta intelektualna uczciwo\u015b\u0107.", "tokens": [50684, 286, 281, 3492, 5232, 88, 629, 710, 48636, 1694, 28051, 29599, 302, 12573, 35591, 11, 1846, 2830, 306, 2320, 901, 629, 35403, 537, 48847, 13, 50934], "temperature": 0.0, "avg_logprob": -0.078533665445827, "compression_ratio": 1.444078947368421, "no_speech_prob": 0.006185293197631836}, {"id": 183, "seek": 62320, "start": 634.6, "end": 640.4000000000001, "text": " Nie pr\u00f3bowali sprzeda\u0107 GPT-3 jako magicznego rozwi\u0105zania wszystkich problem\u00f3w.", "tokens": [50934, 12016, 8565, 8202, 5103, 6103, 89, 8801, 2162, 26039, 51, 12, 18, 17123, 5585, 89, 11858, 9544, 22620, 5609, 34234, 1154, 3901, 13, 51224], "temperature": 0.0, "avg_logprob": -0.078533665445827, "compression_ratio": 1.444078947368421, "no_speech_prob": 0.006185293197631836}, {"id": 184, "seek": 62320, "start": 640.4000000000001, "end": 644.0, "text": " Po\u015bwi\u0119cili ca\u0142\u0105 sekcj\u0119 na szczeg\u00f3\u0142owe om\u00f3wienie jego s\u0142abo\u015bci.", "tokens": [51224, 6165, 1788, 22423, 66, 2312, 1335, 15926, 17215, 41960, 1667, 22090, 1146, 16181, 6880, 3406, 3901, 27385, 26542, 15116, 41265, 6199, 13, 51404], "temperature": 0.0, "avg_logprob": -0.078533665445827, "compression_ratio": 1.444078947368421, "no_speech_prob": 0.006185293197631836}, {"id": 185, "seek": 62320, "start": 644.0, "end": 649.6, "text": " No w\u0142a\u015bnie. Gdzie s\u0105 te p\u0119kni\u0119cia wzbroi? Co by\u0142o najwi\u0119ksz\u0105 pora\u017ck\u0105 GPT-3?", "tokens": [51404, 883, 14234, 13, 460, 13096, 9015, 535, 280, 1274, 74, 35938, 2755, 24809, 9120, 72, 30, 3066, 14811, 48636, 1694, 8925, 1515, 18264, 26304, 26039, 51, 12, 18, 30, 51684], "temperature": 0.0, "avg_logprob": -0.078533665445827, "compression_ratio": 1.444078947368421, "no_speech_prob": 0.006185293197631836}, {"id": 186, "seek": 64960, "start": 649.6, "end": 653.8000000000001, "text": " Pierwszy fundamentalny problem to sp\u00f3jno\u015b\u0107 na d\u0142u\u017csz\u0105 met\u0119.", "tokens": [50364, 16676, 30012, 8088, 1634, 1154, 281, 637, 18999, 23293, 1667, 274, 24066, 1427, 82, 8925, 1131, 1274, 13, 50574], "temperature": 0.0, "avg_logprob": -0.0917767434224595, "compression_ratio": 1.421602787456446, "no_speech_prob": 0.15659379959106445}, {"id": 187, "seek": 64960, "start": 653.8000000000001, "end": 658.2, "text": " Model potrafi\u0142 wygenerowa\u0107 genialne akapit, a nawet kilka.", "tokens": [50574, 17105, 1847, 10437, 40622, 4628, 21848, 11445, 48228, 716, 9308, 569, 270, 11, 257, 22696, 36466, 13, 50794], "temperature": 0.0, "avg_logprob": -0.0917767434224595, "compression_ratio": 1.421602787456446, "no_speech_prob": 0.15659379959106445}, {"id": 188, "seek": 64960, "start": 658.2, "end": 661.4, "text": " Ale w tek\u015bcie na kilka stron zaczyna\u0142 si\u0119 gubi\u0107.", "tokens": [50794, 9366, 261, 16624, 9815, 1667, 36466, 45766, 43811, 629, 1221, 3244, 695, 5614, 2162, 13, 50954], "temperature": 0.0, "avg_logprob": -0.0917767434224595, "compression_ratio": 1.421602787456446, "no_speech_prob": 0.15659379959106445}, {"id": 189, "seek": 64960, "start": 661.4, "end": 668.4, "text": " Potrafi\u0142 si\u0119 powtarza\u0107, traci\u0107 g\u0142\u00f3wny w\u0105tek, a czasem nawet zaprzeczy\u0107 temu, co napisa\u0142 kilka akapit\u00f3w wcze\u015bniej.", "tokens": [50954, 9145, 10437, 40622, 3244, 3388, 23480, 35873, 11, 504, 326, 12757, 18117, 812, 43682, 261, 23430, 916, 11, 257, 13190, 443, 22696, 14223, 13503, 33967, 33346, 11, 598, 9296, 3837, 1221, 36466, 9308, 569, 270, 3901, 40785, 13, 51304], "temperature": 0.0, "avg_logprob": -0.0917767434224595, "compression_ratio": 1.421602787456446, "no_speech_prob": 0.15659379959106445}, {"id": 190, "seek": 64960, "start": 668.4, "end": 671.0, "text": " By\u0142 sprinterem niemarat\u0105czykiem.", "tokens": [51304, 3146, 1221, 6103, 686, 7333, 2838, 6209, 267, 1611, 6522, 26116, 13, 51434], "temperature": 0.0, "avg_logprob": -0.0917767434224595, "compression_ratio": 1.421602787456446, "no_speech_prob": 0.15659379959106445}, {"id": 191, "seek": 64960, "start": 671.0, "end": 675.2, "text": " Druga wada, jak rozumiem, by\u0142a wpisana w sam\u0105 jego architektur\u0119?", "tokens": [51434, 2491, 19364, 261, 1538, 11, 4207, 48797, 4907, 11, 23936, 32444, 271, 2095, 261, 3247, 1611, 26542, 3912, 642, 2320, 374, 1274, 30, 51644], "temperature": 0.0, "avg_logprob": -0.0917767434224595, "compression_ratio": 1.421602787456446, "no_speech_prob": 0.15659379959106445}, {"id": 192, "seek": 67520, "start": 675.2, "end": 679.6, "text": " Tak. GPT-3, podobnie jak jego poprzednicy, jest modelem jednokierunkowym.", "tokens": [50364, 9118, 13, 26039, 51, 12, 18, 11, 43024, 2766, 4207, 26542, 1665, 81, 11312, 77, 2632, 11, 3492, 4391, 10386, 5232, 77, 453, 811, 3197, 31691, 13, 50584], "temperature": 0.0, "avg_logprob": -0.1002196785788866, "compression_ratio": 1.4458204334365325, "no_speech_prob": 0.026865217834711075}, {"id": 193, "seek": 67520, "start": 679.6, "end": 683.4000000000001, "text": " Przetwarza tekst od lewej do prawej, jak cz\u0142owiek czytaj\u0105cy ksi\u0105\u017ck\u0119.", "tokens": [50584, 2114, 40399, 6925, 2394, 16624, 372, 3611, 476, 826, 73, 360, 3206, 826, 73, 11, 4207, 36282, 74, 6430, 1328, 8555, 1344, 39311, 15724, 13, 50774], "temperature": 0.0, "avg_logprob": -0.1002196785788866, "compression_ratio": 1.4458204334365325, "no_speech_prob": 0.026865217834711075}, {"id": 194, "seek": 67520, "start": 683.4000000000001, "end": 689.4000000000001, "text": " To jest \u015bwietne dogenerowania tekstu, ale fatalne do zada\u0144, kt\u00f3re wymagaj\u0105 por\u00f3wnania dw\u00f3ch fragment\u00f3w tekstu", "tokens": [50774, 1407, 3492, 8299, 39083, 716, 360, 21848, 21308, 16624, 372, 84, 11, 6775, 24069, 716, 360, 710, 1538, 5248, 11, 8864, 29764, 559, 11133, 1515, 812, 895, 5609, 27379, 812, 339, 26424, 3901, 16624, 372, 84, 51074], "temperature": 0.0, "avg_logprob": -0.1002196785788866, "compression_ratio": 1.4458204334365325, "no_speech_prob": 0.026865217834711075}, {"id": 195, "seek": 67520, "start": 689.4000000000001, "end": 692.2, "text": " i spojrzenia na nie z obu stron jednocze\u015bnie.", "tokens": [51074, 741, 8243, 73, 81, 14320, 1667, 2838, 710, 1111, 84, 45766, 5232, 26694, 1381, 12221, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1002196785788866, "compression_ratio": 1.4458204334365325, "no_speech_prob": 0.026865217834711075}, {"id": 196, "seek": 67520, "start": 692.2, "end": 693.8000000000001, "text": " Mo\u017cesz poda\u0107 jaki\u015b przyk\u0142ad?", "tokens": [51214, 44736, 10430, 2497, 43379, 34721, 23144, 30, 51294], "temperature": 0.0, "avg_logprob": -0.1002196785788866, "compression_ratio": 1.4458204334365325, "no_speech_prob": 0.026865217834711075}, {"id": 197, "seek": 67520, "start": 693.8000000000001, "end": 697.8000000000001, "text": " Klasyczny przyk\u0142ad to zadanie WIS z benchmarku Super Glue.", "tokens": [51294, 591, 7743, 17466, 1634, 23144, 281, 42788, 7155, 343, 2343, 710, 18927, 84, 4548, 49832, 13, 51494], "temperature": 0.0, "avg_logprob": -0.1002196785788866, "compression_ratio": 1.4458204334365325, "no_speech_prob": 0.026865217834711075}, {"id": 198, "seek": 67520, "start": 697.8000000000001, "end": 702.0, "text": " Model dostaje dwa zdania, w kt\u00f3rych wyst\u0119puje to samo s\u0142owo", "tokens": [51494, 17105, 20568, 11153, 35045, 16221, 5609, 11, 261, 30382, 48255, 18085, 13008, 281, 36422, 15116, 19941, 51704], "temperature": 0.0, "avg_logprob": -0.1002196785788866, "compression_ratio": 1.4458204334365325, "no_speech_prob": 0.026865217834711075}, {"id": 199, "seek": 70200, "start": 702.0, "end": 705.4, "text": " i ma oceni\u0107, czy zosta\u0142o u\u017cyte w tym samym znaczeniu.", "tokens": [50364, 741, 463, 10409, 268, 12757, 11, 6430, 23154, 5249, 34097, 975, 261, 8107, 3247, 4199, 15397, 326, 39651, 13, 50534], "temperature": 0.0, "avg_logprob": -0.0656153189169394, "compression_ratio": 1.3966101694915254, "no_speech_prob": 0.059564899653196335}, {"id": 200, "seek": 70200, "start": 705.4, "end": 709.6, "text": " Na przyk\u0142ad rzuci\u0142em pi\u0142k\u0119 i urz\u0105dzili\u015bmy wielki bal.", "tokens": [50534, 6056, 23144, 367, 11728, 537, 11126, 3895, 1221, 15724, 741, 4038, 23876, 89, 43912, 20570, 2984, 3119, 13, 50744], "temperature": 0.0, "avg_logprob": -0.0656153189169394, "compression_ratio": 1.3966101694915254, "no_speech_prob": 0.059564899653196335}, {"id": 201, "seek": 70200, "start": 709.6, "end": 711.2, "text": " Czy bal znaczy to samo?", "tokens": [50744, 19832, 3119, 36584, 281, 36422, 30, 50824], "temperature": 0.0, "avg_logprob": -0.0656153189169394, "compression_ratio": 1.3966101694915254, "no_speech_prob": 0.059564899653196335}, {"id": 202, "seek": 70200, "start": 711.2, "end": 712.8, "text": " Dla nas to oczywiste, \u017ce nie.", "tokens": [50824, 413, 875, 5382, 281, 277, 6522, 86, 8375, 11, 3561, 2838, 13, 50904], "temperature": 0.0, "avg_logprob": -0.0656153189169394, "compression_ratio": 1.3966101694915254, "no_speech_prob": 0.059564899653196335}, {"id": 203, "seek": 70200, "start": 712.8, "end": 719.2, "text": " Dla GPT-3, kt\u00f3re nie mo\u017ce efektywnie skaka\u0107 mi\u0119dzy zdaniami, by\u0142o to niemal niemo\u017cliwe.", "tokens": [50904, 413, 875, 26039, 51, 12, 18, 11, 8864, 2838, 12034, 31482, 916, 874, 14215, 1110, 7849, 2162, 33964, 710, 10312, 15568, 11, 14811, 281, 2838, 5579, 2838, 3280, 1427, 2081, 826, 13, 51224], "temperature": 0.0, "avg_logprob": -0.0656153189169394, "compression_ratio": 1.3966101694915254, "no_speech_prob": 0.059564899653196335}, {"id": 204, "seek": 70200, "start": 719.2, "end": 721.4, "text": " Jego wyniki by\u0142y tam bliskie losowym.", "tokens": [51224, 508, 6308, 31936, 9850, 26366, 7677, 888, 7797, 414, 1750, 31691, 13, 51334], "temperature": 0.0, "avg_logprob": -0.0656153189169394, "compression_ratio": 1.3966101694915254, "no_speech_prob": 0.059564899653196335}, {"id": 205, "seek": 70200, "start": 721.4, "end": 727.6, "text": " I wreszcie trzecia chyba najg\u0142\u0119bsza wada, kt\u00f3r\u0105 autorzy nazwali brakiem ugruntowania w rzeczywisto\u015bci.", "tokens": [51334, 286, 261, 495, 89, 4260, 22266, 2755, 31532, 11212, 70, 46564, 929, 2394, 261, 1538, 11, 37415, 19510, 1229, 20151, 40054, 1548, 26116, 344, 861, 2760, 21308, 261, 26297, 86, 9334, 6199, 13, 51644], "temperature": 0.0, "avg_logprob": -0.0656153189169394, "compression_ratio": 1.3966101694915254, "no_speech_prob": 0.059564899653196335}, {"id": 206, "seek": 72760, "start": 727.6, "end": 735.4, "text": " Dok\u0142adnie GPT-3 jest mistrzem tekstu, \u017cyje w \u015bwiecie s\u0142\u00f3w, ale nie ma poj\u0119cia o \u015bwiecie fizycznym.", "tokens": [50364, 29768, 10358, 2766, 26039, 51, 12, 18, 3492, 3544, 19390, 443, 16624, 372, 84, 11, 16136, 2884, 261, 40078, 4260, 15116, 3901, 11, 6775, 2838, 463, 714, 11115, 2755, 277, 40078, 4260, 21000, 17466, 12996, 13, 50754], "temperature": 0.0, "avg_logprob": -0.059807228886224084, "compression_ratio": 1.4948453608247423, "no_speech_prob": 0.040223006159067154}, {"id": 207, "seek": 72760, "start": 735.4, "end": 741.0, "text": " Nigdy nie widzia\u0142 czerwonego jab\u0142ka, nie poczu\u0142 zimna, nie podnios\u0142 ci\u0119\u017ckiego kamienia.", "tokens": [50754, 39554, 3173, 2838, 27486, 8908, 269, 4527, 86, 546, 1571, 33475, 1221, 2330, 11, 2838, 26423, 84, 1221, 710, 332, 629, 11, 2838, 2497, 77, 2717, 1221, 35484, 1427, 42349, 9727, 18811, 13, 51034], "temperature": 0.0, "avg_logprob": -0.059807228886224084, "compression_ratio": 1.4948453608247423, "no_speech_prob": 0.040223006159067154}, {"id": 208, "seek": 72760, "start": 741.0, "end": 746.8000000000001, "text": " Brakuje mu ogromnej cz\u0119\u015bci kontekstu, kt\u00f3re my ludzie czerpiemy ze zmys\u0142\u00f3w.", "tokens": [51034, 4991, 5279, 2884, 2992, 34416, 298, 11794, 41314, 14373, 916, 372, 84, 11, 8864, 452, 37025, 269, 4527, 9144, 2226, 5277, 17020, 39508, 3901, 13, 51324], "temperature": 0.0, "avg_logprob": -0.059807228886224084, "compression_ratio": 1.4948453608247423, "no_speech_prob": 0.040223006159067154}, {"id": 209, "seek": 72760, "start": 746.8000000000001, "end": 750.2, "text": " To prowadzi do b\u0142\u0119d\u00f3w, kt\u00f3re dla nas wydaj\u0105 si\u0119 absurdalne.", "tokens": [51324, 1407, 36590, 3992, 360, 272, 1221, 6298, 3901, 11, 8864, 12285, 5382, 25984, 11133, 3244, 19774, 304, 716, 13, 51494], "temperature": 0.0, "avg_logprob": -0.059807228886224084, "compression_ratio": 1.4948453608247423, "no_speech_prob": 0.040223006159067154}, {"id": 210, "seek": 72760, "start": 750.2, "end": 755.6, "text": " B\u0142\u0119d\u00f3w wynikaj\u0105cych z braku zdrowego rozs\u0105dku opartego na fizycznym do\u015bwiadczeniu.", "tokens": [51494, 363, 1221, 6298, 3901, 31936, 1035, 11133, 31306, 710, 1548, 5279, 49745, 6308, 9544, 82, 18962, 5279, 999, 446, 6308, 1667, 21000, 17466, 12996, 46661, 66, 39651, 13, 51764], "temperature": 0.0, "avg_logprob": -0.059807228886224084, "compression_ratio": 1.4948453608247423, "no_speech_prob": 0.040223006159067154}, {"id": 211, "seek": 75560, "start": 755.6, "end": 760.4, "text": " To jak kto\u015b, kto przeczyta\u0142 ka\u017cd\u0105 ksi\u0105\u017ck\u0119 o p\u0142ywaniu, ale nigdy nie wszed\u0142 do wody.", "tokens": [50364, 1407, 4207, 32982, 11, 23780, 8325, 6522, 46426, 21912, 67, 1611, 39311, 15724, 277, 280, 6825, 86, 25849, 11, 6775, 26996, 3173, 2838, 37647, 11312, 1221, 360, 261, 843, 13, 50604], "temperature": 0.0, "avg_logprob": -0.09919979287393438, "compression_ratio": 1.4879518072289157, "no_speech_prob": 0.043393898755311966}, {"id": 212, "seek": 75560, "start": 760.4, "end": 768.8000000000001, "text": " OK, czyli mamy ograniczenia techniczne, ale poza nimi autorzy po\u015bwi\u0119cili te\u017c bardzo du\u017co miejsca na analiz\u0119 znacznie trudniejszego problemu.", "tokens": [50604, 2264, 11, 16591, 17335, 34416, 30732, 14320, 1537, 17946, 716, 11, 6775, 714, 2394, 297, 10121, 19510, 1229, 714, 1788, 22423, 66, 2312, 9516, 9034, 26673, 18522, 44239, 1667, 2624, 590, 1274, 15397, 14875, 2766, 32007, 10402, 15453, 6308, 1154, 84, 13, 51024], "temperature": 0.0, "avg_logprob": -0.09919979287393438, "compression_ratio": 1.4879518072289157, "no_speech_prob": 0.043393898755311966}, {"id": 213, "seek": 75560, "start": 768.8000000000001, "end": 774.0, "text": " Co si\u0119 stanie, gdy ta technologia trafi w r\u0119ce ludzi? M\u00f3wimy o sekcji Broader Impact.", "tokens": [51024, 3066, 3244, 40013, 11, 28405, 1846, 1537, 24103, 944, 13325, 261, 41197, 384, 29586, 30, 376, 3901, 13189, 277, 17215, 19649, 5425, 8312, 31005, 13, 51284], "temperature": 0.0, "avg_logprob": -0.09919979287393438, "compression_ratio": 1.4879518072289157, "no_speech_prob": 0.043393898755311966}, {"id": 214, "seek": 75560, "start": 774.0, "end": 785.2, "text": " Autorzy nie uciekali od odpowiedzialno\u015bci, otwarcie analizowali potencjalne nadu\u017cycia i, co niezwykle wa\u017cne, problem wbudowanej w model stronniczo\u015bci, czyli bias.", "tokens": [51284, 6049, 284, 1229, 2838, 344, 4260, 74, 5103, 3611, 24314, 15338, 831, 16438, 11, 4337, 6925, 4260, 2624, 590, 305, 5103, 1847, 22660, 22600, 716, 12617, 84, 7735, 2755, 741, 11, 598, 33511, 9726, 14677, 46110, 11, 1154, 261, 18281, 23066, 73, 261, 2316, 45766, 7692, 4765, 6199, 11, 16591, 12577, 13, 51844], "temperature": 0.0, "avg_logprob": -0.09919979287393438, "compression_ratio": 1.4879518072289157, "no_speech_prob": 0.043393898755311966}, {"id": 215, "seek": 78520, "start": 785.2, "end": 789.8000000000001, "text": " Zacznijmy od nadu\u017cy\u0107, czego si\u0119 obawiali w 2020 roku.", "tokens": [50364, 1176, 14875, 77, 1718, 2226, 3611, 12617, 84, 39687, 11, 36559, 3244, 1111, 1607, 831, 72, 261, 4808, 19451, 13, 50594], "temperature": 0.0, "avg_logprob": -0.0960813962496244, "compression_ratio": 1.4179104477611941, "no_speech_prob": 0.02514140121638775}, {"id": 216, "seek": 78520, "start": 789.8000000000001, "end": 799.0, "text": " Wskazywali na te oczywiste ryzyka, masowe generowanie dezinformacji z personalizowanego spamu czy fishingu, plagiaty na skal\u0119 przemys\u0142ow\u0105.", "tokens": [50594, 343, 5161, 921, 27112, 5103, 1667, 535, 277, 6522, 86, 8375, 20791, 40940, 11, 2300, 6880, 1337, 22028, 368, 23584, 837, 13152, 710, 2973, 590, 37345, 6308, 24028, 84, 6430, 10180, 84, 11, 33756, 7676, 88, 1667, 16890, 1274, 6541, 443, 39508, 30297, 13, 51054], "temperature": 0.0, "avg_logprob": -0.0960813962496244, "compression_ratio": 1.4179104477611941, "no_speech_prob": 0.02514140121638775}, {"id": 217, "seek": 78520, "start": 799.0, "end": 810.0, "text": " Wtedy w 2020 oceniali, \u017ce zagro\u017cenie nie jest jeszcze bezpo\u015brednie, bo model wci\u0105\u017c pope\u0142nia\u0142 b\u0142\u0119dy i nie by\u0142 wystarczaj\u0105co niezale\u017cny, \u017ceby oszukiwa\u0107 na du\u017c\u0105 skal\u0119.", "tokens": [51054, 343, 83, 6038, 261, 4808, 10409, 268, 831, 72, 11, 3561, 27001, 340, 41118, 2838, 3492, 14168, 10782, 2259, 1788, 986, 2766, 11, 748, 2316, 261, 537, 27242, 42248, 1221, 77, 8908, 272, 46564, 3173, 741, 2838, 16673, 4628, 9710, 3689, 11133, 1291, 33511, 45494, 1634, 11, 11316, 3003, 89, 11788, 25234, 1667, 21783, 1611, 16890, 1274, 13, 51604], "temperature": 0.0, "avg_logprob": -0.0960813962496244, "compression_ratio": 1.4179104477611941, "no_speech_prob": 0.02514140121638775}, {"id": 218, "seek": 81000, "start": 810.0, "end": 823.8, "text": " Ale jasno stwierdzili, \u017ce dalszy post\u0119p mo\u017ce to zmieni\u0107. A ten wynik 52% w te\u015bcie na odr\u00f3\u017cnianie tekst\u00f3w nazwali, jak ju\u017c m\u00f3wi\u0142y\u015bmy, niepokoj\u0105cym kamieniem milowym, kt\u00f3ry pokazuje dok\u0105d to zmierza.", "tokens": [50364, 9366, 361, 296, 1771, 342, 40717, 28168, 2312, 11, 3561, 274, 1124, 1229, 2183, 18085, 12034, 281, 17020, 1053, 12757, 13, 316, 2064, 31936, 1035, 18079, 4, 261, 535, 9815, 1667, 3611, 11721, 1427, 77, 952, 414, 16624, 372, 3901, 20151, 40054, 11, 4207, 10678, 24592, 6825, 10513, 11, 2838, 79, 13704, 8555, 1344, 76, 9727, 1053, 4907, 1962, 31691, 11, 9913, 13010, 43317, 25037, 18962, 281, 17020, 811, 2394, 13, 51054], "temperature": 0.0, "avg_logprob": -0.0733120106468516, "compression_ratio": 1.4052044609665428, "no_speech_prob": 0.05995713919401169}, {"id": 219, "seek": 81000, "start": 823.8, "end": 835.6, "text": " Ale jeszcze wi\u0119kszym problemem, bo bardziej subtelnym, okaza\u0142 si\u0119 bias. Model, ucz\u0105c si\u0119 z internetu, nauczy\u0142 si\u0119 te\u017c wszystkich naszych ludzkich uprzedze\u0144.", "tokens": [51054, 9366, 14168, 29968, 26681, 1154, 443, 11, 748, 27209, 7257, 338, 12996, 11, 3133, 12257, 1221, 3244, 12577, 13, 17105, 11, 35403, 1611, 66, 3244, 710, 4705, 84, 11, 49103, 1229, 1221, 3244, 9516, 34234, 45002, 15946, 30154, 480, 493, 81, 11312, 49689, 13, 51644], "temperature": 0.0, "avg_logprob": -0.0733120106468516, "compression_ratio": 1.4052044609665428, "no_speech_prob": 0.05995713919401169}, {"id": 220, "seek": 83560, "start": 835.6, "end": 846.9, "text": " To jest kluczowa lekcja z tej pracy. Model jest jak lustro, odbija to, co mu pokazano, a internet, na kt\u00f3rym si\u0119 uczy\u0142, no nie jest miejscem neutralnym ani sprawiedliwym.", "tokens": [50364, 1407, 3492, 9671, 1311, 89, 5528, 30863, 34056, 710, 12573, 35591, 13, 17105, 3492, 4207, 24672, 340, 11, 3611, 65, 20642, 281, 11, 598, 2992, 13010, 921, 3730, 11, 257, 4705, 11, 1667, 30120, 3244, 344, 6522, 1221, 11, 572, 2838, 3492, 38122, 76, 10598, 12996, 40477, 22734, 1091, 2081, 86, 4199, 13, 50929], "temperature": 0.0, "avg_logprob": -0.10686800453100312, "compression_ratio": 1.3317073170731708, "no_speech_prob": 0.2704044282436371}, {"id": 221, "seek": 83560, "start": 846.9, "end": 853.6, "text": " Autorzy przeprowadzili seri\u0119 test\u00f3w, \u017ceby zmierzy\u0107 to uprzedzenia i wyniki by\u0142y do\u015b\u0107 ponure.", "tokens": [50929, 6049, 284, 1229, 30829, 1892, 345, 89, 2312, 816, 5034, 1500, 3901, 11, 11316, 17020, 811, 27150, 281, 493, 81, 11312, 14320, 741, 31936, 9850, 26366, 49333, 9224, 540, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10686800453100312, "compression_ratio": 1.3317073170731708, "no_speech_prob": 0.2704044282436371}, {"id": 222, "seek": 85360, "start": 854.6, "end": 865.6, "text": " Odkryli, \u017ce model bardzo silnie skojarzy\u0142 okre\u015blone zawody z p\u0142ci\u0105. A\u017c 83% testowanych profesji mia\u0142o m\u0119skie konotacje.", "tokens": [50414, 12210, 43298, 2081, 11, 3561, 2316, 9034, 3425, 2766, 1110, 78, 10150, 1229, 1221, 3133, 265, 19212, 546, 28165, 843, 710, 28695, 537, 1611, 13, 316, 1427, 30997, 4, 1500, 23341, 339, 22912, 4013, 21290, 5249, 275, 1274, 5161, 414, 5897, 310, 29293, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11404666080269762, "compression_ratio": 1.2901554404145077, "no_speech_prob": 0.6279869079589844}, {"id": 223, "seek": 85360, "start": 865.6, "end": 873.1, "text": " Zawody wymagaj\u0105ce wy\u017cszego wykszta\u0142cenia, jak legislator, bankier, profesor, by\u0142y w jego oczach zdecydowanie m\u0119skie.", "tokens": [50964, 1176, 1607, 843, 29764, 559, 11133, 384, 4628, 1427, 15453, 6308, 4628, 1694, 89, 46426, 13037, 654, 11, 4207, 6593, 1639, 11, 3765, 811, 11, 22912, 284, 11, 26366, 261, 26542, 277, 3689, 608, 49749, 1344, 67, 22028, 275, 1274, 5161, 414, 13, 51339], "temperature": 0.0, "avg_logprob": -0.11404666080269762, "compression_ratio": 1.2901554404145077, "no_speech_prob": 0.6279869079589844}, {"id": 224, "seek": 87310, "start": 873.1, "end": 878.1, "text": " Z kolei rol\u0119, takie jak piel\u0119gniarka, recepcjonistka czy gospodzia, kobiece.", "tokens": [50364, 1176, 18303, 72, 34109, 1274, 11, 15963, 4207, 46065, 1274, 70, 3722, 809, 64, 11, 2268, 79, 45677, 468, 2330, 6430, 37250, 378, 40395, 11, 43057, 46566, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11608379962397557, "compression_ratio": 1.3487394957983194, "no_speech_prob": 0.07139600813388824}, {"id": 225, "seek": 87310, "start": 878.1, "end": 885.6, "text": " Co wi\u0119cej, model znacznie cz\u0119\u015bciej opisywa\u0142 kobiety u\u017cywaj\u0105c przymiotnik\u00f3w zwi\u0105zanych z wygl\u0105dem, jak beautiful czy gorgeous.", "tokens": [50614, 3066, 26004, 11, 2316, 15397, 14875, 2766, 18544, 9815, 73, 999, 14169, 44603, 43057, 4014, 34097, 86, 38757, 6501, 3057, 310, 47447, 27741, 34644, 710, 27947, 1611, 10730, 11, 4207, 2238, 6430, 12291, 13, 50989], "temperature": 0.0, "avg_logprob": -0.11608379962397557, "compression_ratio": 1.3487394957983194, "no_speech_prob": 0.07139600813388824}, {"id": 226, "seek": 87310, "start": 885.6, "end": 893.1, "text": " Czyli odtworzy\u0142 klasyczne stereotypy. A co z ras\u0105? Tu podobno wyniki by\u0142y jeszcze bardziej jednoznaczne.", "tokens": [50989, 37099, 3611, 20270, 284, 1229, 1221, 9671, 5871, 38491, 41182, 8200, 13, 316, 598, 710, 26815, 1611, 30, 7836, 43024, 1771, 31936, 9850, 26366, 14168, 27209, 5232, 1771, 22672, 14875, 716, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11608379962397557, "compression_ratio": 1.3487394957983194, "no_speech_prob": 0.07139600813388824}, {"id": 227, "seek": 89310, "start": 893.1, "end": 904.1, "text": " Tak, tutaj przeprowadzili analiz\u0119 sentymentu. Sprawdzali, jakie nacechowanie emocjonalne maj\u0105 teksty generowane przez model w kontek\u015bcie r\u00f3\u017cnych grup rasowych.", "tokens": [50364, 9118, 11, 12749, 30829, 1892, 345, 89, 2312, 2624, 590, 1274, 2279, 88, 518, 84, 13, 1738, 15889, 89, 5103, 11, 22124, 297, 617, 339, 22028, 28283, 15735, 304, 716, 26064, 16624, 25134, 1337, 23066, 14064, 2316, 261, 14373, 916, 9815, 42602, 12740, 26815, 19605, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0702952880859375, "compression_ratio": 1.3962962962962964, "no_speech_prob": 0.056812431663274765}, {"id": 228, "seek": 89310, "start": 904.1, "end": 914.1, "text": " I jeden z wykres\u00f3w, figure 7.1, pokazuje to bardzo wyra\u017anie. Tre\u015bci powi\u0105zane z ras\u0105 azjatyczk\u0105 mia\u0142y konsekwentnie pozytywny wyd\u017awi\u0119k.", "tokens": [50914, 286, 12906, 710, 39287, 495, 3901, 11, 2573, 1614, 13, 16, 11, 13010, 43317, 281, 9034, 4628, 424, 10659, 2766, 13, 8648, 6199, 3388, 11404, 89, 1929, 710, 26815, 1611, 7883, 73, 21398, 3689, 26304, 21290, 6825, 47020, 74, 34798, 2766, 49358, 874, 43682, 25984, 10659, 22423, 74, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0702952880859375, "compression_ratio": 1.3962962962962964, "no_speech_prob": 0.056812431663274765}, {"id": 229, "seek": 89310, "start": 914.1, "end": 919.1, "text": " Z kolei te dotycz\u0105ce os\u00f3b czarnosku\u0142ych konsekwentnie negatywny.", "tokens": [51414, 1176, 18303, 72, 535, 5893, 17466, 1611, 384, 32089, 6472, 1083, 329, 5279, 47655, 47020, 74, 34798, 2766, 2485, 21398, 43682, 13, 51664], "temperature": 0.0, "avg_logprob": -0.0702952880859375, "compression_ratio": 1.3962962962962964, "no_speech_prob": 0.056812431663274765}, {"id": 230, "seek": 91910, "start": 919.1, "end": 927.1, "text": " To nie jest tak, \u017ce model ma pogl\u0105dy. On po prostu statystycznie odtwarza wzorce, kt\u00f3re znalaz\u0142 w miliardach tekst\u00f3w z internetu.", "tokens": [50364, 1407, 2838, 3492, 991, 11, 3561, 2316, 463, 32037, 75, 1611, 3173, 13, 1282, 714, 19518, 2219, 38593, 17466, 2766, 3611, 83, 6925, 2394, 24809, 284, 384, 11, 8864, 710, 4660, 921, 1221, 261, 1962, 72, 515, 608, 16624, 372, 3901, 710, 4705, 84, 13, 50764], "temperature": 0.0, "avg_logprob": -0.06423149790082659, "compression_ratio": 1.4421768707482994, "no_speech_prob": 0.17403629422187805}, {"id": 231, "seek": 91910, "start": 927.1, "end": 929.1, "text": " I na koniec religia.", "tokens": [50764, 286, 1667, 5897, 35733, 4039, 654, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06423149790082659, "compression_ratio": 1.4421768707482994, "no_speech_prob": 0.17403629422187805}, {"id": 232, "seek": 91910, "start": 929.1, "end": 939.1, "text": " Tutaj by\u0142o podobnie. S\u0142owa takie jak terrorism czy violent pojawia\u0142y si\u0119 ze znacznie wi\u0119ksz\u0105 cz\u0119stotliwo\u015bci\u0105 w kontek\u015bcie Islamu ni\u017c chrze\u015bcija\u0144stwa, budyzmu czy judaizmu.", "tokens": [50864, 41819, 14811, 43024, 2766, 13, 318, 1221, 5528, 15963, 4207, 23917, 6430, 11867, 30655, 654, 6825, 3244, 5277, 15397, 14875, 2766, 29968, 8925, 18544, 372, 310, 2081, 36476, 1611, 261, 14373, 916, 9815, 8571, 84, 28502, 417, 13503, 6199, 2938, 12229, 4151, 11, 3265, 37433, 20140, 6430, 3747, 64, 590, 20140, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06423149790082659, "compression_ratio": 1.4421768707482994, "no_speech_prob": 0.17403629422187805}, {"id": 233, "seek": 91910, "start": 939.1, "end": 944.1, "text": " To znowu jest odbicie tego, jak te tematy s\u0105 reprezentowane w danych treningowych.", "tokens": [51364, 1407, 710, 3785, 84, 3492, 3611, 65, 28434, 8627, 11, 4207, 535, 1383, 21398, 9015, 1085, 265, 14185, 23066, 261, 274, 34644, 2192, 773, 19605, 13, 51614], "temperature": 0.0, "avg_logprob": -0.06423149790082659, "compression_ratio": 1.4421768707482994, "no_speech_prob": 0.17403629422187805}, {"id": 234, "seek": 94410, "start": 944.1, "end": 950.1, "text": " Model jest po prostu niezwykle skutecznym na\u015bladowc\u0105 ze wszystkimi tego konsekwencjami.", "tokens": [50364, 17105, 3492, 714, 19518, 33511, 9726, 14677, 1110, 1169, 3689, 12996, 1667, 1788, 9290, 305, 32557, 5277, 14615, 10121, 8627, 47020, 74, 15615, 66, 73, 4526, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08723610166519408, "compression_ratio": 1.3537906137184115, "no_speech_prob": 0.1910323053598404}, {"id": 235, "seek": 94410, "start": 950.1, "end": 953.1, "text": " Podsumowuj\u0105c, dok\u0105d nas to wszystko prowadzi?", "tokens": [50664, 12646, 82, 449, 305, 44733, 11, 25037, 18962, 5382, 281, 22607, 36590, 3992, 30, 50814], "temperature": 0.0, "avg_logprob": -0.08723610166519408, "compression_ratio": 1.3537906137184115, "no_speech_prob": 0.1910323053598404}, {"id": 236, "seek": 94410, "start": 953.1, "end": 963.1, "text": " Z jednej strony mamy prze\u0142om. GPT-3 pokaza\u0142o, \u017ce sama skala w po\u0142\u0105czeniu z nowym podej\u015bciem in-context learning odblokowuje zdolno\u015bci,", "tokens": [50814, 1176, 5232, 11794, 32406, 17335, 8325, 1221, 298, 13, 26039, 51, 12, 18, 13010, 12257, 5249, 11, 3561, 17768, 1110, 5159, 261, 714, 15926, 66, 39651, 710, 586, 4199, 7468, 73, 9815, 76, 294, 12, 9000, 3828, 2539, 3611, 5199, 453, 305, 13008, 16221, 401, 16438, 11, 51314], "temperature": 0.0, "avg_logprob": -0.08723610166519408, "compression_ratio": 1.3537906137184115, "no_speech_prob": 0.1910323053598404}, {"id": 237, "seek": 94410, "start": 963.1, "end": 969.1, "text": " kt\u00f3re wcze\u015bniej wydawa\u0142y si\u0119 domen\u0105 wyspecjalizowanych modeli. To jest zmiana paradygmatu.", "tokens": [51314, 8864, 40785, 25984, 10449, 6825, 3244, 3285, 268, 1611, 27062, 494, 66, 22600, 590, 23341, 339, 2316, 72, 13, 1407, 3492, 17020, 8497, 13480, 18103, 15677, 84, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08723610166519408, "compression_ratio": 1.3537906137184115, "no_speech_prob": 0.1910323053598404}, {"id": 238, "seek": 96910, "start": 970.1, "end": 978.1, "text": " I co wi\u0119cej, ta praca udowodni\u0142a, \u017ce ogromne modele nie s\u0105 tylko ilo\u015bciowo leksze. One staj\u0105 si\u0119 jako\u015bciowo inne.", "tokens": [50414, 286, 598, 26004, 11, 1846, 582, 6628, 11727, 305, 378, 3722, 5024, 11, 3561, 34416, 298, 716, 4391, 306, 2838, 9015, 13219, 1930, 44468, 19941, 476, 1694, 1381, 13, 1485, 342, 11133, 3244, 17123, 6199, 19941, 24170, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07267858042861476, "compression_ratio": 1.5078125, "no_speech_prob": 0.20549540221691132}, {"id": 239, "seek": 96910, "start": 978.1, "end": 982.1, "text": " Zaczynaj\u0105 dzia\u0142a\u0107 jak meta-learners, ucz\u0105 si\u0119 jak si\u0119 uczy\u0107.", "tokens": [50814, 1176, 14691, 629, 8555, 37903, 2162, 4207, 19616, 12, 306, 1083, 433, 11, 35403, 1611, 3244, 4207, 3244, 344, 33967, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07267858042861476, "compression_ratio": 1.5078125, "no_speech_prob": 0.20549540221691132}, {"id": 240, "seek": 96910, "start": 982.1, "end": 990.1, "text": " Ale jednocze\u015bnie jak lustro o ogromnej mocy powi\u0119kszaj\u0105cej pokaza\u0142a, \u017ce skala wzmaznia te\u017c istniej\u0105ce problemy.", "tokens": [51014, 9366, 5232, 26694, 1381, 12221, 4207, 24672, 340, 277, 34416, 298, 11794, 705, 1344, 3388, 5034, 1694, 89, 11133, 20811, 13010, 12257, 5024, 11, 3561, 1110, 5159, 24809, 21283, 12679, 9516, 1418, 2766, 8555, 384, 1154, 88, 13, 51414], "temperature": 0.0, "avg_logprob": -0.07267858042861476, "compression_ratio": 1.5078125, "no_speech_prob": 0.20549540221691132}, {"id": 241, "seek": 96910, "start": 990.1, "end": 995.1, "text": " Uprzedzenia staj\u0105 si\u0119 ostrzejsze, a potencjalne nadu\u017cycia bardziej realne.", "tokens": [51414, 624, 1424, 11312, 14320, 342, 11133, 3244, 44024, 16920, 82, 1381, 11, 257, 1847, 22660, 22600, 716, 12617, 84, 7735, 2755, 27209, 957, 716, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07267858042861476, "compression_ratio": 1.5078125, "no_speech_prob": 0.20549540221691132}, {"id": 242, "seek": 99510, "start": 995.1, "end": 1003.1, "text": " Te kwestie, podniesione tak wyra\u017anie w 2020 roku, s\u0105 dzi\u015b w samym centrum debaty o odpowiedzialnym rozwoju AI.", "tokens": [50364, 1989, 42035, 414, 11, 2497, 40549, 5328, 991, 4628, 424, 10659, 2766, 261, 4808, 19451, 11, 9015, 31981, 1788, 261, 3247, 4199, 1489, 6247, 3001, 21398, 277, 24314, 15338, 831, 12996, 9544, 6120, 8954, 7318, 13, 50764], "temperature": 0.0, "avg_logprob": -0.054280943789724576, "compression_ratio": 1.3820224719101124, "no_speech_prob": 0.04138452559709549}, {"id": 243, "seek": 99510, "start": 1003.1, "end": 1008.1, "text": " Autorzy ko\u0144cz\u0105 swoj\u0105 prac\u0119 bardzo ciekaw\u0105 refleksj\u0105 na przysz\u0142o\u015b\u0107.", "tokens": [50764, 6049, 284, 1229, 26470, 3689, 1611, 49194, 22404, 1274, 9034, 46419, 1607, 1611, 36549, 1694, 8555, 1667, 44018, 44742, 13, 51014], "temperature": 0.0, "avg_logprob": -0.054280943789724576, "compression_ratio": 1.3820224719101124, "no_speech_prob": 0.04138452559709549}, {"id": 244, "seek": 99510, "start": 1008.1, "end": 1017.1, "text": " Stwierdzaj\u0105, \u017ce samo przewidywanie kolejnego s\u0142owa nawet na niewyobra\u017caln\u0105 skal\u0119 prawdopodobnie kiedy\u015b dojdzie do \u015bciany.", "tokens": [51014, 745, 40717, 28168, 11133, 11, 3561, 36422, 39758, 327, 27112, 7155, 23749, 11858, 15116, 5528, 22696, 1667, 43622, 88, 24393, 1427, 304, 13113, 16890, 1274, 41175, 46684, 996, 2766, 18777, 1788, 360, 73, 13096, 360, 220, 6199, 1325, 13, 51464], "temperature": 0.0, "avg_logprob": -0.054280943789724576, "compression_ratio": 1.3820224719101124, "no_speech_prob": 0.04138452559709549}, {"id": 245, "seek": 99510, "start": 1017.1, "end": 1020.1, "text": " \u017be b\u0119dzie to trzeba uzupe\u0142ni\u0107 o co\u015b wi\u0119cej.", "tokens": [51464, 46864, 10562, 281, 25860, 344, 11728, 31457, 3722, 2162, 277, 19241, 26004, 13, 51614], "temperature": 0.0, "avg_logprob": -0.054280943789724576, "compression_ratio": 1.3820224719101124, "no_speech_prob": 0.04138452559709549}, {"id": 246, "seek": 102010, "start": 1020.1, "end": 1025.1, "text": " Tak, sugeruj\u0105, \u017ce przysz\u0142o\u015b\u0107 mo\u017ce le\u017cy\u0107 w \u0142\u0105czeniu tego podej\u015bcia z innymi.", "tokens": [50364, 9118, 11, 459, 1321, 13263, 11, 3561, 44018, 44742, 12034, 476, 39687, 261, 220, 15926, 66, 39651, 8627, 7468, 73, 1788, 2755, 710, 294, 31813, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08123219999155604, "compression_ratio": 1.4089219330855018, "no_speech_prob": 0.23655302822589874}, {"id": 247, "seek": 102010, "start": 1025.1, "end": 1034.1, "text": " Na przyk\u0142ad z uczeniem si\u0119 bezpo\u015brednio z ludzkich opinii albo z dodaniem innych zmys\u0142\u00f3w, modalno\u015bci, takich jak obrazy, d\u017awi\u0119k czy wideo.", "tokens": [50614, 6056, 23144, 710, 344, 66, 2904, 4907, 3244, 10782, 2259, 1788, 986, 41084, 710, 15946, 30154, 480, 46784, 72, 22622, 710, 13886, 282, 4907, 36286, 17020, 39508, 3901, 11, 1072, 304, 16438, 11, 29607, 4207, 22798, 1229, 11, 274, 10659, 22423, 74, 6430, 4874, 78, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08123219999155604, "compression_ratio": 1.4089219330855018, "no_speech_prob": 0.23655302822589874}, {"id": 248, "seek": 102010, "start": 1034.1, "end": 1038.1, "text": " I to prowadzi nas do ostatniej prowokacyjnej my\u015bli na koniec.", "tokens": [51064, 286, 281, 36590, 3992, 5382, 360, 32686, 10402, 45553, 453, 31285, 11794, 452, 15350, 1667, 5897, 35733, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08123219999155604, "compression_ratio": 1.4089219330855018, "no_speech_prob": 0.23655302822589874}, {"id": 249, "seek": 102010, "start": 1038.1, "end": 1040.1, "text": " Zatem co to za my\u015bl?", "tokens": [51264, 1176, 26851, 598, 281, 7949, 452, 19212, 30, 51364], "temperature": 0.0, "avg_logprob": -0.08123219999155604, "compression_ratio": 1.4089219330855018, "no_speech_prob": 0.23655302822589874}, {"id": 250, "seek": 102010, "start": 1040.1, "end": 1044.1, "text": " Urd\u00f3w w teleturniejach, popisanie wiarygodnych artyku\u0142\u00f3w.", "tokens": [51364, 9533, 67, 3901, 261, 15284, 302, 925, 7764, 608, 11, 1665, 271, 7155, 26393, 822, 21787, 9399, 594, 874, 5279, 1221, 3901, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08123219999155604, "compression_ratio": 1.4089219330855018, "no_speech_prob": 0.23655302822589874}, {"id": 251, "seek": 104410, "start": 1044.1, "end": 1054.1, "text": " To jakie zupe\u0142nie nowe, dzi\u015b niewyobra\u017calne mo\u017cliwo\u015bci zostan\u0105 odblokowane, gdy takie systemy zostan\u0105 wreszcie osadzone w bogatszym \u015bwiecie.", "tokens": [50364, 1407, 22124, 49922, 586, 68, 11, 31981, 1788, 43622, 88, 24393, 1427, 304, 716, 30854, 36476, 31873, 282, 1611, 3611, 5199, 453, 23066, 11, 28405, 15963, 1185, 88, 31873, 282, 1611, 261, 495, 89, 4260, 3003, 345, 16896, 261, 26132, 267, 7706, 76, 40078, 4260, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08335362802637686, "compression_ratio": 1.4514563106796117, "no_speech_prob": 0.4693269431591034}, {"id": 252, "seek": 104410, "start": 1054.1, "end": 1058.1, "text": " W \u015bwiecie obrazu, d\u017awi\u0119ku i fizycznej interakcji.", "tokens": [50864, 343, 40078, 4260, 22798, 11728, 11, 274, 10659, 22423, 5279, 741, 21000, 17466, 11794, 728, 514, 19649, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08335362802637686, "compression_ratio": 1.4514563106796117, "no_speech_prob": 0.4693269431591034}, {"id": 253, "seek": 104410, "start": 1058.1, "end": 1064.1, "text": " I jakie zupe\u0142nie nowe, nieznane nam jeszcze uprzedzenia wnios\u0105 ze sob\u0105 te nowe rodzaje danych.", "tokens": [51064, 286, 22124, 49922, 586, 68, 11, 2838, 22672, 1929, 8835, 14168, 493, 81, 11312, 14320, 261, 77, 2717, 1611, 5277, 18253, 1611, 535, 586, 68, 28607, 11153, 274, 34644, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08335362802637686, "compression_ratio": 1.4514563106796117, "no_speech_prob": 0.4693269431591034}], "language": "pl"}