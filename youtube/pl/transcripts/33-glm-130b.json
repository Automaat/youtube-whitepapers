{"text": " Co by by\u0142o, gdyby pot\u0119\u017cny model j\u0119zykowy, przeweszczaj\u0105cy w wielu zadaniach s\u0142ynny GPT-3, mog\u0142oby dzia\u0142a\u0107 na sprz\u0119cie dost\u0119pnym dla mniejszych firm czy uniwersytet\u00f3w, a nie tylko, wiesz, dla technologicznych gigant\u00f3w. Wydaje si\u0119 to sprzeczne z ca\u0142\u0105 logik\u0105 tego wy\u015bcigu AI, gdzie rewi\u0119kszy niemal zawsze znaczy lepszy i, no c\u00f3\u017c, dro\u017cszy. Dzisiaj zajmiemy si\u0119 prac\u0105 naukow\u0105, kt\u00f3ra, mam wra\u017cenie, rzuca wyzwanie tej logice. B\u0119dziemy bazowa\u0107 na publikacji GLM 130B, an open bilingual pre-trained model od naukowc\u00f3w Tsinghua University i ZIPU AI. A nasza misja jest prosta. Zrozumie\u0107, co tak naprawd\u0119 czyni ten model wyj\u0105tkowy. I od razu powiem, \u017ce nie chodzi tu tylko o wydajno\u015b\u0107, chodzi o architektur\u0119, o fascynuj\u0105c\u0105 opowie\u015b\u0107 in\u017cynierin\u0105, kt\u00f3ra jest pe\u0142na pora\u017cek i prze\u0142om\u00f3w i wreszcie o realn\u0105 pr\u00f3b\u0119 demokratyzacji dost\u0119pu do najpot\u0119\u017cniejszych narz\u0119dzi AI. Dobra, to rozpakujmy to. Na pierwszy rzut oka GLM 130B to po prostu kolejny wielki model. Dwuj\u0119zyczny angielsko-chi\u0144ski ze 130 miliardami parametr\u00f3w. Ale ju\u017c sama jego nazwa, GLM, czyli General Language Model, sugaruje, \u017ce pod mask\u0105 kryje si\u0119 co\u015b innego ni\u017c w standardowych modelach typu GPT. I to jest absolutny klucz do wszystkiego. Tw\u00f3rcy nie poszli na \u0142atwizn\u0119, nie skopiowali po prostu sprawdzonej architektury. Zbudowali go na fundamentzie GLM, kt\u00f3ry dzia\u0142a no fundamentalnie inaczej. Modele GPT s\u0105 czysto autoregresyw, co znaczy, \u017ce dzia\u0142aj\u0105 jednokierunkowo. Czytaj\u0105 tekst odle\u0142ej do prawej i staraj\u0105 si\u0119 przewidzie\u0107, jakie b\u0119dzie nast\u0119pne s\u0142owo. To jest \u015bwietne do generowania tekstu, ale niekoniecznie do g\u0142\u0119bokiego rozumienia kontekstu. A GLM stosuje co\u015b, co w pracy nazywa si\u0119 autoregressive blank in filling. Czyli je\u015bli dobrze to rozumiem, to jest fundamentalna zmiana perspektywy. GPT jest jak taki gaw\u0119dziarz, kt\u00f3ry snuje opowie\u015b\u0107, dodaj\u0105c kolejne s\u0142owa. A GLM jest bardziej jak detektyw albo archeolog, kt\u00f3ry dostaje tekst z dziurami i musi je uzupe\u0142ni\u0107, analizuj\u0105c kontekst z obu stron tego, co by\u0142o przed luk\u0105 i tego, co jest po niej. To jest doskona\u0142a analogia. I co wi\u0119cej, model robi to w bardzo sprypny, hybrydowy spos\u00f3b. W pracy opisuj\u0105 dwie strategie maskowania. Pierwsza z u\u017cyciem tokenu MSK s\u0142u\u017cy do wype\u0142niania kr\u00f3tkich, losowych luk w \u015brodku tekstu. To bardzo przypomina dzia\u0142anie modeli z rodziny BERT, kt\u00f3re s\u0105 mistrzami w zadaniach wymagaj\u0105cych rozumienia semantyki zdania. A druga? Druga strategia z u\u017cyciem tokenu GMSK polega na zamaskowaniu ca\u0142ego ko\u0144ca tekstu, co zmusza model do generowania d\u0142ugich, sp\u00f3jnych fragment\u00f3w. A to z kolei jest domen\u0105 GPT. Czyli po\u0142\u0105czyli to, co najlepsze z obu \u015bwiat\u00f3w. Z jednej strony zdolno\u015b\u0107 BERT-a do g\u0142\u0119bokiego rozumienia kontekstu, a z drugiej zdolno\u015b\u0107 GPT do generowania p\u0142ynnego, d\u0142ugiego tekstu. Ta dwukierunkowo\u015b\u0107 musia\u0142a by\u0107 kluczowa dla jego p\u00f3\u017aniejszych wynik\u00f3w. Dok\u0142adnie. Ta hybrydowa natura daje mu elastyczno\u015b\u0107, kt\u00f3rej brakuje modelom jednotierunkowym. Mo\u017ce jednocze\u015bnie rozumie\u0107 i generowa\u0107 z r\u00f3wn\u0105 bieg\u0142o\u015bci\u0105, co jak zaraz zobaczymy, przek\u0142ada si\u0119 na jego niezwyk\u0142\u0105 wydajno\u015b\u0107. Ale zanim dojdziemy do wynik\u00f3w, musimy porozmawia\u0107 o tym, jak trudne by\u0142o w og\u00f3le doprowadzenie tego modelu do mety. O tak, to jest cz\u0119\u015b\u0107 pracy, kt\u00f3ra naprawd\u0119 mnie uderzy\u0142a. Autorzy s\u0105 niezwykle szczerzy. Przyznaj\u0105 si\u0119 do ponad 30 kompletnie nieudanych pr\u00f3b treningu. Pisz\u0105 o ci\u0105g\u0142ych problemach z tak zwanymi los spikes. Na g\u0142ymi, gwa\u0142townymi skokami b\u0142\u0119d\u00f3w, kt\u00f3re w jednej chwili potrafi\u0142y zniweczy\u0107 tygodnie oblicze\u0144 na superkomputerze. To jest ta brutalna rzeczywisto\u015b\u0107, wiesz, o kt\u00f3rej rzadko si\u0119 czyta w tych wszystkich wypolonorowanych komunikatach prasowych. No w\u0142a\u015bnie i to nie jest odosobniony problem. W zasadzie ka\u017cdy zesp\u00f3\u0142 buduj\u0105cy modele tej skali boryka si\u0119 z niestabilno\u015bci\u0105. Zesp\u00f3\u0142 meta tworz\u0105c OP-T175B musia\u0142 r\u0119cznie interweniowa\u0107 w trakcie treningu, restartowa\u0107 go i dostosowywa\u0107 parametry. To troch\u0119 jak \u0142atanie dziur w kad\u0142ubie p\u0142yn\u0105cego transatlantyku. Dok\u0142adnie. A z kolei tw\u00f3rcy Bloom 176B zastosowali technik\u0119, kt\u00f3r\u0105 nazwali embedding norm, ale sami przyznali, \u017ce zap\u0142acili za to ni\u017csz\u0105 wydajno\u015bci\u0105 ko\u0144cowego modelu. To by\u0142 kompromis, stabilno\u015b\u0107 za cen\u0119 jako\u015bci. A zesp\u00f3\u0142 DLM znalaz\u0142 inne rozwi\u0105zanie. W pracy opisuj\u0105 dwie kluczowe innowacje. Pierwsza to co\u015b, co nazywaj\u0105 deep norm. To jest wariant normalizacji warstw, tak zwany post-LN. I tu musz\u0119 zapyta\u0107, bo wydawa\u0142o mi si\u0119, \u017ce ca\u0142y \u015bwiat EI przeszed\u0142 na pre-LN. Czyli normalizacj\u0119 przed g\u0142\u00f3wn\u0105 operacj\u0105 w warstwie transformer. W\u0142a\u015bnie dlatego, \u017ce mia\u0142a by\u0107 stabilniejsza, a oni poszli pod pr\u0105d. To jest \u015bwietna obserwacja i pokazuje, jak nieszablonowo my\u015bleli. Rzeczywi\u015bcie panowa\u0142a moda na pre-LN. Ale oni udowodnili, \u017ce odpowiednio skalibrowana starsza metoda post-LN mo\u017ce by\u0107 nie tylko r\u00f3wnie stabilna, ale i wydajniejsza. To jednak druga technika okaza\u0142a si\u0119 prawdziwym, ale to prawdziwym prze\u0142omem. Embedding Layer Gradient Shrink w skr\u00f3cie EGS. Dobrze, to musimy to rozpakowa\u0107. Zauwa\u017cyli, \u017ce g\u0142\u00f3wnym \u017ar\u00f3d\u0142em tych eksplozji w treningu s\u0105 nienormalne gradienty. Uproszczaj\u0105c, gradienty to takie sygna\u0142y b\u0142\u0119du, kt\u00f3re m\u00f3wi\u0105 modelowi, jak ma si\u0119 poprawi\u0107. W tym przypadku te sygna\u0142y by\u0142y tak gwa\u0142towne i chaotyczne, \u017ce zamiast korygowa\u0107 kurs modelu, po prostu go wykoleja\u0142y. I co ciekawe, dzia\u0142o si\u0119 to g\u0142\u00f3wnie w tej pierwszej wej\u015bciowej warstwie modelu, czyli w Embedding Layer. Tak, to to warstwa aktowa zamienia s\u0142owa na wektory liczbowe. Jest absolutnie kluczowa i musi si\u0119 uczy\u0107 razem z reszt\u0105 modelu, wi\u0119c nie mo\u017cna jej po prostu zamrozi\u0107. I tu dochodzimy do sedna EGS. Zamiast walczy\u0107 z tymi gradientami na r\u00f3\u017cne skomplikowane sposoby, postanowi\u0107 zrobi\u0107 co\u015b radykalnie prostego. Chwila, czyli oni po prostu zmniejszyli si\u0142\u0119 tych sygna\u0142\u00f3w o 90%. W pracy podaj\u0105 wsp\u00f3\u0142czynnik alpha r\u00f3wny 0,1. To brzmi prawie zbyt prosto, \u017ceby by\u0142o prawdziwe. Czy to nie os\u0142abi\u0142o zdolno\u015bci uczenia si\u0119 tej pierwszej kluczowej warstwy? To jest doskona\u0142y pytanie i w\u0142a\u015bnie tutkwigeniusz tego rozwi\u0105zania. Okazuje si\u0119, \u017ce te gradienty by\u0142y tak przesterowane, \u017ce nawet po \u015bciszeniu ich o 90% wci\u0105\u017c by\u0142y wystarczaj\u0105co silne, by efektywnie uczy\u0107 warstw\u0119. Ale ju\u017c nie na tyle pot\u0119\u017cne, by wysadzi\u0107 w powietrze ca\u0142y trening. Znale\u017ali z\u0142oty \u015brodek. Niesamowite. A i to jest w\u0142a\u015bnie ten rodzaj wiedzy, kt\u00f3ry rzadko przebija si\u0119 do medi\u00f3w. Wszyscy m\u00f3wi\u0105 o miliardach parametr\u00f3w terabajtach danych. A tu kluczem do sukcesu okaza\u0142o si\u0119 jedno, niemal chirurgiczne ci\u0119cie w matematyce modelu. To pokazuje, \u017ce era brutalnego skalowania mo\u017ce powoli ust\u0119powa\u0107 miejsca erze sprytnej in\u017cynierii. Niesamowite, \u017ce taki prosty trick rozwi\u0105za\u0142 problem, z kt\u00f3rym borykali si\u0119 najwi\u0119kszy gracze na \u015bwiecie. Ale ca\u0142a ta in\u017cynierina ekwilibrystyka by\u0142aby na nic, gdyby model na ko\u0144cu okaza\u0142 si\u0119 przeci\u0119tny. Czy to nowa architektura i wymuszona stabilno\u015b\u0107 prze\u0142o\u017cy\u0142y si\u0119 na realn\u0105 przewag\u0119 nad innymi modelami? I to jak? Wyniki s\u0105 naprawd\u0119 imponuj\u0105ce. GLM 130B mimo, \u017ce jest mniejszy, w wielu kluczowych benchmarkach deklasuje GPT-3 175B. Na przyk\u0142ad w te\u015bcie Lambada, kt\u00f3ry mierzy zdolno\u015b\u0107 modelu do przewidzenia ostatniego s\u0142owa w d\u0142ugim, skomplikowanym akapicie, osi\u0105gn\u0105\u0142 dok\u0142adno\u015b\u0107 83,2%. Co by\u0142o wtedy rekordem? A, w momencie publikacji pracy, to by\u0142 rekord. Ale to nie wszystko. Z tego, co czyta\u0142am, poradzi\u0142 sobie te\u017c z du\u017co wi\u0119kszymi przeciwnikami. Zdecydowanie. W zadaniach typu Zero Shot na zbiorze Big Bench Lite. Czyli tam, gdzie model musi rozwi\u0105za\u0107 problem bez \u017cadnych wcze\u015bniejszych przyk\u0142ad\u00f3w. Dok\u0142adnie. Okaza\u0142 si\u0119 lepszy od czterokrotnie wi\u0119kszego modelu Palm 540B od Google. A jako, \u017ce jest to model dwuj\u0119zyczny, to warto doda\u0107, \u017ce znacz\u0105co przewy\u017cszy\u0142 te\u017c najwi\u0119kszy do tej porychi\u0144ski model, Erni Tytan 3.0, kt\u00f3ry ma przecie\u017c dwa razy wi\u0119cej parametr\u00f3w. 260 miliard\u00f3w? Tak. Czyli jest wydajniejszy, inteligentniejszy. Ale w pracy jest jeszcze jeden wynik, kt\u00f3ry wydaje si\u0119 niemal przypadkowym, ale potencjalnie rewolucyjnym odkryciem. Chodzi o analizy etyczne. Ejtorzy sprawdzili model pod ko\u0144cem generowania tre\u015bci toksycznych oraz powielania stereotyp\u00f3w, czyli tak zwanego bias. I wyniki by\u0142y zaskakuj\u0105ce. Okaza\u0142o si\u0119, \u017ce GLM 130B generuje tre\u015bci o znacznie mniejszej toksyczno\u015bci i wykazuje mniej uprzedze\u0144 ni\u017c jego odpowiednicy trenowane wy\u0142\u0105cznie na danych angloj\u0119zycznych. Jak np. GPT-3 czy OPT-175B. Dok\u0142adnie tak. Zatrzymajmy si\u0119 przy tym na chwil\u0119. To jest potencjalnie ogromne odkrycie. To sugeruje, \u017ce problem toksycznej AI mo\u017ce nie by\u0107 tylko kwesti\u0105 lepszego filtrowania danych treningowych, ale fundamentalnie kwesti\u0105 perspektywy. \u017be model, kt\u00f3ry od samego pocz\u0105tku uczy si\u0119 widzie\u0107 \u015bwiat przez pryzmat wi\u0119cej ni\u017c jednej kultury i j\u0119zyka, staje si\u0119 niejako bardziej obyty i mniej podatny na skrajno\u015bci obecne w pojedynczym \u017ar\u00f3dze danych. To jest bardzo trafna interpretacja. Dwuj\u0119zyczno\u015b\u0107 nie tylko nie os\u0142abi\u0142a modelu, ale mog\u0142a go wr\u0119cz uczyni\u0107 z drobszym i bardziej zr\u00f3wnowa\u017conym. Mo\u017cliwe, \u017ce model ucz\u0105c si\u0119 na danych z dw\u00f3ch r\u00f3\u017cnych kultur w pewien spos\u00f3b u\u015bredni\u0142 kulturowe uprzedzenia obecne w obu zbiorach, co doprowadzi\u0142o do bardziej neutralnych odpowiedzi. To fascynuj\u0105cy kierunek bada\u0144 na przysz\u0142o\u015b\u0107. Mamy wi\u0119c model, kt\u00f3ry jest pot\u0119\u017cniejszy, stabilniejszy w treningu i potencjalnie bardziej etyczny. Ale tytu\u0142 pracy wspomina o tym, \u017ce jest to model otwarty. W \u015bwiecie wielkich modeli j\u0119zykowych otwarty zazwyczaj oznacza, \u017ce mo\u017cna sobie co najwy\u017cej poczyta\u0107 prac\u0119 naukow\u0105. Tutaj wydaje si\u0119, \u017ce to s\u0142owo ma znacznie g\u0142\u0119bsze znaczenie. A przesolutnie. I tu dochodzimy do prawdziwej rewolucji, jak\u0105 niesie ze sob\u0105 GGLM 130B. Chodzi o kwantyzacj\u0119 do INT4. Wi\u0119kszo\u015b\u0107 du\u017cych modeli z trudem daje si\u0119 skompresowa\u0107 do 8-bitowej precyzji, czyli INT8, a i to cz\u0119sto wi\u0105\u017ce si\u0119 z pewn\u0105 utrat\u0105 jako\u015bci. A dalsza kompresja to ju\u017c w og\u00f3le katastrofa. Zazwyczaj tak. Tymczasem GGLM 130B dzi\u0119ki swojej unikalnej architekturze mo\u017ce by\u0107 skwantyzowany do 4-bitowej precyzji praktycznie bez \u017cadnej straty na jako\u015bci. W jednym z benchmark\u00f3w MLLu jego wynik po kwantyzacji nawet nieznacznie wzrus\u0142. To jest niewiarygowne. Ale dlaczego tak si\u0119 dzieje? Co jest w tej architekturze takiego, \u017ce pozwala na tak ekstremaln\u0105 kompresj\u0119? W pracy jest fascynuj\u0105cy wykres oznaczony jako rysunek 5. Pokazuje on rozk\u0142ad wak, czyli wewn\u0119trznych parametr\u00f3w modelu, w GGLM w por\u00f3wnaniu do modelu w stylu GPT, jakim jest blum. I jaka jest r\u00f3\u017cnica? Uderzaj\u0105ca. GGLM ma znacznie w\u0119\u017cszy rozk\u0142ad warto\u015bci. Innymi s\u0142owy jest w nim o wiele mniej ekstremalnych odstaj\u0105cych warto\u015bci, kt\u00f3re s\u0105 z mor\u0105 procesu kwantyzacji. Aha, czyli mniej ekstrem\u00f3w oznacza, \u017ce proces zaokr\u0105glania wak do mniejszej ligdy bit\u00f3w jest o wiele mniej stratny. Dok\u0142adnie. To wygl\u0105da na unikaln\u0105, wy\u0142aniaj\u0105c\u0105 si\u0119 w\u0142a\u015bciwo\u015b\u0107 architektury GGLM, kt\u00f3rej nie zaobserwowano w innych modelach na tak\u0105 skal\u0119. A przek\u0142adaj\u0105c to na praktyk\u0119, co to oznacza dla badacza, czy ma\u0142ej firmy? Oznacza to wszystko. Zamiast klastra superkomputer\u00f3w z drogimi akceleratorami skompresowany do Int4 GGLM 130B do inferencji, czyli do u\u017cywania go. Mhm, potrzebuje czego? Potrzebuje serwera z zaledwie czterema kartami graficznymi RTX 3090. Mo\u017cna go te\u017c uruchomi\u0107 na 8 nieco starszych kartach RTX 2080T. To jest sprz\u0119t, kt\u00f3ry jest w zasi\u0119gu setek laboratori\u00f3w badawczych i tysi\u0119cy mniejszych firm na ca\u0142ym \u015bwiecie. To jest w\u0142a\u015bnie ta prawdziwa demokratyzacja dost\u0119pu, o kt\u00f3rej m\u00f3wi\u0142y\u015bmy na pocz\u0105tku. To przej\u015bcie od narz\u0119dzia dost\u0119pnego dla trzech, czterech korporacji na \u015bwiecie do narz\u0119dzia, z kt\u00f3rym mo\u017ce eksperymentowa\u0107 znacznie szersza spo\u0142eczno\u015b\u0107. I na tym nie koniec. Zesp\u00f3\u0142 poszutk okrok dalej i stworzy\u0142 implementacji modelu w C++ z wykorzystaniem biblioteki Faster Transformer od NVIDI, jak podaj\u0105 w pracy. Przyspiesza to dzia\u0142anie modelu od 7 do 8 razy w por\u00f3wnaniu do standardowych implementacji w Pythonie. Czyli mamy nie tylko model, kt\u00f3ry wymaga mniej pami\u0119ci, ale kt\u00f3ry dzia\u0142a te\u017c wielokrutnie, szybciej na tym samym sprz\u0119cie. Podsumowuj\u0105c historia GLM 130B, to znacznie wi\u0119cej ni\u017c tylko kolejny wpis w tabeli z wynikami benchmark\u00f3w. To dow\u00f3d na to, \u017ce inna, bardziej przemy\u015blana architektura mo\u017ce przynie\u015b\u0107 nieoczekiwane korzy\u015bci. To fascynuj\u0105ca opowie\u015b\u0107 in\u017cynieryjna o pokonywaniu problem\u00f3w ze stabilno\u015bci\u0105, za pomoc\u0105 sprytnych, a nie tylko brutalnie si\u0142owych metod. I co chyba najwa\u017cniejsze, to realny, namacalny krok w stron\u0119 bardziej otwartej i dost\u0119pnej dla wszystkich sztucznej inteligencji. Na koniec zostaje jedna prowokuj\u0105ca my\u015bl, kt\u00f3ra wy\u0142ania si\u0119 z tej pracy. Widzieli\u015bmy, \u017ce dwuj\u0119zyczno\u015b\u0107 mog\u0142a nieoczekiwanie zredukowa\u0107 uprzedzenia w modelu. Widzieli\u015bmy, \u017ce inna architektura pozwoli\u0142a na niespodkan\u0105 wcze\u015bniej kompresj\u0119. To wszystko sugeruje, \u017ce by\u0107 mo\u017ce jeste\u015bmy na progu nowej ery w rozwoju AI. Ery, w kt\u00f3rej kluczem do post\u0119pu nie b\u0119dzie ju\u017c tylko \u015blepe skalowanie mocy obliczeniowej i liczby parametr\u00f3w. By\u0107 mo\u017ce kluczem stanie si\u0119 fundamentalne skalowanie lingwistycznej i kulturowej r\u00f3\u017cnorodno\u015bci danych oraz wiesz, inteligentne projektowanie architektur. Co by si\u0119 sta\u0142o, gdyby przysz\u0142e modele uczy\u0142y si\u0119 od samego pocz\u0105tku nie z dw\u00f3ch, ale z dwudziestu j\u0119zyk\u00f3w z r\u00f3\u017cnych rodzin j\u0119zykowych? Jakie zupe\u0142nie nowe, wyjaniaj\u0105ce si\u0119 zdolno\u015bci, tzw. emergent abilities, mogliby\u015bmy w nich wtedy odkry\u0107? To pytanie pozostaje otwarte.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.72, "text": " Co by by\u0142o, gdyby pot\u0119\u017cny model j\u0119zykowy, przeweszczaj\u0105cy w wielu zadaniach s\u0142ynny GPT-3,", "tokens": [50364, 3066, 538, 14811, 11, 28405, 2322, 1847, 1274, 1427, 1634, 2316, 49055, 74, 10089, 11, 39758, 10430, 3689, 11133, 1344, 261, 40437, 42788, 3782, 608, 15116, 2534, 1634, 26039, 51, 12, 18, 11, 50700], "temperature": 0.0, "avg_logprob": -0.17483761371710363, "compression_ratio": 1.396875, "no_speech_prob": 0.008358522318303585}, {"id": 1, "seek": 0, "start": 6.72, "end": 11.8, "text": " mog\u0142oby dzia\u0142a\u0107 na sprz\u0119cie dost\u0119pnym dla mniejszych firm czy uniwersytet\u00f3w,", "tokens": [50700, 13172, 1221, 13944, 37903, 2162, 1667, 6103, 11052, 4260, 48209, 12996, 12285, 39513, 45021, 6174, 6430, 36435, 5364, 4328, 302, 3901, 11, 50954], "temperature": 0.0, "avg_logprob": -0.17483761371710363, "compression_ratio": 1.396875, "no_speech_prob": 0.008358522318303585}, {"id": 2, "seek": 0, "start": 11.8, "end": 14.64, "text": " a nie tylko, wiesz, dla technologicznych gigant\u00f3w.", "tokens": [50954, 257, 2838, 13219, 11, 261, 15347, 11, 12285, 1537, 1132, 17946, 9399, 8741, 394, 3901, 13, 51096], "temperature": 0.0, "avg_logprob": -0.17483761371710363, "compression_ratio": 1.396875, "no_speech_prob": 0.008358522318303585}, {"id": 3, "seek": 0, "start": 14.64, "end": 18.04, "text": " Wydaje si\u0119 to sprzeczne z ca\u0142\u0105 logik\u0105 tego wy\u015bcigu AI,", "tokens": [51096, 343, 6655, 11153, 3244, 281, 6103, 1381, 38491, 710, 1335, 15926, 3565, 1035, 1611, 8627, 4628, 1788, 66, 16397, 7318, 11, 51266], "temperature": 0.0, "avg_logprob": -0.17483761371710363, "compression_ratio": 1.396875, "no_speech_prob": 0.008358522318303585}, {"id": 4, "seek": 0, "start": 18.04, "end": 23.04, "text": " gdzie rewi\u0119kszy niemal zawsze znaczy lepszy i, no c\u00f3\u017c, dro\u017cszy.", "tokens": [51266, 18922, 319, 22423, 1694, 1229, 2838, 5579, 30964, 36584, 476, 1878, 1229, 741, 11, 572, 6333, 1427, 11, 3789, 1427, 7706, 13, 51516], "temperature": 0.0, "avg_logprob": -0.17483761371710363, "compression_ratio": 1.396875, "no_speech_prob": 0.008358522318303585}, {"id": 5, "seek": 0, "start": 23.04, "end": 28.0, "text": " Dzisiaj zajmiemy si\u0119 prac\u0105 naukow\u0105, kt\u00f3ra, mam wra\u017cenie, rzuca wyzwanie tej logice.", "tokens": [51516, 39448, 22356, 33729, 25210, 2226, 3244, 22404, 1611, 35616, 74, 30297, 11, 19456, 11, 13524, 7843, 41118, 11, 367, 11728, 496, 4628, 14406, 7155, 12573, 3565, 573, 13, 51764], "temperature": 0.0, "avg_logprob": -0.17483761371710363, "compression_ratio": 1.396875, "no_speech_prob": 0.008358522318303585}, {"id": 6, "seek": 2800, "start": 28.0, "end": 32.12, "text": " B\u0119dziemy bazowa\u0107 na publikacji GLM 130B,", "tokens": [50364, 363, 42643, 2226, 27147, 11445, 1667, 11227, 1035, 13152, 16225, 44, 19966, 33, 11, 50570], "temperature": 0.0, "avg_logprob": -0.1668501754305256, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.060058366507291794}, {"id": 7, "seek": 2800, "start": 32.12, "end": 38.8, "text": " an open bilingual pre-trained model od naukowc\u00f3w Tsinghua University i ZIPU AI.", "tokens": [50570, 364, 1269, 48757, 659, 12, 17227, 2001, 2316, 3611, 35616, 74, 305, 29268, 16518, 278, 30107, 3535, 741, 1176, 9139, 52, 7318, 13, 50904], "temperature": 0.0, "avg_logprob": -0.1668501754305256, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.060058366507291794}, {"id": 8, "seek": 2800, "start": 38.8, "end": 40.480000000000004, "text": " A nasza misja jest prosta.", "tokens": [50904, 316, 5382, 2394, 3346, 2938, 3492, 582, 8638, 13, 50988], "temperature": 0.0, "avg_logprob": -0.1668501754305256, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.060058366507291794}, {"id": 9, "seek": 2800, "start": 40.480000000000004, "end": 44.0, "text": " Zrozumie\u0107, co tak naprawd\u0119 czyni ten model wyj\u0105tkowy.", "tokens": [50988, 1176, 27857, 449, 414, 2162, 11, 598, 991, 20970, 6430, 3722, 2064, 2316, 4628, 8555, 83, 74, 10089, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1668501754305256, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.060058366507291794}, {"id": 10, "seek": 2800, "start": 44.0, "end": 46.24, "text": " I od razu powiem, \u017ce nie chodzi tu tylko o wydajno\u015b\u0107,", "tokens": [51164, 286, 3611, 367, 8813, 3388, 4907, 11, 3561, 2838, 23998, 2604, 13219, 277, 25984, 1805, 23293, 11, 51276], "temperature": 0.0, "avg_logprob": -0.1668501754305256, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.060058366507291794}, {"id": 11, "seek": 2800, "start": 46.24, "end": 50.24, "text": " chodzi o architektur\u0119, o fascynuj\u0105c\u0105 opowie\u015b\u0107 in\u017cynierin\u0105,", "tokens": [51276, 23998, 277, 3912, 642, 2320, 374, 1274, 11, 277, 30632, 1344, 77, 13263, 32557, 999, 13998, 7753, 294, 1427, 2534, 811, 259, 1611, 11, 51476], "temperature": 0.0, "avg_logprob": -0.1668501754305256, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.060058366507291794}, {"id": 12, "seek": 2800, "start": 50.24, "end": 52.6, "text": " kt\u00f3ra jest pe\u0142na pora\u017cek i prze\u0142om\u00f3w", "tokens": [51476, 19456, 3492, 43205, 629, 1515, 18264, 916, 741, 8325, 1221, 298, 3901, 51594], "temperature": 0.0, "avg_logprob": -0.1668501754305256, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.060058366507291794}, {"id": 13, "seek": 5260, "start": 52.6, "end": 58.760000000000005, "text": " i wreszcie o realn\u0105 pr\u00f3b\u0119 demokratyzacji dost\u0119pu do najpot\u0119\u017cniejszych narz\u0119dzi AI.", "tokens": [50364, 741, 261, 495, 89, 4260, 277, 957, 13113, 8565, 65, 1274, 49432, 37433, 13152, 48209, 84, 360, 11212, 17698, 1274, 1427, 10402, 45021, 6714, 89, 6298, 3992, 7318, 13, 50672], "temperature": 0.0, "avg_logprob": -0.12975335121154785, "compression_ratio": 1.3576388888888888, "no_speech_prob": 0.15388917922973633}, {"id": 14, "seek": 5260, "start": 58.760000000000005, "end": 60.4, "text": " Dobra, to rozpakujmy to.", "tokens": [50672, 413, 24393, 11, 281, 9544, 45944, 4579, 2226, 281, 13, 50754], "temperature": 0.0, "avg_logprob": -0.12975335121154785, "compression_ratio": 1.3576388888888888, "no_speech_prob": 0.15388917922973633}, {"id": 15, "seek": 5260, "start": 60.4, "end": 65.56, "text": " Na pierwszy rzut oka GLM 130B to po prostu kolejny wielki model.", "tokens": [50754, 6056, 34016, 367, 89, 325, 277, 2330, 16225, 44, 19966, 33, 281, 714, 19518, 23749, 1634, 20570, 2984, 2316, 13, 51012], "temperature": 0.0, "avg_logprob": -0.12975335121154785, "compression_ratio": 1.3576388888888888, "no_speech_prob": 0.15388917922973633}, {"id": 16, "seek": 5260, "start": 65.56, "end": 70.96000000000001, "text": " Dwuj\u0119zyczny angielsko-chi\u0144ski ze 130 miliardami parametr\u00f3w.", "tokens": [51012, 41448, 18258, 1229, 3689, 1634, 2562, 1187, 82, 4093, 12, 8036, 5248, 18020, 5277, 19966, 1962, 72, 515, 4526, 6220, 27965, 3901, 13, 51282], "temperature": 0.0, "avg_logprob": -0.12975335121154785, "compression_ratio": 1.3576388888888888, "no_speech_prob": 0.15388917922973633}, {"id": 17, "seek": 5260, "start": 70.96000000000001, "end": 75.4, "text": " Ale ju\u017c sama jego nazwa, GLM, czyli General Language Model,", "tokens": [51282, 9366, 10678, 17768, 26542, 20151, 4151, 11, 16225, 44, 11, 16591, 6996, 24445, 17105, 11, 51504], "temperature": 0.0, "avg_logprob": -0.12975335121154785, "compression_ratio": 1.3576388888888888, "no_speech_prob": 0.15388917922973633}, {"id": 18, "seek": 5260, "start": 75.4, "end": 80.44, "text": " sugaruje, \u017ce pod mask\u0105 kryje si\u0119 co\u015b innego ni\u017c w standardowych modelach typu GPT.", "tokens": [51504, 459, 2976, 13008, 11, 3561, 2497, 6094, 1611, 34847, 2884, 3244, 19241, 294, 11858, 28502, 261, 3832, 19605, 2316, 608, 2125, 84, 26039, 51, 13, 51756], "temperature": 0.0, "avg_logprob": -0.12975335121154785, "compression_ratio": 1.3576388888888888, "no_speech_prob": 0.15388917922973633}, {"id": 19, "seek": 8044, "start": 80.48, "end": 83.56, "text": " I to jest absolutny klucz do wszystkiego.", "tokens": [50366, 286, 281, 3492, 18757, 1634, 9671, 1311, 89, 360, 14615, 12200, 13, 50520], "temperature": 0.0, "avg_logprob": -0.10185043675125025, "compression_ratio": 1.478125, "no_speech_prob": 0.029173821210861206}, {"id": 20, "seek": 8044, "start": 83.56, "end": 88.36, "text": " Tw\u00f3rcy nie poszli na \u0142atwizn\u0119, nie skopiowali po prostu sprawdzonej architektury.", "tokens": [50520, 2574, 15614, 1344, 2838, 1366, 89, 2081, 1667, 47759, 86, 590, 77, 1274, 11, 2838, 1110, 404, 72, 305, 5103, 714, 19518, 46192, 16896, 73, 3912, 642, 2320, 2598, 13, 50760], "temperature": 0.0, "avg_logprob": -0.10185043675125025, "compression_ratio": 1.478125, "no_speech_prob": 0.029173821210861206}, {"id": 21, "seek": 8044, "start": 88.36, "end": 93.88, "text": " Zbudowali go na fundamentzie GLM, kt\u00f3ry dzia\u0142a no fundamentalnie inaczej.", "tokens": [50760, 1176, 18281, 305, 5103, 352, 1667, 6073, 3283, 16225, 44, 11, 9913, 37903, 572, 8088, 2766, 33230, 16920, 13, 51036], "temperature": 0.0, "avg_logprob": -0.10185043675125025, "compression_ratio": 1.478125, "no_speech_prob": 0.029173821210861206}, {"id": 22, "seek": 8044, "start": 93.88, "end": 98.84, "text": " Modele GPT s\u0105 czysto autoregresyw, co znaczy, \u017ce dzia\u0142aj\u0105 jednokierunkowo.", "tokens": [51036, 20500, 306, 26039, 51, 9015, 6430, 20875, 1476, 418, 45189, 27112, 11, 598, 36584, 11, 3561, 27121, 11133, 5232, 77, 453, 811, 3197, 19941, 13, 51284], "temperature": 0.0, "avg_logprob": -0.10185043675125025, "compression_ratio": 1.478125, "no_speech_prob": 0.029173821210861206}, {"id": 23, "seek": 8044, "start": 98.84, "end": 104.0, "text": " Czytaj\u0105 tekst odle\u0142ej do prawej i staraj\u0105 si\u0119 przewidzie\u0107, jakie b\u0119dzie nast\u0119pne s\u0142owo.", "tokens": [51284, 19832, 1328, 8555, 16624, 372, 277, 2285, 19827, 73, 360, 3206, 826, 73, 741, 3543, 11133, 3244, 39758, 327, 21214, 11, 22124, 10562, 39662, 716, 15116, 19941, 13, 51542], "temperature": 0.0, "avg_logprob": -0.10185043675125025, "compression_ratio": 1.478125, "no_speech_prob": 0.029173821210861206}, {"id": 24, "seek": 8044, "start": 104.0, "end": 109.68, "text": " To jest \u015bwietne do generowania tekstu, ale niekoniecznie do g\u0142\u0119bokiego rozumienia kontekstu.", "tokens": [51542, 1407, 3492, 8299, 39083, 716, 360, 1337, 21308, 16624, 372, 84, 11, 6775, 2838, 18295, 414, 19923, 360, 18117, 1274, 21666, 12200, 48797, 18811, 14373, 916, 372, 84, 13, 51826], "temperature": 0.0, "avg_logprob": -0.10185043675125025, "compression_ratio": 1.478125, "no_speech_prob": 0.029173821210861206}, {"id": 25, "seek": 10968, "start": 109.68, "end": 114.60000000000001, "text": " A GLM stosuje co\u015b, co w pracy nazywa si\u0119 autoregressive blank in filling.", "tokens": [50364, 316, 16225, 44, 43581, 13008, 19241, 11, 598, 261, 35591, 20151, 88, 4151, 3244, 1476, 418, 3091, 488, 8247, 294, 10623, 13, 50610], "temperature": 0.0, "avg_logprob": -0.11147593069767607, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.000444661796791479}, {"id": 26, "seek": 10968, "start": 114.60000000000001, "end": 119.28, "text": " Czyli je\u015bli dobrze to rozumiem, to jest fundamentalna zmiana perspektywy.", "tokens": [50610, 37099, 25630, 28335, 281, 48797, 4907, 11, 281, 3492, 8088, 629, 17020, 8497, 868, 32659, 874, 9726, 13, 50844], "temperature": 0.0, "avg_logprob": -0.11147593069767607, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.000444661796791479}, {"id": 27, "seek": 10968, "start": 119.28, "end": 125.08000000000001, "text": " GPT jest jak taki gaw\u0119dziarz, kt\u00f3ry snuje opowie\u015b\u0107, dodaj\u0105c kolejne s\u0142owa.", "tokens": [50844, 26039, 51, 3492, 4207, 20065, 290, 1607, 6298, 3992, 49763, 11, 9913, 2406, 13008, 999, 13998, 7753, 11, 13886, 38757, 23749, 716, 15116, 5528, 13, 51134], "temperature": 0.0, "avg_logprob": -0.11147593069767607, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.000444661796791479}, {"id": 28, "seek": 10968, "start": 125.08000000000001, "end": 129.76000000000002, "text": " A GLM jest bardziej jak detektyw albo archeolog,", "tokens": [51134, 316, 16225, 44, 3492, 27209, 4207, 1141, 916, 874, 86, 22622, 37897, 1132, 11, 51368], "temperature": 0.0, "avg_logprob": -0.11147593069767607, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.000444661796791479}, {"id": 29, "seek": 10968, "start": 129.76000000000002, "end": 133.84, "text": " kt\u00f3ry dostaje tekst z dziurami i musi je uzupe\u0142ni\u0107,", "tokens": [51368, 9913, 20568, 11153, 16624, 372, 710, 31981, 374, 4526, 741, 37587, 1506, 344, 11728, 31457, 3722, 2162, 11, 51572], "temperature": 0.0, "avg_logprob": -0.11147593069767607, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.000444661796791479}, {"id": 30, "seek": 10968, "start": 133.84, "end": 139.4, "text": " analizuj\u0105c kontekst z obu stron tego, co by\u0142o przed luk\u0105 i tego, co jest po niej.", "tokens": [51572, 2624, 590, 44733, 14373, 916, 372, 710, 1111, 84, 45766, 8627, 11, 598, 14811, 18334, 287, 2034, 1611, 741, 8627, 11, 598, 3492, 714, 2838, 73, 13, 51850], "temperature": 0.0, "avg_logprob": -0.11147593069767607, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.000444661796791479}, {"id": 31, "seek": 13940, "start": 139.4, "end": 141.24, "text": " To jest doskona\u0142a analogia.", "tokens": [50364, 1407, 3492, 4491, 74, 4037, 5024, 16660, 654, 13, 50456], "temperature": 0.0, "avg_logprob": -0.14721286569842856, "compression_ratio": 1.355072463768116, "no_speech_prob": 0.0649494156241417}, {"id": 32, "seek": 13940, "start": 141.24, "end": 146.4, "text": " I co wi\u0119cej, model robi to w bardzo sprypny, hybrydowy spos\u00f3b.", "tokens": [50456, 286, 598, 26004, 11, 2316, 47380, 281, 261, 9034, 637, 627, 79, 1634, 11, 2477, 65, 627, 67, 10089, 22904, 13, 50714], "temperature": 0.0, "avg_logprob": -0.14721286569842856, "compression_ratio": 1.355072463768116, "no_speech_prob": 0.0649494156241417}, {"id": 33, "seek": 13940, "start": 146.4, "end": 149.56, "text": " W pracy opisuj\u0105 dwie strategie maskowania.", "tokens": [50714, 343, 35591, 45477, 13263, 274, 8699, 5464, 414, 6094, 21308, 13, 50872], "temperature": 0.0, "avg_logprob": -0.14721286569842856, "compression_ratio": 1.355072463768116, "no_speech_prob": 0.0649494156241417}, {"id": 34, "seek": 13940, "start": 149.56, "end": 156.76, "text": " Pierwsza z u\u017cyciem tokenu MSK s\u0142u\u017cy do wype\u0142niania kr\u00f3tkich, losowych luk w \u015brodku tekstu.", "tokens": [50872, 16676, 14358, 2394, 710, 34097, 4260, 76, 14862, 84, 7395, 42, 48459, 7735, 360, 4628, 31457, 77, 952, 654, 42366, 83, 48349, 11, 1750, 19605, 287, 2034, 261, 28580, 5279, 16624, 372, 84, 13, 51232], "temperature": 0.0, "avg_logprob": -0.14721286569842856, "compression_ratio": 1.355072463768116, "no_speech_prob": 0.0649494156241417}, {"id": 35, "seek": 13940, "start": 156.76, "end": 160.24, "text": " To bardzo przypomina dzia\u0142anie modeli z rodziny BERT,", "tokens": [51232, 1407, 9034, 41780, 49217, 27121, 7155, 2316, 72, 710, 28607, 3519, 363, 31479, 11, 51406], "temperature": 0.0, "avg_logprob": -0.14721286569842856, "compression_ratio": 1.355072463768116, "no_speech_prob": 0.0649494156241417}, {"id": 36, "seek": 13940, "start": 160.24, "end": 164.76, "text": " kt\u00f3re s\u0105 mistrzami w zadaniach wymagaj\u0105cych rozumienia semantyki zdania.", "tokens": [51406, 8864, 9015, 3544, 19390, 4526, 261, 42788, 3782, 608, 29764, 559, 11133, 31306, 48797, 18811, 4361, 394, 88, 2984, 16221, 5609, 13, 51632], "temperature": 0.0, "avg_logprob": -0.14721286569842856, "compression_ratio": 1.355072463768116, "no_speech_prob": 0.0649494156241417}, {"id": 37, "seek": 13940, "start": 164.76, "end": 165.64000000000001, "text": " A druga?", "tokens": [51632, 316, 4110, 64, 30, 51676], "temperature": 0.0, "avg_logprob": -0.14721286569842856, "compression_ratio": 1.355072463768116, "no_speech_prob": 0.0649494156241417}, {"id": 38, "seek": 16564, "start": 165.67999999999998, "end": 172.39999999999998, "text": " Druga strategia z u\u017cyciem tokenu GMSK polega na zamaskowaniu ca\u0142ego ko\u0144ca tekstu,", "tokens": [50366, 2491, 19364, 5464, 654, 710, 34097, 4260, 76, 14862, 84, 460, 10288, 42, 13208, 3680, 1667, 19876, 3863, 305, 25849, 35224, 6308, 26470, 496, 16624, 372, 84, 11, 50702], "temperature": 0.0, "avg_logprob": -0.09789383562305305, "compression_ratio": 1.4580645161290322, "no_speech_prob": 0.002218678593635559}, {"id": 39, "seek": 16564, "start": 172.39999999999998, "end": 176.6, "text": " co zmusza model do generowania d\u0142ugich, sp\u00f3jnych fragment\u00f3w.", "tokens": [50702, 598, 17020, 301, 2394, 2316, 360, 1337, 21308, 274, 34077, 480, 11, 637, 18999, 9399, 26424, 3901, 13, 50912], "temperature": 0.0, "avg_logprob": -0.09789383562305305, "compression_ratio": 1.4580645161290322, "no_speech_prob": 0.002218678593635559}, {"id": 40, "seek": 16564, "start": 176.6, "end": 179.04, "text": " A to z kolei jest domen\u0105 GPT.", "tokens": [50912, 316, 281, 710, 18303, 72, 3492, 3285, 268, 1611, 26039, 51, 13, 51034], "temperature": 0.0, "avg_logprob": -0.09789383562305305, "compression_ratio": 1.4580645161290322, "no_speech_prob": 0.002218678593635559}, {"id": 41, "seek": 16564, "start": 179.04, "end": 182.83999999999997, "text": " Czyli po\u0142\u0105czyli to, co najlepsze z obu \u015bwiat\u00f3w.", "tokens": [51034, 37099, 714, 15926, 6522, 2081, 281, 11, 598, 41903, 1878, 1381, 710, 1111, 84, 36425, 3901, 13, 51224], "temperature": 0.0, "avg_logprob": -0.09789383562305305, "compression_ratio": 1.4580645161290322, "no_speech_prob": 0.002218678593635559}, {"id": 42, "seek": 16564, "start": 182.83999999999997, "end": 187.27999999999997, "text": " Z jednej strony zdolno\u015b\u0107 BERT-a do g\u0142\u0119bokiego rozumienia kontekstu,", "tokens": [51224, 1176, 5232, 11794, 32406, 16221, 401, 23293, 363, 31479, 12, 64, 360, 18117, 1274, 21666, 12200, 48797, 18811, 14373, 916, 372, 84, 11, 51446], "temperature": 0.0, "avg_logprob": -0.09789383562305305, "compression_ratio": 1.4580645161290322, "no_speech_prob": 0.002218678593635559}, {"id": 43, "seek": 16564, "start": 187.27999999999997, "end": 191.6, "text": " a z drugiej zdolno\u015b\u0107 GPT do generowania p\u0142ynnego, d\u0142ugiego tekstu.", "tokens": [51446, 257, 710, 47373, 16221, 401, 23293, 26039, 51, 360, 1337, 21308, 28695, 2534, 11858, 11, 274, 34077, 12200, 16624, 372, 84, 13, 51662], "temperature": 0.0, "avg_logprob": -0.09789383562305305, "compression_ratio": 1.4580645161290322, "no_speech_prob": 0.002218678593635559}, {"id": 44, "seek": 16564, "start": 191.6, "end": 195.48, "text": " Ta dwukierunkowo\u015b\u0107 musia\u0142a by\u0107 kluczowa dla jego p\u00f3\u017aniejszych wynik\u00f3w.", "tokens": [51662, 6551, 27379, 2034, 811, 3197, 19941, 7753, 1038, 25605, 15069, 9671, 1311, 89, 5528, 12285, 26542, 36968, 45021, 31936, 1035, 3901, 13, 51856], "temperature": 0.0, "avg_logprob": -0.09789383562305305, "compression_ratio": 1.4580645161290322, "no_speech_prob": 0.002218678593635559}, {"id": 45, "seek": 19548, "start": 195.48, "end": 198.56, "text": " Dok\u0142adnie. Ta hybrydowa natura daje mu elastyczno\u015b\u0107,", "tokens": [50364, 29768, 10358, 2766, 13, 6551, 2477, 65, 627, 67, 5528, 2249, 2991, 1120, 2884, 2992, 806, 9820, 3689, 23293, 11, 50518], "temperature": 0.0, "avg_logprob": -0.10331892967224121, "compression_ratio": 1.4717514124293785, "no_speech_prob": 0.00387813663110137}, {"id": 46, "seek": 19548, "start": 198.56, "end": 201.04, "text": " kt\u00f3rej brakuje modelom jednotierunkowym.", "tokens": [50518, 36023, 1548, 5279, 2884, 2316, 298, 5232, 2247, 811, 3197, 31691, 13, 50642], "temperature": 0.0, "avg_logprob": -0.10331892967224121, "compression_ratio": 1.4717514124293785, "no_speech_prob": 0.00387813663110137}, {"id": 47, "seek": 19548, "start": 201.04, "end": 206.04, "text": " Mo\u017ce jednocze\u015bnie rozumie\u0107 i generowa\u0107 z r\u00f3wn\u0105 bieg\u0142o\u015bci\u0105, co jak zaraz zobaczymy,", "tokens": [50642, 43774, 5232, 26694, 1381, 12221, 48797, 414, 2162, 741, 1337, 11445, 710, 11416, 895, 1611, 272, 20408, 35059, 1611, 11, 598, 4207, 22675, 921, 37273, 2226, 11, 50892], "temperature": 0.0, "avg_logprob": -0.10331892967224121, "compression_ratio": 1.4717514124293785, "no_speech_prob": 0.00387813663110137}, {"id": 48, "seek": 19548, "start": 206.04, "end": 208.51999999999998, "text": " przek\u0142ada si\u0119 na jego niezwyk\u0142\u0105 wydajno\u015b\u0107.", "tokens": [50892, 29785, 46217, 3244, 1667, 26542, 33511, 9726, 74, 15926, 25984, 1805, 23293, 13, 51016], "temperature": 0.0, "avg_logprob": -0.10331892967224121, "compression_ratio": 1.4717514124293785, "no_speech_prob": 0.00387813663110137}, {"id": 49, "seek": 19548, "start": 208.51999999999998, "end": 212.28, "text": " Ale zanim dojdziemy do wynik\u00f3w, musimy porozmawia\u0107 o tym,", "tokens": [51016, 9366, 710, 17869, 360, 73, 13096, 2226, 360, 31936, 1035, 3901, 11, 43449, 1515, 15151, 76, 34953, 2162, 277, 8107, 11, 51204], "temperature": 0.0, "avg_logprob": -0.10331892967224121, "compression_ratio": 1.4717514124293785, "no_speech_prob": 0.00387813663110137}, {"id": 50, "seek": 19548, "start": 212.28, "end": 215.39999999999998, "text": " jak trudne by\u0142o w og\u00f3le doprowadzenie tego modelu do mety.", "tokens": [51204, 4207, 32007, 716, 14811, 261, 29229, 360, 35019, 16778, 8627, 2316, 84, 360, 1131, 88, 13, 51360], "temperature": 0.0, "avg_logprob": -0.10331892967224121, "compression_ratio": 1.4717514124293785, "no_speech_prob": 0.00387813663110137}, {"id": 51, "seek": 19548, "start": 215.39999999999998, "end": 218.39999999999998, "text": " O tak, to jest cz\u0119\u015b\u0107 pracy, kt\u00f3ra naprawd\u0119 mnie uderzy\u0142a.", "tokens": [51360, 422, 991, 11, 281, 3492, 47149, 35591, 11, 19456, 20970, 17661, 344, 1068, 1229, 5024, 13, 51510], "temperature": 0.0, "avg_logprob": -0.10331892967224121, "compression_ratio": 1.4717514124293785, "no_speech_prob": 0.00387813663110137}, {"id": 52, "seek": 19548, "start": 218.39999999999998, "end": 220.64, "text": " Autorzy s\u0105 niezwykle szczerzy.", "tokens": [51510, 6049, 284, 1229, 9015, 33511, 9726, 14677, 22090, 260, 1229, 13, 51622], "temperature": 0.0, "avg_logprob": -0.10331892967224121, "compression_ratio": 1.4717514124293785, "no_speech_prob": 0.00387813663110137}, {"id": 53, "seek": 19548, "start": 220.64, "end": 225.07999999999998, "text": " Przyznaj\u0105 si\u0119 do ponad 30 kompletnie nieudanych pr\u00f3b treningu.", "tokens": [51622, 39590, 35458, 8555, 3244, 360, 9224, 345, 2217, 5207, 14657, 2766, 2838, 532, 34644, 8565, 65, 2192, 773, 84, 13, 51844], "temperature": 0.0, "avg_logprob": -0.10331892967224121, "compression_ratio": 1.4717514124293785, "no_speech_prob": 0.00387813663110137}, {"id": 54, "seek": 22508, "start": 225.08, "end": 228.12, "text": " Pisz\u0105 o ci\u0105g\u0142ych problemach z tak zwanymi los spikes.", "tokens": [50364, 430, 23848, 1611, 277, 42398, 70, 47655, 1154, 608, 710, 991, 11873, 1325, 3057, 1750, 28997, 13, 50516], "temperature": 0.0, "avg_logprob": -0.13179465702601842, "compression_ratio": 1.4345047923322685, "no_speech_prob": 0.03994855284690857}, {"id": 55, "seek": 22508, "start": 228.12, "end": 233.0, "text": " Na g\u0142ymi, gwa\u0142townymi skokami b\u0142\u0119d\u00f3w, kt\u00f3re w jednej chwili potrafi\u0142y zniweczy\u0107", "tokens": [50516, 6056, 18117, 88, 3057, 11, 290, 44603, 30401, 88, 3057, 1110, 453, 4526, 272, 1221, 6298, 3901, 11, 8864, 261, 5232, 11794, 26237, 2312, 1847, 10437, 72, 6825, 710, 3722, 826, 33967, 50760], "temperature": 0.0, "avg_logprob": -0.13179465702601842, "compression_ratio": 1.4345047923322685, "no_speech_prob": 0.03994855284690857}, {"id": 56, "seek": 22508, "start": 233.0, "end": 235.4, "text": " tygodnie oblicze\u0144 na superkomputerze.", "tokens": [50760, 1104, 21787, 2766, 1111, 1050, 49689, 1667, 1687, 20557, 13849, 1381, 13, 50880], "temperature": 0.0, "avg_logprob": -0.13179465702601842, "compression_ratio": 1.4345047923322685, "no_speech_prob": 0.03994855284690857}, {"id": 57, "seek": 22508, "start": 235.4, "end": 239.64000000000001, "text": " To jest ta brutalna rzeczywisto\u015b\u0107, wiesz, o kt\u00f3rej rzadko si\u0119 czyta w tych wszystkich", "tokens": [50880, 1407, 3492, 1846, 17878, 629, 26297, 86, 9334, 7753, 11, 261, 15347, 11, 277, 36023, 367, 89, 345, 4093, 3244, 6430, 1328, 261, 15180, 34234, 51092], "temperature": 0.0, "avg_logprob": -0.13179465702601842, "compression_ratio": 1.4345047923322685, "no_speech_prob": 0.03994855284690857}, {"id": 58, "seek": 22508, "start": 239.64000000000001, "end": 242.12, "text": " wypolonorowanych komunikatach prasowych.", "tokens": [51092, 4628, 12892, 266, 284, 23341, 339, 45359, 36300, 608, 582, 296, 19605, 13, 51216], "temperature": 0.0, "avg_logprob": -0.13179465702601842, "compression_ratio": 1.4345047923322685, "no_speech_prob": 0.03994855284690857}, {"id": 59, "seek": 22508, "start": 242.12, "end": 244.88000000000002, "text": " No w\u0142a\u015bnie i to nie jest odosobniony problem.", "tokens": [51216, 883, 14234, 741, 281, 2838, 3492, 3611, 329, 996, 77, 46184, 1154, 13, 51354], "temperature": 0.0, "avg_logprob": -0.13179465702601842, "compression_ratio": 1.4345047923322685, "no_speech_prob": 0.03994855284690857}, {"id": 60, "seek": 22508, "start": 244.88000000000002, "end": 250.12, "text": " W zasadzie ka\u017cdy zesp\u00f3\u0142 buduj\u0105cy modele tej skali boryka si\u0119 z niestabilno\u015bci\u0105.", "tokens": [51354, 343, 44585, 3283, 31615, 710, 13361, 16181, 3265, 13263, 1344, 4391, 306, 12573, 1110, 5103, 272, 827, 2330, 3244, 710, 3867, 377, 5177, 16438, 1611, 13, 51616], "temperature": 0.0, "avg_logprob": -0.13179465702601842, "compression_ratio": 1.4345047923322685, "no_speech_prob": 0.03994855284690857}, {"id": 61, "seek": 25012, "start": 250.12, "end": 256.36, "text": " Zesp\u00f3\u0142 meta tworz\u0105c OP-T175B musia\u0142 r\u0119cznie interweniowa\u0107 w trakcie treningu,", "tokens": [50364, 1176, 13361, 16181, 19616, 46288, 8925, 66, 23324, 12, 51, 7773, 20, 33, 1038, 8908, 41197, 19923, 728, 15615, 72, 11445, 261, 944, 74, 4260, 2192, 773, 84, 11, 50676], "temperature": 0.0, "avg_logprob": -0.14098023113451505, "compression_ratio": 1.3876221498371335, "no_speech_prob": 0.06828238070011139}, {"id": 62, "seek": 25012, "start": 256.36, "end": 259.24, "text": " restartowa\u0107 go i dostosowywa\u0107 parametry.", "tokens": [50676, 21022, 11445, 352, 741, 20568, 329, 10089, 25234, 6220, 9889, 13, 50820], "temperature": 0.0, "avg_logprob": -0.14098023113451505, "compression_ratio": 1.3876221498371335, "no_speech_prob": 0.06828238070011139}, {"id": 63, "seek": 25012, "start": 259.24, "end": 262.52, "text": " To troch\u0119 jak \u0142atanie dziur w kad\u0142ubie p\u0142yn\u0105cego transatlantyku.", "tokens": [50820, 1407, 24926, 4207, 25387, 267, 7155, 31981, 374, 261, 8064, 1221, 836, 414, 28695, 2534, 1611, 384, 1571, 1145, 48630, 394, 88, 5279, 13, 50984], "temperature": 0.0, "avg_logprob": -0.14098023113451505, "compression_ratio": 1.3876221498371335, "no_speech_prob": 0.06828238070011139}, {"id": 64, "seek": 25012, "start": 262.52, "end": 263.52, "text": " Dok\u0142adnie.", "tokens": [50984, 29768, 10358, 2766, 13, 51034], "temperature": 0.0, "avg_logprob": -0.14098023113451505, "compression_ratio": 1.3876221498371335, "no_speech_prob": 0.06828238070011139}, {"id": 65, "seek": 25012, "start": 263.52, "end": 269.92, "text": " A z kolei tw\u00f3rcy Bloom 176B zastosowali technik\u0119, kt\u00f3r\u0105 nazwali embedding norm,", "tokens": [51034, 316, 710, 18303, 72, 683, 15614, 1344, 25927, 3282, 21, 33, 36746, 329, 305, 5103, 1537, 1035, 1274, 11, 37415, 20151, 40054, 12240, 3584, 2026, 11, 51354], "temperature": 0.0, "avg_logprob": -0.14098023113451505, "compression_ratio": 1.3876221498371335, "no_speech_prob": 0.06828238070011139}, {"id": 66, "seek": 25012, "start": 269.92, "end": 274.36, "text": " ale sami przyznali, \u017ce zap\u0142acili za to ni\u017csz\u0105 wydajno\u015bci\u0105 ko\u0144cowego modelu.", "tokens": [51354, 6775, 3247, 72, 6501, 89, 4660, 72, 11, 3561, 14223, 1221, 326, 2312, 7949, 281, 28502, 82, 8925, 25984, 1805, 16438, 1611, 26470, 66, 26576, 2316, 84, 13, 51576], "temperature": 0.0, "avg_logprob": -0.14098023113451505, "compression_ratio": 1.3876221498371335, "no_speech_prob": 0.06828238070011139}, {"id": 67, "seek": 25012, "start": 274.36, "end": 277.36, "text": " To by\u0142 kompromis, stabilno\u015b\u0107 za cen\u0119 jako\u015bci.", "tokens": [51576, 1407, 16673, 5207, 28722, 271, 11, 11652, 23293, 7949, 27900, 1274, 17123, 6199, 13, 51726], "temperature": 0.0, "avg_logprob": -0.14098023113451505, "compression_ratio": 1.3876221498371335, "no_speech_prob": 0.06828238070011139}, {"id": 68, "seek": 27736, "start": 277.36, "end": 280.16, "text": " A zesp\u00f3\u0142 DLM znalaz\u0142 inne rozwi\u0105zanie.", "tokens": [50364, 316, 710, 13361, 16181, 413, 43, 44, 710, 4660, 921, 1221, 24170, 9544, 22620, 7155, 13, 50504], "temperature": 0.0, "avg_logprob": -0.14893205745800123, "compression_ratio": 1.3966101694915254, "no_speech_prob": 0.16911180317401886}, {"id": 69, "seek": 27736, "start": 280.16, "end": 283.12, "text": " W pracy opisuj\u0105 dwie kluczowe innowacje.", "tokens": [50504, 343, 35591, 45477, 13263, 274, 8699, 9671, 1311, 89, 6880, 294, 3785, 29293, 13, 50652], "temperature": 0.0, "avg_logprob": -0.14893205745800123, "compression_ratio": 1.3966101694915254, "no_speech_prob": 0.16911180317401886}, {"id": 70, "seek": 27736, "start": 283.12, "end": 285.92, "text": " Pierwsza to co\u015b, co nazywaj\u0105 deep norm.", "tokens": [50652, 16676, 14358, 2394, 281, 19241, 11, 598, 20151, 27112, 11133, 2452, 2026, 13, 50792], "temperature": 0.0, "avg_logprob": -0.14893205745800123, "compression_ratio": 1.3966101694915254, "no_speech_prob": 0.16911180317401886}, {"id": 71, "seek": 27736, "start": 285.92, "end": 290.52000000000004, "text": " To jest wariant normalizacji warstw, tak zwany post-LN.", "tokens": [50792, 1407, 3492, 1516, 5798, 2710, 590, 13152, 1516, 372, 86, 11, 991, 11873, 1325, 2183, 12, 43, 45, 13, 51022], "temperature": 0.0, "avg_logprob": -0.14893205745800123, "compression_ratio": 1.3966101694915254, "no_speech_prob": 0.16911180317401886}, {"id": 72, "seek": 27736, "start": 290.52000000000004, "end": 295.84000000000003, "text": " I tu musz\u0119 zapyta\u0107, bo wydawa\u0142o mi si\u0119, \u017ce ca\u0142y \u015bwiat EI przeszed\u0142 na pre-LN.", "tokens": [51022, 286, 2604, 1038, 11052, 14223, 88, 42931, 11, 748, 25984, 10449, 5249, 2752, 3244, 11, 3561, 35226, 36425, 462, 40, 6541, 10430, 292, 1221, 1667, 659, 12, 43, 45, 13, 51288], "temperature": 0.0, "avg_logprob": -0.14893205745800123, "compression_ratio": 1.3966101694915254, "no_speech_prob": 0.16911180317401886}, {"id": 73, "seek": 27736, "start": 295.84000000000003, "end": 299.96000000000004, "text": " Czyli normalizacj\u0119 przed g\u0142\u00f3wn\u0105 operacj\u0105 w warstwie transformer.", "tokens": [51288, 37099, 2710, 590, 29924, 18334, 18117, 812, 895, 1611, 2208, 326, 8555, 261, 1516, 372, 8699, 31782, 13, 51494], "temperature": 0.0, "avg_logprob": -0.14893205745800123, "compression_ratio": 1.3966101694915254, "no_speech_prob": 0.16911180317401886}, {"id": 74, "seek": 27736, "start": 299.96000000000004, "end": 303.76, "text": " W\u0142a\u015bnie dlatego, \u017ce mia\u0142a by\u0107 stabilniejsza, a oni poszli pod pr\u0105d.", "tokens": [51494, 343, 5024, 12221, 32205, 11, 3561, 21290, 5024, 15069, 11652, 30295, 2394, 11, 257, 36317, 1366, 89, 2081, 2497, 582, 18962, 13, 51684], "temperature": 0.0, "avg_logprob": -0.14893205745800123, "compression_ratio": 1.3966101694915254, "no_speech_prob": 0.16911180317401886}, {"id": 75, "seek": 30376, "start": 303.76, "end": 308.48, "text": " To jest \u015bwietna obserwacja i pokazuje, jak nieszablonowo my\u015bleli.", "tokens": [50364, 1407, 3492, 8299, 39083, 629, 12887, 86, 23395, 741, 13010, 43317, 11, 4207, 297, 15347, 455, 14864, 19941, 48633, 306, 2081, 13, 50600], "temperature": 0.0, "avg_logprob": -0.11892136660489169, "compression_ratio": 1.3745318352059925, "no_speech_prob": 0.020239325240254402}, {"id": 76, "seek": 30376, "start": 308.48, "end": 311.68, "text": " Rzeczywi\u015bcie panowa\u0142a moda na pre-LN.", "tokens": [50600, 497, 1381, 15997, 2462, 5528, 5024, 1072, 64, 1667, 659, 12, 43, 45, 13, 50760], "temperature": 0.0, "avg_logprob": -0.11892136660489169, "compression_ratio": 1.3745318352059925, "no_speech_prob": 0.020239325240254402}, {"id": 77, "seek": 30376, "start": 311.68, "end": 317.32, "text": " Ale oni udowodnili, \u017ce odpowiednio skalibrowana starsza metoda post-LN", "tokens": [50760, 9366, 36317, 11727, 305, 378, 77, 2312, 11, 3561, 36574, 41084, 1110, 304, 897, 1892, 2095, 6105, 2394, 1131, 13449, 2183, 12, 43, 45, 51042], "temperature": 0.0, "avg_logprob": -0.11892136660489169, "compression_ratio": 1.3745318352059925, "no_speech_prob": 0.020239325240254402}, {"id": 78, "seek": 30376, "start": 317.32, "end": 321.56, "text": " mo\u017ce by\u0107 nie tylko r\u00f3wnie stabilna, ale i wydajniejsza.", "tokens": [51042, 12034, 15069, 2838, 13219, 11416, 14215, 11652, 629, 11, 6775, 741, 25984, 1805, 30295, 2394, 13, 51254], "temperature": 0.0, "avg_logprob": -0.11892136660489169, "compression_ratio": 1.3745318352059925, "no_speech_prob": 0.020239325240254402}, {"id": 79, "seek": 30376, "start": 321.56, "end": 326.28, "text": " To jednak druga technika okaza\u0142a si\u0119 prawdziwym, ale to prawdziwym prze\u0142omem.", "tokens": [51254, 1407, 25897, 4110, 64, 1537, 5439, 3133, 12257, 5024, 3244, 41175, 3992, 86, 4199, 11, 6775, 281, 41175, 3992, 86, 4199, 8325, 1221, 298, 443, 13, 51490], "temperature": 0.0, "avg_logprob": -0.11892136660489169, "compression_ratio": 1.3745318352059925, "no_speech_prob": 0.020239325240254402}, {"id": 80, "seek": 30376, "start": 326.28, "end": 330.36, "text": " Embedding Layer Gradient Shrink w skr\u00f3cie EGS.", "tokens": [51490, 24234, 292, 3584, 35166, 16710, 1196, 1160, 81, 475, 261, 1110, 11721, 4260, 462, 24446, 13, 51694], "temperature": 0.0, "avg_logprob": -0.11892136660489169, "compression_ratio": 1.3745318352059925, "no_speech_prob": 0.020239325240254402}, {"id": 81, "seek": 33036, "start": 330.36, "end": 332.40000000000003, "text": " Dobrze, to musimy to rozpakowa\u0107.", "tokens": [50364, 29679, 13503, 11, 281, 43449, 281, 9544, 45944, 11445, 13, 50466], "temperature": 0.0, "avg_logprob": -0.10285518480383832, "compression_ratio": 1.444794952681388, "no_speech_prob": 0.025407841429114342}, {"id": 82, "seek": 33036, "start": 332.40000000000003, "end": 339.16, "text": " Zauwa\u017cyli, \u017ce g\u0142\u00f3wnym \u017ar\u00f3d\u0142em tych eksplozji w treningu s\u0105 nienormalne gradienty.", "tokens": [50466, 1176, 1459, 4151, 7735, 2081, 11, 3561, 18117, 812, 895, 4199, 50212, 43678, 11126, 15180, 30724, 564, 15151, 4013, 261, 2192, 773, 84, 9015, 297, 1053, 24440, 716, 16235, 88, 13, 50804], "temperature": 0.0, "avg_logprob": -0.10285518480383832, "compression_ratio": 1.444794952681388, "no_speech_prob": 0.025407841429114342}, {"id": 83, "seek": 33036, "start": 339.16, "end": 344.64, "text": " Uproszczaj\u0105c, gradienty to takie sygna\u0142y b\u0142\u0119du, kt\u00f3re m\u00f3wi\u0105 modelowi, jak ma si\u0119 poprawi\u0107.", "tokens": [50804, 624, 1424, 329, 43771, 38757, 11, 16235, 88, 281, 15963, 943, 70, 629, 6825, 272, 46564, 769, 11, 8864, 46591, 2316, 24503, 11, 4207, 463, 3244, 1665, 5131, 12757, 13, 51078], "temperature": 0.0, "avg_logprob": -0.10285518480383832, "compression_ratio": 1.444794952681388, "no_speech_prob": 0.025407841429114342}, {"id": 84, "seek": 33036, "start": 344.64, "end": 348.08000000000004, "text": " W tym przypadku te sygna\u0142y by\u0142y tak gwa\u0142towne i chaotyczne,", "tokens": [51078, 343, 8107, 41955, 535, 943, 70, 629, 6825, 26366, 991, 290, 44603, 30401, 68, 741, 6294, 6737, 38491, 11, 51250], "temperature": 0.0, "avg_logprob": -0.10285518480383832, "compression_ratio": 1.444794952681388, "no_speech_prob": 0.025407841429114342}, {"id": 85, "seek": 33036, "start": 348.08000000000004, "end": 352.96000000000004, "text": " \u017ce zamiast korygowa\u0107 kurs modelu, po prostu go wykoleja\u0142y.", "tokens": [51250, 3561, 710, 4526, 525, 350, 827, 70, 11445, 350, 2156, 2316, 84, 11, 714, 19518, 352, 4628, 4093, 306, 2938, 6825, 13, 51494], "temperature": 0.0, "avg_logprob": -0.10285518480383832, "compression_ratio": 1.444794952681388, "no_speech_prob": 0.025407841429114342}, {"id": 86, "seek": 33036, "start": 352.96000000000004, "end": 358.6, "text": " I co ciekawe, dzia\u0142o si\u0119 to g\u0142\u00f3wnie w tej pierwszej wej\u015bciowej warstwie modelu, czyli w Embedding Layer.", "tokens": [51494, 286, 598, 30596, 2330, 826, 11, 9758, 654, 5249, 3244, 281, 18117, 812, 14215, 261, 12573, 27623, 16920, 321, 73, 6199, 21091, 1516, 372, 8699, 2316, 84, 11, 16591, 261, 24234, 292, 3584, 35166, 13, 51776], "temperature": 0.0, "avg_logprob": -0.10285518480383832, "compression_ratio": 1.444794952681388, "no_speech_prob": 0.025407841429114342}, {"id": 87, "seek": 35860, "start": 358.68, "end": 362.52000000000004, "text": " Tak, to to warstwa aktowa zamienia s\u0142owa na wektory liczbowe.", "tokens": [50368, 9118, 11, 281, 281, 1516, 372, 4151, 13680, 5528, 19876, 18811, 15116, 5528, 1667, 321, 2320, 827, 6169, 89, 65, 6880, 13, 50560], "temperature": 0.0, "avg_logprob": -0.13778656798523742, "compression_ratio": 1.4142394822006472, "no_speech_prob": 0.000738719361834228}, {"id": 88, "seek": 35860, "start": 362.52000000000004, "end": 367.08000000000004, "text": " Jest absolutnie kluczowa i musi si\u0119 uczy\u0107 razem z reszt\u0105 modelu,", "tokens": [50560, 24918, 18757, 2766, 9671, 1311, 89, 5528, 741, 37587, 3244, 344, 33967, 40225, 710, 725, 2682, 1611, 2316, 84, 11, 50788], "temperature": 0.0, "avg_logprob": -0.13778656798523742, "compression_ratio": 1.4142394822006472, "no_speech_prob": 0.000738719361834228}, {"id": 89, "seek": 35860, "start": 367.08000000000004, "end": 369.64000000000004, "text": " wi\u0119c nie mo\u017cna jej po prostu zamrozi\u0107.", "tokens": [50788, 16677, 2838, 17790, 28924, 714, 19518, 19876, 340, 28496, 13, 50916], "temperature": 0.0, "avg_logprob": -0.13778656798523742, "compression_ratio": 1.4142394822006472, "no_speech_prob": 0.000738719361834228}, {"id": 90, "seek": 35860, "start": 369.64000000000004, "end": 372.20000000000005, "text": " I tu dochodzimy do sedna EGS.", "tokens": [50916, 286, 2604, 9243, 378, 89, 13189, 360, 9643, 629, 462, 24446, 13, 51044], "temperature": 0.0, "avg_logprob": -0.13778656798523742, "compression_ratio": 1.4142394822006472, "no_speech_prob": 0.000738719361834228}, {"id": 91, "seek": 35860, "start": 372.20000000000005, "end": 375.36, "text": " Zamiast walczy\u0107 z tymi gradientami na r\u00f3\u017cne skomplikowane sposoby,", "tokens": [51044, 1176, 4526, 525, 21346, 33967, 710, 1104, 3057, 16235, 4526, 1667, 47760, 1110, 298, 564, 1035, 23066, 20443, 13944, 11, 51202], "temperature": 0.0, "avg_logprob": -0.13778656798523742, "compression_ratio": 1.4142394822006472, "no_speech_prob": 0.000738719361834228}, {"id": 92, "seek": 35860, "start": 375.36, "end": 377.84000000000003, "text": " postanowi\u0107 zrobi\u0107 co\u015b radykalnie prostego.", "tokens": [51202, 2183, 282, 305, 12757, 31785, 19241, 367, 880, 19990, 2766, 10293, 6308, 13, 51326], "temperature": 0.0, "avg_logprob": -0.13778656798523742, "compression_ratio": 1.4142394822006472, "no_speech_prob": 0.000738719361834228}, {"id": 93, "seek": 35860, "start": 377.84000000000003, "end": 384.0, "text": " Chwila, czyli oni po prostu zmniejszyli si\u0142\u0119 tych sygna\u0142\u00f3w o 90%.", "tokens": [51326, 761, 86, 7371, 11, 16591, 36317, 714, 19518, 17020, 10402, 7706, 2081, 1511, 46564, 15180, 943, 70, 629, 1221, 3901, 277, 4289, 6856, 51634], "temperature": 0.0, "avg_logprob": -0.13778656798523742, "compression_ratio": 1.4142394822006472, "no_speech_prob": 0.000738719361834228}, {"id": 94, "seek": 35860, "start": 384.0, "end": 388.40000000000003, "text": " W pracy podaj\u0105 wsp\u00f3\u0142czynnik alpha r\u00f3wny 0,1.", "tokens": [51634, 343, 35591, 2497, 11133, 39069, 6522, 77, 13123, 8961, 367, 3901, 1634, 1958, 11, 16, 13, 51854], "temperature": 0.0, "avg_logprob": -0.13778656798523742, "compression_ratio": 1.4142394822006472, "no_speech_prob": 0.000738719361834228}, {"id": 95, "seek": 38840, "start": 388.4, "end": 391.23999999999995, "text": " To brzmi prawie zbyt prosto, \u017ceby by\u0142o prawdziwe.", "tokens": [50364, 1407, 738, 89, 3057, 3206, 8699, 710, 2322, 83, 10293, 78, 11, 11316, 14811, 41175, 3992, 826, 13, 50506], "temperature": 0.0, "avg_logprob": -0.09915041636271649, "compression_ratio": 1.4733542319749215, "no_speech_prob": 0.002707981737330556}, {"id": 96, "seek": 38840, "start": 391.23999999999995, "end": 396.0, "text": " Czy to nie os\u0142abi\u0142o zdolno\u015bci uczenia si\u0119 tej pierwszej kluczowej warstwy?", "tokens": [50506, 19832, 281, 2838, 3003, 1221, 18884, 5249, 16221, 401, 16438, 344, 38517, 3244, 12573, 27623, 16920, 9671, 1311, 89, 21091, 1516, 372, 9726, 30, 50744], "temperature": 0.0, "avg_logprob": -0.09915041636271649, "compression_ratio": 1.4733542319749215, "no_speech_prob": 0.002707981737330556}, {"id": 97, "seek": 38840, "start": 396.0, "end": 400.59999999999997, "text": " To jest doskona\u0142y pytanie i w\u0142a\u015bnie tutkwigeniusz tego rozwi\u0105zania.", "tokens": [50744, 1407, 3492, 4491, 74, 4037, 6825, 36610, 741, 14234, 3672, 74, 86, 3213, 4872, 89, 8627, 9544, 22620, 5609, 13, 50974], "temperature": 0.0, "avg_logprob": -0.09915041636271649, "compression_ratio": 1.4733542319749215, "no_speech_prob": 0.002707981737330556}, {"id": 98, "seek": 38840, "start": 400.59999999999997, "end": 403.59999999999997, "text": " Okazuje si\u0119, \u017ce te gradienty by\u0142y tak przesterowane,", "tokens": [50974, 3477, 43317, 3244, 11, 3561, 535, 16235, 88, 26366, 991, 6541, 3011, 23066, 11, 51124], "temperature": 0.0, "avg_logprob": -0.09915041636271649, "compression_ratio": 1.4733542319749215, "no_speech_prob": 0.002707981737330556}, {"id": 99, "seek": 38840, "start": 403.59999999999997, "end": 410.23999999999995, "text": " \u017ce nawet po \u015bciszeniu ich o 90% wci\u0105\u017c by\u0142y wystarczaj\u0105co silne, by efektywnie uczy\u0107 warstw\u0119.", "tokens": [51124, 3561, 22696, 714, 8299, 26720, 39651, 1893, 277, 4289, 4, 261, 537, 27242, 26366, 4628, 9710, 3689, 11133, 1291, 3425, 716, 11, 538, 31482, 916, 874, 14215, 344, 33967, 1516, 372, 86, 1274, 13, 51456], "temperature": 0.0, "avg_logprob": -0.09915041636271649, "compression_ratio": 1.4733542319749215, "no_speech_prob": 0.002707981737330556}, {"id": 100, "seek": 38840, "start": 410.23999999999995, "end": 413.67999999999995, "text": " Ale ju\u017c nie na tyle pot\u0119\u017cne, by wysadzi\u0107 w powietrze ca\u0142y trening.", "tokens": [51456, 9366, 10678, 2838, 1667, 39293, 1847, 1274, 1427, 716, 11, 538, 27062, 345, 28496, 261, 3388, 1684, 13503, 35226, 2192, 773, 13, 51628], "temperature": 0.0, "avg_logprob": -0.09915041636271649, "compression_ratio": 1.4733542319749215, "no_speech_prob": 0.002707981737330556}, {"id": 101, "seek": 38840, "start": 413.67999999999995, "end": 415.08, "text": " Znale\u017ali z\u0142oty \u015brodek.", "tokens": [51628, 1176, 77, 1220, 10659, 2081, 31614, 6737, 28580, 916, 13, 51698], "temperature": 0.0, "avg_logprob": -0.09915041636271649, "compression_ratio": 1.4733542319749215, "no_speech_prob": 0.002707981737330556}, {"id": 102, "seek": 38840, "start": 415.08, "end": 416.28, "text": " Niesamowite.", "tokens": [51698, 426, 530, 335, 305, 642, 13, 51758], "temperature": 0.0, "avg_logprob": -0.09915041636271649, "compression_ratio": 1.4733542319749215, "no_speech_prob": 0.002707981737330556}, {"id": 103, "seek": 41628, "start": 416.28, "end": 420.79999999999995, "text": " A i to jest w\u0142a\u015bnie ten rodzaj wiedzy, kt\u00f3ry rzadko przebija si\u0119 do medi\u00f3w.", "tokens": [50364, 316, 741, 281, 3492, 14234, 2064, 28607, 1805, 46894, 1229, 11, 9913, 367, 89, 345, 4093, 8325, 65, 20642, 3244, 360, 17269, 3901, 13, 50590], "temperature": 0.0, "avg_logprob": -0.08118878641436177, "compression_ratio": 1.4516129032258065, "no_speech_prob": 0.01107187382876873}, {"id": 104, "seek": 41628, "start": 420.79999999999995, "end": 424.84, "text": " Wszyscy m\u00f3wi\u0105 o miliardach parametr\u00f3w terabajtach danych.", "tokens": [50590, 343, 15453, 38966, 46591, 277, 1962, 72, 515, 608, 6220, 27965, 3901, 1796, 455, 1805, 83, 608, 274, 34644, 13, 50792], "temperature": 0.0, "avg_logprob": -0.08118878641436177, "compression_ratio": 1.4516129032258065, "no_speech_prob": 0.01107187382876873}, {"id": 105, "seek": 41628, "start": 424.84, "end": 428.28, "text": " A tu kluczem do sukcesu okaza\u0142o si\u0119 jedno,", "tokens": [50792, 316, 2604, 9671, 1311, 24313, 360, 46432, 887, 84, 3133, 12257, 5249, 3244, 5232, 1771, 11, 50964], "temperature": 0.0, "avg_logprob": -0.08118878641436177, "compression_ratio": 1.4516129032258065, "no_speech_prob": 0.01107187382876873}, {"id": 106, "seek": 41628, "start": 428.28, "end": 431.4, "text": " niemal chirurgiczne ci\u0119cie w matematyce modelu.", "tokens": [50964, 2838, 5579, 23782, 5476, 17946, 716, 35484, 4260, 261, 3803, 8615, 88, 384, 2316, 84, 13, 51120], "temperature": 0.0, "avg_logprob": -0.08118878641436177, "compression_ratio": 1.4516129032258065, "no_speech_prob": 0.01107187382876873}, {"id": 107, "seek": 41628, "start": 431.4, "end": 437.55999999999995, "text": " To pokazuje, \u017ce era brutalnego skalowania mo\u017ce powoli ust\u0119powa\u0107 miejsca erze sprytnej in\u017cynierii.", "tokens": [51120, 1407, 13010, 43317, 11, 3561, 4249, 17878, 11858, 16890, 21308, 12034, 3388, 9384, 26189, 18085, 11445, 18522, 44239, 1189, 1381, 637, 627, 83, 11794, 294, 1427, 2534, 811, 5597, 13, 51428], "temperature": 0.0, "avg_logprob": -0.08118878641436177, "compression_ratio": 1.4516129032258065, "no_speech_prob": 0.01107187382876873}, {"id": 108, "seek": 41628, "start": 437.55999999999995, "end": 440.03999999999996, "text": " Niesamowite, \u017ce taki prosty trick rozwi\u0105za\u0142 problem,", "tokens": [51428, 426, 530, 335, 305, 642, 11, 3561, 20065, 10293, 88, 4282, 9544, 18234, 2394, 1221, 1154, 11, 51552], "temperature": 0.0, "avg_logprob": -0.08118878641436177, "compression_ratio": 1.4516129032258065, "no_speech_prob": 0.01107187382876873}, {"id": 109, "seek": 41628, "start": 440.03999999999996, "end": 442.88, "text": " z kt\u00f3rym borykali si\u0119 najwi\u0119kszy gracze na \u015bwiecie.", "tokens": [51552, 710, 30120, 272, 827, 74, 5103, 3244, 48636, 1694, 1229, 11625, 1381, 1667, 40078, 4260, 13, 51694], "temperature": 0.0, "avg_logprob": -0.08118878641436177, "compression_ratio": 1.4516129032258065, "no_speech_prob": 0.01107187382876873}, {"id": 110, "seek": 44288, "start": 442.88, "end": 448.32, "text": " Ale ca\u0142a ta in\u017cynierina ekwilibrystyka by\u0142aby na nic, gdyby model na ko\u0144cu okaza\u0142 si\u0119 przeci\u0119tny.", "tokens": [50364, 9366, 1335, 5024, 1846, 294, 1427, 2534, 811, 1426, 13359, 86, 388, 897, 627, 25134, 2330, 16673, 2509, 1667, 6201, 11, 28405, 2322, 2316, 1667, 26470, 12032, 3133, 12257, 1221, 3244, 39622, 46788, 1634, 13, 50636], "temperature": 0.0, "avg_logprob": -0.13004461411506898, "compression_ratio": 1.299625468164794, "no_speech_prob": 0.010312017984688282}, {"id": 111, "seek": 44288, "start": 448.32, "end": 453.8, "text": " Czy to nowa architektura i wymuszona stabilno\u015b\u0107 prze\u0142o\u017cy\u0142y si\u0119 na realn\u0105 przewag\u0119 nad innymi modelami?", "tokens": [50636, 19832, 281, 586, 64, 3912, 642, 2320, 2991, 741, 29764, 301, 13383, 11652, 23293, 8325, 5249, 7735, 6825, 3244, 1667, 957, 13113, 39758, 40748, 12617, 294, 31813, 2316, 4526, 30, 50910], "temperature": 0.0, "avg_logprob": -0.13004461411506898, "compression_ratio": 1.299625468164794, "no_speech_prob": 0.010312017984688282}, {"id": 112, "seek": 44288, "start": 453.8, "end": 454.71999999999997, "text": " I to jak?", "tokens": [50910, 286, 281, 4207, 30, 50956], "temperature": 0.0, "avg_logprob": -0.13004461411506898, "compression_ratio": 1.299625468164794, "no_speech_prob": 0.010312017984688282}, {"id": 113, "seek": 44288, "start": 454.71999999999997, "end": 457.2, "text": " Wyniki s\u0105 naprawd\u0119 imponuj\u0105ce.", "tokens": [50956, 343, 2534, 9850, 9015, 20970, 704, 266, 13263, 384, 13, 51080], "temperature": 0.0, "avg_logprob": -0.13004461411506898, "compression_ratio": 1.299625468164794, "no_speech_prob": 0.010312017984688282}, {"id": 114, "seek": 44288, "start": 457.2, "end": 460.6, "text": " GLM 130B mimo, \u017ce jest mniejszy,", "tokens": [51080, 16225, 44, 19966, 33, 275, 6934, 11, 3561, 3492, 39513, 7706, 11, 51250], "temperature": 0.0, "avg_logprob": -0.13004461411506898, "compression_ratio": 1.299625468164794, "no_speech_prob": 0.010312017984688282}, {"id": 115, "seek": 44288, "start": 460.6, "end": 466.28, "text": " w wielu kluczowych benchmarkach deklasuje GPT-3 175B.", "tokens": [51250, 261, 40437, 9671, 1311, 89, 19605, 18927, 608, 368, 74, 7743, 13008, 26039, 51, 12, 18, 41165, 33, 13, 51534], "temperature": 0.0, "avg_logprob": -0.13004461411506898, "compression_ratio": 1.299625468164794, "no_speech_prob": 0.010312017984688282}, {"id": 116, "seek": 46628, "start": 466.32, "end": 468.44, "text": " Na przyk\u0142ad w te\u015bcie Lambada,", "tokens": [50366, 6056, 23144, 261, 535, 9815, 19302, 1538, 11, 50472], "temperature": 0.0, "avg_logprob": -0.15941573513878715, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.1668238639831543}, {"id": 117, "seek": 46628, "start": 468.44, "end": 474.35999999999996, "text": " kt\u00f3ry mierzy zdolno\u015b\u0107 modelu do przewidzenia ostatniego s\u0142owa w d\u0142ugim, skomplikowanym akapicie,", "tokens": [50472, 9913, 47448, 1229, 16221, 401, 23293, 2316, 84, 360, 39758, 327, 14320, 32686, 2766, 1571, 15116, 5528, 261, 274, 34077, 332, 11, 1110, 298, 564, 1035, 23341, 76, 9308, 569, 28434, 11, 50768], "temperature": 0.0, "avg_logprob": -0.15941573513878715, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.1668238639831543}, {"id": 118, "seek": 46628, "start": 474.35999999999996, "end": 477.88, "text": " osi\u0105gn\u0105\u0142 dok\u0142adno\u015b\u0107 83,2%.", "tokens": [50768, 3003, 11404, 4568, 1611, 1221, 45864, 23293, 30997, 11, 17, 6856, 50944], "temperature": 0.0, "avg_logprob": -0.15941573513878715, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.1668238639831543}, {"id": 119, "seek": 46628, "start": 477.88, "end": 479.52, "text": " Co by\u0142o wtedy rekordem?", "tokens": [50944, 3066, 14811, 26959, 33881, 765, 443, 30, 51026], "temperature": 0.0, "avg_logprob": -0.15941573513878715, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.1668238639831543}, {"id": 120, "seek": 46628, "start": 479.52, "end": 483.35999999999996, "text": " A, w momencie publikacji pracy, to by\u0142 rekord.", "tokens": [51026, 316, 11, 261, 40883, 11227, 1035, 13152, 35591, 11, 281, 16673, 33881, 765, 13, 51218], "temperature": 0.0, "avg_logprob": -0.15941573513878715, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.1668238639831543}, {"id": 121, "seek": 46628, "start": 483.35999999999996, "end": 484.79999999999995, "text": " Ale to nie wszystko.", "tokens": [51218, 9366, 281, 2838, 22607, 13, 51290], "temperature": 0.0, "avg_logprob": -0.15941573513878715, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.1668238639831543}, {"id": 122, "seek": 46628, "start": 484.79999999999995, "end": 489.47999999999996, "text": " Z tego, co czyta\u0142am, poradzi\u0142 sobie te\u017c z du\u017co wi\u0119kszymi przeciwnikami.", "tokens": [51290, 1176, 8627, 11, 598, 6430, 1328, 20177, 11, 1515, 345, 3992, 1221, 13652, 9516, 710, 26673, 29968, 1229, 3057, 39622, 895, 1035, 4526, 13, 51524], "temperature": 0.0, "avg_logprob": -0.15941573513878715, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.1668238639831543}, {"id": 123, "seek": 46628, "start": 489.47999999999996, "end": 491.15999999999997, "text": " Zdecydowanie.", "tokens": [51524, 1176, 1479, 1344, 67, 22028, 13, 51608], "temperature": 0.0, "avg_logprob": -0.15941573513878715, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.1668238639831543}, {"id": 124, "seek": 46628, "start": 491.15999999999997, "end": 495.84, "text": " W zadaniach typu Zero Shot na zbiorze Big Bench Lite.", "tokens": [51608, 343, 42788, 3782, 608, 2125, 84, 17182, 28845, 1667, 710, 33362, 1381, 5429, 3964, 339, 32986, 13, 51842], "temperature": 0.0, "avg_logprob": -0.15941573513878715, "compression_ratio": 1.3728813559322033, "no_speech_prob": 0.1668238639831543}, {"id": 125, "seek": 49584, "start": 496.47999999999996, "end": 501.2, "text": " Czyli tam, gdzie model musi rozwi\u0105za\u0107 problem bez \u017cadnych wcze\u015bniejszych przyk\u0142ad\u00f3w.", "tokens": [50396, 37099, 7677, 11, 18922, 2316, 37587, 9544, 18234, 35873, 1154, 10782, 39628, 9399, 40785, 45021, 23144, 3901, 13, 50632], "temperature": 0.0, "avg_logprob": -0.13722179554126882, "compression_ratio": 1.3680555555555556, "no_speech_prob": 0.003031857078894973}, {"id": 126, "seek": 49584, "start": 501.2, "end": 502.4, "text": " Dok\u0142adnie.", "tokens": [50632, 29768, 10358, 2766, 13, 50692], "temperature": 0.0, "avg_logprob": -0.13722179554126882, "compression_ratio": 1.3680555555555556, "no_speech_prob": 0.003031857078894973}, {"id": 127, "seek": 49584, "start": 502.4, "end": 508.35999999999996, "text": " Okaza\u0142 si\u0119 lepszy od czterokrotnie wi\u0119kszego modelu Palm 540B od Google.", "tokens": [50692, 3477, 12257, 1221, 3244, 476, 1878, 1229, 3611, 6472, 391, 453, 10536, 2766, 29968, 27725, 2316, 84, 32668, 1025, 5254, 33, 3611, 3329, 13, 50990], "temperature": 0.0, "avg_logprob": -0.13722179554126882, "compression_ratio": 1.3680555555555556, "no_speech_prob": 0.003031857078894973}, {"id": 128, "seek": 49584, "start": 508.35999999999996, "end": 510.76, "text": " A jako, \u017ce jest to model dwuj\u0119zyczny,", "tokens": [50990, 316, 17123, 11, 3561, 3492, 281, 2316, 27379, 18258, 1229, 3689, 1634, 11, 51110], "temperature": 0.0, "avg_logprob": -0.13722179554126882, "compression_ratio": 1.3680555555555556, "no_speech_prob": 0.003031857078894973}, {"id": 129, "seek": 49584, "start": 510.76, "end": 515.6, "text": " to warto doda\u0107, \u017ce znacz\u0105co przewy\u017cszy\u0142 te\u017c najwi\u0119kszy do tej porychi\u0144ski model,", "tokens": [51110, 281, 31830, 360, 2675, 2162, 11, 3561, 15397, 326, 8925, 1291, 39758, 88, 1427, 7706, 1221, 9516, 48636, 1694, 1229, 360, 12573, 280, 827, 8036, 5248, 18020, 2316, 11, 51352], "temperature": 0.0, "avg_logprob": -0.13722179554126882, "compression_ratio": 1.3680555555555556, "no_speech_prob": 0.003031857078894973}, {"id": 130, "seek": 49584, "start": 515.6, "end": 520.0799999999999, "text": " Erni Tytan 3.0, kt\u00f3ry ma przecie\u017c dwa razy wi\u0119cej parametr\u00f3w.", "tokens": [51352, 3300, 3722, 314, 4328, 282, 805, 13, 15, 11, 9913, 463, 8325, 40082, 35045, 9639, 88, 26004, 6220, 27965, 3901, 13, 51576], "temperature": 0.0, "avg_logprob": -0.13722179554126882, "compression_ratio": 1.3680555555555556, "no_speech_prob": 0.003031857078894973}, {"id": 131, "seek": 49584, "start": 520.0799999999999, "end": 522.04, "text": " 260 miliard\u00f3w?", "tokens": [51576, 44624, 1962, 72, 515, 3901, 30, 51674], "temperature": 0.0, "avg_logprob": -0.13722179554126882, "compression_ratio": 1.3680555555555556, "no_speech_prob": 0.003031857078894973}, {"id": 132, "seek": 49584, "start": 522.04, "end": 522.68, "text": " Tak.", "tokens": [51674, 9118, 13, 51706], "temperature": 0.0, "avg_logprob": -0.13722179554126882, "compression_ratio": 1.3680555555555556, "no_speech_prob": 0.003031857078894973}, {"id": 133, "seek": 52268, "start": 522.68, "end": 526.3599999999999, "text": " Czyli jest wydajniejszy, inteligentniejszy.", "tokens": [50364, 37099, 3492, 25984, 1805, 10402, 7706, 11, 24777, 25002, 10402, 7706, 13, 50548], "temperature": 0.0, "avg_logprob": -0.11526960092824656, "compression_ratio": 1.443298969072165, "no_speech_prob": 0.020584244281053543}, {"id": 134, "seek": 52268, "start": 526.3599999999999, "end": 530.52, "text": " Ale w pracy jest jeszcze jeden wynik, kt\u00f3ry wydaje si\u0119 niemal przypadkowym,", "tokens": [50548, 9366, 261, 35591, 3492, 14168, 12906, 31936, 1035, 11, 9913, 49165, 3244, 2838, 5579, 33100, 74, 31691, 11, 50756], "temperature": 0.0, "avg_logprob": -0.11526960092824656, "compression_ratio": 1.443298969072165, "no_speech_prob": 0.020584244281053543}, {"id": 135, "seek": 52268, "start": 530.52, "end": 533.2399999999999, "text": " ale potencjalnie rewolucyjnym odkryciem.", "tokens": [50756, 6775, 1847, 22660, 22600, 2766, 319, 48481, 1311, 88, 73, 12996, 3611, 43298, 4260, 76, 13, 50892], "temperature": 0.0, "avg_logprob": -0.11526960092824656, "compression_ratio": 1.443298969072165, "no_speech_prob": 0.020584244281053543}, {"id": 136, "seek": 52268, "start": 533.2399999999999, "end": 535.0, "text": " Chodzi o analizy etyczne.", "tokens": [50892, 761, 14543, 277, 2624, 590, 88, 1030, 17466, 716, 13, 50980], "temperature": 0.0, "avg_logprob": -0.11526960092824656, "compression_ratio": 1.443298969072165, "no_speech_prob": 0.020584244281053543}, {"id": 137, "seek": 52268, "start": 535.0, "end": 538.76, "text": " Ejtorzy sprawdzili model pod ko\u0144cem generowania tre\u015bci toksycznych", "tokens": [50980, 462, 73, 21151, 1229, 46192, 89, 2312, 2316, 2497, 26470, 26422, 1337, 21308, 2192, 6199, 281, 1694, 17466, 9399, 51168], "temperature": 0.0, "avg_logprob": -0.11526960092824656, "compression_ratio": 1.443298969072165, "no_speech_prob": 0.020584244281053543}, {"id": 138, "seek": 52268, "start": 538.76, "end": 542.28, "text": " oraz powielania stereotyp\u00f3w, czyli tak zwanego bias.", "tokens": [51168, 28905, 3388, 1187, 5609, 41182, 79, 3901, 11, 16591, 991, 710, 7916, 6308, 12577, 13, 51344], "temperature": 0.0, "avg_logprob": -0.11526960092824656, "compression_ratio": 1.443298969072165, "no_speech_prob": 0.020584244281053543}, {"id": 139, "seek": 52268, "start": 542.28, "end": 544.2399999999999, "text": " I wyniki by\u0142y zaskakuj\u0105ce.", "tokens": [51344, 286, 31936, 9850, 26366, 710, 3863, 514, 13263, 384, 13, 51442], "temperature": 0.0, "avg_logprob": -0.11526960092824656, "compression_ratio": 1.443298969072165, "no_speech_prob": 0.020584244281053543}, {"id": 140, "seek": 52268, "start": 544.2399999999999, "end": 550.1999999999999, "text": " Okaza\u0142o si\u0119, \u017ce GLM 130B generuje tre\u015bci o znacznie mniejszej toksyczno\u015bci", "tokens": [51442, 3477, 12257, 5249, 3244, 11, 3561, 16225, 44, 19966, 33, 1337, 13008, 2192, 6199, 277, 15397, 14875, 2766, 275, 30295, 16920, 281, 1694, 17466, 16438, 51740], "temperature": 0.0, "avg_logprob": -0.11526960092824656, "compression_ratio": 1.443298969072165, "no_speech_prob": 0.020584244281053543}, {"id": 141, "seek": 55020, "start": 550.2, "end": 553.88, "text": " i wykazuje mniej uprzedze\u0144 ni\u017c jego odpowiednicy trenowane wy\u0142\u0105cznie", "tokens": [50364, 741, 39287, 43317, 39513, 493, 81, 11312, 49689, 28502, 26542, 36574, 77, 2632, 23136, 23066, 4628, 15926, 19923, 50548], "temperature": 0.0, "avg_logprob": -0.1308076346098487, "compression_ratio": 1.3465703971119134, "no_speech_prob": 0.00443490082398057}, {"id": 142, "seek": 55020, "start": 553.88, "end": 555.44, "text": " na danych angloj\u0119zycznych.", "tokens": [50548, 1667, 274, 34644, 2562, 752, 11115, 1229, 3689, 9399, 13, 50626], "temperature": 0.0, "avg_logprob": -0.1308076346098487, "compression_ratio": 1.3465703971119134, "no_speech_prob": 0.00443490082398057}, {"id": 143, "seek": 55020, "start": 555.44, "end": 559.48, "text": " Jak np. GPT-3 czy OPT-175B.", "tokens": [50626, 15029, 33808, 13, 26039, 51, 12, 18, 6430, 23324, 51, 12, 7773, 20, 33, 13, 50828], "temperature": 0.0, "avg_logprob": -0.1308076346098487, "compression_ratio": 1.3465703971119134, "no_speech_prob": 0.00443490082398057}, {"id": 144, "seek": 55020, "start": 559.48, "end": 560.6800000000001, "text": " Dok\u0142adnie tak.", "tokens": [50828, 29768, 10358, 2766, 991, 13, 50888], "temperature": 0.0, "avg_logprob": -0.1308076346098487, "compression_ratio": 1.3465703971119134, "no_speech_prob": 0.00443490082398057}, {"id": 145, "seek": 55020, "start": 560.6800000000001, "end": 562.84, "text": " Zatrzymajmy si\u0119 przy tym na chwil\u0119.", "tokens": [50888, 1176, 267, 13047, 1696, 73, 2226, 3244, 6501, 8107, 1667, 41941, 1274, 13, 50996], "temperature": 0.0, "avg_logprob": -0.1308076346098487, "compression_ratio": 1.3465703971119134, "no_speech_prob": 0.00443490082398057}, {"id": 146, "seek": 55020, "start": 562.84, "end": 566.8000000000001, "text": " To jest potencjalnie ogromne odkrycie.", "tokens": [50996, 1407, 3492, 1847, 22660, 22600, 2766, 34416, 298, 716, 3611, 43298, 4260, 13, 51194], "temperature": 0.0, "avg_logprob": -0.1308076346098487, "compression_ratio": 1.3465703971119134, "no_speech_prob": 0.00443490082398057}, {"id": 147, "seek": 55020, "start": 566.8000000000001, "end": 571.6400000000001, "text": " To sugeruje, \u017ce problem toksycznej AI mo\u017ce nie by\u0107 tylko kwesti\u0105", "tokens": [51194, 1407, 459, 1321, 13008, 11, 3561, 1154, 281, 1694, 17466, 11794, 7318, 12034, 2838, 15069, 13219, 42035, 11404, 51436], "temperature": 0.0, "avg_logprob": -0.1308076346098487, "compression_ratio": 1.3465703971119134, "no_speech_prob": 0.00443490082398057}, {"id": 148, "seek": 55020, "start": 571.6400000000001, "end": 574.1600000000001, "text": " lepszego filtrowania danych treningowych,", "tokens": [51436, 476, 1878, 27725, 1387, 6903, 21308, 274, 34644, 2192, 773, 19605, 11, 51562], "temperature": 0.0, "avg_logprob": -0.1308076346098487, "compression_ratio": 1.3465703971119134, "no_speech_prob": 0.00443490082398057}, {"id": 149, "seek": 55020, "start": 574.1600000000001, "end": 578.44, "text": " ale fundamentalnie kwesti\u0105 perspektywy.", "tokens": [51562, 6775, 8088, 2766, 42035, 11404, 868, 32659, 874, 9726, 13, 51776], "temperature": 0.0, "avg_logprob": -0.1308076346098487, "compression_ratio": 1.3465703971119134, "no_speech_prob": 0.00443490082398057}, {"id": 150, "seek": 57844, "start": 578.48, "end": 583.12, "text": " \u017be model, kt\u00f3ry od samego pocz\u0105tku uczy si\u0119 widzie\u0107 \u015bwiat przez pryzmat", "tokens": [50366, 46864, 2316, 11, 9913, 3611, 912, 1571, 43959, 344, 6522, 3244, 5274, 21214, 36425, 14064, 582, 37433, 15677, 50598], "temperature": 0.0, "avg_logprob": -0.1218251795382113, "compression_ratio": 1.4865771812080537, "no_speech_prob": 0.02495930716395378}, {"id": 151, "seek": 57844, "start": 583.12, "end": 585.7600000000001, "text": " wi\u0119cej ni\u017c jednej kultury i j\u0119zyka,", "tokens": [50598, 26004, 28502, 5232, 11794, 350, 723, 2598, 741, 42309, 40940, 11, 50730], "temperature": 0.0, "avg_logprob": -0.1218251795382113, "compression_ratio": 1.4865771812080537, "no_speech_prob": 0.02495930716395378}, {"id": 152, "seek": 57844, "start": 585.7600000000001, "end": 589.5600000000001, "text": " staje si\u0119 niejako bardziej obyty i mniej podatny", "tokens": [50730, 342, 11153, 3244, 2838, 73, 18501, 27209, 1111, 41944, 741, 39513, 2497, 267, 1634, 50920], "temperature": 0.0, "avg_logprob": -0.1218251795382113, "compression_ratio": 1.4865771812080537, "no_speech_prob": 0.02495930716395378}, {"id": 153, "seek": 57844, "start": 589.5600000000001, "end": 592.6400000000001, "text": " na skrajno\u015bci obecne w pojedynczym \u017ar\u00f3dze danych.", "tokens": [50920, 1667, 1110, 48690, 16438, 49141, 716, 261, 714, 40543, 2534, 6522, 76, 50212, 43678, 1381, 274, 34644, 13, 51074], "temperature": 0.0, "avg_logprob": -0.1218251795382113, "compression_ratio": 1.4865771812080537, "no_speech_prob": 0.02495930716395378}, {"id": 154, "seek": 57844, "start": 592.6400000000001, "end": 595.2, "text": " To jest bardzo trafna interpretacja.", "tokens": [51074, 1407, 3492, 9034, 944, 69, 629, 7302, 23395, 13, 51202], "temperature": 0.0, "avg_logprob": -0.1218251795382113, "compression_ratio": 1.4865771812080537, "no_speech_prob": 0.02495930716395378}, {"id": 155, "seek": 57844, "start": 595.2, "end": 598.6, "text": " Dwuj\u0119zyczno\u015b\u0107 nie tylko nie os\u0142abi\u0142a modelu,", "tokens": [51202, 413, 30627, 11115, 1229, 3689, 23293, 2838, 13219, 2838, 3003, 1221, 18884, 5024, 2316, 84, 11, 51372], "temperature": 0.0, "avg_logprob": -0.1218251795382113, "compression_ratio": 1.4865771812080537, "no_speech_prob": 0.02495930716395378}, {"id": 156, "seek": 57844, "start": 598.6, "end": 603.12, "text": " ale mog\u0142a go wr\u0119cz uczyni\u0107 z drobszym i bardziej zr\u00f3wnowa\u017conym.", "tokens": [51372, 6775, 13172, 5024, 352, 928, 1274, 3689, 344, 6522, 3722, 2162, 710, 3789, 929, 26681, 741, 27209, 710, 11721, 895, 5528, 1427, 12732, 13, 51598], "temperature": 0.0, "avg_logprob": -0.1218251795382113, "compression_ratio": 1.4865771812080537, "no_speech_prob": 0.02495930716395378}, {"id": 157, "seek": 57844, "start": 603.12, "end": 607.1600000000001, "text": " Mo\u017cliwe, \u017ce model ucz\u0105c si\u0119 na danych z dw\u00f3ch r\u00f3\u017cnych kultur", "tokens": [51598, 44736, 2081, 826, 11, 3561, 2316, 35403, 1611, 66, 3244, 1667, 274, 34644, 710, 27379, 812, 339, 42602, 350, 26099, 51800], "temperature": 0.0, "avg_logprob": -0.1218251795382113, "compression_ratio": 1.4865771812080537, "no_speech_prob": 0.02495930716395378}, {"id": 158, "seek": 60716, "start": 607.16, "end": 612.28, "text": " w pewien spos\u00f3b u\u015bredni\u0142 kulturowe uprzedzenia obecne w obu zbiorach,", "tokens": [50364, 261, 25889, 1053, 22904, 344, 1788, 986, 3722, 1221, 350, 26099, 6880, 493, 81, 11312, 14320, 49141, 716, 261, 1111, 84, 710, 33362, 608, 11, 50620], "temperature": 0.0, "avg_logprob": -0.11329648464540892, "compression_ratio": 1.4713375796178343, "no_speech_prob": 0.0007160968380048871}, {"id": 159, "seek": 60716, "start": 612.28, "end": 615.68, "text": " co doprowadzi\u0142o do bardziej neutralnych odpowiedzi.", "tokens": [50620, 598, 360, 35019, 3992, 5249, 360, 27209, 10598, 9399, 36574, 3992, 13, 50790], "temperature": 0.0, "avg_logprob": -0.11329648464540892, "compression_ratio": 1.4713375796178343, "no_speech_prob": 0.0007160968380048871}, {"id": 160, "seek": 60716, "start": 615.68, "end": 618.36, "text": " To fascynuj\u0105cy kierunek bada\u0144 na przysz\u0142o\u015b\u0107.", "tokens": [50790, 1407, 30632, 1344, 77, 13263, 1344, 38767, 409, 916, 272, 1538, 5248, 1667, 44018, 44742, 13, 50924], "temperature": 0.0, "avg_logprob": -0.11329648464540892, "compression_ratio": 1.4713375796178343, "no_speech_prob": 0.0007160968380048871}, {"id": 161, "seek": 60716, "start": 618.36, "end": 622.48, "text": " Mamy wi\u0119c model, kt\u00f3ry jest pot\u0119\u017cniejszy, stabilniejszy w treningu", "tokens": [50924, 376, 7804, 16677, 2316, 11, 9913, 3492, 1847, 1274, 1427, 10402, 7706, 11, 11652, 10402, 7706, 261, 2192, 773, 84, 51130], "temperature": 0.0, "avg_logprob": -0.11329648464540892, "compression_ratio": 1.4713375796178343, "no_speech_prob": 0.0007160968380048871}, {"id": 162, "seek": 60716, "start": 622.48, "end": 624.76, "text": " i potencjalnie bardziej etyczny.", "tokens": [51130, 741, 1847, 22660, 22600, 2766, 27209, 1030, 17466, 1634, 13, 51244], "temperature": 0.0, "avg_logprob": -0.11329648464540892, "compression_ratio": 1.4713375796178343, "no_speech_prob": 0.0007160968380048871}, {"id": 163, "seek": 60716, "start": 624.76, "end": 629.24, "text": " Ale tytu\u0142 pracy wspomina o tym, \u017ce jest to model otwarty.", "tokens": [51244, 9366, 1104, 9179, 1221, 35591, 17757, 49217, 277, 8107, 11, 3561, 3492, 281, 2316, 4337, 29587, 88, 13, 51468], "temperature": 0.0, "avg_logprob": -0.11329648464540892, "compression_ratio": 1.4713375796178343, "no_speech_prob": 0.0007160968380048871}, {"id": 164, "seek": 60716, "start": 629.24, "end": 631.36, "text": " W \u015bwiecie wielkich modeli j\u0119zykowych", "tokens": [51468, 343, 40078, 4260, 20570, 48349, 2316, 72, 49055, 74, 19605, 51574], "temperature": 0.0, "avg_logprob": -0.11329648464540892, "compression_ratio": 1.4713375796178343, "no_speech_prob": 0.0007160968380048871}, {"id": 165, "seek": 60716, "start": 631.36, "end": 636.1999999999999, "text": " otwarty zazwyczaj oznacza, \u017ce mo\u017cna sobie co najwy\u017cej poczyta\u0107 prac\u0119 naukow\u0105.", "tokens": [51574, 4337, 29587, 88, 710, 921, 9726, 3689, 1805, 277, 22672, 326, 2394, 11, 3561, 17790, 13652, 598, 11212, 9726, 38493, 714, 6522, 42931, 22404, 1274, 35616, 74, 30297, 13, 51816], "temperature": 0.0, "avg_logprob": -0.11329648464540892, "compression_ratio": 1.4713375796178343, "no_speech_prob": 0.0007160968380048871}, {"id": 166, "seek": 63620, "start": 636.24, "end": 639.8000000000001, "text": " Tutaj wydaje si\u0119, \u017ce to s\u0142owo ma znacznie g\u0142\u0119bsze znaczenie.", "tokens": [50366, 41819, 49165, 3244, 11, 3561, 281, 15116, 19941, 463, 15397, 14875, 2766, 18117, 1274, 929, 1381, 15397, 326, 16778, 13, 50544], "temperature": 0.0, "avg_logprob": -0.11355659289237781, "compression_ratio": 1.3651877133105803, "no_speech_prob": 0.005217071622610092}, {"id": 167, "seek": 63620, "start": 639.8000000000001, "end": 641.1600000000001, "text": " A przesolutnie.", "tokens": [50544, 316, 6541, 279, 2308, 2766, 13, 50612], "temperature": 0.0, "avg_logprob": -0.11355659289237781, "compression_ratio": 1.3651877133105803, "no_speech_prob": 0.005217071622610092}, {"id": 168, "seek": 63620, "start": 641.1600000000001, "end": 647.44, "text": " I tu dochodzimy do prawdziwej rewolucji, jak\u0105 niesie ze sob\u0105 GGLM 130B.", "tokens": [50612, 286, 2604, 9243, 378, 89, 13189, 360, 41175, 3992, 826, 73, 319, 48481, 1311, 4013, 11, 46719, 48100, 414, 5277, 18253, 1611, 460, 19440, 44, 19966, 33, 13, 50926], "temperature": 0.0, "avg_logprob": -0.11355659289237781, "compression_ratio": 1.3651877133105803, "no_speech_prob": 0.005217071622610092}, {"id": 169, "seek": 63620, "start": 647.44, "end": 650.6800000000001, "text": " Chodzi o kwantyzacj\u0119 do INT4.", "tokens": [50926, 761, 14543, 277, 23846, 394, 37433, 29924, 360, 43140, 19, 13, 51088], "temperature": 0.0, "avg_logprob": -0.11355659289237781, "compression_ratio": 1.3651877133105803, "no_speech_prob": 0.005217071622610092}, {"id": 170, "seek": 63620, "start": 650.6800000000001, "end": 655.72, "text": " Wi\u0119kszo\u015b\u0107 du\u017cych modeli z trudem daje si\u0119 skompresowa\u0107 do 8-bitowej precyzji,", "tokens": [51088, 30127, 1694, 4765, 7753, 1581, 7735, 339, 2316, 72, 710, 32007, 443, 1120, 2884, 3244, 1110, 8586, 495, 11445, 360, 1649, 12, 5260, 21091, 659, 1344, 89, 4013, 11, 51340], "temperature": 0.0, "avg_logprob": -0.11355659289237781, "compression_ratio": 1.3651877133105803, "no_speech_prob": 0.005217071622610092}, {"id": 171, "seek": 63620, "start": 655.72, "end": 660.76, "text": " czyli INT8, a i to cz\u0119sto wi\u0105\u017ce si\u0119 z pewn\u0105 utrat\u0105 jako\u015bci.", "tokens": [51340, 16591, 43140, 23, 11, 257, 741, 281, 34369, 261, 11404, 2875, 3244, 710, 47160, 1611, 2839, 4481, 1611, 17123, 6199, 13, 51592], "temperature": 0.0, "avg_logprob": -0.11355659289237781, "compression_ratio": 1.3651877133105803, "no_speech_prob": 0.005217071622610092}, {"id": 172, "seek": 63620, "start": 660.76, "end": 663.4000000000001, "text": " A dalsza kompresja to ju\u017c w og\u00f3le katastrofa.", "tokens": [51592, 316, 274, 1124, 2394, 5207, 14508, 2938, 281, 10678, 261, 29229, 16536, 525, 340, 11771, 13, 51724], "temperature": 0.0, "avg_logprob": -0.11355659289237781, "compression_ratio": 1.3651877133105803, "no_speech_prob": 0.005217071622610092}, {"id": 173, "seek": 63620, "start": 663.4000000000001, "end": 664.96, "text": " Zazwyczaj tak.", "tokens": [51724, 1176, 921, 9726, 3689, 1805, 991, 13, 51802], "temperature": 0.0, "avg_logprob": -0.11355659289237781, "compression_ratio": 1.3651877133105803, "no_speech_prob": 0.005217071622610092}, {"id": 174, "seek": 66496, "start": 664.96, "end": 669.5600000000001, "text": " Tymczasem GGLM 130B dzi\u0119ki swojej unikalnej architekturze", "tokens": [50364, 314, 4199, 30989, 443, 460, 19440, 44, 19966, 33, 45003, 29489, 73, 517, 41216, 11794, 3912, 642, 2320, 374, 1381, 50594], "temperature": 0.0, "avg_logprob": -0.11812942059009106, "compression_ratio": 1.4276094276094276, "no_speech_prob": 0.004277762491255999}, {"id": 175, "seek": 66496, "start": 669.5600000000001, "end": 675.32, "text": " mo\u017ce by\u0107 skwantyzowany do 4-bitowej precyzji praktycznie bez \u017cadnej straty na jako\u015bci.", "tokens": [50594, 12034, 15069, 1110, 86, 394, 37433, 23341, 360, 1017, 12, 5260, 21091, 659, 1344, 89, 4013, 3206, 74, 45586, 10782, 39628, 11794, 1056, 21398, 1667, 17123, 6199, 13, 50882], "temperature": 0.0, "avg_logprob": -0.11812942059009106, "compression_ratio": 1.4276094276094276, "no_speech_prob": 0.004277762491255999}, {"id": 176, "seek": 66496, "start": 675.32, "end": 677.36, "text": " W jednym z benchmark\u00f3w MLLu", "tokens": [50882, 343, 5232, 12996, 710, 18927, 3901, 21601, 43, 84, 50984], "temperature": 0.0, "avg_logprob": -0.11812942059009106, "compression_ratio": 1.4276094276094276, "no_speech_prob": 0.004277762491255999}, {"id": 177, "seek": 66496, "start": 677.36, "end": 680.8000000000001, "text": " jego wynik po kwantyzacji nawet nieznacznie wzrus\u0142.", "tokens": [50984, 26542, 31936, 1035, 714, 23846, 394, 37433, 13152, 22696, 2838, 22672, 14875, 2766, 24809, 13783, 1221, 13, 51156], "temperature": 0.0, "avg_logprob": -0.11812942059009106, "compression_ratio": 1.4276094276094276, "no_speech_prob": 0.004277762491255999}, {"id": 178, "seek": 66496, "start": 680.8000000000001, "end": 682.72, "text": " To jest niewiarygowne.", "tokens": [51156, 1407, 3492, 43622, 29104, 70, 648, 68, 13, 51252], "temperature": 0.0, "avg_logprob": -0.11812942059009106, "compression_ratio": 1.4276094276094276, "no_speech_prob": 0.004277762491255999}, {"id": 179, "seek": 66496, "start": 682.72, "end": 684.2, "text": " Ale dlaczego tak si\u0119 dzieje?", "tokens": [51252, 9366, 37873, 39329, 991, 3244, 17953, 2884, 30, 51326], "temperature": 0.0, "avg_logprob": -0.11812942059009106, "compression_ratio": 1.4276094276094276, "no_speech_prob": 0.004277762491255999}, {"id": 180, "seek": 66496, "start": 684.2, "end": 688.84, "text": " Co jest w tej architekturze takiego, \u017ce pozwala na tak ekstremaln\u0105 kompresj\u0119?", "tokens": [51326, 3066, 3492, 261, 12573, 3912, 642, 2320, 374, 1381, 32296, 11, 3561, 40557, 5159, 1667, 991, 13359, 372, 2579, 304, 13113, 5207, 14508, 11115, 30, 51558], "temperature": 0.0, "avg_logprob": -0.11812942059009106, "compression_ratio": 1.4276094276094276, "no_speech_prob": 0.004277762491255999}, {"id": 181, "seek": 66496, "start": 688.84, "end": 692.84, "text": " W pracy jest fascynuj\u0105cy wykres oznaczony jako rysunek 5.", "tokens": [51558, 343, 35591, 3492, 30632, 1344, 77, 13263, 1344, 39287, 495, 277, 22672, 14875, 2526, 17123, 367, 749, 409, 916, 1025, 13, 51758], "temperature": 0.0, "avg_logprob": -0.11812942059009106, "compression_ratio": 1.4276094276094276, "no_speech_prob": 0.004277762491255999}, {"id": 182, "seek": 69284, "start": 692.88, "end": 696.84, "text": " Pokazuje on rozk\u0142ad wak, czyli wewn\u0119trznych parametr\u00f3w modelu,", "tokens": [50366, 14958, 43317, 322, 9544, 15317, 261, 514, 11, 16591, 321, 895, 1274, 6903, 89, 9399, 6220, 27965, 3901, 2316, 84, 11, 50564], "temperature": 0.0, "avg_logprob": -0.12123256776391006, "compression_ratio": 1.5451388888888888, "no_speech_prob": 0.10771346092224121}, {"id": 183, "seek": 69284, "start": 696.84, "end": 700.6800000000001, "text": " w GGLM w por\u00f3wnaniu do modelu w stylu GPT, jakim jest blum.", "tokens": [50564, 261, 460, 19440, 44, 261, 1515, 812, 895, 25849, 360, 2316, 84, 261, 7952, 2781, 26039, 51, 11, 49410, 3492, 888, 449, 13, 50756], "temperature": 0.0, "avg_logprob": -0.12123256776391006, "compression_ratio": 1.5451388888888888, "no_speech_prob": 0.10771346092224121}, {"id": 184, "seek": 69284, "start": 700.6800000000001, "end": 702.12, "text": " I jaka jest r\u00f3\u017cnica?", "tokens": [50756, 286, 4207, 64, 3492, 19637, 32687, 30, 50828], "temperature": 0.0, "avg_logprob": -0.12123256776391006, "compression_ratio": 1.5451388888888888, "no_speech_prob": 0.10771346092224121}, {"id": 185, "seek": 69284, "start": 702.12, "end": 703.36, "text": " Uderzaj\u0105ca.", "tokens": [50828, 624, 1068, 89, 11133, 496, 13, 50890], "temperature": 0.0, "avg_logprob": -0.12123256776391006, "compression_ratio": 1.5451388888888888, "no_speech_prob": 0.10771346092224121}, {"id": 186, "seek": 69284, "start": 703.36, "end": 706.88, "text": " GGLM ma znacznie w\u0119\u017cszy rozk\u0142ad warto\u015bci.", "tokens": [50890, 460, 19440, 44, 463, 15397, 14875, 2766, 261, 1274, 1427, 7706, 9544, 15317, 31830, 6199, 13, 51066], "temperature": 0.0, "avg_logprob": -0.12123256776391006, "compression_ratio": 1.5451388888888888, "no_speech_prob": 0.10771346092224121}, {"id": 187, "seek": 69284, "start": 706.88, "end": 711.88, "text": " Innymi s\u0142owy jest w nim o wiele mniej ekstremalnych odstaj\u0105cych warto\u015bci,", "tokens": [51066, 682, 31813, 15116, 10089, 3492, 261, 24887, 277, 33137, 39513, 13359, 372, 2579, 304, 9399, 3611, 372, 11133, 31306, 31830, 6199, 11, 51316], "temperature": 0.0, "avg_logprob": -0.12123256776391006, "compression_ratio": 1.5451388888888888, "no_speech_prob": 0.10771346092224121}, {"id": 188, "seek": 69284, "start": 711.88, "end": 714.2, "text": " kt\u00f3re s\u0105 z mor\u0105 procesu kwantyzacji.", "tokens": [51316, 8864, 9015, 710, 1896, 1611, 17565, 84, 23846, 394, 37433, 13152, 13, 51432], "temperature": 0.0, "avg_logprob": -0.12123256776391006, "compression_ratio": 1.5451388888888888, "no_speech_prob": 0.10771346092224121}, {"id": 189, "seek": 69284, "start": 714.2, "end": 720.52, "text": " Aha, czyli mniej ekstrem\u00f3w oznacza, \u017ce proces zaokr\u0105glania wak do mniejszej ligdy bit\u00f3w", "tokens": [51432, 27448, 11, 16591, 39513, 13359, 372, 2579, 3901, 277, 22672, 326, 2394, 11, 3561, 17565, 7949, 453, 32881, 7191, 5609, 261, 514, 360, 275, 30295, 16920, 11742, 3173, 857, 3901, 51748], "temperature": 0.0, "avg_logprob": -0.12123256776391006, "compression_ratio": 1.5451388888888888, "no_speech_prob": 0.10771346092224121}, {"id": 190, "seek": 69284, "start": 720.52, "end": 722.48, "text": " jest o wiele mniej stratny.", "tokens": [51748, 3492, 277, 33137, 39513, 23674, 1634, 13, 51846], "temperature": 0.0, "avg_logprob": -0.12123256776391006, "compression_ratio": 1.5451388888888888, "no_speech_prob": 0.10771346092224121}, {"id": 191, "seek": 72248, "start": 722.48, "end": 724.0, "text": " Dok\u0142adnie.", "tokens": [50364, 29768, 10358, 2766, 13, 50440], "temperature": 0.0, "avg_logprob": -0.13103327880034576, "compression_ratio": 1.3433333333333333, "no_speech_prob": 0.0011910966131836176}, {"id": 192, "seek": 72248, "start": 724.0, "end": 730.4, "text": " To wygl\u0105da na unikaln\u0105, wy\u0142aniaj\u0105c\u0105 si\u0119 w\u0142a\u015bciwo\u015b\u0107 architektury GGLM,", "tokens": [50440, 1407, 32015, 1667, 517, 41216, 13113, 11, 4628, 1221, 5609, 8555, 32557, 3244, 40112, 48847, 3912, 642, 2320, 2598, 460, 19440, 44, 11, 50760], "temperature": 0.0, "avg_logprob": -0.13103327880034576, "compression_ratio": 1.3433333333333333, "no_speech_prob": 0.0011910966131836176}, {"id": 193, "seek": 72248, "start": 730.4, "end": 734.2, "text": " kt\u00f3rej nie zaobserwowano w innych modelach na tak\u0105 skal\u0119.", "tokens": [50760, 36023, 2838, 7949, 16537, 260, 34354, 3730, 261, 36286, 2316, 608, 1667, 31069, 16890, 1274, 13, 50950], "temperature": 0.0, "avg_logprob": -0.13103327880034576, "compression_ratio": 1.3433333333333333, "no_speech_prob": 0.0011910966131836176}, {"id": 194, "seek": 72248, "start": 734.2, "end": 739.04, "text": " A przek\u0142adaj\u0105c to na praktyk\u0119, co to oznacza dla badacza, czy ma\u0142ej firmy?", "tokens": [50950, 316, 29785, 46217, 8555, 66, 281, 1667, 3206, 74, 874, 15724, 11, 598, 281, 277, 22672, 326, 2394, 12285, 1578, 326, 2394, 11, 6430, 463, 19827, 73, 12159, 2226, 30, 51192], "temperature": 0.0, "avg_logprob": -0.13103327880034576, "compression_ratio": 1.3433333333333333, "no_speech_prob": 0.0011910966131836176}, {"id": 195, "seek": 72248, "start": 739.04, "end": 740.76, "text": " Oznacza to wszystko.", "tokens": [51192, 422, 22672, 326, 2394, 281, 22607, 13, 51278], "temperature": 0.0, "avg_logprob": -0.13103327880034576, "compression_ratio": 1.3433333333333333, "no_speech_prob": 0.0011910966131836176}, {"id": 196, "seek": 72248, "start": 740.76, "end": 744.76, "text": " Zamiast klastra superkomputer\u00f3w z drogimi akceleratorami", "tokens": [51278, 1176, 4526, 525, 9671, 525, 424, 1687, 20557, 13849, 3901, 710, 3789, 70, 10121, 9308, 4933, 260, 1639, 4526, 51478], "temperature": 0.0, "avg_logprob": -0.13103327880034576, "compression_ratio": 1.3433333333333333, "no_speech_prob": 0.0011910966131836176}, {"id": 197, "seek": 72248, "start": 744.76, "end": 749.88, "text": " skompresowany do Int4 GGLM 130B do inferencji, czyli do u\u017cywania go.", "tokens": [51478, 1110, 8586, 495, 23341, 360, 5681, 19, 460, 19440, 44, 19966, 33, 360, 13596, 268, 19649, 11, 16591, 360, 34097, 86, 5609, 352, 13, 51734], "temperature": 0.0, "avg_logprob": -0.13103327880034576, "compression_ratio": 1.3433333333333333, "no_speech_prob": 0.0011910966131836176}, {"id": 198, "seek": 72248, "start": 749.88, "end": 751.24, "text": " Mhm, potrzebuje czego?", "tokens": [51734, 26272, 11, 28577, 6021, 2884, 36559, 30, 51802], "temperature": 0.0, "avg_logprob": -0.13103327880034576, "compression_ratio": 1.3433333333333333, "no_speech_prob": 0.0011910966131836176}, {"id": 199, "seek": 75124, "start": 751.24, "end": 757.6800000000001, "text": " Potrzebuje serwera z zaledwie czterema kartami graficznymi RTX 3090.", "tokens": [50364, 9145, 13503, 6021, 2884, 816, 1554, 64, 710, 710, 5573, 8699, 269, 2682, 323, 1696, 29120, 4526, 1295, 1786, 89, 31813, 44573, 2217, 7771, 13, 50686], "temperature": 0.0, "avg_logprob": -0.096102203641619, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.033991385251283646}, {"id": 200, "seek": 75124, "start": 757.6800000000001, "end": 763.32, "text": " Mo\u017cna go te\u017c uruchomi\u0107 na 8 nieco starszych kartach RTX 2080T.", "tokens": [50686, 44736, 629, 352, 9516, 4038, 625, 9220, 2162, 1667, 1649, 2838, 1291, 6105, 28051, 29120, 608, 44573, 945, 4702, 51, 13, 50968], "temperature": 0.0, "avg_logprob": -0.096102203641619, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.033991385251283646}, {"id": 201, "seek": 75124, "start": 763.32, "end": 766.92, "text": " To jest sprz\u0119t, kt\u00f3ry jest w zasi\u0119gu setek laboratori\u00f3w badawczych", "tokens": [50968, 1407, 3492, 6103, 11052, 83, 11, 9913, 3492, 261, 26530, 5034, 2794, 992, 916, 5938, 39842, 3901, 272, 1538, 86, 6522, 339, 51148], "temperature": 0.0, "avg_logprob": -0.096102203641619, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.033991385251283646}, {"id": 202, "seek": 75124, "start": 766.92, "end": 769.6, "text": " i tysi\u0119cy mniejszych firm na ca\u0142ym \u015bwiecie.", "tokens": [51148, 741, 38156, 47303, 39513, 45021, 6174, 1667, 35224, 4199, 40078, 4260, 13, 51282], "temperature": 0.0, "avg_logprob": -0.096102203641619, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.033991385251283646}, {"id": 203, "seek": 75124, "start": 769.6, "end": 774.28, "text": " To jest w\u0142a\u015bnie ta prawdziwa demokratyzacja dost\u0119pu, o kt\u00f3rej m\u00f3wi\u0142y\u015bmy na pocz\u0105tku.", "tokens": [51282, 1407, 3492, 14234, 1846, 41175, 3992, 4151, 49432, 37433, 23395, 48209, 84, 11, 277, 36023, 24592, 6825, 10513, 1667, 43959, 13, 51516], "temperature": 0.0, "avg_logprob": -0.096102203641619, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.033991385251283646}, {"id": 204, "seek": 75124, "start": 774.28, "end": 778.6800000000001, "text": " To przej\u015bcie od narz\u0119dzia dost\u0119pnego dla trzech, czterech korporacji na \u015bwiecie", "tokens": [51516, 1407, 8325, 73, 9815, 3611, 6714, 89, 6298, 40395, 48209, 11858, 12285, 504, 19439, 11, 269, 2682, 323, 339, 14784, 2816, 13152, 1667, 40078, 4260, 51736], "temperature": 0.0, "avg_logprob": -0.096102203641619, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.033991385251283646}, {"id": 205, "seek": 77868, "start": 778.68, "end": 782.88, "text": " do narz\u0119dzia, z kt\u00f3rym mo\u017ce eksperymentowa\u0107 znacznie szersza spo\u0142eczno\u015b\u0107.", "tokens": [50364, 360, 6714, 89, 6298, 40395, 11, 710, 30120, 12034, 30724, 610, 88, 518, 11445, 15397, 14875, 2766, 7870, 433, 2394, 36851, 89, 23293, 13, 50574], "temperature": 0.0, "avg_logprob": -0.12723068674658514, "compression_ratio": 1.48, "no_speech_prob": 0.052605025470256805}, {"id": 206, "seek": 77868, "start": 782.88, "end": 784.16, "text": " I na tym nie koniec.", "tokens": [50574, 286, 1667, 8107, 2838, 5897, 35733, 13, 50638], "temperature": 0.0, "avg_logprob": -0.12723068674658514, "compression_ratio": 1.48, "no_speech_prob": 0.052605025470256805}, {"id": 207, "seek": 77868, "start": 784.16, "end": 788.76, "text": " Zesp\u00f3\u0142 poszutk okrok dalej i stworzy\u0142 implementacji modelu w C++", "tokens": [50638, 1176, 13361, 16181, 1366, 89, 325, 74, 3133, 31621, 34257, 741, 342, 28321, 1229, 1221, 4445, 13152, 2316, 84, 261, 383, 25472, 50868], "temperature": 0.0, "avg_logprob": -0.12723068674658514, "compression_ratio": 1.48, "no_speech_prob": 0.052605025470256805}, {"id": 208, "seek": 77868, "start": 788.76, "end": 793.16, "text": " z wykorzystaniem biblioteki Faster Transformer od NVIDI, jak podaj\u0105 w pracy.", "tokens": [50868, 710, 43606, 1229, 18758, 4907, 34344, 310, 14753, 46665, 27938, 260, 3611, 426, 3958, 40, 11, 4207, 2497, 11133, 261, 35591, 13, 51088], "temperature": 0.0, "avg_logprob": -0.12723068674658514, "compression_ratio": 1.48, "no_speech_prob": 0.052605025470256805}, {"id": 209, "seek": 77868, "start": 793.16, "end": 796.5999999999999, "text": " Przyspiesza to dzia\u0142anie modelu od 7 do 8 razy w por\u00f3wnaniu", "tokens": [51088, 2114, 89, 749, 79, 530, 2394, 281, 27121, 7155, 2316, 84, 3611, 1614, 360, 1649, 9639, 88, 261, 1515, 812, 895, 25849, 51260], "temperature": 0.0, "avg_logprob": -0.12723068674658514, "compression_ratio": 1.48, "no_speech_prob": 0.052605025470256805}, {"id": 210, "seek": 77868, "start": 796.5999999999999, "end": 799.0, "text": " do standardowych implementacji w Pythonie.", "tokens": [51260, 360, 3832, 19605, 4445, 13152, 261, 15329, 414, 13, 51380], "temperature": 0.0, "avg_logprob": -0.12723068674658514, "compression_ratio": 1.48, "no_speech_prob": 0.052605025470256805}, {"id": 211, "seek": 77868, "start": 799.0, "end": 802.28, "text": " Czyli mamy nie tylko model, kt\u00f3ry wymaga mniej pami\u0119ci,", "tokens": [51380, 37099, 17335, 2838, 13219, 2316, 11, 9913, 29764, 9286, 39513, 31088, 537, 11, 51544], "temperature": 0.0, "avg_logprob": -0.12723068674658514, "compression_ratio": 1.48, "no_speech_prob": 0.052605025470256805}, {"id": 212, "seek": 77868, "start": 802.28, "end": 806.0, "text": " ale kt\u00f3ry dzia\u0142a te\u017c wielokrutnie, szybciej na tym samym sprz\u0119cie.", "tokens": [51544, 6775, 9913, 37903, 9516, 20570, 453, 24316, 2766, 11, 36456, 4260, 73, 1667, 8107, 3247, 4199, 6103, 11052, 4260, 13, 51730], "temperature": 0.0, "avg_logprob": -0.12723068674658514, "compression_ratio": 1.48, "no_speech_prob": 0.052605025470256805}, {"id": 213, "seek": 80600, "start": 806.0, "end": 811.6, "text": " Podsumowuj\u0105c historia GLM 130B, to znacznie wi\u0119cej ni\u017c tylko kolejny wpis", "tokens": [50364, 12646, 82, 449, 305, 44733, 18385, 16225, 44, 19966, 33, 11, 281, 15397, 14875, 2766, 26004, 28502, 13219, 23749, 1634, 32444, 271, 50644], "temperature": 0.0, "avg_logprob": -0.11561411421820028, "compression_ratio": 1.403174603174603, "no_speech_prob": 0.19371749460697174}, {"id": 214, "seek": 80600, "start": 811.6, "end": 813.72, "text": " w tabeli z wynikami benchmark\u00f3w.", "tokens": [50644, 261, 4421, 10148, 710, 31936, 1035, 4526, 18927, 3901, 13, 50750], "temperature": 0.0, "avg_logprob": -0.11561411421820028, "compression_ratio": 1.403174603174603, "no_speech_prob": 0.19371749460697174}, {"id": 215, "seek": 80600, "start": 813.72, "end": 820.84, "text": " To dow\u00f3d na to, \u017ce inna, bardziej przemy\u015blana architektura mo\u017ce przynie\u015b\u0107 nieoczekiwane korzy\u015bci.", "tokens": [50750, 1407, 9459, 17081, 1667, 281, 11, 3561, 294, 629, 11, 27209, 6541, 3633, 19212, 2095, 3912, 642, 2320, 2991, 12034, 6501, 2766, 7753, 2838, 905, 89, 14753, 86, 1929, 14784, 1229, 6199, 13, 51106], "temperature": 0.0, "avg_logprob": -0.11561411421820028, "compression_ratio": 1.403174603174603, "no_speech_prob": 0.19371749460697174}, {"id": 216, "seek": 80600, "start": 820.84, "end": 825.8, "text": " To fascynuj\u0105ca opowie\u015b\u0107 in\u017cynieryjna o pokonywaniu problem\u00f3w ze stabilno\u015bci\u0105,", "tokens": [51106, 1407, 30632, 1344, 77, 13263, 496, 999, 13998, 7753, 294, 1427, 2534, 811, 88, 73, 629, 277, 13010, 2526, 86, 25849, 1154, 3901, 5277, 11652, 16438, 1611, 11, 51354], "temperature": 0.0, "avg_logprob": -0.11561411421820028, "compression_ratio": 1.403174603174603, "no_speech_prob": 0.19371749460697174}, {"id": 217, "seek": 80600, "start": 825.8, "end": 829.72, "text": " za pomoc\u0105 sprytnych, a nie tylko brutalnie si\u0142owych metod.", "tokens": [51354, 7949, 48962, 1611, 637, 627, 83, 9399, 11, 257, 2838, 13219, 17878, 2766, 1511, 1221, 19605, 1131, 378, 13, 51550], "temperature": 0.0, "avg_logprob": -0.11561411421820028, "compression_ratio": 1.403174603174603, "no_speech_prob": 0.19371749460697174}, {"id": 218, "seek": 80600, "start": 829.72, "end": 835.4, "text": " I co chyba najwa\u017cniejsze, to realny, namacalny krok w stron\u0119 bardziej otwartej", "tokens": [51550, 286, 598, 31532, 11212, 27111, 44258, 11, 281, 957, 1634, 11, 8835, 326, 304, 1634, 350, 31621, 261, 45766, 1274, 27209, 4337, 86, 11026, 73, 51834], "temperature": 0.0, "avg_logprob": -0.11561411421820028, "compression_ratio": 1.403174603174603, "no_speech_prob": 0.19371749460697174}, {"id": 219, "seek": 83540, "start": 835.4, "end": 838.52, "text": " i dost\u0119pnej dla wszystkich sztucznej inteligencji.", "tokens": [50364, 741, 48209, 11794, 12285, 34234, 262, 2682, 1311, 89, 11794, 24777, 3213, 19649, 13, 50520], "temperature": 0.0, "avg_logprob": -0.09454173450322115, "compression_ratio": 1.4511278195488722, "no_speech_prob": 0.011365458369255066}, {"id": 220, "seek": 83540, "start": 838.52, "end": 843.8, "text": " Na koniec zostaje jedna prowokuj\u0105ca my\u015bl, kt\u00f3ra wy\u0142ania si\u0119 z tej pracy.", "tokens": [50520, 6056, 5897, 35733, 31873, 11153, 5232, 629, 45553, 453, 13263, 496, 452, 19212, 11, 19456, 4628, 1221, 5609, 3244, 710, 12573, 35591, 13, 50784], "temperature": 0.0, "avg_logprob": -0.09454173450322115, "compression_ratio": 1.4511278195488722, "no_speech_prob": 0.011365458369255066}, {"id": 221, "seek": 83540, "start": 843.8, "end": 849.72, "text": " Widzieli\u015bmy, \u017ce dwuj\u0119zyczno\u015b\u0107 mog\u0142a nieoczekiwanie zredukowa\u0107 uprzedzenia w modelu.", "tokens": [50784, 28331, 89, 23099, 10513, 11, 3561, 27379, 18258, 1229, 3689, 23293, 13172, 5024, 2838, 905, 89, 14753, 86, 7155, 710, 265, 769, 74, 11445, 493, 81, 11312, 14320, 261, 2316, 84, 13, 51080], "temperature": 0.0, "avg_logprob": -0.09454173450322115, "compression_ratio": 1.4511278195488722, "no_speech_prob": 0.011365458369255066}, {"id": 222, "seek": 83540, "start": 849.72, "end": 855.4, "text": " Widzieli\u015bmy, \u017ce inna architektura pozwoli\u0142a na niespodkan\u0105 wcze\u015bniej kompresj\u0119.", "tokens": [51080, 28331, 89, 23099, 10513, 11, 3561, 294, 629, 3912, 642, 2320, 2991, 40557, 9384, 5024, 1667, 48100, 43388, 5225, 1611, 40785, 5207, 14508, 11115, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09454173450322115, "compression_ratio": 1.4511278195488722, "no_speech_prob": 0.011365458369255066}, {"id": 223, "seek": 83540, "start": 855.4, "end": 860.68, "text": " To wszystko sugeruje, \u017ce by\u0107 mo\u017ce jeste\u015bmy na progu nowej ery w rozwoju AI.", "tokens": [51364, 1407, 22607, 459, 1321, 13008, 11, 3561, 15069, 12034, 35928, 1667, 447, 2794, 586, 40779, 1189, 88, 261, 9544, 6120, 8954, 7318, 13, 51628], "temperature": 0.0, "avg_logprob": -0.09454173450322115, "compression_ratio": 1.4511278195488722, "no_speech_prob": 0.011365458369255066}, {"id": 224, "seek": 86068, "start": 860.68, "end": 867.76, "text": " Ery, w kt\u00f3rej kluczem do post\u0119pu nie b\u0119dzie ju\u017c tylko \u015blepe skalowanie mocy obliczeniowej i liczby parametr\u00f3w.", "tokens": [50364, 462, 627, 11, 261, 36023, 9671, 1311, 24313, 360, 2183, 18085, 84, 2838, 10562, 10678, 13219, 8299, 306, 494, 16890, 22028, 705, 1344, 1111, 1050, 42124, 21091, 741, 6169, 89, 2322, 6220, 27965, 3901, 13, 50718], "temperature": 0.0, "avg_logprob": -0.11886754631996155, "compression_ratio": 1.487719298245614, "no_speech_prob": 0.028521263971924782}, {"id": 225, "seek": 86068, "start": 867.76, "end": 874.04, "text": " By\u0107 mo\u017ce kluczem stanie si\u0119 fundamentalne skalowanie lingwistycznej i kulturowej r\u00f3\u017cnorodno\u015bci danych", "tokens": [50718, 3146, 2162, 12034, 9671, 1311, 24313, 40013, 3244, 8088, 716, 16890, 22028, 22949, 86, 468, 17466, 11794, 741, 350, 26099, 21091, 19637, 19048, 378, 16438, 274, 34644, 51032], "temperature": 0.0, "avg_logprob": -0.11886754631996155, "compression_ratio": 1.487719298245614, "no_speech_prob": 0.028521263971924782}, {"id": 226, "seek": 86068, "start": 874.04, "end": 878.0799999999999, "text": " oraz wiesz, inteligentne projektowanie architektur.", "tokens": [51032, 28905, 261, 15347, 11, 24777, 25002, 716, 26261, 22028, 3912, 642, 2320, 374, 13, 51234], "temperature": 0.0, "avg_logprob": -0.11886754631996155, "compression_ratio": 1.487719298245614, "no_speech_prob": 0.028521263971924782}, {"id": 227, "seek": 86068, "start": 878.0799999999999, "end": 886.28, "text": " Co by si\u0119 sta\u0142o, gdyby przysz\u0142e modele uczy\u0142y si\u0119 od samego pocz\u0105tku nie z dw\u00f3ch, ale z dwudziestu j\u0119zyk\u00f3w z r\u00f3\u017cnych rodzin j\u0119zykowych?", "tokens": [51234, 3066, 538, 3244, 11135, 5249, 11, 28405, 2322, 44018, 19827, 4391, 306, 344, 6522, 6825, 3244, 3611, 912, 1571, 43959, 2838, 710, 27379, 812, 339, 11, 6775, 710, 27379, 532, 37567, 84, 49055, 23849, 710, 42602, 8685, 23584, 49055, 74, 19605, 30, 51644], "temperature": 0.0, "avg_logprob": -0.11886754631996155, "compression_ratio": 1.487719298245614, "no_speech_prob": 0.028521263971924782}, {"id": 228, "seek": 88628, "start": 886.28, "end": 893.92, "text": " Jakie zupe\u0142nie nowe, wyjaniaj\u0105ce si\u0119 zdolno\u015bci, tzw. emergent abilities, mogliby\u015bmy w nich wtedy odkry\u0107?", "tokens": [50364, 15029, 414, 49922, 586, 68, 11, 4628, 73, 5609, 8555, 384, 3244, 16221, 401, 16438, 11, 256, 14406, 13, 4345, 6930, 11582, 11, 13172, 38270, 88, 10513, 261, 25570, 26959, 3611, 43298, 2162, 30, 50746], "temperature": 0.0, "avg_logprob": -0.21573561429977417, "compression_ratio": 1.0852713178294573, "no_speech_prob": 0.1447049081325531}, {"id": 229, "seek": 88628, "start": 893.92, "end": 895.92, "text": " To pytanie pozostaje otwarte.", "tokens": [50746, 1407, 36610, 21281, 555, 11153, 4337, 86, 11026, 13, 50846], "temperature": 0.0, "avg_logprob": -0.21573561429977417, "compression_ratio": 1.0852713178294573, "no_speech_prob": 0.1447049081325531}], "language": "pl"}