{"text": " Wyobra\u017amy sobie taki \u015bwiat, w kt\u00f3rym od lat, no w zasadzie od zawsze, obowi\u0105zuje jedna zasada. W technologii wi\u0119kszy znaczy lepszy. Mhm. Ka\u017cdy buduje coraz pot\u0119\u017cniejsze, coraz dro\u017csze, no i coraz bardziej energoch\u0142onne potwory. A\u017c tu nagle, na pocz\u0105tku 2023 roku, pojawia si\u0119 praca naukowa, kt\u00f3ra jest jak, nie wiem, jak herezja. No dok\u0142adnie. Udowadnia, \u017ce model sztucznej inteligencji, 10 razy mniejszy od panuj\u0105cego kr\u00f3la, mo\u017ce go pobi\u0107 w wi\u0119kszo\u015bci zada\u0144, a co wi\u0119cej, ten Dawid w walce z goliatem zosta\u0142 wytranowany wy\u0142a\u015bnie na publicznie dost\u0119pnych danych. I to by\u0142 wiesz moment, kt\u00f3ry zatrz\u0105s\u0142 posadami ca\u0142ej bran\u017cy. Zdecydowanie. I to jest w\u0142a\u015bnie ten dokument, kt\u00f3remu si\u0119 dzisiaj przyjrzymy. Ta naukowa zespo\u0142u Meta AI zatytu\u0142owana LLMA Open and Efficient Foundation Language Models. To nie jest jaka\u015b tam kolejna, inkrementalna poprawka, nie, to co\u015b wi\u0119cej. To jest jeden z tych tekst\u00f3w, kt\u00f3re, no, po prostu wyznaczaj\u0105 now\u0105 epok\u0119. Zmieniaj\u0105 zasady gry i, co chyba najwa\u017cniejsze, otwieraj\u0105 drzwi do rewolucji, kt\u00f3ra dzieje si\u0119 na naszych oczach w \u015bwiecie Open Source AI. Nasza misja na dzi\u015b jest wi\u0119c prosta. Chcemy roz\u0142o\u017cy\u0107 t\u0119 prac\u0119 na czynniki pierwsze. Zrozumie\u0107, co tak dok\u0142adnie sprawi\u0142o, \u017ce te modele okaza\u0142y si\u0119, no, tak prze\u0142omowe. Zajrzymy pod mask\u0119. Zajrzymy pod mask\u0119, przeanalizujemy ich diet\u0119, na kt\u00f3rej by\u0142y karmione i oczywi\u015bcie przyjrzymy si\u0119 wynikom, kt\u00f3re wywo\u0142a\u0142y takie poruszenie. To b\u0119dzie analiza dla ka\u017cdego, kto chce poj\u0105\u0107, jak dosz\u0142o do jednej z najwi\u0119kszych rewolucji w najnowszej historii sztucznej inteligencji. No to zaczynajmy. Dobrze, to zacznijmy od tej centralnej, niemal wywrotowej tezy. Co autorzy tak naprawd\u0119 chcieli udowodni\u0107? Bo przecie\u017c, no, ca\u0142y \u015bwiat goni\u0142 za setkami miliard\u00f3w, a nawet bilionami parametr\u00f3w. Znaczony przez GPT-3 by\u0142 jasny. Wi\u0119cej, wi\u0119cej i jeszcze raz wi\u0119cej. G\u0142\u00f3wne za\u0142o\u017cenie tej pracy by\u0142o w\u0142a\u015bnie, wiesz, tak\u0105 kontr\u0105 do tego trendu. Autorzy postanowili zada\u0107 zupe\u0142nie inne pytanie, bo do tej pory wszyscy optymalizowali swoje dzia\u0142ania pod k\u0105tem bud\u017cetu na trening. Czyli na to jednorazowe, pot\u0119\u017cne odpalenie. Dok\u0142adnie. Jak najefektywniej wykorzysta\u0107 ogromn\u0105 moc obliczeniow\u0105, by jednorazowo stworzy\u0107 jak najwi\u0119kszy model? A zesp\u00f3\u0142 Meta stwierdzi\u0142. A co je\u015bli to jest z\u0142e podej\u015bcie? Co je\u015bli prawdziwym kosztem, tym w\u0105skim gard\u0142em, jest bud\u017cet na inferencj\u0119? Czyli nato faktyczne codzienne u\u017cywanie modelu? Tak, milion razy na sekund\u0119 przez miliony u\u017cytkownik\u00f3w. Czyli ich hipoteza by\u0142a, jak ju\u017c powiedzia\u0142am, wr\u0119cz heretyczna jak na tamte czasy. Stwierdzili. A co je\u015bli wszyscy w bran\u017cy biegn\u0105 w z\u0142\u0105 stron\u0119? Zamiast budowa\u0107 coraz dro\u017csze silniki rakietowe, kt\u00f3re odpalasz raz, skupmy si\u0119 na stworzeniu czego\u015b, co b\u0119dzie dzia\u0142a\u0107 tanio i niezawodnie na masow\u0105 skal\u0119. To by\u0142a fundamentalna zmiana my\u015blenia o ekonomii AI. Dok\u0142adnie. I oni postawili tez\u0119, \u017ce dla osi\u0105gni\u0119cia okre\u015blonego poziomu, powiedzmy inteligencji, znacznie lepszy jest mniejszy model, ale wytrenowany na absurdalnie du\u017cej ilo\u015bci danych, ni\u017c gigantyczny model, kt\u00f3rego trening przerwano wcze\u015bniej z powodu koszt\u00f3w. Taki mniejszy model jest na ko\u0144cu dnia nieporywnywalnie ta\u0144szy i szybsz w utrzymaniu. I praca pokaza\u0142a co\u015b fascynuj\u0105cego na przyk\u0142ad. Wydajno\u015b\u0107 modelu 7B, czyli z siedmioma miliardami parametr\u00f3w, ona wci\u0105\u017c ros\u0142a i ros\u0142a nawet po przetworzeniu biliona token\u00f3w danych. Co by\u0142o wbrew logice, tak. Wydajno\u015bci jest fascynuj\u0105ce, ale wiesz, teoria to jedno. Diabe\u0142d kwi w szczeg\u00f3\u0142ach, jak oni w\u0142a\u015bciwie tego dokonali. Domy\u015blam, \u017ce kluczem nie by\u0142a tylko architektura, ale przede wszystkim to, czym ten model karmiono. Trafiasz w sedno. Zbi\u00f3r danych by\u0142 absolutnie kluczowy i wyr\u00f3\u017cnia\u0142 si\u0119 jedn\u0105 fundamentaln\u0105 cech\u0105. By\u0142 w 100% jawny, ca\u0142kowicie publiczny. Tak, skomponowany z publicznie dost\u0119pnych \u017ar\u00f3de\u0142. I to by\u0142, no powiedzmy sobie szczerze, policzek wymierzony w dotychczasow\u0105 praktyk\u0119, gdzie modele takie jak GPT-3 trenowano na cz\u0119\u015bciowo tajnych zbiorach. Pami\u0119tam, by\u0142 ten s\u0142ynny enigmatyczny zbi\u00f3r Bux 2. Dok\u0142adnie, o rzemie o zbiarze dw\u00f3ch terabajt\u00f3w, o kt\u00f3rym nikt nic nie wiedzia\u0142. A meta pokaza\u0142a, \u017ce hej, nie trzeba mie\u0107 sekrepnych sk\u0142adnik\u00f3w. Mo\u017cna to zrobi\u0107 na otwartych danych. No dobrze, to co znalaz\u0142o si\u0119 w tym jawnym menu? Co z\u0142o\u017cy\u0142o si\u0119 na te 1,4 biliona token\u00f3w? Lwi\u0105 cz\u0119\u015b\u0107, bo a\u017c 67% stanowi\u0142 common crawl, czyli gigantyczny zrzut publicznej cz\u0119\u015bci internetu. Ale co wa\u017cne, on zosta\u0142 bardzo starannie przefiltrowany, \u017ceby odsia\u0107 tre\u015bci niskiej jako\u015bci. No a po prostu \u015bmieci. Do tego do\u0142o\u017cono 15% ze zbioru C4, kt\u00f3ry jest inn\u0105 r\u00f3wnie\u017c oczyszczon\u0105 wersj\u0105 common crawl. A dalej mamy cztery sk\u0142adniki po 4,5% ka\u017cdy. Czyli co to by\u0142o? Publiczne repozytoria kodu z GitHub'a, artyku\u0142y z Wikipedia w 20 j\u0119zykach, zbiory ksi\u0105\u017cek, tak jak projekt Gutenberg i Bookstree. A i chyba co\u015b jeszcze z nauki. I co ciekawe, prace naukowe z serwisu Archive to by\u0142o 2,5%. I wysokiej jako\u015bci pytania i odpowiedzi z forum Stack Exchange, czyli 2%. Czyli pe\u0142na transparentno\u015b\u0107 i du\u017ca r\u00f3\u017cnorodno\u015b\u0107. A co z samym silnikiem, z architektur\u0105, czy oni wymy\u015blili jaki\u015b zupe\u0142nie nowy, rewolucyjny rodzaj sieci neuronowej? I tu dochodzimy do kolejnego fascynuj\u0105cego elementu. Nie, nie wymy\u015blili. I to jest w tym wszystkim pi\u0119kne. Czyli nie rewolucja? Zamiast rewolucji postawili na inteligentn\u0105 ewolucj\u0119. Mo\u017cna to por\u00f3wna\u0107 do, nie wiem, tuninmu silnika samochodowego. Nie zaprojektowali nowego bloku silnika, ale wzi\u0119li sprawdzon\u0105 architektur\u0119 Transformer i zastosowali trzy najlepsze sprawdzone ju\u017c na rynku ulepszenia. Okej, jakie to by\u0142y ulepszenia? Czym dok\u0142adnie podkr\u0119cili ten silnik? Pierwsze to pre-normalization z u\u017cyciem RMS norm. To jest bardzo wa\u017cny detal, bo wiesz, trening gigantycznego modelu jest jak balansowanie wie\u017cy sklodzk\u00f3w. \u0141atwo o katastrof\u0119. Dok\u0142adnie. Bez odpowiedniej stabilizacji warto\u015bci wewn\u0105trz sieci mog\u0105 eksplodowa\u0107 lub zanika\u0107, a ca\u0142y wielotygodniowy piekielnie drugi proces ko\u0144czy si\u0119 fiaskiem. A pre-normalization dzia\u0142a jak taki solidny fundament i amortyzator. Okej, czyli to jest gwarancja stabilno\u015bci, a drugie ulepszenie? Drugie to zamiana standardowej funkcji aktywacji relu na swiglu podpatrzonej w modelu Palm od Google. To jak zamiana zwyk\u0142ego filkra powietrza w samochodzie na sportowy. Ma\u0142a zmiana, du\u017cy efekt. Ma\u0142y, ale sprytny zabieg, kt\u00f3ry daje wymierny kilkuprocentowy wzrost wydajno\u015bci bez fundamentalnych zmian. A trzeci element to ju\u017c Majstersztyk. Czyli? Zrezygnowali z absolutnych osadze\u0144 pozycyjnych na rzecz Rotary Embeddings, znanych jako ropey. Rope, okej. Zamiast m\u00f3wi\u0107 modelowi na sztywno, to s\u0142owo jest na pi\u0105tej pozycji w zdaniu. Rope daje mu co\u015b w rodzaju wewn\u0119trznego kompasu. To pozwala mu znacznie lepiej rozumie\u0107 relacje i odleg\u0142o\u015bci mi\u0119dzy s\u0142owami, zw\u0142aszcza w bardzo d\u0142ugich tekstach. W pracy wspomniano te\u017c o czym\u015b, co pozwoli\u0142o im wytrenowa\u0107 najwi\u0119kszy model w zaledwie 21 dni. To brzmi jak szalenie kr\u00f3tki czas. Czy to te\u017c efekt tych ulepsze\u0144? Nie. To ju\u017c czysta, genialna in\u017cynieria. Wykorzystali dwie kluczowe techniki. Po pierwsze, niezwykle wydajn\u0105 implementacj\u0119 mechanizmu E-Tension z biblioteki X-Formers. I jak to dzia\u0142a? W klasycznym podej\u015bciu, \u017ceby obliczy\u0107 uwag\u0119, trzeba zapisa\u0107 w pami\u0119ci gigantyczn\u0105 macie\u017c wak, co jest bardzo pami\u0119ciorzerne. Implementacja w X-Formers pozwala tego unikn\u0105\u0107, drastycznie oszcz\u0119dzaj\u0105c zasoby. A ta druga technika? Polega ona na tym, \u017ce podczas przej\u015bcia w prz\u00f3d model nie zapisuje wszystkich po\u015brednich wynik\u00f3w, a tylko te kluczowe. Reszt\u0119 odtwarza w locie podczas przej\u015bcia wstecznego. Czyli znowu. Ogromna oszcz\u0119dno\u015b\u0107 pami\u0119ci. Ogromna. Bez tych dw\u00f3ch sztuczek ten projekt w tej skali i w tym czasie po prostu nie by\u0142by mo\u017cliwy. Dobrze. Mamy wi\u0119c rewolucyjn\u0105 tez\u0119, transparentne dane i inteligentnie stunigowan\u0105 architektor\u0119. Ale na koniec dnia licz\u0105 si\u0119 wyniki. Przejd\u017amy do dowod\u00f3w w liczbach. Co tak naprawd\u0119 pokaza\u0142y te wszystkie tabele z benchmark\u00f3w? Wyniki by\u0142y, no, wstrz\u0105sem dla ca\u0142ej bran\u017cy. Najwa\u017cniejszy i najcz\u0119\u015bciej cytowany wniosek jest taki. Lama 13B, czyli model z 13 miliardami parametr\u00f3w, czyli ten wystarczaj\u0105co ma\u0142y, \u017ceby go uruchomi\u0107 powiedzmy na domowym sprz\u0119cie. Tak. Wystarczaj\u0105co ma\u0142y by z pewnym wysi\u0142kiem uruchomi\u0107 go na jednej mocnej karcie graficznej. Dla graczy w wi\u0119kszo\u015bci zada\u0144 pokona\u0142 giganta, jakim by\u0142 175 miliardowy GPT-3. Zaraz, zaraz. Zatrzymajmy si\u0119 tu na chwil\u0119. Model ponad 10 razy mniejszy dzia\u0142aj\u0105cy na publicznych danych pokona\u0142 zamkni\u0119tego, komercyjnego potwora, kt\u00f3ry zdefiniowa\u0142 ca\u0142\u0105 epok\u0119? To brzmi jak historia z filmu. A jednak. I to w szerokim zakresie test\u00f3w. Na przyk\u0142ad w rozumowaniu potocznym, czyli common sense reasoning, na takich benchmarkach jak Helleswag czy Winogrand. A w jakich jeszcze? Ale tak\u017ce w odpowiadaniu na pytania w trybie closed book na testie Trivia QA, gdzie model musi odpowiedzie\u0107 bazuj\u0105c wy\u0142\u0105cznie na wiedz\u0119 zapami\u0119tanej podczas treningu. To pokaza\u0142o, \u017ce mniejszy model, ale trenowany d\u0142u\u017cej, mo\u017ce mie\u0107 g\u0119stsz\u0105 i lepiej zorganizowan\u0105 wiedz\u0119. To ju\u017c samo w sobie jest niesamowite. Ale czy to skalowa\u0142o si\u0119 w g\u00f3r\u0119? By\u0142o \u0142atwo sobie wyobrazi\u0107, \u017ce ta magia mniejszego modelu dzia\u0142a tylko do pewnego momentu. Jak najwi\u0119ksze z rodziny Lama 65B wypad\u0142 w starciu z prawdziwymi tytanami tamtych czas\u00f3w, jak Palm od Google. Okaza\u0142 si\u0119 w pe\u0142ni konkurencyjny. W zadaniach zrozumowania potocznego przewy\u017ccza\u0142 nawet nieco wi\u0119kszy model Chi\u0144czyla 70B, na prawie wszystkich testach. Ale najbardziej zdumiewaj\u0105cy wynik pojawi\u0142 si\u0119 gdzie indziej. Gdzie? W te\u015bcie rozumowania matematycznego GSM 8K. Lama 65B, kt\u00f3ry przecie\u017c nie by\u0142 w \u017caden specjalny spos\u00f3b trenowany na danych matematycznych, okaza\u0142 si\u0119 lepszy. Okaza\u0142 si\u0119 lepszy ni\u017c model Minerva 62B, specjalistyczny model od Google, kt\u00f3ry przeszed\u0142 dodatkowy fine tuning w\u0142a\u015bnie na danych naukowych i matematycznych. To ju\u017c jest kompletne zaskoczenie. Wychodzi na to, \u017ce pobi\u0142 specjalist\u0119 w jego w\u0142asnej dyscyplinie, nawet si\u0119 do niej specjalnie nie przygotowuj\u0105c. Czy by\u0142y w og\u00f3le jakie\u015b obszary, w kt\u00f3rych Lama wyra\u017cnie ust\u0119powa\u0142a pola? Tak, i autorzy byli w tej kwestii bardzo szczerzy. W benchmarku MMLU, czyli Massive Multitask Language Understanding, to jest ten wielki test wiedzy, taka matura dla AI. Dok\u0142adnie. Tam Lama 65B wypad\u0142a nieco s\u0142abiej od Chinjili i Palm. Sami autorze sugeruj\u0105, \u017ce najbardziej prawdopodobodn\u0105 przyczyn\u0105 by\u0142a po prostu mniejsza ilo\u015b\u0107 ksi\u0105\u017cek i artyku\u0142\u00f3w naukowych w ich danych treningowych. Ich zbiory archive i ksi\u0105\u017cek to \u0142\u0105cznie 177 GB podczas gdy konkurencja mog\u0142a u\u017cywa\u0107 nawet 2 TB danych ksi\u0105\u017ckowych. To doskonale pokazuje, jak bardzo sk\u0142ad diety model\u00f3w wp\u0142ywa na jego ko\u0144cowe zdolno\u015bci. Pr\u00f3bujmy teraz spojrze\u0107 na to z szerszej perspektywy. Co ta praca tak naprawd\u0119 oznacza\u0142a dla \u015bwiata technologii? Jaki by\u0142 jej realny wp\u0142yw? Moim zdaniem Lama mia\u0142a dwa fundamentalne skutki. Pierwszy to demokratyzacja bada\u0144 nad du\u017cymi modelami j\u0119zykowymi. W jakim sensie? Udowadniaj\u0105c, \u017ce mo\u017cna osi\u0105gn\u0105\u0107 \u015bwiatowej klasy wyniki na publicznych danych i co by\u0142o absolutnie kluczowe, udost\u0119pniaj\u0105c wagi swoich modeli spo\u0142eczno\u015bci naukowej, meta otworzy\u0142a wrota dla tysi\u0119cy mniejszych zespo\u0142\u00f3w badawczych, start-up\u00f3w, uniwersytet\u00f3w. Nagle okaza\u0142o si\u0119, \u017ce nie trzeba by\u0107 Google'em czy OpenAI, \u017ceby prowadzi\u0107 badania na najwy\u017cszym poziomie. Dok\u0142adnie. To wywo\u0142a\u0142o eksplozy innowacji w \u015bwiecie OpenSource. A ten drugi skutek? Drugi to ta wspomniana ju\u017c zmiana paradygmatu, czyli przeniesienie uwagi z kosztu treningu na wydajno\u015b\u0107 inferencji. To mia\u0142o kolosalne znaczenie dla praktycznych zastosowa\u0144. Dla biznesu? Biznes na bezrozumia\u0142, \u017ce nie musi inwestowa\u0107 w utrzymanie potwora o 500 miliardach parametr\u00f3w, \u017ceby mie\u0107 topow\u0105 wydajno\u015b\u0107. Mo\u017cna mie\u0107 mniejsze, zwinniejsze i znacznie ta\u0144sze w codziennym u\u017cytkowaniu model, kt\u00f3ry dostarczy r\u00f3wnie\u017c dobre, a czasem nawet lepsze wyniki. To ca\u0142kowicie zmieni\u0142o ekonomi\u0105 sztucznej inteligencji. Autorzy zbudowali to pot\u0119\u017cne narz\u0119dzie, ale czy byli te\u017c szczerzy co do jego potencjalnych wad i zagro\u017ce\u0144? Wiele prac naukowych woli przemilcze\u0107 te trudne tematy. I to jest jeden z najmocniejszych punkt\u00f3w tej pracy. Podchodz\u0105 do tematu z du\u017c\u0105 odpowiedzialno\u015bci\u0105. Przeprowadzili np. testy na toksyczno\u015b\u0107 u\u017cywaj\u0105c benchmarku Real Toxicity Prompts. I co im wysz\u0142o? Potwierdzili co\u015b, co obserwowano ju\u017c wcze\u015bniej. Niestety toksyczno\u015b\u0107 generowanych tre\u015bci ro\u015bnie wraz z rozmiarem modelu. Lama 65B generowa\u0142a bardziej toksyczne odpowiedzi ni\u017c jej mniejsza siostra Lama 7B. A co z uprzedzeniami, czyli tzw. Bayas? Model uczy si\u0119 przecie\u017c na danych z internetu, kt\u00f3ry jest delikatnie m\u00f3wi\u0105c pe\u0142en stereotyp\u00f3w. Dok\u0142adnie. Przeanalizowali to u\u017cywaj\u0105c benchmark\u00f3w CrossPairs i Winogender. W pierwszym, kt\u00f3ry mierzy stereotypy w 9 r\u00f3\u017cnych kategoriach, model Lama 65B okaza\u0142 si\u0119 szczeg\u00f3lnie stronniczy w kategorii religii. A w drugim? Z kolei w drugim testie, kt\u00f3ry bada uprzedzenia p\u0142ciowe zwi\u0105zane z zawodami, model cz\u0119\u015bciej pope\u0142nia\u0142 b\u0142\u0119dy, gdy p\u0142e\u0107 w zdaniu by\u0142a sprzeczna ze stereotypem, np. gdy piel\u0119gniarz by\u0142 m\u0119\u017cczyzn\u0105, a nie kobiet\u0105. Czyli to bardzo uczciwa samo ocena. Pokazuje, \u017ce model jest lustrem danych, uczy si\u0119 zar\u00f3wno wiedzy i kreatywno\u015bci, jak i naszych spo\u0142ecznych uprzedze\u0144. I na sam koniec swojej pracy, niemal jako post-scriptum, wspomnieli kr\u00f3tko o czym\u015b, co nazywa si\u0119 Instruction Fine Tuning. To by\u0142 wtedy bardzo gor\u0105cy temat. Tak, to by\u0142 niemal zwiastun tego, co mia\u0142o nadej\u015b\u0107 kilka miesi\u0119cy p\u00f3\u017aniej i co dzi\u015b jest standardem. Pokazali, \u017ce nawet bardzo kr\u00f3tki i prosty proces dostrajania modelu na zbior\u0119 instrukcji. Tworz\u0105c model Lama i. Tak, tworz\u0105c wersj\u0119, kt\u00f3r\u0105 nazwali Lama i, \u017ce ten proces znacz\u0105co poprawi\u0142 jego wyniki na tym trudnym benchmarku MMLU. Wynik skoczy\u0142 z 63,4% do prawie 69%. To by\u0142 wyra\u017cny sygna\u0142 dla ca\u0142ej bran\u017cy, \u017ce surowe, pot\u0119\u017cne modele bazowe to jedno, ale prawdziwa u\u017cyteczno\u015b\u0107 le\u017cy gdzie indziej. Prawdziwa u\u017cyteczno\u015b\u0107 i bezpiecze\u0144stwo le\u017c\u0105 w dostrajaniu ich do tego, by pod\u0105\u017ca\u0142y za ludzkimi poleceniami. Podsumowuj\u0105c, praca nad Lama to bez dw\u00f3ch zdanie kamie\u0144 milowy. Pokaza\u0142a, \u017ce mniejsze, ale intensywniej trenowane modele na otwartych, publicznych danych mog\u0105 by\u0107 wydajniejsze i cz\u0119sto lepsze od zamkni\u0119tych gigant\u00f3w. Zmieni\u0142a spos\u00f3b my\u015blenia o skalowaniu AI, k\u0142ad\u0105c nacisk na praktyczn\u0105 u\u017cyteczno\u015b\u0107 i koszty codziennego dzia\u0142ania. Kluczowe by\u0142o to przesuni\u0119cie perspektywy. Z pytania, jak najtaniej wytrenowa\u0107 model do poziomu X, na pytanie, jaki model o poziomie X b\u0119dzie najta\u0144szy w u\u017cyciu na masow\u0105 skal\u0119. To filozofia, kt\u00f3ra wci\u0105\u017c jest aktualna. To jest filozofia, kt\u00f3ra do dzi\u015b kszta\u0142tuje ca\u0142y rozw\u00f3j sztucznej inteligencji, zw\u0142aszcza w ekosystemie Open Source, kt\u00f3ry dzi\u0119ki Lamie po prostu eksplodowa\u0142. I na koniec zostawmy naszych s\u0142uchaczy z pewn\u0105 my\u015bl\u0105. W konkluzji swojej pracy autorzy zapowiedzieli, \u017ce planuj\u0105 trenowa\u0107 jeszcze wi\u0119ksze modele na jeszcze wi\u0119kszych zbiorach danych. Bior\u0105c pod uwag\u0119 ich w\u0142asne szczere odkrycia, \u017ce uprzedzenia i toksyczno\u015b\u0107 r\u00f3wnie\u017c skaluj\u0105 si\u0119 wraz z rozmiarem? To rodzi si\u0119 fundamentalne pytanie. Czy buduj\u0105c kora z pot\u0119\u017cniejsze modele na danych z ca\u0142ego internetu, tworzymy lepsze narz\u0119dzia do rozumienia \u015bwiata, czy raczej coraz doskonosze lustra, kt\u00f3re bezkrytycznie odbijaj\u0105 wszystkie jego niedoskona\u0142o\u015bci? I jak m\u0105drze skalowa\u0107 mo\u017cliwo\u015bci, nieskaluj\u0105c proporcjonalnie potencjalnych szk\u00f3d? To jest wyzwanie, z kt\u00f3rym ca\u0142a bran\u017ca mierzy si\u0119 do dzisiaj.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.5, "text": " Wyobra\u017amy sobie taki \u015bwiat, w kt\u00f3rym od lat, no w zasadzie od zawsze, obowi\u0105zuje jedna zasada.", "tokens": [50364, 14458, 24393, 10659, 2226, 13652, 20065, 36425, 11, 261, 30120, 3611, 4465, 11, 572, 261, 44585, 3283, 3611, 30964, 11, 1111, 47886, 11728, 2884, 5232, 629, 26530, 1538, 13, 50739], "temperature": 0.0, "avg_logprob": -0.16365994000044026, "compression_ratio": 1.3822393822393821, "no_speech_prob": 0.024487342685461044}, {"id": 1, "seek": 0, "start": 7.5, "end": 10.5, "text": " W technologii wi\u0119kszy znaczy lepszy.", "tokens": [50739, 343, 1537, 1132, 5597, 29968, 1229, 36584, 476, 1878, 1229, 13, 50889], "temperature": 0.0, "avg_logprob": -0.16365994000044026, "compression_ratio": 1.3822393822393821, "no_speech_prob": 0.024487342685461044}, {"id": 2, "seek": 0, "start": 10.5, "end": 11.5, "text": " Mhm.", "tokens": [50889, 26272, 13, 50939], "temperature": 0.0, "avg_logprob": -0.16365994000044026, "compression_ratio": 1.3822393822393821, "no_speech_prob": 0.024487342685461044}, {"id": 3, "seek": 0, "start": 11.5, "end": 17.5, "text": " Ka\u017cdy buduje coraz pot\u0119\u017cniejsze, coraz dro\u017csze, no i coraz bardziej energoch\u0142onne potwory.", "tokens": [50939, 10988, 1427, 3173, 3265, 13008, 25899, 1847, 1274, 1427, 44258, 11, 25899, 3789, 1427, 82, 1381, 11, 572, 741, 25899, 27209, 2043, 1571, 339, 1221, 22419, 1847, 86, 827, 13, 51239], "temperature": 0.0, "avg_logprob": -0.16365994000044026, "compression_ratio": 1.3822393822393821, "no_speech_prob": 0.024487342685461044}, {"id": 4, "seek": 0, "start": 17.5, "end": 26.0, "text": " A\u017c tu nagle, na pocz\u0105tku 2023 roku, pojawia si\u0119 praca naukowa, kt\u00f3ra jest jak, nie wiem, jak herezja.", "tokens": [51239, 316, 1427, 2604, 297, 15088, 11, 1667, 43959, 44377, 19451, 11, 30655, 654, 3244, 582, 6628, 35616, 74, 5528, 11, 19456, 3492, 4207, 11, 2838, 26522, 11, 4207, 720, 4371, 2938, 13, 51664], "temperature": 0.0, "avg_logprob": -0.16365994000044026, "compression_ratio": 1.3822393822393821, "no_speech_prob": 0.024487342685461044}, {"id": 5, "seek": 0, "start": 26.0, "end": 27.0, "text": " No dok\u0142adnie.", "tokens": [51664, 883, 45864, 2766, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16365994000044026, "compression_ratio": 1.3822393822393821, "no_speech_prob": 0.024487342685461044}, {"id": 6, "seek": 2700, "start": 27.0, "end": 33.0, "text": " Udowadnia, \u017ce model sztucznej inteligencji, 10 razy mniejszy od panuj\u0105cego kr\u00f3la,", "tokens": [50364, 624, 67, 22647, 12679, 11, 3561, 2316, 262, 2682, 1311, 89, 11794, 24777, 3213, 19649, 11, 1266, 9639, 88, 39513, 7706, 3611, 2462, 13263, 384, 1571, 42366, 875, 11, 50664], "temperature": 0.0, "avg_logprob": -0.11165865262349446, "compression_ratio": 1.3970588235294117, "no_speech_prob": 0.12724557518959045}, {"id": 7, "seek": 2700, "start": 33.0, "end": 39.0, "text": " mo\u017ce go pobi\u0107 w wi\u0119kszo\u015bci zada\u0144, a co wi\u0119cej, ten Dawid w walce z goliatem", "tokens": [50664, 12034, 352, 714, 5614, 2162, 261, 29968, 4765, 6199, 710, 1538, 5248, 11, 257, 598, 26004, 11, 2064, 28407, 327, 261, 21346, 384, 710, 352, 2081, 26851, 50964], "temperature": 0.0, "avg_logprob": -0.11165865262349446, "compression_ratio": 1.3970588235294117, "no_speech_prob": 0.12724557518959045}, {"id": 8, "seek": 2700, "start": 39.0, "end": 43.0, "text": " zosta\u0142 wytranowany wy\u0142a\u015bnie na publicznie dost\u0119pnych danych.", "tokens": [50964, 23154, 1221, 261, 4328, 4257, 23341, 4628, 5024, 12221, 1667, 1908, 89, 2766, 48209, 9399, 274, 34644, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11165865262349446, "compression_ratio": 1.3970588235294117, "no_speech_prob": 0.12724557518959045}, {"id": 9, "seek": 2700, "start": 43.0, "end": 46.0, "text": " I to by\u0142 wiesz moment, kt\u00f3ry zatrz\u0105s\u0142 posadami ca\u0142ej bran\u017cy.", "tokens": [51164, 286, 281, 16673, 261, 15347, 1623, 11, 9913, 35802, 81, 8925, 82, 1221, 1366, 345, 4526, 47631, 73, 12029, 7735, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11165865262349446, "compression_ratio": 1.3970588235294117, "no_speech_prob": 0.12724557518959045}, {"id": 10, "seek": 2700, "start": 46.0, "end": 47.0, "text": " Zdecydowanie.", "tokens": [51314, 1176, 1479, 1344, 67, 22028, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11165865262349446, "compression_ratio": 1.3970588235294117, "no_speech_prob": 0.12724557518959045}, {"id": 11, "seek": 2700, "start": 47.0, "end": 51.0, "text": " I to jest w\u0142a\u015bnie ten dokument, kt\u00f3remu si\u0119 dzisiaj przyjrzymy.", "tokens": [51364, 286, 281, 3492, 14234, 2064, 40858, 11, 4695, 2579, 84, 3244, 25772, 6501, 73, 13047, 2226, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11165865262349446, "compression_ratio": 1.3970588235294117, "no_speech_prob": 0.12724557518959045}, {"id": 12, "seek": 5100, "start": 51.0, "end": 59.0, "text": " Ta naukowa zespo\u0142u Meta AI zatytu\u0142owana LLMA Open and Efficient Foundation Language Models.", "tokens": [50364, 6551, 35616, 74, 5528, 710, 279, 2259, 24066, 6377, 64, 7318, 35802, 4328, 84, 1221, 40458, 441, 43, 9998, 7238, 293, 462, 7816, 10335, 24445, 6583, 1625, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12350495220863655, "compression_ratio": 1.4262295081967213, "no_speech_prob": 0.3904341161251068}, {"id": 13, "seek": 5100, "start": 59.0, "end": 63.0, "text": " To nie jest jaka\u015b tam kolejna, inkrementalna poprawka, nie, to co\u015b wi\u0119cej.", "tokens": [50764, 1407, 2838, 3492, 4207, 64, 1788, 7677, 23749, 629, 11, 11276, 265, 15875, 629, 1665, 5131, 2330, 11, 2838, 11, 281, 19241, 26004, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12350495220863655, "compression_ratio": 1.4262295081967213, "no_speech_prob": 0.3904341161251068}, {"id": 14, "seek": 5100, "start": 63.0, "end": 68.0, "text": " To jest jeden z tych tekst\u00f3w, kt\u00f3re, no, po prostu wyznaczaj\u0105 now\u0105 epok\u0119.", "tokens": [50964, 1407, 3492, 12906, 710, 15180, 16624, 372, 3901, 11, 8864, 11, 572, 11, 714, 19518, 4628, 22672, 14875, 11133, 586, 1611, 2388, 453, 1274, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12350495220863655, "compression_ratio": 1.4262295081967213, "no_speech_prob": 0.3904341161251068}, {"id": 15, "seek": 5100, "start": 68.0, "end": 72.0, "text": " Zmieniaj\u0105 zasady gry i, co chyba najwa\u017cniejsze, otwieraj\u0105 drzwi do rewolucji,", "tokens": [51214, 1176, 76, 18811, 8555, 26530, 880, 41974, 741, 11, 598, 31532, 11212, 27111, 44258, 11, 4337, 40717, 11133, 1224, 89, 6253, 360, 319, 48481, 1311, 4013, 11, 51414], "temperature": 0.0, "avg_logprob": -0.12350495220863655, "compression_ratio": 1.4262295081967213, "no_speech_prob": 0.3904341161251068}, {"id": 16, "seek": 5100, "start": 72.0, "end": 76.0, "text": " kt\u00f3ra dzieje si\u0119 na naszych oczach w \u015bwiecie Open Source AI.", "tokens": [51414, 19456, 17953, 2884, 3244, 1667, 45002, 277, 3689, 608, 261, 40078, 4260, 7238, 29629, 7318, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12350495220863655, "compression_ratio": 1.4262295081967213, "no_speech_prob": 0.3904341161251068}, {"id": 17, "seek": 5100, "start": 76.0, "end": 78.0, "text": " Nasza misja na dzi\u015b jest wi\u0119c prosta.", "tokens": [51614, 16151, 2394, 3346, 2938, 1667, 31981, 1788, 3492, 16677, 582, 8638, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12350495220863655, "compression_ratio": 1.4262295081967213, "no_speech_prob": 0.3904341161251068}, {"id": 18, "seek": 7800, "start": 78.0, "end": 81.0, "text": " Chcemy roz\u0142o\u017cy\u0107 t\u0119 prac\u0119 na czynniki pierwsze.", "tokens": [50364, 761, 384, 2226, 9544, 5249, 39687, 32489, 22404, 1274, 1667, 6430, 26384, 9850, 45994, 13, 50514], "temperature": 0.0, "avg_logprob": -0.05818867255113796, "compression_ratio": 1.5305466237942122, "no_speech_prob": 0.009187011979520321}, {"id": 19, "seek": 7800, "start": 81.0, "end": 86.0, "text": " Zrozumie\u0107, co tak dok\u0142adnie sprawi\u0142o, \u017ce te modele okaza\u0142y si\u0119, no, tak prze\u0142omowe.", "tokens": [50514, 1176, 27857, 449, 414, 2162, 11, 598, 991, 45864, 2766, 22734, 72, 5249, 11, 3561, 535, 4391, 306, 3133, 12257, 6825, 3244, 11, 572, 11, 991, 8325, 1221, 298, 6880, 13, 50764], "temperature": 0.0, "avg_logprob": -0.05818867255113796, "compression_ratio": 1.5305466237942122, "no_speech_prob": 0.009187011979520321}, {"id": 20, "seek": 7800, "start": 86.0, "end": 88.0, "text": " Zajrzymy pod mask\u0119.", "tokens": [50764, 1176, 1805, 13047, 2226, 2497, 6094, 1274, 13, 50864], "temperature": 0.0, "avg_logprob": -0.05818867255113796, "compression_ratio": 1.5305466237942122, "no_speech_prob": 0.009187011979520321}, {"id": 21, "seek": 7800, "start": 88.0, "end": 92.0, "text": " Zajrzymy pod mask\u0119, przeanalizujemy ich diet\u0119, na kt\u00f3rej by\u0142y karmione", "tokens": [50864, 1176, 1805, 13047, 2226, 2497, 6094, 1274, 11, 8325, 29702, 590, 21767, 1893, 6339, 1274, 11, 1667, 36023, 26366, 350, 4452, 5328, 51064], "temperature": 0.0, "avg_logprob": -0.05818867255113796, "compression_ratio": 1.5305466237942122, "no_speech_prob": 0.009187011979520321}, {"id": 22, "seek": 7800, "start": 92.0, "end": 96.0, "text": " i oczywi\u015bcie przyjrzymy si\u0119 wynikom, kt\u00f3re wywo\u0142a\u0142y takie poruszenie.", "tokens": [51064, 741, 23862, 6501, 73, 13047, 2226, 3244, 31936, 1035, 298, 11, 8864, 4628, 6120, 5024, 6825, 15963, 1515, 301, 16778, 13, 51264], "temperature": 0.0, "avg_logprob": -0.05818867255113796, "compression_ratio": 1.5305466237942122, "no_speech_prob": 0.009187011979520321}, {"id": 23, "seek": 7800, "start": 96.0, "end": 102.0, "text": " To b\u0119dzie analiza dla ka\u017cdego, kto chce poj\u0105\u0107, jak dosz\u0142o do jednej z najwi\u0119kszych rewolucji", "tokens": [51264, 1407, 10562, 2624, 13427, 12285, 21912, 67, 6308, 11, 23780, 28928, 714, 8555, 2162, 11, 4207, 4491, 89, 5249, 360, 5232, 11794, 710, 48636, 1694, 28051, 319, 48481, 1311, 4013, 51564], "temperature": 0.0, "avg_logprob": -0.05818867255113796, "compression_ratio": 1.5305466237942122, "no_speech_prob": 0.009187011979520321}, {"id": 24, "seek": 7800, "start": 102.0, "end": 104.0, "text": " w najnowszej historii sztucznej inteligencji.", "tokens": [51564, 261, 11212, 77, 1509, 16920, 4058, 5597, 262, 2682, 1311, 89, 11794, 24777, 3213, 19649, 13, 51664], "temperature": 0.0, "avg_logprob": -0.05818867255113796, "compression_ratio": 1.5305466237942122, "no_speech_prob": 0.009187011979520321}, {"id": 25, "seek": 7800, "start": 104.0, "end": 105.0, "text": " No to zaczynajmy.", "tokens": [51664, 883, 281, 43811, 20981, 2226, 13, 51714], "temperature": 0.0, "avg_logprob": -0.05818867255113796, "compression_ratio": 1.5305466237942122, "no_speech_prob": 0.009187011979520321}, {"id": 26, "seek": 10500, "start": 105.0, "end": 109.0, "text": " Dobrze, to zacznijmy od tej centralnej, niemal wywrotowej tezy.", "tokens": [50364, 29679, 13503, 11, 281, 710, 14875, 77, 1718, 2226, 3611, 12573, 5777, 11794, 11, 2838, 5579, 4628, 7449, 310, 21091, 535, 1229, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08365742027336824, "compression_ratio": 1.3911564625850341, "no_speech_prob": 0.03390674665570259}, {"id": 27, "seek": 10500, "start": 109.0, "end": 112.0, "text": " Co autorzy tak naprawd\u0119 chcieli udowodni\u0107?", "tokens": [50564, 3066, 19510, 1229, 991, 20970, 417, 537, 10148, 11727, 305, 378, 3722, 2162, 30, 50714], "temperature": 0.0, "avg_logprob": -0.08365742027336824, "compression_ratio": 1.3911564625850341, "no_speech_prob": 0.03390674665570259}, {"id": 28, "seek": 10500, "start": 112.0, "end": 118.0, "text": " Bo przecie\u017c, no, ca\u0142y \u015bwiat goni\u0142 za setkami miliard\u00f3w, a nawet bilionami parametr\u00f3w.", "tokens": [50714, 3286, 8325, 40082, 11, 572, 11, 35226, 36425, 290, 17049, 1221, 7949, 992, 48737, 1962, 72, 515, 3901, 11, 257, 22696, 8588, 313, 4526, 6220, 27965, 3901, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08365742027336824, "compression_ratio": 1.3911564625850341, "no_speech_prob": 0.03390674665570259}, {"id": 29, "seek": 10500, "start": 118.0, "end": 120.0, "text": " Znaczony przez GPT-3 by\u0142 jasny.", "tokens": [51014, 1176, 77, 14875, 2526, 14064, 26039, 51, 12, 18, 16673, 361, 296, 1634, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08365742027336824, "compression_ratio": 1.3911564625850341, "no_speech_prob": 0.03390674665570259}, {"id": 30, "seek": 10500, "start": 120.0, "end": 124.0, "text": " Wi\u0119cej, wi\u0119cej i jeszcze raz wi\u0119cej.", "tokens": [51114, 30127, 20811, 11, 26004, 741, 14168, 9639, 26004, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08365742027336824, "compression_ratio": 1.3911564625850341, "no_speech_prob": 0.03390674665570259}, {"id": 31, "seek": 10500, "start": 124.0, "end": 129.0, "text": " G\u0142\u00f3wne za\u0142o\u017cenie tej pracy by\u0142o w\u0142a\u015bnie, wiesz, tak\u0105 kontr\u0105 do tego trendu.", "tokens": [51314, 460, 1221, 3901, 716, 7949, 5249, 41118, 12573, 35591, 14811, 14234, 11, 261, 15347, 11, 31069, 14373, 32881, 360, 8627, 6028, 84, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08365742027336824, "compression_ratio": 1.3911564625850341, "no_speech_prob": 0.03390674665570259}, {"id": 32, "seek": 10500, "start": 129.0, "end": 133.0, "text": " Autorzy postanowili zada\u0107 zupe\u0142nie inne pytanie,", "tokens": [51564, 6049, 284, 1229, 2183, 282, 305, 2312, 710, 1538, 2162, 49922, 24170, 36610, 11, 51764], "temperature": 0.0, "avg_logprob": -0.08365742027336824, "compression_ratio": 1.3911564625850341, "no_speech_prob": 0.03390674665570259}, {"id": 33, "seek": 13300, "start": 133.0, "end": 138.0, "text": " bo do tej pory wszyscy optymalizowali swoje dzia\u0142ania pod k\u0105tem bud\u017cetu na trening.", "tokens": [50364, 748, 360, 12573, 280, 827, 44232, 2427, 4199, 304, 590, 305, 5103, 29489, 27121, 5609, 2497, 350, 1611, 18275, 3265, 1427, 41236, 1667, 2192, 773, 13, 50614], "temperature": 0.0, "avg_logprob": -0.0761683352885802, "compression_ratio": 1.4688524590163934, "no_speech_prob": 0.022973528131842613}, {"id": 34, "seek": 13300, "start": 138.0, "end": 142.0, "text": " Czyli na to jednorazowe, pot\u0119\u017cne odpalenie.", "tokens": [50614, 37099, 1667, 281, 5232, 19048, 921, 6880, 11, 1847, 1274, 1427, 716, 3611, 79, 21745, 414, 13, 50814], "temperature": 0.0, "avg_logprob": -0.0761683352885802, "compression_ratio": 1.4688524590163934, "no_speech_prob": 0.022973528131842613}, {"id": 35, "seek": 13300, "start": 142.0, "end": 143.0, "text": " Dok\u0142adnie.", "tokens": [50814, 29768, 10358, 2766, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0761683352885802, "compression_ratio": 1.4688524590163934, "no_speech_prob": 0.022973528131842613}, {"id": 36, "seek": 13300, "start": 143.0, "end": 149.0, "text": " Jak najefektywniej wykorzysta\u0107 ogromn\u0105 moc obliczeniow\u0105, by jednorazowo stworzy\u0107 jak najwi\u0119kszy model?", "tokens": [50864, 15029, 11212, 5666, 916, 874, 895, 7764, 43606, 49590, 2162, 34416, 298, 13113, 34962, 1111, 1050, 42124, 30297, 11, 538, 5232, 19048, 921, 19941, 342, 28321, 27150, 4207, 48636, 1694, 1229, 2316, 30, 51164], "temperature": 0.0, "avg_logprob": -0.0761683352885802, "compression_ratio": 1.4688524590163934, "no_speech_prob": 0.022973528131842613}, {"id": 37, "seek": 13300, "start": 149.0, "end": 151.0, "text": " A zesp\u00f3\u0142 Meta stwierdzi\u0142.", "tokens": [51164, 316, 710, 13361, 16181, 6377, 64, 342, 40717, 67, 3992, 1221, 13, 51264], "temperature": 0.0, "avg_logprob": -0.0761683352885802, "compression_ratio": 1.4688524590163934, "no_speech_prob": 0.022973528131842613}, {"id": 38, "seek": 13300, "start": 151.0, "end": 153.0, "text": " A co je\u015bli to jest z\u0142e podej\u015bcie?", "tokens": [51264, 316, 598, 25630, 281, 3492, 710, 19827, 7468, 73, 9815, 30, 51364], "temperature": 0.0, "avg_logprob": -0.0761683352885802, "compression_ratio": 1.4688524590163934, "no_speech_prob": 0.022973528131842613}, {"id": 39, "seek": 13300, "start": 153.0, "end": 158.0, "text": " Co je\u015bli prawdziwym kosztem, tym w\u0105skim gard\u0142em, jest bud\u017cet na inferencj\u0119?", "tokens": [51364, 3066, 25630, 41175, 3992, 86, 4199, 19532, 2682, 443, 11, 8107, 261, 1611, 5161, 332, 5628, 11126, 11, 3492, 3265, 1427, 302, 1667, 13596, 22660, 11115, 30, 51614], "temperature": 0.0, "avg_logprob": -0.0761683352885802, "compression_ratio": 1.4688524590163934, "no_speech_prob": 0.022973528131842613}, {"id": 40, "seek": 13300, "start": 158.0, "end": 161.0, "text": " Czyli nato faktyczne codzienne u\u017cywanie modelu?", "tokens": [51614, 37099, 2249, 78, 33647, 874, 38491, 17656, 89, 21262, 34097, 86, 7155, 2316, 84, 30, 51764], "temperature": 0.0, "avg_logprob": -0.0761683352885802, "compression_ratio": 1.4688524590163934, "no_speech_prob": 0.022973528131842613}, {"id": 41, "seek": 16100, "start": 161.0, "end": 165.0, "text": " Tak, milion razy na sekund\u0119 przez miliony u\u017cytkownik\u00f3w.", "tokens": [50364, 9118, 11, 1962, 313, 9639, 88, 1667, 17215, 997, 1274, 14064, 1962, 46184, 344, 1427, 4328, 74, 44895, 3901, 13, 50564], "temperature": 0.0, "avg_logprob": -0.067017027537028, "compression_ratio": 1.4155844155844155, "no_speech_prob": 0.000689624110236764}, {"id": 42, "seek": 16100, "start": 165.0, "end": 170.0, "text": " Czyli ich hipoteza by\u0142a, jak ju\u017c powiedzia\u0142am, wr\u0119cz heretyczna jak na tamte czasy.", "tokens": [50564, 37099, 1893, 8103, 1370, 2394, 23936, 11, 4207, 10678, 48539, 335, 11, 928, 1274, 3689, 720, 2210, 3689, 629, 4207, 1667, 7677, 975, 6472, 5871, 13, 50814], "temperature": 0.0, "avg_logprob": -0.067017027537028, "compression_ratio": 1.4155844155844155, "no_speech_prob": 0.000689624110236764}, {"id": 43, "seek": 16100, "start": 170.0, "end": 171.0, "text": " Stwierdzili.", "tokens": [50814, 745, 40717, 28168, 2312, 13, 50864], "temperature": 0.0, "avg_logprob": -0.067017027537028, "compression_ratio": 1.4155844155844155, "no_speech_prob": 0.000689624110236764}, {"id": 44, "seek": 16100, "start": 171.0, "end": 174.0, "text": " A co je\u015bli wszyscy w bran\u017cy biegn\u0105 w z\u0142\u0105 stron\u0119?", "tokens": [50864, 316, 598, 25630, 44232, 261, 12029, 7735, 272, 414, 4568, 1611, 261, 710, 15926, 45766, 1274, 30, 51014], "temperature": 0.0, "avg_logprob": -0.067017027537028, "compression_ratio": 1.4155844155844155, "no_speech_prob": 0.000689624110236764}, {"id": 45, "seek": 16100, "start": 174.0, "end": 180.0, "text": " Zamiast budowa\u0107 coraz dro\u017csze silniki rakietowe, kt\u00f3re odpalasz raz,", "tokens": [51014, 1176, 4526, 525, 3265, 11445, 25899, 3789, 1427, 82, 1381, 3425, 77, 9850, 35544, 1684, 6880, 11, 8864, 3611, 31862, 19601, 9639, 11, 51314], "temperature": 0.0, "avg_logprob": -0.067017027537028, "compression_ratio": 1.4155844155844155, "no_speech_prob": 0.000689624110236764}, {"id": 46, "seek": 16100, "start": 180.0, "end": 186.0, "text": " skupmy si\u0119 na stworzeniu czego\u015b, co b\u0119dzie dzia\u0142a\u0107 tanio i niezawodnie na masow\u0105 skal\u0119.", "tokens": [51314, 1110, 1010, 2226, 3244, 1667, 342, 28321, 39651, 36559, 1788, 11, 598, 10562, 37903, 2162, 7603, 1004, 741, 33511, 1607, 378, 2766, 1667, 2300, 30297, 16890, 1274, 13, 51614], "temperature": 0.0, "avg_logprob": -0.067017027537028, "compression_ratio": 1.4155844155844155, "no_speech_prob": 0.000689624110236764}, {"id": 47, "seek": 16100, "start": 186.0, "end": 190.0, "text": " To by\u0142a fundamentalna zmiana my\u015blenia o ekonomii AI.", "tokens": [51614, 1407, 23936, 8088, 629, 17020, 8497, 48633, 6698, 654, 277, 13359, 12481, 5597, 7318, 13, 51814], "temperature": 0.0, "avg_logprob": -0.067017027537028, "compression_ratio": 1.4155844155844155, "no_speech_prob": 0.000689624110236764}, {"id": 48, "seek": 19000, "start": 190.0, "end": 194.0, "text": " Dok\u0142adnie. I oni postawili tez\u0119, \u017ce dla osi\u0105gni\u0119cia okre\u015blonego poziomu,", "tokens": [50364, 29768, 10358, 2766, 13, 286, 36317, 2183, 1607, 2312, 535, 11052, 11, 3561, 12285, 3003, 11404, 70, 35938, 2755, 3133, 265, 19212, 546, 1571, 38503, 298, 84, 11, 50564], "temperature": 0.0, "avg_logprob": -0.06870954686945135, "compression_ratio": 1.4628975265017667, "no_speech_prob": 0.018843984231352806}, {"id": 49, "seek": 19000, "start": 194.0, "end": 199.0, "text": " powiedzmy inteligencji, znacznie lepszy jest mniejszy model,", "tokens": [50564, 27617, 2226, 24777, 3213, 19649, 11, 15397, 14875, 2766, 476, 1878, 1229, 3492, 39513, 7706, 2316, 11, 50814], "temperature": 0.0, "avg_logprob": -0.06870954686945135, "compression_ratio": 1.4628975265017667, "no_speech_prob": 0.018843984231352806}, {"id": 50, "seek": 19000, "start": 199.0, "end": 202.0, "text": " ale wytrenowany na absurdalnie du\u017cej ilo\u015bci danych,", "tokens": [50814, 6775, 261, 4328, 1095, 23341, 1667, 19774, 304, 2766, 1581, 38493, 1930, 44468, 274, 34644, 11, 50964], "temperature": 0.0, "avg_logprob": -0.06870954686945135, "compression_ratio": 1.4628975265017667, "no_speech_prob": 0.018843984231352806}, {"id": 51, "seek": 19000, "start": 202.0, "end": 207.0, "text": " ni\u017c gigantyczny model, kt\u00f3rego trening przerwano wcze\u015bniej z powodu koszt\u00f3w.", "tokens": [50964, 28502, 8741, 394, 17466, 1634, 2316, 11, 46951, 2192, 773, 582, 4527, 86, 3730, 40785, 710, 3388, 34873, 19532, 2682, 3901, 13, 51214], "temperature": 0.0, "avg_logprob": -0.06870954686945135, "compression_ratio": 1.4628975265017667, "no_speech_prob": 0.018843984231352806}, {"id": 52, "seek": 19000, "start": 207.0, "end": 212.0, "text": " Taki mniejszy model jest na ko\u0144cu dnia nieporywnywalnie ta\u0144szy i szybsz w utrzymaniu.", "tokens": [51214, 314, 7421, 39513, 7706, 2316, 3492, 1667, 26470, 12032, 274, 12679, 2838, 79, 827, 43682, 29530, 2766, 1846, 5248, 7706, 741, 30526, 929, 89, 261, 2839, 13047, 1601, 5951, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06870954686945135, "compression_ratio": 1.4628975265017667, "no_speech_prob": 0.018843984231352806}, {"id": 53, "seek": 19000, "start": 212.0, "end": 215.0, "text": " I praca pokaza\u0142a co\u015b fascynuj\u0105cego na przyk\u0142ad.", "tokens": [51464, 286, 582, 6628, 13010, 12257, 5024, 19241, 30632, 1344, 77, 13263, 384, 1571, 1667, 23144, 13, 51614], "temperature": 0.0, "avg_logprob": -0.06870954686945135, "compression_ratio": 1.4628975265017667, "no_speech_prob": 0.018843984231352806}, {"id": 54, "seek": 21500, "start": 215.0, "end": 219.0, "text": " Wydajno\u015b\u0107 modelu 7B, czyli z siedmioma miliardami parametr\u00f3w,", "tokens": [50364, 343, 6655, 1805, 23293, 2316, 84, 1614, 33, 11, 16591, 710, 262, 1091, 3057, 6440, 1962, 72, 515, 4526, 6220, 27965, 3901, 11, 50564], "temperature": 0.0, "avg_logprob": -0.08328888898978204, "compression_ratio": 1.4627831715210355, "no_speech_prob": 0.42855173349380493}, {"id": 55, "seek": 21500, "start": 219.0, "end": 223.0, "text": " ona wci\u0105\u017c ros\u0142a i ros\u0142a nawet po przetworzeniu biliona token\u00f3w danych.", "tokens": [50564, 20325, 261, 537, 27242, 18953, 5024, 741, 18953, 5024, 22696, 714, 6541, 302, 28321, 39651, 8588, 21758, 14862, 3901, 274, 34644, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08328888898978204, "compression_ratio": 1.4627831715210355, "no_speech_prob": 0.42855173349380493}, {"id": 56, "seek": 21500, "start": 223.0, "end": 225.0, "text": " Co by\u0142o wbrew logice, tak.", "tokens": [50764, 3066, 14811, 261, 65, 2236, 3565, 573, 11, 991, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08328888898978204, "compression_ratio": 1.4627831715210355, "no_speech_prob": 0.42855173349380493}, {"id": 57, "seek": 21500, "start": 225.0, "end": 229.0, "text": " Wydajno\u015bci jest fascynuj\u0105ce, ale wiesz, teoria to jedno.", "tokens": [50864, 343, 6655, 1805, 16438, 3492, 30632, 1344, 77, 13263, 384, 11, 6775, 261, 15347, 11, 535, 8172, 281, 5232, 1771, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08328888898978204, "compression_ratio": 1.4627831715210355, "no_speech_prob": 0.42855173349380493}, {"id": 58, "seek": 21500, "start": 229.0, "end": 233.0, "text": " Diabe\u0142d kwi w szczeg\u00f3\u0142ach, jak oni w\u0142a\u015bciwie tego dokonali.", "tokens": [51064, 8789, 4488, 1221, 67, 350, 6253, 261, 22090, 1146, 16181, 608, 11, 4207, 36317, 50108, 8627, 360, 18295, 5103, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08328888898978204, "compression_ratio": 1.4627831715210355, "no_speech_prob": 0.42855173349380493}, {"id": 59, "seek": 21500, "start": 233.0, "end": 236.0, "text": " Domy\u015blam, \u017ce kluczem nie by\u0142a tylko architektura,", "tokens": [51264, 413, 8488, 1788, 4326, 11, 3561, 9671, 1311, 24313, 2838, 23936, 13219, 3912, 642, 2320, 2991, 11, 51414], "temperature": 0.0, "avg_logprob": -0.08328888898978204, "compression_ratio": 1.4627831715210355, "no_speech_prob": 0.42855173349380493}, {"id": 60, "seek": 21500, "start": 236.0, "end": 239.0, "text": " ale przede wszystkim to, czym ten model karmiono.", "tokens": [51414, 6775, 44786, 30481, 281, 11, 31466, 2064, 2316, 350, 4452, 49020, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08328888898978204, "compression_ratio": 1.4627831715210355, "no_speech_prob": 0.42855173349380493}, {"id": 61, "seek": 21500, "start": 239.0, "end": 243.0, "text": " Trafiasz w sedno. Zbi\u00f3r danych by\u0142 absolutnie kluczowy", "tokens": [51564, 5403, 69, 4609, 89, 261, 9643, 1771, 13, 1176, 5614, 15614, 274, 34644, 16673, 18757, 2766, 9671, 1311, 89, 10089, 51764], "temperature": 0.0, "avg_logprob": -0.08328888898978204, "compression_ratio": 1.4627831715210355, "no_speech_prob": 0.42855173349380493}, {"id": 62, "seek": 24300, "start": 243.0, "end": 246.0, "text": " i wyr\u00f3\u017cnia\u0142 si\u0119 jedn\u0105 fundamentaln\u0105 cech\u0105.", "tokens": [50364, 741, 4628, 11721, 1427, 77, 8908, 3244, 5232, 13113, 8088, 13113, 1769, 339, 1611, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1346925518469896, "compression_ratio": 1.3819875776397517, "no_speech_prob": 0.006434869486838579}, {"id": 63, "seek": 24300, "start": 246.0, "end": 250.0, "text": " By\u0142 w 100% jawny, ca\u0142kowicie publiczny.", "tokens": [50514, 3146, 1221, 261, 2319, 4, 2784, 43682, 11, 35224, 74, 305, 28434, 1908, 89, 1634, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1346925518469896, "compression_ratio": 1.3819875776397517, "no_speech_prob": 0.006434869486838579}, {"id": 64, "seek": 24300, "start": 250.0, "end": 253.0, "text": " Tak, skomponowany z publicznie dost\u0119pnych \u017ar\u00f3de\u0142.", "tokens": [50714, 9118, 11, 1110, 8586, 266, 23341, 710, 1908, 89, 2766, 48209, 9399, 50212, 11721, 1479, 1221, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1346925518469896, "compression_ratio": 1.3819875776397517, "no_speech_prob": 0.006434869486838579}, {"id": 65, "seek": 24300, "start": 253.0, "end": 256.0, "text": " I to by\u0142, no powiedzmy sobie szczerze,", "tokens": [50864, 286, 281, 16673, 11, 572, 27617, 2226, 13652, 22090, 260, 1381, 11, 51014], "temperature": 0.0, "avg_logprob": -0.1346925518469896, "compression_ratio": 1.3819875776397517, "no_speech_prob": 0.006434869486838579}, {"id": 66, "seek": 24300, "start": 256.0, "end": 259.0, "text": " policzek wymierzony w dotychczasow\u0105 praktyk\u0119,", "tokens": [51014, 6285, 19878, 29764, 811, 44479, 261, 5893, 16384, 30989, 30297, 3206, 74, 874, 15724, 11, 51164], "temperature": 0.0, "avg_logprob": -0.1346925518469896, "compression_ratio": 1.3819875776397517, "no_speech_prob": 0.006434869486838579}, {"id": 67, "seek": 24300, "start": 259.0, "end": 263.0, "text": " gdzie modele takie jak GPT-3 trenowano na cz\u0119\u015bciowo tajnych zbiorach.", "tokens": [51164, 18922, 4391, 306, 15963, 4207, 26039, 51, 12, 18, 23136, 305, 3730, 1667, 41314, 19941, 256, 1805, 9399, 710, 33362, 608, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1346925518469896, "compression_ratio": 1.3819875776397517, "no_speech_prob": 0.006434869486838579}, {"id": 68, "seek": 24300, "start": 263.0, "end": 267.0, "text": " Pami\u0119tam, by\u0142 ten s\u0142ynny enigmatyczny zbi\u00f3r Bux 2.", "tokens": [51364, 430, 23806, 37323, 11, 16673, 2064, 15116, 2534, 1634, 465, 46496, 17466, 1634, 710, 5614, 15614, 363, 2449, 568, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1346925518469896, "compression_ratio": 1.3819875776397517, "no_speech_prob": 0.006434869486838579}, {"id": 69, "seek": 24300, "start": 267.0, "end": 271.0, "text": " Dok\u0142adnie, o rzemie o zbiarze dw\u00f3ch terabajt\u00f3w, o kt\u00f3rym nikt nic nie wiedzia\u0142.", "tokens": [51564, 29768, 10358, 2766, 11, 277, 367, 24313, 414, 277, 710, 5614, 289, 1381, 27379, 812, 339, 1796, 455, 1805, 83, 3901, 11, 277, 30120, 297, 9874, 6201, 2838, 261, 15338, 8908, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1346925518469896, "compression_ratio": 1.3819875776397517, "no_speech_prob": 0.006434869486838579}, {"id": 70, "seek": 27100, "start": 271.0, "end": 275.0, "text": " A meta pokaza\u0142a, \u017ce hej, nie trzeba mie\u0107 sekrepnych sk\u0142adnik\u00f3w.", "tokens": [50364, 316, 19616, 13010, 12257, 5024, 11, 3561, 415, 73, 11, 2838, 25860, 35612, 17215, 19919, 9399, 1110, 10358, 47447, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13333662350972494, "compression_ratio": 1.3563218390804597, "no_speech_prob": 0.05076412111520767}, {"id": 71, "seek": 27100, "start": 275.0, "end": 278.0, "text": " Mo\u017cna to zrobi\u0107 na otwartych danych.", "tokens": [50564, 44736, 629, 281, 31785, 1667, 4337, 29587, 16384, 274, 34644, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13333662350972494, "compression_ratio": 1.3563218390804597, "no_speech_prob": 0.05076412111520767}, {"id": 72, "seek": 27100, "start": 278.0, "end": 282.0, "text": " No dobrze, to co znalaz\u0142o si\u0119 w tym jawnym menu?", "tokens": [50714, 883, 28335, 11, 281, 598, 710, 4660, 921, 5249, 3244, 261, 8107, 2784, 895, 4199, 6510, 30, 50914], "temperature": 0.0, "avg_logprob": -0.13333662350972494, "compression_ratio": 1.3563218390804597, "no_speech_prob": 0.05076412111520767}, {"id": 73, "seek": 27100, "start": 282.0, "end": 286.0, "text": " Co z\u0142o\u017cy\u0142o si\u0119 na te 1,4 biliona token\u00f3w?", "tokens": [50914, 3066, 710, 5249, 7735, 5249, 3244, 1667, 535, 502, 11, 19, 8588, 21758, 14862, 3901, 30, 51114], "temperature": 0.0, "avg_logprob": -0.13333662350972494, "compression_ratio": 1.3563218390804597, "no_speech_prob": 0.05076412111520767}, {"id": 74, "seek": 27100, "start": 286.0, "end": 290.0, "text": " Lwi\u0105 cz\u0119\u015b\u0107, bo a\u017c 67% stanowi\u0142 common crawl,", "tokens": [51114, 441, 86, 11404, 47149, 11, 748, 48134, 23879, 4, 27984, 24503, 1221, 2689, 24767, 11, 51314], "temperature": 0.0, "avg_logprob": -0.13333662350972494, "compression_ratio": 1.3563218390804597, "no_speech_prob": 0.05076412111520767}, {"id": 75, "seek": 27100, "start": 290.0, "end": 293.0, "text": " czyli gigantyczny zrzut publicznej cz\u0119\u015bci internetu.", "tokens": [51314, 16591, 8741, 394, 17466, 1634, 710, 19390, 325, 1908, 89, 11794, 41314, 4705, 84, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13333662350972494, "compression_ratio": 1.3563218390804597, "no_speech_prob": 0.05076412111520767}, {"id": 76, "seek": 27100, "start": 293.0, "end": 296.0, "text": " Ale co wa\u017cne, on zosta\u0142 bardzo starannie", "tokens": [51464, 9366, 598, 46110, 11, 322, 23154, 1221, 9034, 3543, 43433, 51614], "temperature": 0.0, "avg_logprob": -0.13333662350972494, "compression_ratio": 1.3563218390804597, "no_speech_prob": 0.05076412111520767}, {"id": 77, "seek": 29600, "start": 296.0, "end": 300.0, "text": " przefiltrowany, \u017ceby odsia\u0107 tre\u015bci niskiej jako\u015bci.", "tokens": [50364, 8325, 69, 2352, 1892, 1325, 11, 11316, 3611, 82, 654, 2162, 2192, 6199, 297, 7797, 7764, 17123, 6199, 13, 50564], "temperature": 0.0, "avg_logprob": -0.14044790903727214, "compression_ratio": 1.3258064516129033, "no_speech_prob": 0.29437577724456787}, {"id": 78, "seek": 29600, "start": 300.0, "end": 302.0, "text": " No a po prostu \u015bmieci.", "tokens": [50564, 883, 257, 714, 19518, 8299, 25210, 537, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14044790903727214, "compression_ratio": 1.3258064516129033, "no_speech_prob": 0.29437577724456787}, {"id": 79, "seek": 29600, "start": 302.0, "end": 305.0, "text": " Do tego do\u0142o\u017cono 15% ze zbioru C4,", "tokens": [50664, 1144, 8627, 360, 5249, 1427, 8957, 2119, 4, 5277, 710, 33362, 84, 383, 19, 11, 50814], "temperature": 0.0, "avg_logprob": -0.14044790903727214, "compression_ratio": 1.3258064516129033, "no_speech_prob": 0.29437577724456787}, {"id": 80, "seek": 29600, "start": 305.0, "end": 309.0, "text": " kt\u00f3ry jest inn\u0105 r\u00f3wnie\u017c oczyszczon\u0105 wersj\u0105 common crawl.", "tokens": [50814, 9913, 3492, 7714, 1611, 20532, 277, 3689, 20589, 3689, 266, 1611, 261, 433, 8555, 2689, 24767, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14044790903727214, "compression_ratio": 1.3258064516129033, "no_speech_prob": 0.29437577724456787}, {"id": 81, "seek": 29600, "start": 309.0, "end": 313.0, "text": " A dalej mamy cztery sk\u0142adniki po 4,5% ka\u017cdy.", "tokens": [51014, 316, 34257, 17335, 6472, 12733, 1110, 10358, 77, 9850, 714, 1017, 11, 20, 4, 31615, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14044790903727214, "compression_ratio": 1.3258064516129033, "no_speech_prob": 0.29437577724456787}, {"id": 82, "seek": 29600, "start": 313.0, "end": 314.0, "text": " Czyli co to by\u0142o?", "tokens": [51214, 37099, 598, 281, 14811, 30, 51264], "temperature": 0.0, "avg_logprob": -0.14044790903727214, "compression_ratio": 1.3258064516129033, "no_speech_prob": 0.29437577724456787}, {"id": 83, "seek": 29600, "start": 314.0, "end": 319.0, "text": " Publiczne repozytoria kodu z GitHub'a, artyku\u0142y z Wikipedia w 20 j\u0119zykach,", "tokens": [51264, 9489, 43077, 1085, 78, 1229, 83, 8172, 350, 34873, 710, 23331, 6, 64, 11, 594, 874, 5279, 6825, 710, 28999, 261, 945, 49055, 41326, 11, 51514], "temperature": 0.0, "avg_logprob": -0.14044790903727214, "compression_ratio": 1.3258064516129033, "no_speech_prob": 0.29437577724456787}, {"id": 84, "seek": 29600, "start": 319.0, "end": 322.0, "text": " zbiory ksi\u0105\u017cek, tak jak projekt Gutenberg i Bookstree.", "tokens": [51514, 710, 5614, 827, 39311, 916, 11, 991, 4207, 26261, 42833, 6873, 741, 9476, 372, 701, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14044790903727214, "compression_ratio": 1.3258064516129033, "no_speech_prob": 0.29437577724456787}, {"id": 85, "seek": 29600, "start": 322.0, "end": 324.0, "text": " A i chyba co\u015b jeszcze z nauki.", "tokens": [51664, 316, 741, 31532, 19241, 14168, 710, 35616, 2984, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14044790903727214, "compression_ratio": 1.3258064516129033, "no_speech_prob": 0.29437577724456787}, {"id": 86, "seek": 32400, "start": 324.0, "end": 330.0, "text": " I co ciekawe, prace naukowe z serwisu Archive to by\u0142o 2,5%.", "tokens": [50364, 286, 598, 30596, 2330, 826, 11, 582, 617, 35616, 74, 6880, 710, 816, 86, 25871, 10984, 488, 281, 14811, 568, 11, 20, 6856, 50664], "temperature": 0.0, "avg_logprob": -0.08793816604013519, "compression_ratio": 1.3455882352941178, "no_speech_prob": 0.07720557600259781}, {"id": 87, "seek": 32400, "start": 330.0, "end": 336.0, "text": " I wysokiej jako\u015bci pytania i odpowiedzi z forum Stack Exchange, czyli 2%.", "tokens": [50664, 286, 27062, 453, 7764, 17123, 6199, 25878, 5609, 741, 36574, 3992, 710, 17542, 37649, 31169, 11, 16591, 568, 6856, 50964], "temperature": 0.0, "avg_logprob": -0.08793816604013519, "compression_ratio": 1.3455882352941178, "no_speech_prob": 0.07720557600259781}, {"id": 88, "seek": 32400, "start": 336.0, "end": 340.0, "text": " Czyli pe\u0142na transparentno\u015b\u0107 i du\u017ca r\u00f3\u017cnorodno\u015b\u0107.", "tokens": [50964, 37099, 43205, 629, 12737, 23293, 741, 21783, 64, 19637, 19048, 378, 23293, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08793816604013519, "compression_ratio": 1.3455882352941178, "no_speech_prob": 0.07720557600259781}, {"id": 89, "seek": 32400, "start": 340.0, "end": 342.0, "text": " A co z samym silnikiem, z architektur\u0105,", "tokens": [51164, 316, 598, 710, 3247, 4199, 3425, 13123, 4907, 11, 710, 3912, 642, 2320, 374, 1611, 11, 51264], "temperature": 0.0, "avg_logprob": -0.08793816604013519, "compression_ratio": 1.3455882352941178, "no_speech_prob": 0.07720557600259781}, {"id": 90, "seek": 32400, "start": 342.0, "end": 348.0, "text": " czy oni wymy\u015blili jaki\u015b zupe\u0142nie nowy, rewolucyjny rodzaj sieci neuronowej?", "tokens": [51264, 6430, 36317, 4628, 2226, 19212, 2312, 34721, 49922, 586, 88, 11, 319, 48481, 1311, 88, 73, 1634, 28607, 1805, 2804, 537, 34090, 21091, 30, 51564], "temperature": 0.0, "avg_logprob": -0.08793816604013519, "compression_ratio": 1.3455882352941178, "no_speech_prob": 0.07720557600259781}, {"id": 91, "seek": 32400, "start": 348.0, "end": 352.0, "text": " I tu dochodzimy do kolejnego fascynuj\u0105cego elementu.", "tokens": [51564, 286, 2604, 9243, 378, 89, 13189, 360, 23749, 11858, 30632, 1344, 77, 13263, 384, 1571, 4478, 84, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08793816604013519, "compression_ratio": 1.3455882352941178, "no_speech_prob": 0.07720557600259781}, {"id": 92, "seek": 35200, "start": 352.0, "end": 354.0, "text": " Nie, nie wymy\u015blili.", "tokens": [50364, 12016, 11, 2838, 4628, 2226, 19212, 2312, 13, 50464], "temperature": 0.0, "avg_logprob": -0.05795343608072359, "compression_ratio": 1.4641509433962263, "no_speech_prob": 0.006294562015682459}, {"id": 93, "seek": 35200, "start": 354.0, "end": 356.0, "text": " I to jest w tym wszystkim pi\u0119kne.", "tokens": [50464, 286, 281, 3492, 261, 8107, 30481, 48085, 716, 13, 50564], "temperature": 0.0, "avg_logprob": -0.05795343608072359, "compression_ratio": 1.4641509433962263, "no_speech_prob": 0.006294562015682459}, {"id": 94, "seek": 35200, "start": 356.0, "end": 358.0, "text": " Czyli nie rewolucja?", "tokens": [50564, 37099, 2838, 319, 48481, 1311, 2938, 30, 50664], "temperature": 0.0, "avg_logprob": -0.05795343608072359, "compression_ratio": 1.4641509433962263, "no_speech_prob": 0.006294562015682459}, {"id": 95, "seek": 35200, "start": 358.0, "end": 362.0, "text": " Zamiast rewolucji postawili na inteligentn\u0105 ewolucj\u0119.", "tokens": [50664, 1176, 4526, 525, 319, 48481, 1311, 4013, 2183, 1607, 2312, 1667, 24777, 25002, 13113, 43364, 401, 1311, 11115, 13, 50864], "temperature": 0.0, "avg_logprob": -0.05795343608072359, "compression_ratio": 1.4641509433962263, "no_speech_prob": 0.006294562015682459}, {"id": 96, "seek": 35200, "start": 362.0, "end": 366.0, "text": " Mo\u017cna to por\u00f3wna\u0107 do, nie wiem, tuninmu silnika samochodowego.", "tokens": [50864, 44736, 629, 281, 1515, 3901, 629, 2162, 360, 11, 2838, 26522, 11, 4267, 259, 20140, 3425, 77, 5439, 3247, 8997, 378, 26576, 13, 51064], "temperature": 0.0, "avg_logprob": -0.05795343608072359, "compression_ratio": 1.4641509433962263, "no_speech_prob": 0.006294562015682459}, {"id": 97, "seek": 35200, "start": 366.0, "end": 369.0, "text": " Nie zaprojektowali nowego bloku silnika,", "tokens": [51064, 12016, 14223, 340, 14930, 305, 5103, 586, 6308, 888, 13275, 3425, 77, 5439, 11, 51214], "temperature": 0.0, "avg_logprob": -0.05795343608072359, "compression_ratio": 1.4641509433962263, "no_speech_prob": 0.006294562015682459}, {"id": 98, "seek": 35200, "start": 369.0, "end": 372.0, "text": " ale wzi\u0119li sprawdzon\u0105 architektur\u0119 Transformer", "tokens": [51214, 6775, 261, 16706, 2081, 46192, 35296, 1611, 3912, 642, 2320, 374, 1274, 27938, 260, 51364], "temperature": 0.0, "avg_logprob": -0.05795343608072359, "compression_ratio": 1.4641509433962263, "no_speech_prob": 0.006294562015682459}, {"id": 99, "seek": 35200, "start": 372.0, "end": 377.0, "text": " i zastosowali trzy najlepsze sprawdzone ju\u017c na rynku ulepszenia.", "tokens": [51364, 741, 36746, 329, 305, 5103, 34573, 41903, 1878, 1381, 46192, 16896, 10678, 1667, 367, 2534, 5279, 344, 306, 1878, 14320, 13, 51614], "temperature": 0.0, "avg_logprob": -0.05795343608072359, "compression_ratio": 1.4641509433962263, "no_speech_prob": 0.006294562015682459}, {"id": 100, "seek": 35200, "start": 377.0, "end": 380.0, "text": " Okej, jakie to by\u0142y ulepszenia?", "tokens": [51614, 29094, 73, 11, 22124, 281, 26366, 344, 306, 1878, 14320, 30, 51764], "temperature": 0.0, "avg_logprob": -0.05795343608072359, "compression_ratio": 1.4641509433962263, "no_speech_prob": 0.006294562015682459}, {"id": 101, "seek": 38000, "start": 380.0, "end": 382.0, "text": " Czym dok\u0142adnie podkr\u0119cili ten silnik?", "tokens": [50364, 19832, 76, 45864, 2766, 2497, 38553, 1274, 66, 2312, 2064, 3425, 13123, 30, 50464], "temperature": 0.0, "avg_logprob": -0.10702860270831602, "compression_ratio": 1.3693379790940767, "no_speech_prob": 0.010315642692148685}, {"id": 102, "seek": 38000, "start": 382.0, "end": 386.0, "text": " Pierwsze to pre-normalization z u\u017cyciem RMS norm.", "tokens": [50464, 16676, 14358, 1381, 281, 659, 12, 23157, 2144, 710, 34097, 4260, 76, 497, 10288, 2026, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10702860270831602, "compression_ratio": 1.3693379790940767, "no_speech_prob": 0.010315642692148685}, {"id": 103, "seek": 38000, "start": 386.0, "end": 388.0, "text": " To jest bardzo wa\u017cny detal, bo wiesz,", "tokens": [50664, 1407, 3492, 9034, 27777, 1634, 33185, 11, 748, 261, 15347, 11, 50764], "temperature": 0.0, "avg_logprob": -0.10702860270831602, "compression_ratio": 1.3693379790940767, "no_speech_prob": 0.010315642692148685}, {"id": 104, "seek": 38000, "start": 388.0, "end": 392.0, "text": " trening gigantycznego modelu jest jak balansowanie wie\u017cy sklodzk\u00f3w.", "tokens": [50764, 2192, 773, 8741, 394, 17466, 11858, 2316, 84, 3492, 4207, 3119, 599, 22028, 3355, 7735, 1110, 75, 378, 89, 23849, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10702860270831602, "compression_ratio": 1.3693379790940767, "no_speech_prob": 0.010315642692148685}, {"id": 105, "seek": 38000, "start": 392.0, "end": 394.0, "text": " \u0141atwo o katastrof\u0119.", "tokens": [50964, 36901, 267, 6120, 277, 16536, 525, 340, 69, 1274, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10702860270831602, "compression_ratio": 1.3693379790940767, "no_speech_prob": 0.010315642692148685}, {"id": 106, "seek": 38000, "start": 394.0, "end": 395.0, "text": " Dok\u0142adnie.", "tokens": [51064, 29768, 10358, 2766, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10702860270831602, "compression_ratio": 1.3693379790940767, "no_speech_prob": 0.010315642692148685}, {"id": 107, "seek": 38000, "start": 395.0, "end": 401.0, "text": " Bez odpowiedniej stabilizacji warto\u015bci wewn\u0105trz sieci mog\u0105 eksplodowa\u0107 lub zanika\u0107,", "tokens": [51114, 879, 89, 36574, 10402, 11652, 590, 13152, 31830, 6199, 321, 895, 1611, 6903, 89, 2804, 537, 34123, 30724, 564, 378, 11445, 15980, 710, 282, 5439, 2162, 11, 51414], "temperature": 0.0, "avg_logprob": -0.10702860270831602, "compression_ratio": 1.3693379790940767, "no_speech_prob": 0.010315642692148685}, {"id": 108, "seek": 38000, "start": 401.0, "end": 406.0, "text": " a ca\u0142y wielotygodniowy piekielnie drugi proces ko\u0144czy si\u0119 fiaskiem.", "tokens": [51414, 257, 35226, 20570, 6737, 21787, 3722, 10089, 1730, 74, 1187, 2766, 4110, 72, 17565, 26470, 6522, 3244, 15848, 3863, 4907, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10702860270831602, "compression_ratio": 1.3693379790940767, "no_speech_prob": 0.010315642692148685}, {"id": 109, "seek": 40600, "start": 406.0, "end": 411.0, "text": " A pre-normalization dzia\u0142a jak taki solidny fundament i amortyzator.", "tokens": [50364, 316, 659, 12, 23157, 2144, 37903, 4207, 20065, 5100, 1634, 6073, 741, 669, 477, 37433, 1639, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08072877752369848, "compression_ratio": 1.40084388185654, "no_speech_prob": 0.006701876875013113}, {"id": 110, "seek": 40600, "start": 411.0, "end": 415.0, "text": " Okej, czyli to jest gwarancja stabilno\u015bci, a drugie ulepszenie?", "tokens": [50614, 29094, 73, 11, 16591, 281, 3492, 290, 6925, 4463, 2938, 11652, 16438, 11, 257, 4110, 414, 344, 306, 1878, 16778, 30, 50814], "temperature": 0.0, "avg_logprob": -0.08072877752369848, "compression_ratio": 1.40084388185654, "no_speech_prob": 0.006701876875013113}, {"id": 111, "seek": 40600, "start": 415.0, "end": 419.0, "text": " Drugie to zamiana standardowej funkcji aktywacji relu", "tokens": [50814, 35806, 414, 281, 19876, 8497, 3832, 21091, 26476, 19649, 9308, 874, 86, 13152, 1039, 84, 51014], "temperature": 0.0, "avg_logprob": -0.08072877752369848, "compression_ratio": 1.40084388185654, "no_speech_prob": 0.006701876875013113}, {"id": 112, "seek": 40600, "start": 419.0, "end": 423.0, "text": " na swiglu podpatrzonej w modelu Palm od Google.", "tokens": [51014, 1667, 1693, 328, 2781, 2497, 11584, 81, 16896, 73, 261, 2316, 84, 32668, 3611, 3329, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08072877752369848, "compression_ratio": 1.40084388185654, "no_speech_prob": 0.006701876875013113}, {"id": 113, "seek": 40600, "start": 423.0, "end": 427.0, "text": " To jak zamiana zwyk\u0142ego filkra powietrza w samochodzie na sportowy.", "tokens": [51214, 1407, 4207, 19876, 8497, 43436, 74, 1221, 6308, 1387, 42913, 3388, 1684, 81, 2394, 261, 3247, 8997, 378, 3283, 1667, 7282, 10089, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08072877752369848, "compression_ratio": 1.40084388185654, "no_speech_prob": 0.006701876875013113}, {"id": 114, "seek": 40600, "start": 427.0, "end": 429.0, "text": " Ma\u0142a zmiana, du\u017cy efekt.", "tokens": [51414, 4042, 5024, 17020, 8497, 11, 1581, 7735, 31482, 8192, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08072877752369848, "compression_ratio": 1.40084388185654, "no_speech_prob": 0.006701876875013113}, {"id": 115, "seek": 42900, "start": 429.0, "end": 434.0, "text": " Ma\u0142y, ale sprytny zabieg, kt\u00f3ry daje wymierny kilkuprocentowy wzrost wydajno\u015bci", "tokens": [50364, 4042, 6825, 11, 6775, 637, 627, 83, 1634, 24838, 20408, 11, 9913, 1120, 2884, 29764, 811, 1634, 5128, 74, 1010, 340, 2207, 10089, 24809, 27494, 25984, 1805, 16438, 50614], "temperature": 0.0, "avg_logprob": -0.10463261766498591, "compression_ratio": 1.3957597173144876, "no_speech_prob": 0.09177505224943161}, {"id": 116, "seek": 42900, "start": 434.0, "end": 437.0, "text": " bez fundamentalnych zmian.", "tokens": [50614, 10782, 8088, 9399, 43591, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10463261766498591, "compression_ratio": 1.3957597173144876, "no_speech_prob": 0.09177505224943161}, {"id": 117, "seek": 42900, "start": 437.0, "end": 439.0, "text": " A trzeci element to ju\u017c Majstersztyk.", "tokens": [50764, 316, 22266, 537, 4478, 281, 10678, 7048, 10130, 2682, 46127, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10463261766498591, "compression_ratio": 1.3957597173144876, "no_speech_prob": 0.09177505224943161}, {"id": 118, "seek": 42900, "start": 439.0, "end": 440.0, "text": " Czyli?", "tokens": [50864, 37099, 30, 50914], "temperature": 0.0, "avg_logprob": -0.10463261766498591, "compression_ratio": 1.3957597173144876, "no_speech_prob": 0.09177505224943161}, {"id": 119, "seek": 42900, "start": 440.0, "end": 445.0, "text": " Zrezygnowali z absolutnych osadze\u0144 pozycyjnych na rzecz Rotary Embeddings,", "tokens": [50914, 1176, 265, 1229, 70, 3785, 5103, 710, 18757, 9399, 3003, 345, 49689, 49358, 42949, 9399, 1667, 36833, 17681, 822, 24234, 292, 29432, 11, 51164], "temperature": 0.0, "avg_logprob": -0.10463261766498591, "compression_ratio": 1.3957597173144876, "no_speech_prob": 0.09177505224943161}, {"id": 120, "seek": 42900, "start": 445.0, "end": 447.0, "text": " znanych jako ropey.", "tokens": [51164, 15397, 34644, 17123, 13540, 88, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10463261766498591, "compression_ratio": 1.3957597173144876, "no_speech_prob": 0.09177505224943161}, {"id": 121, "seek": 42900, "start": 447.0, "end": 449.0, "text": " Rope, okej.", "tokens": [51264, 497, 1114, 11, 40043, 73, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10463261766498591, "compression_ratio": 1.3957597173144876, "no_speech_prob": 0.09177505224943161}, {"id": 122, "seek": 42900, "start": 449.0, "end": 454.0, "text": " Zamiast m\u00f3wi\u0107 modelowi na sztywno, to s\u0142owo jest na pi\u0105tej pozycji w zdaniu.", "tokens": [51364, 1176, 4526, 525, 13489, 12757, 2316, 24503, 1667, 7870, 874, 20944, 11, 281, 15116, 19941, 3492, 1667, 3895, 1611, 975, 73, 49358, 19649, 261, 16221, 25849, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10463261766498591, "compression_ratio": 1.3957597173144876, "no_speech_prob": 0.09177505224943161}, {"id": 123, "seek": 42900, "start": 454.0, "end": 458.0, "text": " Rope daje mu co\u015b w rodzaju wewn\u0119trznego kompasu.", "tokens": [51614, 497, 1114, 1120, 2884, 2992, 19241, 261, 28607, 33166, 321, 895, 1274, 6903, 89, 11858, 5207, 20990, 84, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10463261766498591, "compression_ratio": 1.3957597173144876, "no_speech_prob": 0.09177505224943161}, {"id": 124, "seek": 45800, "start": 458.0, "end": 463.0, "text": " To pozwala mu znacznie lepiej rozumie\u0107 relacje i odleg\u0142o\u015bci mi\u0119dzy s\u0142owami,", "tokens": [50364, 1407, 40557, 5159, 2992, 15397, 14875, 2766, 476, 39699, 48797, 414, 2162, 1039, 29293, 741, 277, 2285, 70, 35059, 33964, 15116, 305, 4526, 11, 50614], "temperature": 0.0, "avg_logprob": -0.05449013814438869, "compression_ratio": 1.376865671641791, "no_speech_prob": 0.0022023324854671955}, {"id": 125, "seek": 45800, "start": 463.0, "end": 466.0, "text": " zw\u0142aszcza w bardzo d\u0142ugich tekstach.", "tokens": [50614, 11873, 1221, 19601, 41524, 261, 9034, 274, 34077, 480, 16624, 372, 608, 13, 50764], "temperature": 0.0, "avg_logprob": -0.05449013814438869, "compression_ratio": 1.376865671641791, "no_speech_prob": 0.0022023324854671955}, {"id": 126, "seek": 45800, "start": 466.0, "end": 470.0, "text": " W pracy wspomniano te\u017c o czym\u015b, co pozwoli\u0142o im wytrenowa\u0107 najwi\u0119kszy model", "tokens": [50764, 343, 35591, 17757, 38131, 6254, 9516, 277, 31466, 1788, 11, 598, 40557, 9384, 5249, 566, 261, 4328, 1095, 11445, 48636, 1694, 1229, 2316, 50964], "temperature": 0.0, "avg_logprob": -0.05449013814438869, "compression_ratio": 1.376865671641791, "no_speech_prob": 0.0022023324854671955}, {"id": 127, "seek": 45800, "start": 470.0, "end": 472.0, "text": " w zaledwie 21 dni.", "tokens": [50964, 261, 710, 5573, 8699, 5080, 46125, 13, 51064], "temperature": 0.0, "avg_logprob": -0.05449013814438869, "compression_ratio": 1.376865671641791, "no_speech_prob": 0.0022023324854671955}, {"id": 128, "seek": 45800, "start": 472.0, "end": 474.0, "text": " To brzmi jak szalenie kr\u00f3tki czas.", "tokens": [51064, 1407, 738, 89, 3057, 4207, 7870, 21745, 414, 42366, 83, 2984, 13190, 13, 51164], "temperature": 0.0, "avg_logprob": -0.05449013814438869, "compression_ratio": 1.376865671641791, "no_speech_prob": 0.0022023324854671955}, {"id": 129, "seek": 45800, "start": 474.0, "end": 476.0, "text": " Czy to te\u017c efekt tych ulepsze\u0144?", "tokens": [51164, 19832, 281, 9516, 31482, 8192, 15180, 344, 306, 1878, 49689, 30, 51264], "temperature": 0.0, "avg_logprob": -0.05449013814438869, "compression_ratio": 1.376865671641791, "no_speech_prob": 0.0022023324854671955}, {"id": 130, "seek": 45800, "start": 476.0, "end": 479.0, "text": " Nie. To ju\u017c czysta, genialna in\u017cynieria.", "tokens": [51264, 12016, 13, 1407, 10678, 6430, 9140, 11, 48228, 629, 294, 1427, 2534, 811, 654, 13, 51414], "temperature": 0.0, "avg_logprob": -0.05449013814438869, "compression_ratio": 1.376865671641791, "no_speech_prob": 0.0022023324854671955}, {"id": 131, "seek": 45800, "start": 479.0, "end": 482.0, "text": " Wykorzystali dwie kluczowe techniki.", "tokens": [51414, 14458, 19339, 36049, 5103, 274, 8699, 9671, 1311, 89, 6880, 1537, 9850, 13, 51564], "temperature": 0.0, "avg_logprob": -0.05449013814438869, "compression_ratio": 1.376865671641791, "no_speech_prob": 0.0022023324854671955}, {"id": 132, "seek": 48200, "start": 482.0, "end": 488.0, "text": " Po pierwsze, niezwykle wydajn\u0105 implementacj\u0119 mechanizmu E-Tension z biblioteki X-Formers.", "tokens": [50364, 6165, 45994, 11, 33511, 9726, 14677, 25984, 1805, 13113, 4445, 29924, 4236, 590, 20140, 462, 12, 51, 3378, 710, 34344, 310, 14753, 1783, 12, 49855, 433, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10508765548956199, "compression_ratio": 1.3686274509803922, "no_speech_prob": 0.5538004636764526}, {"id": 133, "seek": 48200, "start": 488.0, "end": 490.0, "text": " I jak to dzia\u0142a?", "tokens": [50664, 286, 4207, 281, 37903, 30, 50764], "temperature": 0.0, "avg_logprob": -0.10508765548956199, "compression_ratio": 1.3686274509803922, "no_speech_prob": 0.5538004636764526}, {"id": 134, "seek": 48200, "start": 490.0, "end": 496.0, "text": " W klasycznym podej\u015bciu, \u017ceby obliczy\u0107 uwag\u0119, trzeba zapisa\u0107 w pami\u0119ci gigantyczn\u0105 macie\u017c wak,", "tokens": [50764, 343, 9671, 5871, 3689, 12996, 7468, 73, 6199, 84, 11, 11316, 1111, 1050, 27150, 43696, 11, 25860, 14223, 3837, 2162, 261, 31088, 537, 8741, 394, 17466, 13113, 7912, 414, 1427, 261, 514, 11, 51064], "temperature": 0.0, "avg_logprob": -0.10508765548956199, "compression_ratio": 1.3686274509803922, "no_speech_prob": 0.5538004636764526}, {"id": 135, "seek": 48200, "start": 496.0, "end": 498.0, "text": " co jest bardzo pami\u0119ciorzerne.", "tokens": [51064, 598, 3492, 9034, 31088, 537, 284, 4527, 716, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10508765548956199, "compression_ratio": 1.3686274509803922, "no_speech_prob": 0.5538004636764526}, {"id": 136, "seek": 48200, "start": 498.0, "end": 504.0, "text": " Implementacja w X-Formers pozwala tego unikn\u0105\u0107, drastycznie oszcz\u0119dzaj\u0105c zasoby.", "tokens": [51164, 4331, 43704, 23395, 261, 1783, 12, 49855, 433, 40557, 5159, 8627, 517, 1035, 13113, 2162, 11, 1224, 9820, 19923, 3003, 43771, 6298, 89, 38757, 26530, 13944, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10508765548956199, "compression_ratio": 1.3686274509803922, "no_speech_prob": 0.5538004636764526}, {"id": 137, "seek": 48200, "start": 504.0, "end": 506.0, "text": " A ta druga technika?", "tokens": [51464, 316, 1846, 4110, 64, 1537, 5439, 30, 51564], "temperature": 0.0, "avg_logprob": -0.10508765548956199, "compression_ratio": 1.3686274509803922, "no_speech_prob": 0.5538004636764526}, {"id": 138, "seek": 50600, "start": 507.0, "end": 512.0, "text": " Polega ona na tym, \u017ce podczas przej\u015bcia w prz\u00f3d model nie zapisuje wszystkich po\u015brednich wynik\u00f3w,", "tokens": [50414, 34212, 3680, 20325, 1667, 8107, 11, 3561, 2497, 30989, 8325, 73, 1788, 2755, 261, 6541, 17081, 2316, 2838, 14223, 271, 13008, 34234, 714, 1788, 986, 77, 480, 31936, 1035, 3901, 11, 50664], "temperature": 0.0, "avg_logprob": -0.07824224727168964, "compression_ratio": 1.465986394557823, "no_speech_prob": 0.5071479082107544}, {"id": 139, "seek": 50600, "start": 512.0, "end": 514.0, "text": " a tylko te kluczowe.", "tokens": [50664, 257, 13219, 535, 9671, 1311, 89, 6880, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07824224727168964, "compression_ratio": 1.465986394557823, "no_speech_prob": 0.5071479082107544}, {"id": 140, "seek": 50600, "start": 514.0, "end": 517.0, "text": " Reszt\u0119 odtwarza w locie podczas przej\u015bcia wstecznego.", "tokens": [50764, 5015, 2682, 1274, 3611, 83, 6925, 2394, 261, 1628, 414, 2497, 30989, 8325, 73, 1788, 2755, 261, 2941, 3689, 11858, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07824224727168964, "compression_ratio": 1.465986394557823, "no_speech_prob": 0.5071479082107544}, {"id": 141, "seek": 50600, "start": 517.0, "end": 520.0, "text": " Czyli znowu. Ogromna oszcz\u0119dno\u015b\u0107 pami\u0119ci.", "tokens": [50914, 37099, 710, 3785, 84, 13, 422, 861, 298, 629, 3003, 43771, 6298, 23293, 31088, 537, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07824224727168964, "compression_ratio": 1.465986394557823, "no_speech_prob": 0.5071479082107544}, {"id": 142, "seek": 50600, "start": 520.0, "end": 521.0, "text": " Ogromna.", "tokens": [51064, 422, 861, 298, 629, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07824224727168964, "compression_ratio": 1.465986394557823, "no_speech_prob": 0.5071479082107544}, {"id": 143, "seek": 50600, "start": 521.0, "end": 526.0, "text": " Bez tych dw\u00f3ch sztuczek ten projekt w tej skali i w tym czasie po prostu nie by\u0142by mo\u017cliwy.", "tokens": [51114, 879, 89, 15180, 27379, 812, 339, 262, 2682, 1311, 19878, 2064, 26261, 261, 12573, 1110, 5103, 741, 261, 8107, 42667, 714, 19518, 2838, 16673, 2322, 30854, 9726, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07824224727168964, "compression_ratio": 1.465986394557823, "no_speech_prob": 0.5071479082107544}, {"id": 144, "seek": 50600, "start": 526.0, "end": 533.0, "text": " Dobrze. Mamy wi\u0119c rewolucyjn\u0105 tez\u0119, transparentne dane i inteligentnie stunigowan\u0105 architektor\u0119.", "tokens": [51364, 29679, 13503, 13, 376, 7804, 16677, 319, 48481, 1311, 88, 73, 13113, 535, 11052, 11, 12737, 716, 49206, 741, 24777, 25002, 2766, 11885, 328, 37345, 1611, 3912, 642, 2320, 284, 1274, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07824224727168964, "compression_ratio": 1.465986394557823, "no_speech_prob": 0.5071479082107544}, {"id": 145, "seek": 53300, "start": 533.0, "end": 536.0, "text": " Ale na koniec dnia licz\u0105 si\u0119 wyniki.", "tokens": [50364, 9366, 1667, 5897, 35733, 274, 12679, 6169, 8925, 3244, 31936, 9850, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07754943245335628, "compression_ratio": 1.3535714285714286, "no_speech_prob": 0.09775415807962418}, {"id": 146, "seek": 53300, "start": 536.0, "end": 538.0, "text": " Przejd\u017amy do dowod\u00f3w w liczbach.", "tokens": [50514, 2114, 16920, 67, 10659, 2226, 360, 9459, 378, 3901, 261, 6169, 89, 32096, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07754943245335628, "compression_ratio": 1.3535714285714286, "no_speech_prob": 0.09775415807962418}, {"id": 147, "seek": 53300, "start": 538.0, "end": 542.0, "text": " Co tak naprawd\u0119 pokaza\u0142y te wszystkie tabele z benchmark\u00f3w?", "tokens": [50614, 3066, 991, 20970, 13010, 12257, 6825, 535, 31723, 4421, 16884, 710, 18927, 3901, 30, 50814], "temperature": 0.0, "avg_logprob": -0.07754943245335628, "compression_ratio": 1.3535714285714286, "no_speech_prob": 0.09775415807962418}, {"id": 148, "seek": 53300, "start": 542.0, "end": 547.0, "text": " Wyniki by\u0142y, no, wstrz\u0105sem dla ca\u0142ej bran\u017cy.", "tokens": [50814, 343, 2534, 9850, 26366, 11, 572, 11, 261, 9733, 8925, 19872, 12285, 47631, 73, 12029, 7735, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07754943245335628, "compression_ratio": 1.3535714285714286, "no_speech_prob": 0.09775415807962418}, {"id": 149, "seek": 53300, "start": 547.0, "end": 551.0, "text": " Najwa\u017cniejszy i najcz\u0119\u015bciej cytowany wniosek jest taki.", "tokens": [51064, 31576, 27111, 10402, 7706, 741, 11212, 41151, 9815, 73, 40248, 23341, 261, 3722, 541, 74, 3492, 20065, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07754943245335628, "compression_ratio": 1.3535714285714286, "no_speech_prob": 0.09775415807962418}, {"id": 150, "seek": 53300, "start": 551.0, "end": 557.0, "text": " Lama 13B, czyli model z 13 miliardami parametr\u00f3w, czyli ten wystarczaj\u0105co ma\u0142y,", "tokens": [51264, 441, 2404, 3705, 33, 11, 16591, 2316, 710, 3705, 1962, 72, 515, 4526, 6220, 27965, 3901, 11, 16591, 2064, 4628, 9710, 3689, 11133, 1291, 463, 6825, 11, 51564], "temperature": 0.0, "avg_logprob": -0.07754943245335628, "compression_ratio": 1.3535714285714286, "no_speech_prob": 0.09775415807962418}, {"id": 151, "seek": 53300, "start": 557.0, "end": 560.0, "text": " \u017ceby go uruchomi\u0107 powiedzmy na domowym sprz\u0119cie.", "tokens": [51564, 11316, 352, 4038, 625, 9220, 2162, 27617, 2226, 1667, 3285, 31691, 6103, 11052, 4260, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07754943245335628, "compression_ratio": 1.3535714285714286, "no_speech_prob": 0.09775415807962418}, {"id": 152, "seek": 56000, "start": 560.0, "end": 566.0, "text": " Tak. Wystarczaj\u0105co ma\u0142y by z pewnym wysi\u0142kiem uruchomi\u0107 go na jednej mocnej karcie graficznej.", "tokens": [50364, 9118, 13, 14458, 9710, 3689, 11133, 1291, 463, 6825, 538, 710, 47160, 4199, 27062, 40622, 26116, 4038, 625, 9220, 2162, 352, 1667, 5232, 11794, 34962, 11794, 7917, 4260, 1295, 1786, 89, 11794, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06786555091806706, "compression_ratio": 1.3569023569023568, "no_speech_prob": 0.0020066960714757442}, {"id": 153, "seek": 56000, "start": 566.0, "end": 573.0, "text": " Dla graczy w wi\u0119kszo\u015bci zada\u0144 pokona\u0142 giganta, jakim by\u0142 175 miliardowy GPT-3.", "tokens": [50664, 413, 875, 11625, 1229, 261, 29968, 4765, 6199, 710, 1538, 5248, 13010, 4037, 1221, 8741, 5983, 11, 49410, 16673, 41165, 1962, 72, 515, 10089, 26039, 51, 12, 18, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06786555091806706, "compression_ratio": 1.3569023569023568, "no_speech_prob": 0.0020066960714757442}, {"id": 154, "seek": 56000, "start": 573.0, "end": 576.0, "text": " Zaraz, zaraz. Zatrzymajmy si\u0119 tu na chwil\u0119.", "tokens": [51014, 41580, 921, 11, 22675, 921, 13, 1176, 267, 13047, 1696, 73, 2226, 3244, 2604, 1667, 41941, 1274, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06786555091806706, "compression_ratio": 1.3569023569023568, "no_speech_prob": 0.0020066960714757442}, {"id": 155, "seek": 56000, "start": 576.0, "end": 582.0, "text": " Model ponad 10 razy mniejszy dzia\u0142aj\u0105cy na publicznych danych pokona\u0142 zamkni\u0119tego,", "tokens": [51164, 17105, 9224, 345, 1266, 9639, 88, 39513, 7706, 27121, 11133, 1344, 1667, 1908, 89, 9399, 274, 34644, 13010, 4037, 1221, 19876, 74, 35938, 975, 1571, 11, 51464], "temperature": 0.0, "avg_logprob": -0.06786555091806706, "compression_ratio": 1.3569023569023568, "no_speech_prob": 0.0020066960714757442}, {"id": 156, "seek": 56000, "start": 582.0, "end": 586.0, "text": " komercyjnego potwora, kt\u00f3ry zdefiniowa\u0142 ca\u0142\u0105 epok\u0119?", "tokens": [51464, 5207, 260, 42949, 11858, 1847, 86, 3252, 11, 9913, 710, 20595, 3812, 30105, 1335, 15926, 2388, 453, 1274, 30, 51664], "temperature": 0.0, "avg_logprob": -0.06786555091806706, "compression_ratio": 1.3569023569023568, "no_speech_prob": 0.0020066960714757442}, {"id": 157, "seek": 56000, "start": 586.0, "end": 588.0, "text": " To brzmi jak historia z filmu.", "tokens": [51664, 1407, 738, 89, 3057, 4207, 18385, 710, 2007, 84, 13, 51764], "temperature": 0.0, "avg_logprob": -0.06786555091806706, "compression_ratio": 1.3569023569023568, "no_speech_prob": 0.0020066960714757442}, {"id": 158, "seek": 58800, "start": 588.0, "end": 591.0, "text": " A jednak. I to w szerokim zakresie test\u00f3w.", "tokens": [50364, 316, 25897, 13, 286, 281, 261, 36160, 453, 332, 23810, 495, 414, 1500, 3901, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1044923718770345, "compression_ratio": 1.4641744548286604, "no_speech_prob": 0.017805300652980804}, {"id": 159, "seek": 58800, "start": 591.0, "end": 595.0, "text": " Na przyk\u0142ad w rozumowaniu potocznym, czyli common sense reasoning,", "tokens": [50514, 6056, 23144, 261, 48797, 305, 25849, 1847, 905, 89, 12996, 11, 16591, 2689, 2020, 21577, 11, 50714], "temperature": 0.0, "avg_logprob": -0.1044923718770345, "compression_ratio": 1.4641744548286604, "no_speech_prob": 0.017805300652980804}, {"id": 160, "seek": 58800, "start": 595.0, "end": 598.0, "text": " na takich benchmarkach jak Helleswag czy Winogrand.", "tokens": [50714, 1667, 29607, 18927, 608, 4207, 12090, 279, 86, 559, 6430, 10427, 664, 3699, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1044923718770345, "compression_ratio": 1.4641744548286604, "no_speech_prob": 0.017805300652980804}, {"id": 161, "seek": 58800, "start": 598.0, "end": 599.0, "text": " A w jakich jeszcze?", "tokens": [50864, 316, 261, 4207, 480, 14168, 30, 50914], "temperature": 0.0, "avg_logprob": -0.1044923718770345, "compression_ratio": 1.4641744548286604, "no_speech_prob": 0.017805300652980804}, {"id": 162, "seek": 58800, "start": 599.0, "end": 604.0, "text": " Ale tak\u017ce w odpowiadaniu na pytania w trybie closed book na testie Trivia QA,", "tokens": [50914, 9366, 23306, 261, 24314, 38069, 25849, 1667, 25878, 5609, 261, 853, 7392, 5395, 1446, 1667, 1500, 414, 10931, 11617, 1249, 32, 11, 51164], "temperature": 0.0, "avg_logprob": -0.1044923718770345, "compression_ratio": 1.4641744548286604, "no_speech_prob": 0.017805300652980804}, {"id": 163, "seek": 58800, "start": 604.0, "end": 610.0, "text": " gdzie model musi odpowiedzie\u0107 bazuj\u0105c wy\u0142\u0105cznie na wiedz\u0119 zapami\u0119tanej podczas treningu.", "tokens": [51164, 18922, 2316, 37587, 24314, 22078, 27147, 44733, 4628, 15926, 19923, 1667, 46894, 11052, 14223, 23806, 83, 1929, 73, 2497, 30989, 2192, 773, 84, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1044923718770345, "compression_ratio": 1.4641744548286604, "no_speech_prob": 0.017805300652980804}, {"id": 164, "seek": 58800, "start": 610.0, "end": 613.0, "text": " To pokaza\u0142o, \u017ce mniejszy model, ale trenowany d\u0142u\u017cej,", "tokens": [51464, 1407, 13010, 12257, 5249, 11, 3561, 39513, 7706, 2316, 11, 6775, 23136, 23341, 274, 24066, 38493, 11, 51614], "temperature": 0.0, "avg_logprob": -0.1044923718770345, "compression_ratio": 1.4641744548286604, "no_speech_prob": 0.017805300652980804}, {"id": 165, "seek": 58800, "start": 613.0, "end": 616.0, "text": " mo\u017ce mie\u0107 g\u0119stsz\u0105 i lepiej zorganizowan\u0105 wiedz\u0119.", "tokens": [51614, 12034, 35612, 290, 1274, 372, 82, 8925, 741, 476, 39699, 710, 12372, 590, 37345, 1611, 46894, 11052, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1044923718770345, "compression_ratio": 1.4641744548286604, "no_speech_prob": 0.017805300652980804}, {"id": 166, "seek": 61600, "start": 616.0, "end": 619.0, "text": " To ju\u017c samo w sobie jest niesamowite.", "tokens": [50364, 1407, 10678, 36422, 261, 13652, 3492, 48100, 335, 305, 642, 13, 50514], "temperature": 0.0, "avg_logprob": -0.12175015802983638, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.035808008164167404}, {"id": 167, "seek": 61600, "start": 619.0, "end": 622.0, "text": " Ale czy to skalowa\u0142o si\u0119 w g\u00f3r\u0119?", "tokens": [50514, 9366, 6430, 281, 16890, 5528, 5249, 3244, 261, 290, 15614, 1274, 30, 50664], "temperature": 0.0, "avg_logprob": -0.12175015802983638, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.035808008164167404}, {"id": 168, "seek": 61600, "start": 622.0, "end": 628.0, "text": " By\u0142o \u0142atwo sobie wyobrazi\u0107, \u017ce ta magia mniejszego modelu dzia\u0142a tylko do pewnego momentu.", "tokens": [50664, 3146, 5249, 47759, 6120, 13652, 4628, 24393, 28496, 11, 3561, 1846, 2258, 654, 39513, 15453, 6308, 2316, 84, 37903, 13219, 360, 25889, 11858, 1623, 84, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12175015802983638, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.035808008164167404}, {"id": 169, "seek": 61600, "start": 628.0, "end": 636.0, "text": " Jak najwi\u0119ksze z rodziny Lama 65B wypad\u0142 w starciu z prawdziwymi tytanami tamtych czas\u00f3w,", "tokens": [50964, 15029, 48636, 1694, 1381, 710, 28607, 3519, 441, 2404, 11624, 33, 4628, 13647, 1221, 261, 3543, 30795, 710, 41175, 3992, 9726, 3057, 1104, 20356, 4526, 7677, 874, 339, 13190, 3901, 11, 51364], "temperature": 0.0, "avg_logprob": -0.12175015802983638, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.035808008164167404}, {"id": 170, "seek": 61600, "start": 636.0, "end": 638.0, "text": " jak Palm od Google.", "tokens": [51364, 4207, 32668, 3611, 3329, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12175015802983638, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.035808008164167404}, {"id": 171, "seek": 61600, "start": 638.0, "end": 640.0, "text": " Okaza\u0142 si\u0119 w pe\u0142ni konkurencyjny.", "tokens": [51464, 3477, 12257, 1221, 3244, 261, 43205, 3722, 21428, 9873, 42949, 1634, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12175015802983638, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.035808008164167404}, {"id": 172, "seek": 61600, "start": 640.0, "end": 645.0, "text": " W zadaniach zrozumowania potocznego przewy\u017ccza\u0142 nawet nieco wi\u0119kszy model Chi\u0144czyla 70B,", "tokens": [51564, 343, 42788, 3782, 608, 710, 27857, 449, 21308, 1847, 905, 89, 11858, 39758, 88, 1427, 41524, 1221, 22696, 2838, 1291, 29968, 1229, 2316, 17730, 5248, 6522, 875, 5285, 33, 11, 51814], "temperature": 0.0, "avg_logprob": -0.12175015802983638, "compression_ratio": 1.3846153846153846, "no_speech_prob": 0.035808008164167404}, {"id": 173, "seek": 64500, "start": 645.0, "end": 647.0, "text": " na prawie wszystkich testach.", "tokens": [50364, 1667, 3206, 8699, 34234, 1500, 608, 13, 50464], "temperature": 0.0, "avg_logprob": -0.08182705222786248, "compression_ratio": 1.526143790849673, "no_speech_prob": 0.023445840924978256}, {"id": 174, "seek": 64500, "start": 647.0, "end": 650.0, "text": " Ale najbardziej zdumiewaj\u0105cy wynik pojawi\u0142 si\u0119 gdzie indziej.", "tokens": [50464, 9366, 41857, 16221, 449, 1093, 11133, 1344, 31936, 1035, 30655, 40622, 3244, 18922, 1016, 19554, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08182705222786248, "compression_ratio": 1.526143790849673, "no_speech_prob": 0.023445840924978256}, {"id": 175, "seek": 64500, "start": 650.0, "end": 651.0, "text": " Gdzie?", "tokens": [50614, 460, 13096, 30, 50664], "temperature": 0.0, "avg_logprob": -0.08182705222786248, "compression_ratio": 1.526143790849673, "no_speech_prob": 0.023445840924978256}, {"id": 176, "seek": 64500, "start": 651.0, "end": 655.0, "text": " W te\u015bcie rozumowania matematycznego GSM 8K.", "tokens": [50664, 343, 535, 9815, 9544, 449, 21308, 3803, 8615, 17466, 11858, 460, 26693, 1649, 42, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08182705222786248, "compression_ratio": 1.526143790849673, "no_speech_prob": 0.023445840924978256}, {"id": 177, "seek": 64500, "start": 655.0, "end": 660.0, "text": " Lama 65B, kt\u00f3ry przecie\u017c nie by\u0142 w \u017caden specjalny spos\u00f3b trenowany na danych matematycznych,", "tokens": [50864, 441, 2404, 11624, 33, 11, 9913, 8325, 40082, 2838, 16673, 261, 19625, 14771, 46433, 1634, 22904, 23136, 23341, 1667, 274, 34644, 3803, 8615, 17466, 9399, 11, 51114], "temperature": 0.0, "avg_logprob": -0.08182705222786248, "compression_ratio": 1.526143790849673, "no_speech_prob": 0.023445840924978256}, {"id": 178, "seek": 64500, "start": 660.0, "end": 662.0, "text": " okaza\u0142 si\u0119 lepszy.", "tokens": [51114, 3133, 12257, 1221, 3244, 476, 1878, 1229, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08182705222786248, "compression_ratio": 1.526143790849673, "no_speech_prob": 0.023445840924978256}, {"id": 179, "seek": 64500, "start": 662.0, "end": 664.0, "text": " Okaza\u0142 si\u0119 lepszy ni\u017c model Minerva 62B,", "tokens": [51214, 3477, 12257, 1221, 3244, 476, 1878, 1229, 28502, 2316, 2829, 1978, 64, 24536, 33, 11, 51314], "temperature": 0.0, "avg_logprob": -0.08182705222786248, "compression_ratio": 1.526143790849673, "no_speech_prob": 0.023445840924978256}, {"id": 180, "seek": 64500, "start": 664.0, "end": 668.0, "text": " specjalistyczny model od Google, kt\u00f3ry przeszed\u0142 dodatkowy fine tuning", "tokens": [51314, 46433, 468, 17466, 1634, 2316, 3611, 3329, 11, 9913, 6541, 10430, 292, 1221, 13886, 33525, 10089, 2489, 15164, 51514], "temperature": 0.0, "avg_logprob": -0.08182705222786248, "compression_ratio": 1.526143790849673, "no_speech_prob": 0.023445840924978256}, {"id": 181, "seek": 64500, "start": 668.0, "end": 670.0, "text": " w\u0142a\u015bnie na danych naukowych i matematycznych.", "tokens": [51514, 14234, 1667, 274, 34644, 35616, 74, 19605, 741, 3803, 8615, 17466, 9399, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08182705222786248, "compression_ratio": 1.526143790849673, "no_speech_prob": 0.023445840924978256}, {"id": 182, "seek": 64500, "start": 670.0, "end": 672.0, "text": " To ju\u017c jest kompletne zaskoczenie.", "tokens": [51614, 1407, 10678, 3492, 5207, 14657, 716, 710, 3863, 905, 16778, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08182705222786248, "compression_ratio": 1.526143790849673, "no_speech_prob": 0.023445840924978256}, {"id": 183, "seek": 67200, "start": 672.0, "end": 676.0, "text": " Wychodzi na to, \u017ce pobi\u0142 specjalist\u0119 w jego w\u0142asnej dyscyplinie,", "tokens": [50364, 14458, 34616, 1667, 281, 11, 3561, 714, 5614, 1221, 46433, 468, 1274, 261, 26542, 43572, 11794, 15243, 1344, 48102, 414, 11, 50564], "temperature": 0.0, "avg_logprob": -0.08819218604795394, "compression_ratio": 1.3462897526501767, "no_speech_prob": 0.044480860233306885}, {"id": 184, "seek": 67200, "start": 676.0, "end": 679.0, "text": " nawet si\u0119 do niej specjalnie nie przygotowuj\u0105c.", "tokens": [50564, 22696, 3244, 360, 2838, 73, 46433, 2766, 2838, 35914, 305, 44733, 13, 50714], "temperature": 0.0, "avg_logprob": -0.08819218604795394, "compression_ratio": 1.3462897526501767, "no_speech_prob": 0.044480860233306885}, {"id": 185, "seek": 67200, "start": 679.0, "end": 684.0, "text": " Czy by\u0142y w og\u00f3le jakie\u015b obszary, w kt\u00f3rych Lama wyra\u017cnie ust\u0119powa\u0142a pola?", "tokens": [50714, 19832, 26366, 261, 29229, 31163, 3181, 89, 822, 11, 261, 30382, 441, 2404, 4628, 424, 1427, 2766, 26189, 18085, 5528, 5024, 1180, 64, 30, 50964], "temperature": 0.0, "avg_logprob": -0.08819218604795394, "compression_ratio": 1.3462897526501767, "no_speech_prob": 0.044480860233306885}, {"id": 186, "seek": 67200, "start": 684.0, "end": 687.0, "text": " Tak, i autorzy byli w tej kwestii bardzo szczerzy.", "tokens": [50964, 9118, 11, 741, 19510, 1229, 538, 2081, 261, 12573, 42035, 5597, 9034, 22090, 260, 1229, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08819218604795394, "compression_ratio": 1.3462897526501767, "no_speech_prob": 0.044480860233306885}, {"id": 187, "seek": 67200, "start": 687.0, "end": 692.0, "text": " W benchmarku MMLU, czyli Massive Multitask Language Understanding,", "tokens": [51114, 343, 18927, 84, 376, 12683, 52, 11, 16591, 10482, 488, 14665, 270, 3863, 24445, 36858, 11, 51364], "temperature": 0.0, "avg_logprob": -0.08819218604795394, "compression_ratio": 1.3462897526501767, "no_speech_prob": 0.044480860233306885}, {"id": 188, "seek": 67200, "start": 692.0, "end": 695.0, "text": " to jest ten wielki test wiedzy, taka matura dla AI.", "tokens": [51364, 281, 3492, 2064, 20570, 2984, 1500, 46894, 1229, 11, 28017, 3803, 2991, 12285, 7318, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08819218604795394, "compression_ratio": 1.3462897526501767, "no_speech_prob": 0.044480860233306885}, {"id": 189, "seek": 67200, "start": 695.0, "end": 697.0, "text": " Dok\u0142adnie.", "tokens": [51514, 29768, 10358, 2766, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08819218604795394, "compression_ratio": 1.3462897526501767, "no_speech_prob": 0.044480860233306885}, {"id": 190, "seek": 69700, "start": 697.0, "end": 702.0, "text": " Tam Lama 65B wypad\u0142a nieco s\u0142abiej od Chinjili i Palm.", "tokens": [50364, 8540, 441, 2404, 11624, 33, 4628, 13647, 5024, 2838, 1291, 15116, 455, 7764, 3611, 4430, 73, 2312, 741, 32668, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09870470591953823, "compression_ratio": 1.4077669902912622, "no_speech_prob": 0.05072960630059242}, {"id": 191, "seek": 69700, "start": 702.0, "end": 706.0, "text": " Sami autorze sugeruj\u0105, \u017ce najbardziej prawdopodobodn\u0105 przyczyn\u0105", "tokens": [50614, 44029, 19510, 1381, 459, 1321, 13263, 11, 3561, 41857, 41175, 46684, 996, 378, 13113, 6501, 6522, 13113, 50814], "temperature": 0.0, "avg_logprob": -0.09870470591953823, "compression_ratio": 1.4077669902912622, "no_speech_prob": 0.05072960630059242}, {"id": 192, "seek": 69700, "start": 706.0, "end": 711.0, "text": " by\u0142a po prostu mniejsza ilo\u015b\u0107 ksi\u0105\u017cek i artyku\u0142\u00f3w naukowych w ich danych treningowych.", "tokens": [50814, 23936, 714, 19518, 275, 30295, 2394, 1930, 78, 7753, 39311, 916, 741, 594, 874, 5279, 1221, 3901, 35616, 74, 19605, 261, 1893, 274, 34644, 2192, 773, 19605, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09870470591953823, "compression_ratio": 1.4077669902912622, "no_speech_prob": 0.05072960630059242}, {"id": 193, "seek": 69700, "start": 711.0, "end": 716.0, "text": " Ich zbiory archive i ksi\u0105\u017cek to \u0142\u0105cznie 177 GB", "tokens": [51064, 3141, 710, 5614, 827, 23507, 741, 39311, 916, 281, 220, 15926, 19923, 3282, 22, 26809, 51314], "temperature": 0.0, "avg_logprob": -0.09870470591953823, "compression_ratio": 1.4077669902912622, "no_speech_prob": 0.05072960630059242}, {"id": 194, "seek": 69700, "start": 716.0, "end": 721.0, "text": " podczas gdy konkurencja mog\u0142a u\u017cywa\u0107 nawet 2 TB danych ksi\u0105\u017ckowych.", "tokens": [51314, 2497, 30989, 28405, 21428, 9873, 34056, 13172, 5024, 34097, 25234, 22696, 568, 29711, 274, 34644, 39311, 74, 19605, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09870470591953823, "compression_ratio": 1.4077669902912622, "no_speech_prob": 0.05072960630059242}, {"id": 195, "seek": 69700, "start": 721.0, "end": 726.0, "text": " To doskonale pokazuje, jak bardzo sk\u0142ad diety model\u00f3w wp\u0142ywa na jego ko\u0144cowe zdolno\u015bci.", "tokens": [51564, 1407, 4491, 18295, 1220, 13010, 43317, 11, 4207, 9034, 1110, 10358, 1026, 2210, 2316, 3901, 32444, 6825, 4151, 1667, 26542, 26470, 66, 6880, 16221, 401, 16438, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09870470591953823, "compression_ratio": 1.4077669902912622, "no_speech_prob": 0.05072960630059242}, {"id": 196, "seek": 72600, "start": 726.0, "end": 729.0, "text": " Pr\u00f3bujmy teraz spojrze\u0107 na to z szerszej perspektywy.", "tokens": [50364, 2114, 14216, 4579, 2226, 16854, 8243, 73, 13503, 2162, 1667, 281, 710, 7870, 433, 16920, 868, 32659, 874, 9726, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07175235274415578, "compression_ratio": 1.401840490797546, "no_speech_prob": 0.029216734692454338}, {"id": 197, "seek": 72600, "start": 729.0, "end": 733.0, "text": " Co ta praca tak naprawd\u0119 oznacza\u0142a dla \u015bwiata technologii?", "tokens": [50514, 3066, 1846, 582, 6628, 991, 20970, 277, 22672, 326, 2394, 5024, 12285, 21485, 3274, 1537, 1132, 5597, 30, 50714], "temperature": 0.0, "avg_logprob": -0.07175235274415578, "compression_ratio": 1.401840490797546, "no_speech_prob": 0.029216734692454338}, {"id": 198, "seek": 72600, "start": 733.0, "end": 735.0, "text": " Jaki by\u0142 jej realny wp\u0142yw?", "tokens": [50714, 508, 7421, 16673, 28924, 957, 1634, 32444, 6825, 86, 30, 50814], "temperature": 0.0, "avg_logprob": -0.07175235274415578, "compression_ratio": 1.401840490797546, "no_speech_prob": 0.029216734692454338}, {"id": 199, "seek": 72600, "start": 735.0, "end": 738.0, "text": " Moim zdaniem Lama mia\u0142a dwa fundamentalne skutki.", "tokens": [50814, 3335, 332, 710, 10312, 4907, 441, 2404, 21290, 5024, 35045, 8088, 716, 1110, 325, 2984, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07175235274415578, "compression_ratio": 1.401840490797546, "no_speech_prob": 0.029216734692454338}, {"id": 200, "seek": 72600, "start": 738.0, "end": 743.0, "text": " Pierwszy to demokratyzacja bada\u0144 nad du\u017cymi modelami j\u0119zykowymi.", "tokens": [50964, 16676, 30012, 281, 49432, 37433, 23395, 272, 1538, 5248, 12617, 1581, 7735, 3057, 2316, 4526, 49055, 74, 10089, 3057, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07175235274415578, "compression_ratio": 1.401840490797546, "no_speech_prob": 0.029216734692454338}, {"id": 201, "seek": 72600, "start": 743.0, "end": 745.0, "text": " W jakim sensie?", "tokens": [51214, 343, 49410, 2923, 414, 30, 51314], "temperature": 0.0, "avg_logprob": -0.07175235274415578, "compression_ratio": 1.401840490797546, "no_speech_prob": 0.029216734692454338}, {"id": 202, "seek": 72600, "start": 745.0, "end": 749.0, "text": " Udowadniaj\u0105c, \u017ce mo\u017cna osi\u0105gn\u0105\u0107 \u015bwiatowej klasy wyniki na publicznych danych", "tokens": [51314, 624, 67, 22647, 12679, 8555, 66, 11, 3561, 17790, 3003, 11404, 4568, 36374, 36425, 21091, 9671, 5871, 31936, 9850, 1667, 1908, 89, 9399, 274, 34644, 51514], "temperature": 0.0, "avg_logprob": -0.07175235274415578, "compression_ratio": 1.401840490797546, "no_speech_prob": 0.029216734692454338}, {"id": 203, "seek": 72600, "start": 749.0, "end": 754.0, "text": " i co by\u0142o absolutnie kluczowe, udost\u0119pniaj\u0105c wagi swoich modeli spo\u0142eczno\u015bci naukowej,", "tokens": [51514, 741, 598, 14811, 18757, 2766, 9671, 1311, 89, 6880, 11, 11727, 555, 18085, 12679, 8555, 66, 261, 20291, 13291, 480, 2316, 72, 36851, 89, 16438, 35616, 74, 21091, 11, 51764], "temperature": 0.0, "avg_logprob": -0.07175235274415578, "compression_ratio": 1.401840490797546, "no_speech_prob": 0.029216734692454338}, {"id": 204, "seek": 75400, "start": 754.0, "end": 760.0, "text": " meta otworzy\u0142a wrota dla tysi\u0119cy mniejszych zespo\u0142\u00f3w badawczych, start-up\u00f3w, uniwersytet\u00f3w.", "tokens": [50364, 19616, 4337, 28321, 1229, 5024, 928, 5377, 12285, 38156, 47303, 39513, 45021, 710, 279, 2259, 1221, 3901, 272, 1538, 86, 6522, 339, 11, 722, 12, 1010, 3901, 11, 36435, 5364, 4328, 302, 3901, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08087030324068936, "compression_ratio": 1.4454277286135693, "no_speech_prob": 0.0067090862430632114}, {"id": 205, "seek": 75400, "start": 760.0, "end": 767.0, "text": " Nagle okaza\u0142o si\u0119, \u017ce nie trzeba by\u0107 Google'em czy OpenAI, \u017ceby prowadzi\u0107 badania na najwy\u017cszym poziomie.", "tokens": [50664, 426, 15088, 3133, 12257, 5249, 3244, 11, 3561, 2838, 25860, 15069, 3329, 6, 443, 6430, 7238, 48698, 11, 11316, 36590, 28496, 1578, 5609, 1667, 11212, 9726, 1427, 7706, 76, 38503, 40120, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08087030324068936, "compression_ratio": 1.4454277286135693, "no_speech_prob": 0.0067090862430632114}, {"id": 206, "seek": 75400, "start": 767.0, "end": 771.0, "text": " Dok\u0142adnie. To wywo\u0142a\u0142o eksplozy innowacji w \u015bwiecie OpenSource.", "tokens": [51014, 29768, 10358, 2766, 13, 1407, 4628, 6120, 5024, 5249, 30724, 21132, 1229, 294, 3785, 13152, 261, 40078, 4260, 7238, 50, 2948, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08087030324068936, "compression_ratio": 1.4454277286135693, "no_speech_prob": 0.0067090862430632114}, {"id": 207, "seek": 75400, "start": 771.0, "end": 772.0, "text": " A ten drugi skutek?", "tokens": [51214, 316, 2064, 4110, 72, 1110, 325, 916, 30, 51264], "temperature": 0.0, "avg_logprob": -0.08087030324068936, "compression_ratio": 1.4454277286135693, "no_speech_prob": 0.0067090862430632114}, {"id": 208, "seek": 75400, "start": 772.0, "end": 778.0, "text": " Drugi to ta wspomniana ju\u017c zmiana paradygmatu, czyli przeniesienie uwagi z kosztu treningu na wydajno\u015b\u0107 inferencji.", "tokens": [51264, 2491, 24780, 281, 1846, 17757, 38131, 8497, 10678, 17020, 8497, 13480, 18103, 15677, 84, 11, 16591, 582, 2904, 530, 27385, 23147, 20291, 710, 19532, 2682, 84, 2192, 773, 84, 1667, 25984, 1805, 23293, 13596, 268, 19649, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08087030324068936, "compression_ratio": 1.4454277286135693, "no_speech_prob": 0.0067090862430632114}, {"id": 209, "seek": 75400, "start": 778.0, "end": 781.0, "text": " To mia\u0142o kolosalne znaczenie dla praktycznych zastosowa\u0144.", "tokens": [51564, 1407, 21290, 5249, 17818, 329, 304, 716, 15397, 326, 16778, 12285, 3206, 74, 874, 3689, 9399, 36746, 329, 5528, 5248, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08087030324068936, "compression_ratio": 1.4454277286135693, "no_speech_prob": 0.0067090862430632114}, {"id": 210, "seek": 75400, "start": 781.0, "end": 783.0, "text": " Dla biznesu?", "tokens": [51714, 413, 875, 7390, 4081, 84, 30, 51814], "temperature": 0.0, "avg_logprob": -0.08087030324068936, "compression_ratio": 1.4454277286135693, "no_speech_prob": 0.0067090862430632114}, {"id": 211, "seek": 78300, "start": 783.0, "end": 792.0, "text": " Biznes na bezrozumia\u0142, \u017ce nie musi inwestowa\u0107 w utrzymanie potwora o 500 miliardach parametr\u00f3w, \u017ceby mie\u0107 topow\u0105 wydajno\u015b\u0107.", "tokens": [50364, 16619, 4081, 1667, 10782, 27857, 449, 8908, 11, 3561, 2838, 37587, 294, 8750, 11445, 261, 2839, 13047, 1601, 414, 1847, 86, 3252, 277, 5923, 1962, 72, 515, 608, 6220, 27965, 3901, 11, 11316, 35612, 1192, 30297, 25984, 1805, 23293, 13, 50814], "temperature": 0.0, "avg_logprob": -0.06984054125272311, "compression_ratio": 1.4398734177215189, "no_speech_prob": 0.000839917513076216}, {"id": 212, "seek": 78300, "start": 792.0, "end": 801.0, "text": " Mo\u017cna mie\u0107 mniejsze, zwinniejsze i znacznie ta\u0144sze w codziennym u\u017cytkowaniu model, kt\u00f3ry dostarczy r\u00f3wnie\u017c dobre, a czasem nawet lepsze wyniki.", "tokens": [50814, 44736, 629, 35612, 275, 44258, 11, 11873, 259, 44258, 741, 15397, 14875, 2766, 1846, 5248, 82, 1381, 261, 17656, 89, 1053, 12996, 344, 1427, 4328, 74, 305, 25849, 2316, 11, 9913, 20568, 289, 6522, 20532, 41959, 11, 257, 13190, 443, 22696, 476, 1878, 1381, 31936, 9850, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06984054125272311, "compression_ratio": 1.4398734177215189, "no_speech_prob": 0.000839917513076216}, {"id": 213, "seek": 78300, "start": 801.0, "end": 805.0, "text": " To ca\u0142kowicie zmieni\u0142o ekonomi\u0105 sztucznej inteligencji.", "tokens": [51264, 1407, 35224, 74, 305, 28434, 17020, 35462, 5249, 13359, 12481, 11404, 262, 2682, 1311, 89, 11794, 24777, 3213, 19649, 13, 51464], "temperature": 0.0, "avg_logprob": -0.06984054125272311, "compression_ratio": 1.4398734177215189, "no_speech_prob": 0.000839917513076216}, {"id": 214, "seek": 78300, "start": 805.0, "end": 812.0, "text": " Autorzy zbudowali to pot\u0119\u017cne narz\u0119dzie, ale czy byli te\u017c szczerzy co do jego potencjalnych wad i zagro\u017ce\u0144?", "tokens": [51464, 6049, 284, 1229, 710, 18281, 305, 5103, 281, 1847, 1274, 1427, 716, 6714, 89, 42643, 11, 6775, 6430, 538, 2081, 9516, 22090, 260, 1229, 598, 360, 26542, 1847, 22660, 22600, 9399, 261, 345, 741, 27001, 340, 2875, 5248, 30, 51814], "temperature": 0.0, "avg_logprob": -0.06984054125272311, "compression_ratio": 1.4398734177215189, "no_speech_prob": 0.000839917513076216}, {"id": 215, "seek": 81200, "start": 812.0, "end": 816.0, "text": " Wiele prac naukowych woli przemilcze\u0107 te trudne tematy.", "tokens": [50364, 9233, 306, 22404, 35616, 74, 19605, 261, 9384, 6541, 443, 388, 9680, 2162, 535, 32007, 716, 1383, 21398, 13, 50564], "temperature": 0.0, "avg_logprob": -0.059765123437952115, "compression_ratio": 1.437956204379562, "no_speech_prob": 0.025424307212233543}, {"id": 216, "seek": 81200, "start": 816.0, "end": 819.0, "text": " I to jest jeden z najmocniejszych punkt\u00f3w tej pracy.", "tokens": [50564, 286, 281, 3492, 12906, 710, 11212, 76, 905, 10402, 45021, 39561, 3901, 12573, 35591, 13, 50714], "temperature": 0.0, "avg_logprob": -0.059765123437952115, "compression_ratio": 1.437956204379562, "no_speech_prob": 0.025424307212233543}, {"id": 217, "seek": 81200, "start": 819.0, "end": 822.0, "text": " Podchodz\u0105 do tematu z du\u017c\u0105 odpowiedzialno\u015bci\u0105.", "tokens": [50714, 12646, 29914, 8925, 360, 1383, 20546, 710, 21783, 1611, 24314, 15338, 831, 16438, 1611, 13, 50864], "temperature": 0.0, "avg_logprob": -0.059765123437952115, "compression_ratio": 1.437956204379562, "no_speech_prob": 0.025424307212233543}, {"id": 218, "seek": 81200, "start": 822.0, "end": 827.0, "text": " Przeprowadzili np. testy na toksyczno\u015b\u0107 u\u017cywaj\u0105c benchmarku Real Toxicity Prompts.", "tokens": [50864, 2114, 46342, 1892, 345, 89, 2312, 33808, 13, 1500, 88, 1667, 281, 1694, 17466, 23293, 34097, 86, 38757, 18927, 84, 8467, 1407, 87, 44198, 15833, 39280, 13, 51114], "temperature": 0.0, "avg_logprob": -0.059765123437952115, "compression_ratio": 1.437956204379562, "no_speech_prob": 0.025424307212233543}, {"id": 219, "seek": 81200, "start": 827.0, "end": 829.0, "text": " I co im wysz\u0142o?", "tokens": [51114, 286, 598, 566, 261, 20589, 5249, 30, 51214], "temperature": 0.0, "avg_logprob": -0.059765123437952115, "compression_ratio": 1.437956204379562, "no_speech_prob": 0.025424307212233543}, {"id": 220, "seek": 81200, "start": 829.0, "end": 832.0, "text": " Potwierdzili co\u015b, co obserwowano ju\u017c wcze\u015bniej.", "tokens": [51214, 9145, 40717, 28168, 2312, 19241, 11, 598, 12887, 34354, 3730, 10678, 40785, 13, 51364], "temperature": 0.0, "avg_logprob": -0.059765123437952115, "compression_ratio": 1.437956204379562, "no_speech_prob": 0.025424307212233543}, {"id": 221, "seek": 81200, "start": 832.0, "end": 837.0, "text": " Niestety toksyczno\u015b\u0107 generowanych tre\u015bci ro\u015bnie wraz z rozmiarem modelu.", "tokens": [51364, 426, 6495, 2210, 281, 1694, 17466, 23293, 1337, 23341, 339, 2192, 6199, 744, 12221, 7843, 89, 710, 9544, 3057, 19183, 2316, 84, 13, 51614], "temperature": 0.0, "avg_logprob": -0.059765123437952115, "compression_ratio": 1.437956204379562, "no_speech_prob": 0.025424307212233543}, {"id": 222, "seek": 83700, "start": 837.0, "end": 844.0, "text": " Lama 65B generowa\u0142a bardziej toksyczne odpowiedzi ni\u017c jej mniejsza siostra Lama 7B.", "tokens": [50364, 441, 2404, 11624, 33, 1337, 5528, 5024, 27209, 281, 1694, 17466, 716, 36574, 3992, 28502, 28924, 275, 30295, 2394, 1511, 555, 424, 441, 2404, 1614, 33, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12426246717138198, "compression_ratio": 1.3063829787234043, "no_speech_prob": 0.05241558700799942}, {"id": 223, "seek": 83700, "start": 844.0, "end": 848.0, "text": " A co z uprzedzeniami, czyli tzw. Bayas?", "tokens": [50714, 316, 598, 710, 493, 81, 11312, 2904, 15568, 11, 16591, 256, 14406, 13, 7840, 296, 30, 50914], "temperature": 0.0, "avg_logprob": -0.12426246717138198, "compression_ratio": 1.3063829787234043, "no_speech_prob": 0.05241558700799942}, {"id": 224, "seek": 83700, "start": 848.0, "end": 854.0, "text": " Model uczy si\u0119 przecie\u017c na danych z internetu, kt\u00f3ry jest delikatnie m\u00f3wi\u0105c pe\u0142en stereotyp\u00f3w.", "tokens": [50914, 17105, 344, 6522, 3244, 8325, 40082, 1667, 274, 34644, 710, 4705, 84, 11, 9913, 3492, 1103, 36300, 2766, 46591, 66, 43205, 268, 41182, 79, 3901, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12426246717138198, "compression_ratio": 1.3063829787234043, "no_speech_prob": 0.05241558700799942}, {"id": 225, "seek": 83700, "start": 854.0, "end": 860.0, "text": " Dok\u0142adnie. Przeanalizowali to u\u017cywaj\u0105c benchmark\u00f3w CrossPairs i Winogender.", "tokens": [51214, 29768, 10358, 2766, 13, 2114, 1381, 29702, 590, 305, 5103, 281, 34097, 86, 38757, 18927, 3901, 11623, 47, 4094, 741, 10427, 664, 3216, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12426246717138198, "compression_ratio": 1.3063829787234043, "no_speech_prob": 0.05241558700799942}, {"id": 226, "seek": 86000, "start": 861.0, "end": 871.0, "text": " W pierwszym, kt\u00f3ry mierzy stereotypy w 9 r\u00f3\u017cnych kategoriach, model Lama 65B okaza\u0142 si\u0119 szczeg\u00f3lnie stronniczy w kategorii religii.", "tokens": [50414, 343, 34016, 76, 11, 9913, 47448, 1229, 41182, 8200, 261, 1722, 42602, 350, 2968, 7386, 608, 11, 2316, 441, 2404, 11624, 33, 3133, 12257, 1221, 3244, 49624, 2766, 1056, 19968, 299, 1229, 261, 350, 2968, 284, 5597, 4039, 5597, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08585856358210246, "compression_ratio": 1.4221453287197232, "no_speech_prob": 0.5515382289886475}, {"id": 227, "seek": 86000, "start": 871.0, "end": 872.0, "text": " A w drugim?", "tokens": [50914, 316, 261, 4110, 332, 30, 50964], "temperature": 0.0, "avg_logprob": -0.08585856358210246, "compression_ratio": 1.4221453287197232, "no_speech_prob": 0.5515382289886475}, {"id": 228, "seek": 86000, "start": 872.0, "end": 879.0, "text": " Z kolei w drugim testie, kt\u00f3ry bada uprzedzenia p\u0142ciowe zwi\u0105zane z zawodami, model cz\u0119\u015bciej pope\u0142nia\u0142 b\u0142\u0119dy,", "tokens": [50964, 1176, 18303, 72, 261, 4110, 332, 1500, 414, 11, 9913, 272, 1538, 493, 81, 11312, 14320, 28695, 537, 6880, 27741, 1929, 710, 28165, 378, 4526, 11, 2316, 18544, 9815, 73, 42248, 1221, 77, 8908, 272, 46564, 3173, 11, 51314], "temperature": 0.0, "avg_logprob": -0.08585856358210246, "compression_ratio": 1.4221453287197232, "no_speech_prob": 0.5515382289886475}, {"id": 229, "seek": 86000, "start": 879.0, "end": 885.0, "text": " gdy p\u0142e\u0107 w zdaniu by\u0142a sprzeczna ze stereotypem, np. gdy piel\u0119gniarz by\u0142 m\u0119\u017cczyzn\u0105, a nie kobiet\u0105.", "tokens": [51314, 28405, 280, 19827, 2162, 261, 16221, 25849, 23936, 6103, 1381, 3689, 629, 5277, 41182, 79, 443, 11, 33808, 13, 28405, 46065, 1274, 70, 3722, 49763, 16673, 275, 1274, 1427, 6522, 89, 13113, 11, 257, 2838, 43057, 1684, 1611, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08585856358210246, "compression_ratio": 1.4221453287197232, "no_speech_prob": 0.5515382289886475}, {"id": 230, "seek": 86000, "start": 885.0, "end": 888.0, "text": " Czyli to bardzo uczciwa samo ocena.", "tokens": [51614, 37099, 281, 9034, 35403, 537, 4151, 36422, 10409, 4118, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08585856358210246, "compression_ratio": 1.4221453287197232, "no_speech_prob": 0.5515382289886475}, {"id": 231, "seek": 88800, "start": 888.0, "end": 895.0, "text": " Pokazuje, \u017ce model jest lustrem danych, uczy si\u0119 zar\u00f3wno wiedzy i kreatywno\u015bci, jak i naszych spo\u0142ecznych uprzedze\u0144.", "tokens": [50364, 14958, 43317, 11, 3561, 2316, 3492, 24672, 2579, 274, 34644, 11, 344, 6522, 3244, 22675, 812, 20944, 46894, 1229, 741, 350, 620, 88, 20944, 6199, 11, 4207, 741, 45002, 36851, 89, 9399, 493, 81, 11312, 49689, 13, 50714], "temperature": 0.0, "avg_logprob": -0.05757219141179865, "compression_ratio": 1.4772036474164134, "no_speech_prob": 0.06153447553515434}, {"id": 232, "seek": 88800, "start": 895.0, "end": 903.0, "text": " I na sam koniec swojej pracy, niemal jako post-scriptum, wspomnieli kr\u00f3tko o czym\u015b, co nazywa si\u0119 Instruction Fine Tuning.", "tokens": [50714, 286, 1667, 3247, 5897, 35733, 29489, 73, 35591, 11, 2838, 5579, 17123, 2183, 12, 82, 5944, 449, 11, 17757, 38131, 23099, 42366, 83, 4093, 277, 31466, 1788, 11, 598, 20151, 88, 4151, 3244, 2730, 3826, 12024, 21363, 278, 13, 51114], "temperature": 0.0, "avg_logprob": -0.05757219141179865, "compression_ratio": 1.4772036474164134, "no_speech_prob": 0.06153447553515434}, {"id": 233, "seek": 88800, "start": 903.0, "end": 905.0, "text": " To by\u0142 wtedy bardzo gor\u0105cy temat.", "tokens": [51114, 1407, 16673, 26959, 9034, 24012, 1611, 1344, 32954, 13, 51214], "temperature": 0.0, "avg_logprob": -0.05757219141179865, "compression_ratio": 1.4772036474164134, "no_speech_prob": 0.06153447553515434}, {"id": 234, "seek": 88800, "start": 905.0, "end": 911.0, "text": " Tak, to by\u0142 niemal zwiastun tego, co mia\u0142o nadej\u015b\u0107 kilka miesi\u0119cy p\u00f3\u017aniej i co dzi\u015b jest standardem.", "tokens": [51214, 9118, 11, 281, 16673, 2838, 5579, 710, 6253, 525, 409, 8627, 11, 598, 21290, 5249, 297, 762, 44536, 36466, 41543, 47303, 36968, 741, 598, 31981, 1788, 3492, 3832, 443, 13, 51514], "temperature": 0.0, "avg_logprob": -0.05757219141179865, "compression_ratio": 1.4772036474164134, "no_speech_prob": 0.06153447553515434}, {"id": 235, "seek": 88800, "start": 911.0, "end": 917.0, "text": " Pokazali, \u017ce nawet bardzo kr\u00f3tki i prosty proces dostrajania modelu na zbior\u0119 instrukcji.", "tokens": [51514, 14958, 921, 5103, 11, 3561, 22696, 9034, 42366, 83, 2984, 741, 10293, 88, 17565, 20568, 48690, 5609, 2316, 84, 1667, 710, 33362, 1274, 1058, 25126, 19649, 13, 51814], "temperature": 0.0, "avg_logprob": -0.05757219141179865, "compression_ratio": 1.4772036474164134, "no_speech_prob": 0.06153447553515434}, {"id": 236, "seek": 91700, "start": 917.0, "end": 919.0, "text": " Tworz\u0105c model Lama i.", "tokens": [50364, 2574, 284, 8925, 66, 2316, 441, 2404, 741, 13, 50464], "temperature": 0.0, "avg_logprob": -0.08863036888690035, "compression_ratio": 1.345679012345679, "no_speech_prob": 0.08853801339864731}, {"id": 237, "seek": 91700, "start": 919.0, "end": 929.0, "text": " Tak, tworz\u0105c wersj\u0119, kt\u00f3r\u0105 nazwali Lama i, \u017ce ten proces znacz\u0105co poprawi\u0142 jego wyniki na tym trudnym benchmarku MMLU.", "tokens": [50464, 9118, 11, 46288, 8925, 66, 261, 433, 11115, 11, 37415, 20151, 40054, 441, 2404, 741, 11, 3561, 2064, 17565, 15397, 326, 8925, 1291, 1665, 5131, 40622, 26542, 31936, 9850, 1667, 8107, 32007, 12996, 18927, 84, 376, 12683, 52, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08863036888690035, "compression_ratio": 1.345679012345679, "no_speech_prob": 0.08853801339864731}, {"id": 238, "seek": 91700, "start": 929.0, "end": 934.0, "text": " Wynik skoczy\u0142 z 63,4% do prawie 69%.", "tokens": [50964, 343, 2534, 1035, 1110, 905, 1229, 1221, 710, 25082, 11, 19, 4, 360, 3206, 8699, 28267, 6856, 51214], "temperature": 0.0, "avg_logprob": -0.08863036888690035, "compression_ratio": 1.345679012345679, "no_speech_prob": 0.08853801339864731}, {"id": 239, "seek": 91700, "start": 934.0, "end": 943.0, "text": " To by\u0142 wyra\u017cny sygna\u0142 dla ca\u0142ej bran\u017cy, \u017ce surowe, pot\u0119\u017cne modele bazowe to jedno, ale prawdziwa u\u017cyteczno\u015b\u0107 le\u017cy gdzie indziej.", "tokens": [51214, 1407, 16673, 4628, 424, 1427, 1634, 943, 70, 629, 1221, 12285, 47631, 73, 12029, 7735, 11, 3561, 1022, 6880, 11, 1847, 1274, 1427, 716, 4391, 306, 27147, 6880, 281, 5232, 1771, 11, 6775, 41175, 3992, 4151, 34097, 975, 3689, 23293, 476, 7735, 18922, 1016, 19554, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08863036888690035, "compression_ratio": 1.345679012345679, "no_speech_prob": 0.08853801339864731}, {"id": 240, "seek": 94300, "start": 943.0, "end": 949.0, "text": " Prawdziwa u\u017cyteczno\u015b\u0107 i bezpiecze\u0144stwo le\u017c\u0105 w dostrajaniu ich do tego, by pod\u0105\u017ca\u0142y za ludzkimi poleceniami.", "tokens": [50364, 430, 15889, 3992, 4151, 34097, 975, 3689, 23293, 741, 47153, 9680, 12229, 6120, 476, 1427, 1611, 261, 20568, 48690, 25849, 1893, 360, 8627, 11, 538, 2497, 27242, 64, 6825, 7949, 15946, 89, 74, 10121, 13208, 13037, 15568, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06611658334732055, "compression_ratio": 1.4548286604361371, "no_speech_prob": 0.07165329158306122}, {"id": 241, "seek": 94300, "start": 949.0, "end": 953.0, "text": " Podsumowuj\u0105c, praca nad Lama to bez dw\u00f3ch zdanie kamie\u0144 milowy.", "tokens": [50664, 12646, 82, 449, 305, 44733, 11, 582, 6628, 12617, 441, 2404, 281, 10782, 27379, 812, 339, 16221, 7155, 9727, 414, 5248, 1962, 10089, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06611658334732055, "compression_ratio": 1.4548286604361371, "no_speech_prob": 0.07165329158306122}, {"id": 242, "seek": 94300, "start": 953.0, "end": 963.0, "text": " Pokaza\u0142a, \u017ce mniejsze, ale intensywniej trenowane modele na otwartych, publicznych danych mog\u0105 by\u0107 wydajniejsze i cz\u0119sto lepsze od zamkni\u0119tych gigant\u00f3w.", "tokens": [50864, 14958, 12257, 5024, 11, 3561, 275, 44258, 11, 6775, 14056, 88, 895, 7764, 23136, 23066, 4391, 306, 1667, 4337, 29587, 16384, 11, 1908, 89, 9399, 274, 34644, 34123, 15069, 25984, 1805, 44258, 741, 34369, 476, 1878, 1381, 3611, 19876, 74, 35938, 874, 339, 8741, 394, 3901, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06611658334732055, "compression_ratio": 1.4548286604361371, "no_speech_prob": 0.07165329158306122}, {"id": 243, "seek": 94300, "start": 963.0, "end": 971.0, "text": " Zmieni\u0142a spos\u00f3b my\u015blenia o skalowaniu AI, k\u0142ad\u0105c nacisk na praktyczn\u0105 u\u017cyteczno\u015b\u0107 i koszty codziennego dzia\u0142ania.", "tokens": [51364, 1176, 76, 35462, 5024, 22904, 48633, 6698, 654, 277, 16890, 305, 25849, 7318, 11, 350, 10358, 1611, 66, 42071, 7797, 1667, 3206, 74, 874, 3689, 13113, 34097, 975, 3689, 23293, 741, 19532, 89, 874, 17656, 89, 1053, 11858, 27121, 5609, 13, 51764], "temperature": 0.0, "avg_logprob": -0.06611658334732055, "compression_ratio": 1.4548286604361371, "no_speech_prob": 0.07165329158306122}, {"id": 244, "seek": 97100, "start": 971.0, "end": 981.0, "text": " Kluczowe by\u0142o to przesuni\u0119cie perspektywy. Z pytania, jak najtaniej wytrenowa\u0107 model do poziomu X, na pytanie, jaki model o poziomie X b\u0119dzie najta\u0144szy w u\u017cyciu na masow\u0105 skal\u0119.", "tokens": [50364, 16053, 1311, 89, 6880, 14811, 281, 6541, 279, 409, 5034, 4260, 868, 32659, 874, 9726, 13, 1176, 25878, 5609, 11, 4207, 11212, 83, 7155, 73, 261, 4328, 1095, 11445, 2316, 360, 38503, 298, 84, 1783, 11, 1667, 36610, 11, 24492, 2316, 277, 38503, 40120, 1783, 10562, 11212, 1328, 5248, 7706, 261, 34097, 30795, 1667, 2300, 30297, 16890, 1274, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06613341374183769, "compression_ratio": 1.4649446494464944, "no_speech_prob": 0.11554666608572006}, {"id": 245, "seek": 97100, "start": 981.0, "end": 984.0, "text": " To filozofia, kt\u00f3ra wci\u0105\u017c jest aktualna.", "tokens": [50864, 1407, 1387, 15151, 2670, 654, 11, 19456, 261, 537, 27242, 3492, 13680, 901, 629, 13, 51014], "temperature": 0.0, "avg_logprob": -0.06613341374183769, "compression_ratio": 1.4649446494464944, "no_speech_prob": 0.11554666608572006}, {"id": 246, "seek": 97100, "start": 984.0, "end": 992.0, "text": " To jest filozofia, kt\u00f3ra do dzi\u015b kszta\u0142tuje ca\u0142y rozw\u00f3j sztucznej inteligencji, zw\u0142aszcza w ekosystemie Open Source, kt\u00f3ry dzi\u0119ki Lamie po prostu eksplodowa\u0142.", "tokens": [51014, 1407, 3492, 1387, 15151, 2670, 654, 11, 19456, 360, 31981, 1788, 350, 15453, 46426, 9179, 2884, 35226, 9544, 86, 18999, 262, 2682, 1311, 89, 11794, 24777, 3213, 19649, 11, 11873, 1221, 19601, 41524, 261, 13359, 329, 9321, 414, 7238, 29629, 11, 9913, 45003, 18825, 414, 714, 19518, 30724, 564, 378, 30105, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06613341374183769, "compression_ratio": 1.4649446494464944, "no_speech_prob": 0.11554666608572006}, {"id": 247, "seek": 99200, "start": 992.0, "end": 1003.0, "text": " I na koniec zostawmy naszych s\u0142uchaczy z pewn\u0105 my\u015bl\u0105. W konkluzji swojej pracy autorzy zapowiedzieli, \u017ce planuj\u0105 trenowa\u0107 jeszcze wi\u0119ksze modele na jeszcze wi\u0119kszych zbiorach danych.", "tokens": [50364, 286, 1667, 5897, 35733, 31873, 1607, 2226, 45002, 15116, 625, 14691, 710, 47160, 1611, 452, 19212, 1611, 13, 343, 21428, 2781, 89, 4013, 29489, 73, 35591, 19510, 1229, 14223, 305, 15338, 23099, 11, 3561, 1393, 13263, 23136, 11445, 14168, 29968, 1381, 4391, 306, 1667, 14168, 29968, 28051, 710, 33362, 608, 274, 34644, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07131342636911493, "compression_ratio": 1.353448275862069, "no_speech_prob": 0.6348615288734436}, {"id": 248, "seek": 99200, "start": 1003.0, "end": 1011.0, "text": " Bior\u0105c pod uwag\u0119 ich w\u0142asne szczere odkrycia, \u017ce uprzedzenia i toksyczno\u015b\u0107 r\u00f3wnie\u017c skaluj\u0105 si\u0119 wraz z rozmiarem?", "tokens": [50914, 363, 1973, 1611, 66, 2497, 43696, 1893, 43572, 716, 22090, 323, 3611, 43298, 2755, 11, 3561, 493, 81, 11312, 14320, 741, 281, 1694, 17466, 23293, 20532, 16890, 13263, 3244, 7843, 89, 710, 9544, 3057, 19183, 30, 51314], "temperature": 0.0, "avg_logprob": -0.07131342636911493, "compression_ratio": 1.353448275862069, "no_speech_prob": 0.6348615288734436}, {"id": 249, "seek": 101100, "start": 1012.0, "end": 1029.0, "text": " To rodzi si\u0119 fundamentalne pytanie. Czy buduj\u0105c kora z pot\u0119\u017cniejsze modele na danych z ca\u0142ego internetu, tworzymy lepsze narz\u0119dzia do rozumienia \u015bwiata, czy raczej coraz doskonosze lustra, kt\u00f3re bezkrytycznie odbijaj\u0105 wszystkie jego niedoskona\u0142o\u015bci?", "tokens": [50414, 1407, 8685, 3992, 3244, 8088, 716, 36610, 13, 19832, 3265, 44733, 350, 3252, 710, 1847, 1274, 1427, 44258, 4391, 306, 1667, 274, 34644, 710, 35224, 6308, 4705, 84, 11, 46288, 1229, 2226, 476, 1878, 1381, 6714, 89, 6298, 40395, 360, 48797, 18811, 21485, 3274, 11, 6430, 4129, 16920, 25899, 4491, 18295, 329, 1381, 24672, 424, 11, 8864, 10782, 43298, 45586, 3611, 30418, 11133, 31723, 26542, 32488, 329, 74, 4037, 35059, 30, 51264], "temperature": 0.0, "avg_logprob": -0.0800341526667277, "compression_ratio": 1.4212328767123288, "no_speech_prob": 0.16358128190040588}, {"id": 250, "seek": 101100, "start": 1029.0, "end": 1039.0, "text": " I jak m\u0105drze skalowa\u0107 mo\u017cliwo\u015bci, nieskaluj\u0105c proporcjonalnie potencjalnych szk\u00f3d? To jest wyzwanie, z kt\u00f3rym ca\u0142a bran\u017ca mierzy si\u0119 do dzisiaj.", "tokens": [51264, 286, 4207, 275, 18962, 13503, 16890, 11445, 30854, 36476, 11, 48100, 19990, 44733, 2365, 36003, 15735, 304, 2766, 1847, 22660, 22600, 9399, 7870, 74, 17081, 30, 1407, 3492, 4628, 14406, 7155, 11, 710, 30120, 1335, 5024, 12029, 35075, 47448, 1229, 3244, 360, 25772, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0800341526667277, "compression_ratio": 1.4212328767123288, "no_speech_prob": 0.16358128190040588}], "language": "pl"}